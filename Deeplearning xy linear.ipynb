{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow.keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "import random\n",
    "import math\n",
    "from datetime import datetime\n",
    "from numpy import *\n",
    "import pylab as p\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "array_len = 100\n",
    "range_width = 1000\n",
    "\n",
    "first_input = np.empty(array_len)\n",
    "second_input = np.empty(array_len)\n",
    "for i in range(array_len):\n",
    "  first_input[i] = random.randint(0, range_width)\n",
    "  second_input[i] = random.randint(0, range_width)\n",
    "\n",
    "x = np.vstack((first_input, second_input)).T\n",
    "x1 = first_input*2 + second_input*3 + 4\n",
    "x2 = first_input*5 - second_input*6 - 7\n",
    "y = x1*8+ x2*9 + 10\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#training_frac = 0.85\n",
    "#train_max_index = math.floor(array_len * training_frac)\n",
    "#X_train = x[:train_max_index,:]\n",
    "#Y_train = y[:train_max_index]\n",
    "#X_test = x[train_max_index:,:]\n",
    "#Y_test = y[train_max_index:]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.15)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68 samples, validate on 17 samples\n",
      "Epoch 1/10000\n",
      "68/68 [==============================] - 1s 17ms/sample - loss: 648315980.2353 - val_loss: 645128233.4118\n",
      "Epoch 2/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 645897306.3529 - val_loss: 642819994.3529\n",
      "Epoch 3/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 643446064.0000 - val_loss: 640058514.8235\n",
      "Epoch 4/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 640260026.3529 - val_loss: 636922095.0588\n",
      "Epoch 5/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 636979747.2941 - val_loss: 633184918.5882\n",
      "Epoch 6/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 632889421.1765 - val_loss: 629049537.8824\n",
      "Epoch 7/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 628447775.0588 - val_loss: 624437020.2353\n",
      "Epoch 8/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 623257220.7059 - val_loss: 619346887.5294\n",
      "Epoch 9/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 618074643.7647 - val_loss: 613570693.6471\n",
      "Epoch 10/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 611818285.1765 - val_loss: 607285086.1176\n",
      "Epoch 11/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 604904121.4118 - val_loss: 600537172.7059\n",
      "Epoch 12/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 598025556.7059 - val_loss: 593105200.9412\n",
      "Epoch 13/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 589736409.4118 - val_loss: 585402944.0000\n",
      "Epoch 14/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 581527059.2941 - val_loss: 577145940.7059\n",
      "Epoch 15/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 572628870.5882 - val_loss: 568434725.6471\n",
      "Epoch 16/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 563293917.6471 - val_loss: 559402100.7059\n",
      "Epoch 17/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 553581896.4706 - val_loss: 549870343.5294\n",
      "Epoch 18/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 543773914.3529 - val_loss: 539795115.2941\n",
      "Epoch 19/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 532834304.9412 - val_loss: 529469554.8235\n",
      "Epoch 20/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 522607864.4706 - val_loss: 518603392.0000\n",
      "Epoch 21/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 510799305.4118 - val_loss: 507981005.1765\n",
      "Epoch 22/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 500089074.3529 - val_loss: 496706770.8235\n",
      "Epoch 23/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 488484932.7059 - val_loss: 485332498.8235\n",
      "Epoch 24/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 475913991.5294 - val_loss: 474261420.2353\n",
      "Epoch 25/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 464344285.1765 - val_loss: 462856813.1765\n",
      "Epoch 26/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 452726910.1176 - val_loss: 451078413.1765\n",
      "Epoch 27/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 440844398.1176 - val_loss: 439392382.1176\n",
      "Epoch 28/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 428840087.0588 - val_loss: 427775173.6471\n",
      "Epoch 29/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 417032188.7059 - val_loss: 416263273.4118\n",
      "Epoch 30/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 405903216.9412 - val_loss: 404458928.9412\n",
      "Epoch 31/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 393685628.2353 - val_loss: 393209291.2941\n",
      "Epoch 32/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 381657749.6471 - val_loss: 382300365.1765\n",
      "Epoch 33/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 371280919.5294 - val_loss: 370850584.4706\n",
      "Epoch 34/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 359844930.3529 - val_loss: 359782448.0000\n",
      "Epoch 35/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 348511744.9412 - val_loss: 349137198.1176\n",
      "Epoch 36/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 338013109.6471 - val_loss: 338427849.4118\n",
      "Epoch 37/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 326507359.5294 - val_loss: 328268197.6471\n",
      "Epoch 38/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 316385487.7647 - val_loss: 317778132.7059\n",
      "Epoch 39/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 305948929.8824 - val_loss: 307290348.2353\n",
      "Epoch 40/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 295035340.7059 - val_loss: 297182704.9412\n",
      "Epoch 41/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 284698214.1176 - val_loss: 287102245.6471\n",
      "Epoch 42/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 274719691.7647 - val_loss: 276887290.3529\n",
      "Epoch 43/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 264551269.1765 - val_loss: 266852954.3529\n",
      "Epoch 44/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 254317412.2353 - val_loss: 257084442.3529\n",
      "Epoch 45/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 244781346.5882 - val_loss: 247251527.5294\n",
      "Epoch 46/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 234724520.9412 - val_loss: 237793241.4118\n",
      "Epoch 47/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 225939505.4118 - val_loss: 228045764.7059\n",
      "Epoch 48/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 216406325.6471 - val_loss: 218790327.5294\n",
      "Epoch 49/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 207759601.5294 - val_loss: 209498658.3529\n",
      "Epoch 50/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 197917618.8235 - val_loss: 200844984.4706\n",
      "Epoch 51/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 189099352.8235 - val_loss: 192552163.7647\n",
      "Epoch 52/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 181137017.1765 - val_loss: 184023499.2941\n",
      "Epoch 53/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 172550745.1765 - val_loss: 175833869.1765\n",
      "Epoch 54/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 164473785.6471 - val_loss: 167818034.8235\n",
      "Epoch 55/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 157292954.5882 - val_loss: 159627883.2941\n",
      "Epoch 56/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 149410737.6471 - val_loss: 151762624.0000\n",
      "Epoch 57/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 141749178.5882 - val_loss: 144357594.8235\n",
      "Epoch 58/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 134351340.4706 - val_loss: 137276237.1765\n",
      "Epoch 59/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 128277345.5294 - val_loss: 129940061.1765\n",
      "Epoch 60/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 120730292.2353 - val_loss: 123365304.4706\n",
      "Epoch 61/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 114251082.7059 - val_loss: 117103266.8235\n",
      "Epoch 62/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 108269775.8824 - val_loss: 110934548.2353\n",
      "Epoch 63/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 102316981.5294 - val_loss: 104894683.7647\n",
      "Epoch 64/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 96621562.5882 - val_loss: 99022861.1765\n",
      "Epoch 65/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 90567925.7059 - val_loss: 93804020.2353\n",
      "Epoch 66/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 85896558.0000 - val_loss: 88292764.5882\n",
      "Epoch 67/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 80774857.3529 - val_loss: 83087538.8235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 75513826.2353 - val_loss: 78346069.6471\n",
      "Epoch 69/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 70933117.8824 - val_loss: 73795518.9412\n",
      "Epoch 70/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 66715815.5294 - val_loss: 69288685.6471\n",
      "Epoch 71/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 62574291.4706 - val_loss: 64947742.1176\n",
      "Epoch 72/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 58430326.3529 - val_loss: 60929414.5882\n",
      "Epoch 73/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 54442337.7647 - val_loss: 57292801.1765\n",
      "Epoch 74/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 51136925.3529 - val_loss: 53666436.9412\n",
      "Epoch 75/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 47674042.8824 - val_loss: 50298354.1176\n",
      "Epoch 76/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 44330581.8529 - val_loss: 47285270.8235\n",
      "Epoch 77/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 41663027.5588 - val_loss: 44200360.5882\n",
      "Epoch 78/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 38736302.9118 - val_loss: 41368467.0000\n",
      "Epoch 79/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 36131050.4706 - val_loss: 38679525.8824\n",
      "Epoch 80/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 33602338.5000 - val_loss: 36207090.8235\n",
      "Epoch 81/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 31477819.3529 - val_loss: 33783412.1176\n",
      "Epoch 82/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 29181965.1471 - val_loss: 31644725.7059\n",
      "Epoch 83/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 27270354.3235 - val_loss: 29601113.7059\n",
      "Epoch 84/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 25361631.3971 - val_loss: 27770992.8235\n",
      "Epoch 85/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 23762867.0147 - val_loss: 25995774.1176\n",
      "Epoch 86/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 22227229.6176 - val_loss: 24336456.8235\n",
      "Epoch 87/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 20627162.3309 - val_loss: 22902679.4118\n",
      "Epoch 88/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 19462697.1618 - val_loss: 21412132.8824\n",
      "Epoch 89/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 18134548.6765 - val_loss: 20106168.2353\n",
      "Epoch 90/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 17001488.3897 - val_loss: 18886626.4706\n",
      "Epoch 91/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 16005032.4393 - val_loss: 17706877.1471\n",
      "Epoch 92/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15030384.4706 - val_loss: 16603975.4706\n",
      "Epoch 93/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14069576.8860 - val_loss: 15624555.5294\n",
      "Epoch 94/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13374684.2270 - val_loss: 14629905.0165\n",
      "Epoch 95/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12519367.4118 - val_loss: 13779137.8235\n",
      "Epoch 96/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11818719.2206 - val_loss: 12994500.6057\n",
      "Epoch 97/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11228116.4706 - val_loss: 12230290.1765\n",
      "Epoch 98/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10534160.6691 - val_loss: 11591504.1838\n",
      "Epoch 99/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10057313.5149 - val_loss: 10935197.0000\n",
      "Epoch 100/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9468244.2426 - val_loss: 10383224.0588\n",
      "Epoch 101/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9075364.6654 - val_loss: 9795217.9191\n",
      "Epoch 102/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8578981.8235 - val_loss: 9279298.0882\n",
      "Epoch 103/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8230325.4818 - val_loss: 8738042.5588\n",
      "Epoch 104/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7745364.5239 - val_loss: 8292698.5294\n",
      "Epoch 105/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7370369.8989 - val_loss: 7879978.3824\n",
      "Epoch 106/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7047121.6158 - val_loss: 7470841.9412\n",
      "Epoch 107/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6705084.4779 - val_loss: 7089824.1273\n",
      "Epoch 108/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6387429.1213 - val_loss: 6719727.8235\n",
      "Epoch 109/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6104650.8768 - val_loss: 6353716.7647\n",
      "Epoch 110/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5779394.4614 - val_loss: 6036646.5882\n",
      "Epoch 111/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5521675.1121 - val_loss: 5720748.3290\n",
      "Epoch 112/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5245170.7426 - val_loss: 5438981.8676\n",
      "Epoch 113/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5057443.8277 - val_loss: 5138334.5294\n",
      "Epoch 114/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4765808.8015 - val_loss: 4905374.2353\n",
      "Epoch 115/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4584502.3196 - val_loss: 4657290.0882\n",
      "Epoch 116/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4362148.2206 - val_loss: 4438659.5248\n",
      "Epoch 117/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4165906.6218 - val_loss: 4238395.3529\n",
      "Epoch 118/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3977621.5441 - val_loss: 4046204.1618\n",
      "Epoch 119/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3833272.0497 - val_loss: 3834292.6765\n",
      "Epoch 120/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3644986.9152 - val_loss: 3638218.5303\n",
      "Epoch 121/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3442170.5369 - val_loss: 3479208.7353\n",
      "Epoch 122/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3299856.9844 - val_loss: 3304366.6250\n",
      "Epoch 123/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3152698.2013 - val_loss: 3135383.3235\n",
      "Epoch 124/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3009520.4963 - val_loss: 2975530.4118\n",
      "Epoch 125/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2869154.4994 - val_loss: 2827552.2500\n",
      "Epoch 126/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2706579.3860 - val_loss: 2710295.4044\n",
      "Epoch 127/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2602207.5818 - val_loss: 2579213.9044\n",
      "Epoch 128/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2486058.5331 - val_loss: 2450574.3309\n",
      "Epoch 129/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2352192.6110 - val_loss: 2339186.1750\n",
      "Epoch 130/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2242569.6862 - val_loss: 2235655.0515\n",
      "Epoch 131/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2163148.0119 - val_loss: 2119593.7096\n",
      "Epoch 132/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2043513.5809 - val_loss: 2019959.3529\n",
      "Epoch 133/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1951611.9378 - val_loss: 1924437.8529\n",
      "Epoch 134/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1846331.8618 - val_loss: 1842731.0257\n",
      "Epoch 135/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1772488.5776 - val_loss: 1754154.6636\n",
      "Epoch 136/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1683440.4770 - val_loss: 1674036.2096\n",
      "Epoch 137/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1595920.9763 - val_loss: 1606871.4798\n",
      "Epoch 138/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1543106.3847 - val_loss: 1524179.2318\n",
      "Epoch 139/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1460144.5844 - val_loss: 1453559.4485\n",
      "Epoch 140/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1394278.0874 - val_loss: 1387527.9917\n",
      "Epoch 141/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1317893.6581 - val_loss: 1333804.8520\n",
      "Epoch 142/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1261120.0318 - val_loss: 1279490.6949\n",
      "Epoch 143/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1212250.2730 - val_loss: 1222445.0726\n",
      "Epoch 144/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1149611.8097 - val_loss: 1174366.5846\n",
      "Epoch 145/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1099979.7091 - val_loss: 1127085.2132\n",
      "Epoch 146/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1053830.4462 - val_loss: 1080328.1176\n",
      "Epoch 147/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1005382.0257 - val_loss: 1038661.3750\n",
      "Epoch 148/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 960266.7682 - val_loss: 999503.2279\n",
      "Epoch 149/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 919609.9447 - val_loss: 961160.7139\n",
      "Epoch 150/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 881646.3616 - val_loss: 924202.2757\n",
      "Epoch 151/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 835177.2324 - val_loss: 893703.5341\n",
      "Epoch 152/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 802612.5996 - val_loss: 862022.0973\n",
      "Epoch 153/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 767394.6416 - val_loss: 832727.5465\n",
      "Epoch 154/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 731016.3835 - val_loss: 806958.1066\n",
      "Epoch 155/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 699887.9440 - val_loss: 779519.5809\n",
      "Epoch 156/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 673439.3846 - val_loss: 751445.6544\n",
      "Epoch 157/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 645271.7284 - val_loss: 724644.3824\n",
      "Epoch 158/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 616428.7319 - val_loss: 700210.1011\n",
      "Epoch 159/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 587414.2750 - val_loss: 679540.2482\n",
      "Epoch 160/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 565916.8104 - val_loss: 655647.1526\n",
      "Epoch 161/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 541942.5830 - val_loss: 633165.4508\n",
      "Epoch 162/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 521089.6633 - val_loss: 611191.8911\n",
      "Epoch 163/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 500900.5137 - val_loss: 590097.2736\n",
      "Epoch 164/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 478648.4854 - val_loss: 572495.9890\n",
      "Epoch 165/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 463077.7859 - val_loss: 553473.4262\n",
      "Epoch 166/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 445119.5929 - val_loss: 535608.3865\n",
      "Epoch 167/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 426427.8348 - val_loss: 520240.5520\n",
      "Epoch 168/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 414027.0704 - val_loss: 502757.5276\n",
      "Epoch 169/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 396763.6045 - val_loss: 487337.9147\n",
      "Epoch 170/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 381310.8197 - val_loss: 473349.4707\n",
      "Epoch 171/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 372361.5516 - val_loss: 456303.3676\n",
      "Epoch 172/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 356797.6734 - val_loss: 441624.6911\n",
      "Epoch 173/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 344434.8959 - val_loss: 427830.9992\n",
      "Epoch 174/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 332364.6736 - val_loss: 414987.0974\n",
      "Epoch 175/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 320994.5186 - val_loss: 402969.4382\n",
      "Epoch 176/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 309016.4960 - val_loss: 393121.5292\n",
      "Epoch 177/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 301346.0432 - val_loss: 380310.3364\n",
      "Epoch 178/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 293151.0581 - val_loss: 367542.8162\n",
      "Epoch 179/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 279847.6139 - val_loss: 358038.0360\n",
      "Epoch 180/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 272772.1858 - val_loss: 346676.8961\n",
      "Epoch 181/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 263367.6515 - val_loss: 336305.6507\n",
      "Epoch 182/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 254466.8388 - val_loss: 326610.2923\n",
      "Epoch 183/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 247009.3392 - val_loss: 316777.6103\n",
      "Epoch 184/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 239389.7421 - val_loss: 307099.2219\n",
      "Epoch 185/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 232655.2809 - val_loss: 297421.6849\n",
      "Epoch 186/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 224956.0990 - val_loss: 288757.2381\n",
      "Epoch 187/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 219468.6532 - val_loss: 279157.8054\n",
      "Epoch 188/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 211236.6306 - val_loss: 271555.1507\n",
      "Epoch 189/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 206271.1600 - val_loss: 263159.9172\n",
      "Epoch 190/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 198950.9569 - val_loss: 255975.4021\n",
      "Epoch 191/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 193245.3556 - val_loss: 249073.1526\n",
      "Epoch 192/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 187716.8929 - val_loss: 242236.9740\n",
      "Epoch 193/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 183827.5337 - val_loss: 234004.1710\n",
      "Epoch 194/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 176694.8265 - val_loss: 227798.2068\n",
      "Epoch 195/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 171996.2944 - val_loss: 221439.8713\n",
      "Epoch 196/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 166780.7301 - val_loss: 215149.3974\n",
      "Epoch 197/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 162221.9970 - val_loss: 208903.1333\n",
      "Epoch 198/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 158673.2394 - val_loss: 202095.9843\n",
      "Epoch 199/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 153683.9287 - val_loss: 196119.9970\n",
      "Epoch 200/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 149535.8499 - val_loss: 190189.3722\n",
      "Epoch 201/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 144734.4002 - val_loss: 185159.7929\n",
      "Epoch 202/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 141006.8793 - val_loss: 180053.5828\n",
      "Epoch 203/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 137850.7825 - val_loss: 174444.3244\n",
      "Epoch 204/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 134090.7715 - val_loss: 168796.3791\n",
      "Epoch 205/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 129276.8111 - val_loss: 164235.9283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 206/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 125785.1115 - val_loss: 159658.5460\n",
      "Epoch 207/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 122621.5810 - val_loss: 154757.0267\n",
      "Epoch 208/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 119247.4363 - val_loss: 150054.4718\n",
      "Epoch 209/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 116112.8553 - val_loss: 145397.4968\n",
      "Epoch 210/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 112838.0810 - val_loss: 141002.9780\n",
      "Epoch 211/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 109464.5246 - val_loss: 136969.8125\n",
      "Epoch 212/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 106083.0747 - val_loss: 133477.3385\n",
      "Epoch 213/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 103604.6724 - val_loss: 129522.0561\n",
      "Epoch 214/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 101294.7702 - val_loss: 125278.3295\n",
      "Epoch 215/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 97584.2861 - val_loss: 122217.6926\n",
      "Epoch 216/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 95823.9798 - val_loss: 117888.5960\n",
      "Epoch 217/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 92485.7235 - val_loss: 114391.2238\n",
      "Epoch 218/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 90375.8154 - val_loss: 110618.4251\n",
      "Epoch 219/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 87445.5702 - val_loss: 107331.0515\n",
      "Epoch 220/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 84805.2357 - val_loss: 104484.8839\n",
      "Epoch 221/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 83502.4125 - val_loss: 100692.5896\n",
      "Epoch 222/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 80485.4336 - val_loss: 97649.0076\n",
      "Epoch 223/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 78154.3964 - val_loss: 94803.9417\n",
      "Epoch 224/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 76058.8863 - val_loss: 92038.4219\n",
      "Epoch 225/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 74697.2162 - val_loss: 88621.3897\n",
      "Epoch 226/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 72086.6487 - val_loss: 85870.2625\n",
      "Epoch 227/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 69933.3130 - val_loss: 83472.6037\n",
      "Epoch 228/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 68581.8056 - val_loss: 80502.6250\n",
      "Epoch 229/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 66097.7809 - val_loss: 78387.1716\n",
      "Epoch 230/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 236.378 - 0s 147us/sample - loss: 64605.4120 - val_loss: 75803.0337\n",
      "Epoch 231/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 63059.8410 - val_loss: 73102.9353\n",
      "Epoch 232/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 61077.5623 - val_loss: 70893.3073\n",
      "Epoch 233/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 59703.6938 - val_loss: 68532.9542\n",
      "Epoch 234/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 58101.3695 - val_loss: 66327.8417\n",
      "Epoch 235/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 56572.2970 - val_loss: 64330.6976\n",
      "Epoch 236/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 55264.4031 - val_loss: 62245.5757\n",
      "Epoch 237/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 53751.3487 - val_loss: 60393.9226\n",
      "Epoch 238/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 52426.6925 - val_loss: 58595.4962\n",
      "Epoch 239/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 51173.1463 - val_loss: 56703.4680\n",
      "Epoch 240/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 50235.2868 - val_loss: 54614.8828\n",
      "Epoch 241/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 48829.7754 - val_loss: 52788.6507\n",
      "Epoch 242/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 47496.3630 - val_loss: 51227.5524\n",
      "Epoch 243/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 46304.7851 - val_loss: 49815.2269\n",
      "Epoch 244/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 45327.5569 - val_loss: 48203.7131\n",
      "Epoch 245/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 44250.8882 - val_loss: 46667.1450\n",
      "Epoch 246/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 43303.8713 - val_loss: 45042.8199\n",
      "Epoch 247/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 42351.5755 - val_loss: 43515.0896\n",
      "Epoch 248/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 41335.9681 - val_loss: 42095.7955\n",
      "Epoch 249/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 40433.9789 - val_loss: 40685.9375\n",
      "Epoch 250/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 39527.5438 - val_loss: 39375.9928\n",
      "Epoch 251/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 38562.4427 - val_loss: 38227.1450\n",
      "Epoch 252/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 37867.0053 - val_loss: 36945.5807\n",
      "Epoch 253/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 37069.3454 - val_loss: 35709.0018\n",
      "Epoch 254/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 36358.9096 - val_loss: 34472.8015\n",
      "Epoch 255/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 35561.2702 - val_loss: 33350.2279\n",
      "Epoch 256/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 34793.1444 - val_loss: 32352.3593\n",
      "Epoch 257/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 34230.6785 - val_loss: 31278.9409\n",
      "Epoch 258/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 33440.2826 - val_loss: 30435.4882\n",
      "Epoch 259/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 32993.4535 - val_loss: 29366.8141\n",
      "Epoch 260/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 32327.3072 - val_loss: 28424.3617\n",
      "Epoch 261/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 31692.7149 - val_loss: 27611.3907\n",
      "Epoch 262/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 31292.1155 - val_loss: 26629.4705\n",
      "Epoch 263/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 30613.6445 - val_loss: 25843.9588\n",
      "Epoch 264/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 30136.6325 - val_loss: 25015.2934\n",
      "Epoch 265/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 29678.3684 - val_loss: 24180.8988\n",
      "Epoch 266/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 29165.1295 - val_loss: 23421.8332\n",
      "Epoch 267/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 28656.4407 - val_loss: 22772.3326\n",
      "Epoch 268/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 28246.0911 - val_loss: 22079.8361\n",
      "Epoch 269/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 27897.6886 - val_loss: 21322.6018\n",
      "Epoch 270/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 27439.7134 - val_loss: 20672.4570\n",
      "Epoch 271/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 27030.6848 - val_loss: 20082.0348\n",
      "Epoch 272/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26773.3728 - val_loss: 19366.3069\n",
      "Epoch 273/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 26311.8932 - val_loss: 18793.3196\n",
      "Epoch 274/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 25958.4456 - val_loss: 18249.3366\n",
      "Epoch 275/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 25657.9049 - val_loss: 17686.5952\n",
      "Epoch 276/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 25329.5288 - val_loss: 17155.6711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 25027.9774 - val_loss: 16620.8197\n",
      "Epoch 278/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 24812.0755 - val_loss: 16048.0341\n",
      "Epoch 279/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 24411.3411 - val_loss: 15654.9949\n",
      "Epoch 280/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 24175.1165 - val_loss: 15163.1078\n",
      "Epoch 281/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 23937.2491 - val_loss: 14664.6024\n",
      "Epoch 282/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 23690.2189 - val_loss: 14213.5023\n",
      "Epoch 283/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 23421.5232 - val_loss: 13827.6164\n",
      "Epoch 284/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 23239.3854 - val_loss: 13396.6284\n",
      "Epoch 285/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 22979.9381 - val_loss: 13037.1933\n",
      "Epoch 286/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 22776.2301 - val_loss: 12696.1223\n",
      "Epoch 287/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 22583.1370 - val_loss: 12322.0454\n",
      "Epoch 288/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 22451.6945 - val_loss: 11899.0709\n",
      "Epoch 289/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 22198.0303 - val_loss: 11595.2350\n",
      "Epoch 290/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 22033.1378 - val_loss: 11264.7907\n",
      "Epoch 291/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21910.6718 - val_loss: 10884.4979\n",
      "Epoch 292/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21709.6024 - val_loss: 10590.3146\n",
      "Epoch 293/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21593.8899 - val_loss: 10272.1314\n",
      "Epoch 294/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21408.4327 - val_loss: 10031.9290\n",
      "Epoch 295/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21277.4996 - val_loss: 9795.1429\n",
      "Epoch 296/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 21159.1876 - val_loss: 9520.3415\n",
      "Epoch 297/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 21033.6854 - val_loss: 9259.5460\n",
      "Epoch 298/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 20899.5781 - val_loss: 9016.1046\n",
      "Epoch 299/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 20778.9486 - val_loss: 8768.7775\n",
      "Epoch 300/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 20655.7860 - val_loss: 8528.5333\n",
      "Epoch 301/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 20549.4268 - val_loss: 8276.4857\n",
      "Epoch 302/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20438.4216 - val_loss: 8039.0967\n",
      "Epoch 303/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 20301.6481 - val_loss: 7876.0575\n",
      "Epoch 304/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20192.1843 - val_loss: 7686.5254\n",
      "Epoch 305/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 20145.1179 - val_loss: 7420.3222\n",
      "Epoch 306/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 19986.8599 - val_loss: 7242.7420\n",
      "Epoch 307/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 19909.8996 - val_loss: 7036.4634\n",
      "Epoch 308/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 19784.9520 - val_loss: 6860.2681\n",
      "Epoch 309/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19711.6916 - val_loss: 6640.2000\n",
      "Epoch 310/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19610.5650 - val_loss: 6444.2720\n",
      "Epoch 311/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 19482.5830 - val_loss: 6302.2228\n",
      "Epoch 312/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 19414.1602 - val_loss: 6092.3561\n",
      "Epoch 313/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19308.2501 - val_loss: 5915.4794\n",
      "Epoch 314/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 19229.9498 - val_loss: 5725.8916\n",
      "Epoch 315/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19133.3947 - val_loss: 5548.9744\n",
      "Epoch 316/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 19007.9026 - val_loss: 5431.0091\n",
      "Epoch 317/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 18934.4627 - val_loss: 5258.8043\n",
      "Epoch 318/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18847.1357 - val_loss: 5091.3196\n",
      "Epoch 319/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18763.6228 - val_loss: 4926.1205\n",
      "Epoch 320/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 18671.1144 - val_loss: 4771.4890\n",
      "Epoch 321/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18562.8619 - val_loss: 4651.9517\n",
      "Epoch 322/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 18506.6810 - val_loss: 4485.9369\n",
      "Epoch 323/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18389.9384 - val_loss: 4364.1242\n",
      "Epoch 324/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 18323.9669 - val_loss: 4220.2801\n",
      "Epoch 325/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18226.5625 - val_loss: 4095.4378\n",
      "Epoch 326/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 18125.2362 - val_loss: 3989.2892\n",
      "Epoch 327/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 18094.6891 - val_loss: 3818.9177\n",
      "Epoch 328/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 17952.1019 - val_loss: 3713.4036\n",
      "Epoch 329/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17877.1905 - val_loss: 3594.1615\n",
      "Epoch 330/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17801.2274 - val_loss: 3467.2897\n",
      "Epoch 331/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17701.9804 - val_loss: 3365.6350\n",
      "Epoch 332/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17645.8172 - val_loss: 3227.6517\n",
      "Epoch 333/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17532.8727 - val_loss: 3124.9610\n",
      "Epoch 334/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17495.5454 - val_loss: 2996.3409\n",
      "Epoch 335/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17388.0696 - val_loss: 2889.6597\n",
      "Epoch 336/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17302.4924 - val_loss: 2792.6554\n",
      "Epoch 337/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17231.5806 - val_loss: 2694.2165\n",
      "Epoch 338/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 17140.3281 - val_loss: 2610.8933\n",
      "Epoch 339/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 17057.3857 - val_loss: 2531.2202\n",
      "Epoch 340/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16994.6633 - val_loss: 2443.7784\n",
      "Epoch 341/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 16904.2383 - val_loss: 2373.5547\n",
      "Epoch 342/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 16854.4718 - val_loss: 2282.8472\n",
      "Epoch 343/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16768.1812 - val_loss: 2204.9917\n",
      "Epoch 344/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16698.9554 - val_loss: 2128.6706\n",
      "Epoch 345/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16614.5596 - val_loss: 2060.7971\n",
      "Epoch 346/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16537.0784 - val_loss: 2003.0484\n",
      "Epoch 347/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16498.5007 - val_loss: 1926.0155\n",
      "Epoch 348/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16428.4522 - val_loss: 1854.0408\n",
      "Epoch 349/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16329.9906 - val_loss: 1798.8401\n",
      "Epoch 350/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16243.8551 - val_loss: 1754.3953\n",
      "Epoch 351/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16187.8530 - val_loss: 1695.8530\n",
      "Epoch 352/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 16130.8639 - val_loss: 1632.7040\n",
      "Epoch 353/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16039.5691 - val_loss: 1587.1384\n",
      "Epoch 354/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15984.9006 - val_loss: 1535.7141\n",
      "Epoch 355/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 15904.4902 - val_loss: 1491.7862\n",
      "Epoch 356/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15838.4836 - val_loss: 1450.6907\n",
      "Epoch 357/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15781.8384 - val_loss: 1408.6504\n",
      "Epoch 358/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15695.7651 - val_loss: 1376.2559\n",
      "Epoch 359/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 15660.7375 - val_loss: 1331.8803\n",
      "Epoch 360/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15570.2686 - val_loss: 1294.1278\n",
      "Epoch 361/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 15508.6009 - val_loss: 1258.0887\n",
      "Epoch 362/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15447.6031 - val_loss: 1219.7968\n",
      "Epoch 363/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 15369.0082 - val_loss: 1190.7064\n",
      "Epoch 364/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15320.6646 - val_loss: 1149.3820\n",
      "Epoch 365/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15251.8480 - val_loss: 1111.7200\n",
      "Epoch 366/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 15215.1775 - val_loss: 1066.2673\n",
      "Epoch 367/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15115.9096 - val_loss: 1035.3984\n",
      "Epoch 368/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15064.5165 - val_loss: 1002.5178\n",
      "Epoch 369/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15017.2093 - val_loss: 965.4567\n",
      "Epoch 370/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14935.2899 - val_loss: 934.6925\n",
      "Epoch 371/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14883.3167 - val_loss: 903.8941\n",
      "Epoch 372/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14823.9840 - val_loss: 875.2505\n",
      "Epoch 373/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14769.7685 - val_loss: 846.2718\n",
      "Epoch 374/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 14699.3817 - val_loss: 819.0516\n",
      "Epoch 375/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14659.6180 - val_loss: 786.2065\n",
      "Epoch 376/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14587.6867 - val_loss: 757.8125\n",
      "Epoch 377/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14536.2079 - val_loss: 727.8178\n",
      "Epoch 378/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14489.1452 - val_loss: 697.3663\n",
      "Epoch 379/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 120.872 - 0s 162us/sample - loss: 14414.5030 - val_loss: 672.8576\n",
      "Epoch 380/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 14360.3869 - val_loss: 647.4017\n",
      "Epoch 381/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14301.7323 - val_loss: 623.0314\n",
      "Epoch 382/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14244.8575 - val_loss: 598.0440\n",
      "Epoch 383/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14193.4307 - val_loss: 576.1679\n",
      "Epoch 384/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 14142.2426 - val_loss: 547.1719\n",
      "Epoch 385/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 14072.5408 - val_loss: 524.7899\n",
      "Epoch 386/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 14018.3264 - val_loss: 503.8867\n",
      "Epoch 387/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13955.4280 - val_loss: 483.1412\n",
      "Epoch 388/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13903.1144 - val_loss: 461.1424\n",
      "Epoch 389/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13856.3204 - val_loss: 437.7618\n",
      "Epoch 390/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13791.3425 - val_loss: 417.5552\n",
      "Epoch 391/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 13739.2588 - val_loss: 401.4952\n",
      "Epoch 392/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 13696.5070 - val_loss: 376.1499\n",
      "Epoch 393/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13612.4789 - val_loss: 357.3397\n",
      "Epoch 394/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13581.2627 - val_loss: 337.3264\n",
      "Epoch 395/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13516.4105 - val_loss: 318.8941\n",
      "Epoch 396/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13455.4420 - val_loss: 301.6624\n",
      "Epoch 397/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13401.7242 - val_loss: 282.8482\n",
      "Epoch 398/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13324.9252 - val_loss: 269.8917\n",
      "Epoch 399/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13280.6186 - val_loss: 258.2750\n",
      "Epoch 400/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13246.3426 - val_loss: 242.0344\n",
      "Epoch 401/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13170.4940 - val_loss: 224.5854\n",
      "Epoch 402/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13100.6370 - val_loss: 213.7420\n",
      "Epoch 403/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13054.0903 - val_loss: 208.8568\n",
      "Epoch 404/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12995.7473 - val_loss: 192.3856\n",
      "Epoch 405/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12936.5194 - val_loss: 179.0700\n",
      "Epoch 406/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12883.0830 - val_loss: 169.5150\n",
      "Epoch 407/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12830.6050 - val_loss: 156.4708\n",
      "Epoch 408/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12751.0454 - val_loss: 144.1776\n",
      "Epoch 409/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12720.8023 - val_loss: 136.2245\n",
      "Epoch 410/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12647.3104 - val_loss: 124.2453\n",
      "Epoch 411/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12596.4323 - val_loss: 120.1671\n",
      "Epoch 412/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12524.4641 - val_loss: 110.5625\n",
      "Epoch 413/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 12466.4009 - val_loss: 104.1952\n",
      "Epoch 414/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12421.8254 - val_loss: 103.6309\n",
      "Epoch 415/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12352.7417 - val_loss: 92.9814\n",
      "Epoch 416/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 12311.1920 - val_loss: 92.1047\n",
      "Epoch 417/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 12249.4835 - val_loss: 85.9208\n",
      "Epoch 418/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 12189.7232 - val_loss: 80.4310\n",
      "Epoch 419/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12107.3586 - val_loss: 76.8548\n",
      "Epoch 420/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 12072.4328 - val_loss: 82.9247\n",
      "Epoch 421/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11994.7155 - val_loss: 75.8228\n",
      "Epoch 422/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11943.6127 - val_loss: 74.6890\n",
      "Epoch 423/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11886.7026 - val_loss: 80.0289\n",
      "Epoch 424/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 11847.3287 - val_loss: 80.5833\n",
      "Epoch 425/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11770.6236 - val_loss: 77.1320\n",
      "Epoch 426/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11721.3819 - val_loss: 80.0017\n",
      "Epoch 427/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11688.6139 - val_loss: 90.8064\n",
      "Epoch 428/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11606.2328 - val_loss: 88.0942\n",
      "Epoch 429/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11550.9684 - val_loss: 95.7861\n",
      "Epoch 430/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11495.7914 - val_loss: 96.4274\n",
      "Epoch 431/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11446.4164 - val_loss: 93.4237\n",
      "Epoch 432/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 11391.9947 - val_loss: 92.4265\n",
      "Epoch 433/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11350.7888 - val_loss: 95.9105\n",
      "Epoch 434/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 11285.5259 - val_loss: 90.8672\n",
      "Epoch 435/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 11217.5007 - val_loss: 90.9230\n",
      "Epoch 436/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11166.3113 - val_loss: 99.2927\n",
      "Epoch 437/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 11111.3693 - val_loss: 92.9272\n",
      "Epoch 438/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11065.9280 - val_loss: 94.7208\n",
      "Epoch 439/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10995.3315 - val_loss: 94.8826\n",
      "Epoch 440/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 10959.4214 - val_loss: 97.8281\n",
      "Epoch 441/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10909.6262 - val_loss: 94.8653\n",
      "Epoch 442/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 10828.9322 - val_loss: 91.7474\n",
      "Epoch 443/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10784.4994 - val_loss: 99.3040\n",
      "Epoch 444/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10725.3462 - val_loss: 92.9916\n",
      "Epoch 445/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10657.0566 - val_loss: 89.9324\n",
      "Epoch 446/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10621.9876 - val_loss: 101.3570\n",
      "Epoch 447/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 10556.1879 - val_loss: 91.6147\n",
      "Epoch 448/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10503.7899 - val_loss: 90.9062\n",
      "Epoch 449/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10441.9236 - val_loss: 98.5910\n",
      "Epoch 450/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 10425.7937 - val_loss: 95.4815\n",
      "Epoch 451/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10339.2618 - val_loss: 92.3148\n",
      "Epoch 452/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10307.7766 - val_loss: 97.4316\n",
      "Epoch 453/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10242.6033 - val_loss: 91.3185\n",
      "Epoch 454/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 10190.8326 - val_loss: 95.4329\n",
      "Epoch 455/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10129.5698 - val_loss: 93.5212\n",
      "Epoch 456/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10073.9969 - val_loss: 90.6190\n",
      "Epoch 457/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 10033.1421 - val_loss: 95.0594\n",
      "Epoch 458/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9970.9887 - val_loss: 94.5712\n",
      "Epoch 459/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9938.2587 - val_loss: 93.5483\n",
      "Epoch 460/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9883.4755 - val_loss: 94.6286\n",
      "Epoch 461/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9815.8610 - val_loss: 91.2666\n",
      "Epoch 462/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9784.6823 - val_loss: 102.1069\n",
      "Epoch 463/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9712.2208 - val_loss: 91.6010\n",
      "Epoch 464/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9660.9621 - val_loss: 92.2652\n",
      "Epoch 465/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9649.4144 - val_loss: 103.2185\n",
      "Epoch 466/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9549.2329 - val_loss: 93.7424\n",
      "Epoch 467/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9519.3997 - val_loss: 92.2177\n",
      "Epoch 468/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9457.1750 - val_loss: 97.9859\n",
      "Epoch 469/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9394.2228 - val_loss: 90.0085\n",
      "Epoch 470/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9356.0251 - val_loss: 103.1567\n",
      "Epoch 471/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9303.7593 - val_loss: 92.2652\n",
      "Epoch 472/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9245.5377 - val_loss: 89.8900\n",
      "Epoch 473/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9224.2920 - val_loss: 100.1439\n",
      "Epoch 474/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9153.2100 - val_loss: 93.1595\n",
      "Epoch 475/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9090.6572 - val_loss: 90.7300\n",
      "Epoch 476/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9071.4804 - val_loss: 97.7193\n",
      "Epoch 477/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8994.9046 - val_loss: 94.6873\n",
      "Epoch 478/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8943.1298 - val_loss: 92.9805\n",
      "Epoch 479/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8893.1207 - val_loss: 93.8843\n",
      "Epoch 480/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8865.7033 - val_loss: 92.0017\n",
      "Epoch 481/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8808.0282 - val_loss: 92.2079\n",
      "Epoch 482/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8750.1040 - val_loss: 92.6187\n",
      "Epoch 483/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8691.9152 - val_loss: 94.1463\n",
      "Epoch 484/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 8646.1364 - val_loss: 93.2820\n",
      "Epoch 485/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8602.7893 - val_loss: 92.0318\n",
      "Epoch 486/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 8558.3651 - val_loss: 92.7529\n",
      "Epoch 487/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8531.8150 - val_loss: 96.8339\n",
      "Epoch 488/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8450.9124 - val_loss: 91.6859\n",
      "Epoch 489/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8405.0704 - val_loss: 92.0340\n",
      "Epoch 490/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8363.1714 - val_loss: 94.5693\n",
      "Epoch 491/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8320.6672 - val_loss: 98.7983\n",
      "Epoch 492/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 53887.289 - 0s 132us/sample - loss: 8285.5907 - val_loss: 93.3577\n",
      "Epoch 493/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8208.6042 - val_loss: 97.5769\n",
      "Epoch 494/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8161.3417 - val_loss: 95.1223\n",
      "Epoch 495/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 8114.3995 - val_loss: 90.2357\n",
      "Epoch 496/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8086.7478 - val_loss: 99.4150\n",
      "Epoch 497/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 8029.8181 - val_loss: 94.4775\n",
      "Epoch 498/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7967.8663 - val_loss: 93.4352\n",
      "Epoch 499/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7933.0891 - val_loss: 94.9878\n",
      "Epoch 500/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 7866.5715 - val_loss: 90.5751\n",
      "Epoch 501/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7828.6707 - val_loss: 94.8916\n",
      "Epoch 502/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7801.6766 - val_loss: 92.4305\n",
      "Epoch 503/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7732.6855 - val_loss: 94.9925\n",
      "Epoch 504/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7706.4600 - val_loss: 91.6364\n",
      "Epoch 505/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 7650.1206 - val_loss: 93.6109\n",
      "Epoch 506/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7602.9947 - val_loss: 93.2790\n",
      "Epoch 507/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7557.3956 - val_loss: 96.5074\n",
      "Epoch 508/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 106.251 - 0s 118us/sample - loss: 7508.5689 - val_loss: 92.2004\n",
      "Epoch 509/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7471.5764 - val_loss: 95.0572\n",
      "Epoch 510/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 7421.4985 - val_loss: 93.2935\n",
      "Epoch 511/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7370.6689 - val_loss: 91.0421\n",
      "Epoch 512/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7347.0814 - val_loss: 100.2791\n",
      "Epoch 513/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7276.7414 - val_loss: 91.1532\n",
      "Epoch 514/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7252.1088 - val_loss: 96.5632\n",
      "Epoch 515/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7208.3171 - val_loss: 96.1156\n",
      "Epoch 516/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7149.2796 - val_loss: 90.8614\n",
      "Epoch 517/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7082.1475 - val_loss: 93.2125\n",
      "Epoch 518/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7079.5078 - val_loss: 93.9166\n",
      "Epoch 519/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7011.0844 - val_loss: 89.8459\n",
      "Epoch 520/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6949.3989 - val_loss: 100.3826\n",
      "Epoch 521/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6927.9914 - val_loss: 93.5136\n",
      "Epoch 522/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6875.5349 - val_loss: 91.5383\n",
      "Epoch 523/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6840.8183 - val_loss: 96.9804\n",
      "Epoch 524/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6779.2420 - val_loss: 90.2029\n",
      "Epoch 525/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6740.6122 - val_loss: 92.9241\n",
      "Epoch 526/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6712.5031 - val_loss: 96.7862\n",
      "Epoch 527/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6669.7658 - val_loss: 93.3012\n",
      "Epoch 528/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6620.3452 - val_loss: 93.6925\n",
      "Epoch 529/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6585.1882 - val_loss: 94.7994\n",
      "Epoch 530/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6538.2743 - val_loss: 91.0557\n",
      "Epoch 531/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6506.1575 - val_loss: 96.2981\n",
      "Epoch 532/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6447.7154 - val_loss: 89.5605\n",
      "Epoch 533/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6405.3838 - val_loss: 98.4508\n",
      "Epoch 534/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6363.4096 - val_loss: 90.5753\n",
      "Epoch 535/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 6323.7296 - val_loss: 95.8053\n",
      "Epoch 536/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6292.7963 - val_loss: 93.5886\n",
      "Epoch 537/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6244.2367 - val_loss: 92.3704\n",
      "Epoch 538/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6194.5075 - val_loss: 97.0788\n",
      "Epoch 539/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6148.8070 - val_loss: 91.4461\n",
      "Epoch 540/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6123.6817 - val_loss: 91.9481\n",
      "Epoch 541/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6074.9153 - val_loss: 93.6892\n",
      "Epoch 542/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6032.0607 - val_loss: 93.0547\n",
      "Epoch 543/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5993.0557 - val_loss: 89.7364\n",
      "Epoch 544/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5958.2004 - val_loss: 93.9049\n",
      "Epoch 545/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5914.0348 - val_loss: 90.6464\n",
      "Epoch 546/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5871.1869 - val_loss: 92.0434\n",
      "Epoch 547/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5843.7422 - val_loss: 102.8816\n",
      "Epoch 548/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5801.5311 - val_loss: 90.7682\n",
      "Epoch 549/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5774.1582 - val_loss: 94.8747\n",
      "Epoch 550/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5713.3419 - val_loss: 91.7462\n",
      "Epoch 551/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5685.5165 - val_loss: 92.2724\n",
      "Epoch 552/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5642.9963 - val_loss: 97.8666\n",
      "Epoch 553/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5597.4638 - val_loss: 88.5849\n",
      "Epoch 554/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5569.6763 - val_loss: 95.4321\n",
      "Epoch 555/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5538.2931 - val_loss: 91.9347\n",
      "Epoch 556/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5494.7406 - val_loss: 91.9052\n",
      "Epoch 557/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5450.1383 - val_loss: 89.7620\n",
      "Epoch 558/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5426.1177 - val_loss: 100.5696\n",
      "Epoch 559/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5382.9559 - val_loss: 89.2192\n",
      "Epoch 560/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5341.8554 - val_loss: 93.8444\n",
      "Epoch 561/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5309.5371 - val_loss: 90.9338\n",
      "Epoch 562/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 5275.0442 - val_loss: 93.6041\n",
      "Epoch 563/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5241.7889 - val_loss: 95.8427\n",
      "Epoch 564/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5213.6628 - val_loss: 95.1205\n",
      "Epoch 565/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5168.7909 - val_loss: 90.3280\n",
      "Epoch 566/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 5131.6104 - val_loss: 94.7399\n",
      "Epoch 567/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5096.6543 - val_loss: 88.6554\n",
      "Epoch 568/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5062.8314 - val_loss: 95.0447\n",
      "Epoch 569/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5039.6510 - val_loss: 96.4809\n",
      "Epoch 570/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 4997.5735 - val_loss: 90.6321\n",
      "Epoch 571/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4956.3518 - val_loss: 96.4386\n",
      "Epoch 572/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4928.2646 - val_loss: 90.8997\n",
      "Epoch 573/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4884.3996 - val_loss: 89.7093\n",
      "Epoch 574/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4872.5086 - val_loss: 92.4026\n",
      "Epoch 575/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4819.1436 - val_loss: 90.7029\n",
      "Epoch 576/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4786.7950 - val_loss: 96.3314\n",
      "Epoch 577/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4753.4858 - val_loss: 91.6513\n",
      "Epoch 578/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4727.0279 - val_loss: 92.4144\n",
      "Epoch 579/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4673.9944 - val_loss: 87.8338\n",
      "Epoch 580/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4652.0746 - val_loss: 92.9914\n",
      "Epoch 581/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4622.7748 - val_loss: 91.7372\n",
      "Epoch 582/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4580.0379 - val_loss: 89.1999\n",
      "Epoch 583/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4546.0117 - val_loss: 101.9626\n",
      "Epoch 584/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 4523.6114 - val_loss: 89.2736\n",
      "Epoch 585/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4494.4510 - val_loss: 90.4261\n",
      "Epoch 586/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4444.6597 - val_loss: 89.8230\n",
      "Epoch 587/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4414.3839 - val_loss: 92.7609\n",
      "Epoch 588/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4388.6118 - val_loss: 93.3539\n",
      "Epoch 589/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4355.8598 - val_loss: 90.6377\n",
      "Epoch 590/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4340.8996 - val_loss: 92.7834\n",
      "Epoch 591/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4293.1245 - val_loss: 88.6079\n",
      "Epoch 592/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4257.5893 - val_loss: 92.8475\n",
      "Epoch 593/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4239.6665 - val_loss: 90.9538\n",
      "Epoch 594/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4192.1170 - val_loss: 87.8565\n",
      "Epoch 595/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4167.3368 - val_loss: 93.2144\n",
      "Epoch 596/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 63.06 - 0s 147us/sample - loss: 4139.2449 - val_loss: 89.8242\n",
      "Epoch 597/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4107.8056 - val_loss: 91.2982\n",
      "Epoch 598/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4078.5192 - val_loss: 90.5775\n",
      "Epoch 599/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4062.1190 - val_loss: 91.0051\n",
      "Epoch 600/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4026.1211 - val_loss: 91.6038\n",
      "Epoch 601/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3996.9737 - val_loss: 94.0365\n",
      "Epoch 602/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3966.4704 - val_loss: 90.3110\n",
      "Epoch 603/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3937.9984 - val_loss: 92.3527\n",
      "Epoch 604/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3911.4648 - val_loss: 90.6877\n",
      "Epoch 605/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3876.0226 - val_loss: 91.3637\n",
      "Epoch 606/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3858.5847 - val_loss: 90.8541\n",
      "Epoch 607/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3815.3798 - val_loss: 89.8295\n",
      "Epoch 608/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3790.0861 - val_loss: 88.9185\n",
      "Epoch 609/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3777.8467 - val_loss: 91.1690\n",
      "Epoch 610/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3737.6002 - val_loss: 92.0904\n",
      "Epoch 611/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3707.8599 - val_loss: 89.1527\n",
      "Epoch 612/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3674.2375 - val_loss: 93.6041\n",
      "Epoch 613/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3647.3531 - val_loss: 92.2136\n",
      "Epoch 614/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3627.8859 - val_loss: 88.1785\n",
      "Epoch 615/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3601.1947 - val_loss: 89.4546\n",
      "Epoch 616/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3568.8626 - val_loss: 88.7372\n",
      "Epoch 617/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3551.0837 - val_loss: 90.9853\n",
      "Epoch 618/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3517.0880 - val_loss: 87.6665\n",
      "Epoch 619/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3489.5828 - val_loss: 89.1940\n",
      "Epoch 620/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3463.8019 - val_loss: 89.0178\n",
      "Epoch 621/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3439.8465 - val_loss: 88.9227\n",
      "Epoch 622/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3428.5961 - val_loss: 87.7637\n",
      "Epoch 623/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3391.2404 - val_loss: 88.9133\n",
      "Epoch 624/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3360.1369 - val_loss: 93.4036\n",
      "Epoch 625/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3344.5162 - val_loss: 89.3050\n",
      "Epoch 626/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3324.1113 - val_loss: 89.4090\n",
      "Epoch 627/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3293.5199 - val_loss: 87.9315\n",
      "Epoch 628/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3274.2192 - val_loss: 92.4156\n",
      "Epoch 629/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3245.4169 - val_loss: 87.8097\n",
      "Epoch 630/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3219.5684 - val_loss: 90.9907\n",
      "Epoch 631/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3191.0687 - val_loss: 87.7688\n",
      "Epoch 632/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3170.2899 - val_loss: 89.3105\n",
      "Epoch 633/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3143.9199 - val_loss: 87.5685\n",
      "Epoch 634/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3113.4649 - val_loss: 89.5042\n",
      "Epoch 635/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3094.5607 - val_loss: 88.0779\n",
      "Epoch 636/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3062.4133 - val_loss: 90.6255\n",
      "Epoch 637/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3049.4812 - val_loss: 89.0232\n",
      "Epoch 638/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3013.2486 - val_loss: 86.3460\n",
      "Epoch 639/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2990.3209 - val_loss: 87.2596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 640/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2981.6976 - val_loss: 90.9998\n",
      "Epoch 641/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2947.1380 - val_loss: 89.1117\n",
      "Epoch 642/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2910.7466 - val_loss: 87.1567\n",
      "Epoch 643/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2899.5975 - val_loss: 88.7111\n",
      "Epoch 644/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2868.2954 - val_loss: 88.7047\n",
      "Epoch 645/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2845.0205 - val_loss: 86.2635\n",
      "Epoch 646/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2811.3192 - val_loss: 89.0446\n",
      "Epoch 647/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2796.6941 - val_loss: 87.8746\n",
      "Epoch 648/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2776.3664 - val_loss: 88.7677\n",
      "Epoch 649/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2738.0888 - val_loss: 85.3914\n",
      "Epoch 650/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2725.2864 - val_loss: 91.9854\n",
      "Epoch 651/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2696.0868 - val_loss: 86.2058\n",
      "Epoch 652/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2669.5120 - val_loss: 86.2688\n",
      "Epoch 653/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2648.6861 - val_loss: 88.7770\n",
      "Epoch 654/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2629.9960 - val_loss: 86.8100\n",
      "Epoch 655/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2622.1234 - val_loss: 86.2753\n",
      "Epoch 656/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2596.6291 - val_loss: 89.3489\n",
      "Epoch 657/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2558.1737 - val_loss: 84.9901\n",
      "Epoch 658/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2536.3001 - val_loss: 88.3223\n",
      "Epoch 659/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2520.2129 - val_loss: 85.1899\n",
      "Epoch 660/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2506.6782 - val_loss: 87.0380\n",
      "Epoch 661/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2468.5437 - val_loss: 86.1439\n",
      "Epoch 662/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2454.1216 - val_loss: 89.1798\n",
      "Epoch 663/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2427.7329 - val_loss: 85.9347\n",
      "Epoch 664/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2410.2979 - val_loss: 87.7569\n",
      "Epoch 665/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2395.5577 - val_loss: 87.2010\n",
      "Epoch 666/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2371.4248 - val_loss: 85.7827\n",
      "Epoch 667/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2347.6179 - val_loss: 85.3809\n",
      "Epoch 668/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2317.8189 - val_loss: 86.6239\n",
      "Epoch 669/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2298.4006 - val_loss: 84.0511\n",
      "Epoch 670/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2278.3089 - val_loss: 88.8614\n",
      "Epoch 671/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2284.4438 - val_loss: 88.2212\n",
      "Epoch 672/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2254.2770 - val_loss: 85.9417\n",
      "Epoch 673/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2215.6005 - val_loss: 86.5527\n",
      "Epoch 674/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2203.7714 - val_loss: 86.2193\n",
      "Epoch 675/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2170.3116 - val_loss: 84.8220\n",
      "Epoch 676/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2154.5852 - val_loss: 83.9600\n",
      "Epoch 677/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2135.8092 - val_loss: 87.5008\n",
      "Epoch 678/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2117.1208 - val_loss: 86.6380\n",
      "Epoch 679/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2088.3128 - val_loss: 86.2265\n",
      "Epoch 680/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2076.7726 - val_loss: 82.9126\n",
      "Epoch 681/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2055.1795 - val_loss: 84.3459\n",
      "Epoch 682/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2032.4264 - val_loss: 87.4381\n",
      "Epoch 683/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2012.9535 - val_loss: 85.8497\n",
      "Epoch 684/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1991.8712 - val_loss: 84.3103\n",
      "Epoch 685/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1973.1231 - val_loss: 82.8444\n",
      "Epoch 686/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1970.0325 - val_loss: 86.1421\n",
      "Epoch 687/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1940.6536 - val_loss: 83.8062\n",
      "Epoch 688/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1916.7501 - val_loss: 84.8400\n",
      "Epoch 689/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1912.5253 - val_loss: 83.4575\n",
      "Epoch 690/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1883.8778 - val_loss: 88.5093\n",
      "Epoch 691/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1869.1128 - val_loss: 84.7789\n",
      "Epoch 692/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1851.0454 - val_loss: 86.8864\n",
      "Epoch 693/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1826.6318 - val_loss: 82.3416\n",
      "Epoch 694/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1803.8822 - val_loss: 85.5415\n",
      "Epoch 695/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1799.8339 - val_loss: 83.0237\n",
      "Epoch 696/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1777.1439 - val_loss: 83.4453\n",
      "Epoch 697/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1759.3922 - val_loss: 84.0493\n",
      "Epoch 698/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1741.5126 - val_loss: 81.5133\n",
      "Epoch 699/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1731.7774 - val_loss: 83.9719\n",
      "Epoch 700/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1704.7423 - val_loss: 83.2227\n",
      "Epoch 701/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1705.3135 - val_loss: 89.0183\n",
      "Epoch 702/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1669.8182 - val_loss: 81.6372\n",
      "Epoch 703/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1657.2805 - val_loss: 85.0288\n",
      "Epoch 704/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1636.6778 - val_loss: 82.0775\n",
      "Epoch 705/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1628.3615 - val_loss: 86.0728\n",
      "Epoch 706/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1604.1862 - val_loss: 82.0000\n",
      "Epoch 707/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1583.6224 - val_loss: 82.6494\n",
      "Epoch 708/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1577.8022 - val_loss: 81.6846\n",
      "Epoch 709/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1555.8597 - val_loss: 85.0887\n",
      "Epoch 710/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1545.8496 - val_loss: 82.1335\n",
      "Epoch 711/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1526.0583 - val_loss: 83.9388\n",
      "Epoch 712/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1509.5344 - val_loss: 81.7578\n",
      "Epoch 713/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 1492.2952 - val_loss: 82.4058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 714/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1477.8145 - val_loss: 82.8381\n",
      "Epoch 715/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1459.8832 - val_loss: 83.0267\n",
      "Epoch 716/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1456.3057 - val_loss: 80.7941\n",
      "Epoch 717/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1435.6368 - val_loss: 86.4771\n",
      "Epoch 718/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1426.5524 - val_loss: 80.9242\n",
      "Epoch 719/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1402.2428 - val_loss: 82.0387\n",
      "Epoch 720/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1406.5755 - val_loss: 79.9913\n",
      "Epoch 721/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1382.2045 - val_loss: 82.0948\n",
      "Epoch 722/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 1360.0485 - val_loss: 83.1475\n",
      "Epoch 723/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1346.2087 - val_loss: 80.7459\n",
      "Epoch 724/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1327.5865 - val_loss: 80.7987\n",
      "Epoch 725/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1315.1178 - val_loss: 82.8952\n",
      "Epoch 726/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1307.7999 - val_loss: 82.8612\n",
      "Epoch 727/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1283.5225 - val_loss: 78.7417\n",
      "Epoch 728/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1267.6236 - val_loss: 81.9151\n",
      "Epoch 729/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1260.3137 - val_loss: 83.6384\n",
      "Epoch 730/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1247.1150 - val_loss: 80.7890\n",
      "Epoch 731/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1226.6426 - val_loss: 82.3041\n",
      "Epoch 732/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1219.4507 - val_loss: 79.0578\n",
      "Epoch 733/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1197.1905 - val_loss: 83.8590\n",
      "Epoch 734/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1187.7518 - val_loss: 79.1399\n",
      "Epoch 735/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1182.8139 - val_loss: 80.0123\n",
      "Epoch 736/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1166.5312 - val_loss: 80.0252\n",
      "Epoch 737/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1146.1748 - val_loss: 79.1107\n",
      "Epoch 738/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1134.8650 - val_loss: 82.0677\n",
      "Epoch 739/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1120.4380 - val_loss: 78.5664\n",
      "Epoch 740/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1119.8358 - val_loss: 83.1376\n",
      "Epoch 741/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1095.6865 - val_loss: 78.0028\n",
      "Epoch 742/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1092.0892 - val_loss: 82.5722\n",
      "Epoch 743/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1068.7460 - val_loss: 77.5383\n",
      "Epoch 744/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1077.8404 - val_loss: 79.1828\n",
      "Epoch 745/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1040.9738 - val_loss: 78.0481\n",
      "Epoch 746/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1033.8973 - val_loss: 84.8525\n",
      "Epoch 747/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1024.5607 - val_loss: 77.2445\n",
      "Epoch 748/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1008.0516 - val_loss: 81.7892\n",
      "Epoch 749/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 996.2587 - val_loss: 78.4279\n",
      "Epoch 750/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1001.6798 - val_loss: 78.4624\n",
      "Epoch 751/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 975.2464 - val_loss: 81.1098\n",
      "Epoch 752/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 954.9923 - val_loss: 78.0990\n",
      "Epoch 753/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 946.5936 - val_loss: 79.7396\n",
      "Epoch 754/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 942.6651 - val_loss: 77.2832\n",
      "Epoch 755/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 923.2195 - val_loss: 80.1038\n",
      "Epoch 756/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 913.3444 - val_loss: 76.7190\n",
      "Epoch 757/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 900.6443 - val_loss: 76.2207\n",
      "Epoch 758/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 891.6934 - val_loss: 79.4649\n",
      "Epoch 759/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 883.2406 - val_loss: 81.2043\n",
      "Epoch 760/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 872.5279 - val_loss: 84.9452\n",
      "Epoch 761/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 857.8091 - val_loss: 76.8243\n",
      "Epoch 762/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 851.3835 - val_loss: 86.2448\n",
      "Epoch 763/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 848.1612 - val_loss: 78.9410\n",
      "Epoch 764/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 829.6087 - val_loss: 88.2709\n",
      "Epoch 765/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 825.8786 - val_loss: 76.2973\n",
      "Epoch 766/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 811.0695 - val_loss: 78.0308\n",
      "Epoch 767/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 797.5377 - val_loss: 74.8490\n",
      "Epoch 768/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 788.9486 - val_loss: 77.0215\n",
      "Epoch 769/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 773.9002 - val_loss: 78.7689\n",
      "Epoch 770/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 769.9506 - val_loss: 74.9208\n",
      "Epoch 771/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 761.4285 - val_loss: 75.9763\n",
      "Epoch 772/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 751.5006 - val_loss: 74.1939\n",
      "Epoch 773/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 738.1175 - val_loss: 74.7829\n",
      "Epoch 774/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 727.2681 - val_loss: 75.3392\n",
      "Epoch 775/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 725.0997 - val_loss: 76.7614\n",
      "Epoch 776/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 708.7714 - val_loss: 78.7424\n",
      "Epoch 777/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 703.9437 - val_loss: 74.3167\n",
      "Epoch 778/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 690.0076 - val_loss: 79.1160\n",
      "Epoch 779/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 680.2719 - val_loss: 75.7934\n",
      "Epoch 780/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 672.2624 - val_loss: 78.2126\n",
      "Epoch 781/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 672.4329 - val_loss: 75.7973\n",
      "Epoch 782/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 660.6273 - val_loss: 74.7756\n",
      "Epoch 783/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 644.4363 - val_loss: 72.7946\n",
      "Epoch 784/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 639.4540 - val_loss: 74.3780\n",
      "Epoch 785/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 630.5845 - val_loss: 75.0542\n",
      "Epoch 786/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 628.8175 - val_loss: 85.7077\n",
      "Epoch 787/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 621.7688 - val_loss: 85.4149\n",
      "Epoch 788/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 616.1157 - val_loss: 96.0754\n",
      "Epoch 789/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 606.7628 - val_loss: 81.9079\n",
      "Epoch 790/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 598.3517 - val_loss: 86.7972\n",
      "Epoch 791/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 589.6750 - val_loss: 73.1083\n",
      "Epoch 792/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 581.2951 - val_loss: 71.7050\n",
      "Epoch 793/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 573.1848 - val_loss: 78.9168\n",
      "Epoch 794/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 565.1713 - val_loss: 78.0170\n",
      "Epoch 795/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 553.1363 - val_loss: 71.3882\n",
      "Epoch 796/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 548.7227 - val_loss: 72.3065\n",
      "Epoch 797/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 537.4627 - val_loss: 72.9424\n",
      "Epoch 798/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 530.9154 - val_loss: 73.4928\n",
      "Epoch 799/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 523.4574 - val_loss: 72.1255\n",
      "Epoch 800/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 523.2974 - val_loss: 72.0438\n",
      "Epoch 801/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 510.3677 - val_loss: 73.5780\n",
      "Epoch 802/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 503.7081 - val_loss: 72.5771\n",
      "Epoch 803/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 492.9602 - val_loss: 70.3510\n",
      "Epoch 804/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 498.9975 - val_loss: 107.6056\n",
      "Epoch 805/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 494.8546 - val_loss: 92.8965\n",
      "Epoch 806/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 504.2144 - val_loss: 88.8614\n",
      "Epoch 807/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 472.2071 - val_loss: 72.2759\n",
      "Epoch 808/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 461.4985 - val_loss: 70.5948\n",
      "Epoch 809/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 460.0159 - val_loss: 74.2345\n",
      "Epoch 810/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 449.1483 - val_loss: 70.2921\n",
      "Epoch 811/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 449.8579 - val_loss: 72.8428\n",
      "Epoch 812/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 441.8962 - val_loss: 74.6017\n",
      "Epoch 813/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 429.7013 - val_loss: 72.0772\n",
      "Epoch 814/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 439.8169 - val_loss: 76.8210\n",
      "Epoch 815/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 419.3518 - val_loss: 69.1498\n",
      "Epoch 816/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 413.7702 - val_loss: 78.9894\n",
      "Epoch 817/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 405.4533 - val_loss: 72.3807\n",
      "Epoch 818/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 405.5799 - val_loss: 74.5909\n",
      "Epoch 819/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 404.1635 - val_loss: 68.0360\n",
      "Epoch 820/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 409.6933 - val_loss: 73.1289\n",
      "Epoch 821/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 394.6097 - val_loss: 76.1552\n",
      "Epoch 822/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 380.7167 - val_loss: 67.9780\n",
      "Epoch 823/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 375.0089 - val_loss: 69.0204\n",
      "Epoch 824/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 372.8863 - val_loss: 69.7275\n",
      "Epoch 825/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 365.1944 - val_loss: 68.1777\n",
      "Epoch 826/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 361.7529 - val_loss: 68.0712\n",
      "Epoch 827/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 359.1777 - val_loss: 79.7199\n",
      "Epoch 828/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 359.2905 - val_loss: 67.9797\n",
      "Epoch 829/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 352.0182 - val_loss: 68.4620\n",
      "Epoch 830/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 351.7584 - val_loss: 70.9430\n",
      "Epoch 831/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 343.8939 - val_loss: 74.8336\n",
      "Epoch 832/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 336.2391 - val_loss: 75.8740\n",
      "Epoch 833/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 329.9032 - val_loss: 77.8851\n",
      "Epoch 834/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 323.6506 - val_loss: 72.7766\n",
      "Epoch 835/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 320.7770 - val_loss: 69.2102\n",
      "Epoch 836/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 315.5221 - val_loss: 69.5652\n",
      "Epoch 837/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 308.8685 - val_loss: 65.4308\n",
      "Epoch 838/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 309.5812 - val_loss: 65.5577\n",
      "Epoch 839/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 299.6636 - val_loss: 67.2341\n",
      "Epoch 840/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 296.7300 - val_loss: 66.0719\n",
      "Epoch 841/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 294.0980 - val_loss: 72.9375\n",
      "Epoch 842/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 290.2423 - val_loss: 70.5690\n",
      "Epoch 843/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 290.5525 - val_loss: 72.3393\n",
      "Epoch 844/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 285.5355 - val_loss: 64.8885\n",
      "Epoch 845/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 281.9541 - val_loss: 69.2201\n",
      "Epoch 846/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 284.9595 - val_loss: 94.6919\n",
      "Epoch 847/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 281.1801 - val_loss: 68.2002\n",
      "Epoch 848/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 271.3291 - val_loss: 64.3205\n",
      "Epoch 849/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 264.4717 - val_loss: 89.3917\n",
      "Epoch 850/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 263.0410 - val_loss: 68.5879\n",
      "Epoch 851/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 259.0321 - val_loss: 67.5125\n",
      "Epoch 852/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 251.3993 - val_loss: 65.2077\n",
      "Epoch 853/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 244.6570 - val_loss: 63.9467\n",
      "Epoch 854/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 243.2976 - val_loss: 72.9534\n",
      "Epoch 855/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 242.6056 - val_loss: 63.9516\n",
      "Epoch 856/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 236.9353 - val_loss: 75.7683\n",
      "Epoch 857/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 236.1686 - val_loss: 63.7298\n",
      "Epoch 858/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 230.8020 - val_loss: 62.7163\n",
      "Epoch 859/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 233.3347 - val_loss: 70.8929\n",
      "Epoch 860/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 231.0480 - val_loss: 75.2924\n",
      "Epoch 861/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 223.9720 - val_loss: 64.1668\n",
      "Epoch 862/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 219.6376 - val_loss: 66.9345\n",
      "Epoch 863/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 214.5732 - val_loss: 61.9594\n",
      "Epoch 864/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 208.8183 - val_loss: 64.1993\n",
      "Epoch 865/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 211.1603 - val_loss: 61.1592\n",
      "Epoch 866/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 203.1925 - val_loss: 69.7050\n",
      "Epoch 867/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 206.4046 - val_loss: 64.2065\n",
      "Epoch 868/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 199.3704 - val_loss: 60.5553\n",
      "Epoch 869/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 195.9937 - val_loss: 63.7284\n",
      "Epoch 870/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 196.0357 - val_loss: 64.9508\n",
      "Epoch 871/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 195.9176 - val_loss: 62.3220\n",
      "Epoch 872/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 194.0558 - val_loss: 80.5143\n",
      "Epoch 873/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 193.0121 - val_loss: 67.4266\n",
      "Epoch 874/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 183.6795 - val_loss: 61.0970\n",
      "Epoch 875/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 177.3934 - val_loss: 61.7689\n",
      "Epoch 876/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 175.3314 - val_loss: 63.0443\n",
      "Epoch 877/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 177.1723 - val_loss: 59.8924\n",
      "Epoch 878/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 172.3186 - val_loss: 62.1782\n",
      "Epoch 879/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 169.7887 - val_loss: 60.1499\n",
      "Epoch 880/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 165.3771 - val_loss: 59.9901\n",
      "Epoch 881/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 164.3233 - val_loss: 70.7707\n",
      "Epoch 882/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 173.2763 - val_loss: 58.6066\n",
      "Epoch 883/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 159.1723 - val_loss: 62.4440\n",
      "Epoch 884/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 161.1658 - val_loss: 61.9491\n",
      "Epoch 885/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 156.8657 - val_loss: 60.0819\n",
      "Epoch 886/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 160.8891 - val_loss: 60.3446\n",
      "Epoch 887/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 150.0687 - val_loss: 64.6203\n",
      "Epoch 888/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 153.9264 - val_loss: 58.0196\n",
      "Epoch 889/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 156.4494 - val_loss: 73.9313\n",
      "Epoch 890/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 165.1818 - val_loss: 85.8076\n",
      "Epoch 891/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 158.7241 - val_loss: 70.4464\n",
      "Epoch 892/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 162.4511 - val_loss: 59.7892\n",
      "Epoch 893/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 137.9663 - val_loss: 57.9527\n",
      "Epoch 894/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 135.5083 - val_loss: 60.1709\n",
      "Epoch 895/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 137.6171 - val_loss: 57.8066\n",
      "Epoch 896/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 134.5313 - val_loss: 68.8649\n",
      "Epoch 897/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 143.1616 - val_loss: 79.8165\n",
      "Epoch 898/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 147.9158 - val_loss: 58.5291\n",
      "Epoch 899/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 131.3952 - val_loss: 76.1537\n",
      "Epoch 900/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 127.6788 - val_loss: 60.4023\n",
      "Epoch 901/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 36.21 - 0s 206us/sample - loss: 126.0711 - val_loss: 60.3493\n",
      "Epoch 902/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 127.4795 - val_loss: 60.7438\n",
      "Epoch 903/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 121.5633 - val_loss: 55.3082\n",
      "Epoch 904/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 122.0652 - val_loss: 54.9944\n",
      "Epoch 905/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 125.8089 - val_loss: 76.0350\n",
      "Epoch 906/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 122.3283 - val_loss: 54.9983\n",
      "Epoch 907/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 124.0585 - val_loss: 60.7632\n",
      "Epoch 908/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 120.9985 - val_loss: 59.3442\n",
      "Epoch 909/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 116.3738 - val_loss: 64.9033\n",
      "Epoch 910/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 113.4726 - val_loss: 53.7362\n",
      "Epoch 911/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 107.7531 - val_loss: 55.3991\n",
      "Epoch 912/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 105.3874 - val_loss: 54.0315\n",
      "Epoch 913/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 104.9020 - val_loss: 53.1326\n",
      "Epoch 914/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 119.4502 - val_loss: 91.6128\n",
      "Epoch 915/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 109.9212 - val_loss: 54.1996\n",
      "Epoch 916/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 107.5593 - val_loss: 62.4016\n",
      "Epoch 917/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 104.0663 - val_loss: 94.6906\n",
      "Epoch 918/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 107.8535 - val_loss: 68.5788\n",
      "Epoch 919/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 98.5337 - val_loss: 65.6914\n",
      "Epoch 920/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 111.3613 - val_loss: 83.1207\n",
      "Epoch 921/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 121.7123 - val_loss: 52.0154\n",
      "Epoch 922/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 124.2359 - val_loss: 68.5895\n",
      "Epoch 923/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 94.1271 - val_loss: 58.7233\n",
      "Epoch 924/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 91.9132 - val_loss: 53.1446\n",
      "Epoch 925/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 90.3893 - val_loss: 53.7260\n",
      "Epoch 926/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 90.3594 - val_loss: 52.0190\n",
      "Epoch 927/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 93.5051 - val_loss: 65.1834\n",
      "Epoch 928/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 102.2106 - val_loss: 51.1949\n",
      "Epoch 929/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 85.8928 - val_loss: 51.2890\n",
      "Epoch 930/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 86.3140 - val_loss: 50.6749\n",
      "Epoch 931/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 86.5739 - val_loss: 51.2980\n",
      "Epoch 932/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 82.4061 - val_loss: 54.4415\n",
      "Epoch 933/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 82.9431 - val_loss: 55.8619\n",
      "Epoch 934/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 88.9145 - val_loss: 55.0128\n",
      "Epoch 935/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 84.2425 - val_loss: 51.6053\n",
      "Epoch 936/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 86.2511 - val_loss: 70.4147\n",
      "Epoch 937/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 93.6958 - val_loss: 73.9128\n",
      "Epoch 938/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 115.9078 - val_loss: 70.2129\n",
      "Epoch 939/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 77.4940 - val_loss: 52.0945\n",
      "Epoch 940/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 77.2831 - val_loss: 55.2152\n",
      "Epoch 941/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 80.5512 - val_loss: 50.8852\n",
      "Epoch 942/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 74.2934 - val_loss: 50.5841\n",
      "Epoch 943/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 76.3320 - val_loss: 50.5658\n",
      "Epoch 944/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 73.6547 - val_loss: 52.4475\n",
      "Epoch 945/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 78.8893 - val_loss: 68.3976\n",
      "Epoch 946/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 92.9535 - val_loss: 51.9979\n",
      "Epoch 947/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 90.7819 - val_loss: 73.2991\n",
      "Epoch 948/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 81.9213 - val_loss: 52.0032\n",
      "Epoch 949/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 68.5157 - val_loss: 53.9497\n",
      "Epoch 950/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 72.8470 - val_loss: 47.5290\n",
      "Epoch 951/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 82.2078 - val_loss: 92.9934\n",
      "Epoch 952/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 83.5449 - val_loss: 48.4466\n",
      "Epoch 953/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 72.3475 - val_loss: 82.3823\n",
      "Epoch 954/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 90.6884 - val_loss: 45.8085\n",
      "Epoch 955/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 94.1612 - val_loss: 84.1055\n",
      "Epoch 956/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 79.2745 - val_loss: 50.7934\n",
      "Epoch 957/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 65.5896 - val_loss: 55.4120\n",
      "Epoch 958/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 66.1047 - val_loss: 53.1388\n",
      "Epoch 959/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 63.0728 - val_loss: 45.0367\n",
      "Epoch 960/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 59.9219 - val_loss: 44.9635\n",
      "Epoch 961/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 58.3935 - val_loss: 44.9053\n",
      "Epoch 962/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 63.9056 - val_loss: 44.5358\n",
      "Epoch 963/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 67.9850 - val_loss: 71.8944\n",
      "Epoch 964/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 72.4703 - val_loss: 66.2204\n",
      "Epoch 965/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 69.4611 - val_loss: 45.3351\n",
      "Epoch 966/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 54.7057 - val_loss: 54.4475\n",
      "Epoch 967/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 76.7959 - val_loss: 63.6866\n",
      "Epoch 968/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 66.8691 - val_loss: 57.6124\n",
      "Epoch 969/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 55.4097 - val_loss: 47.5746\n",
      "Epoch 970/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 57.0578 - val_loss: 43.0572\n",
      "Epoch 971/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 55.4449 - val_loss: 51.6722\n",
      "Epoch 972/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 54.2024 - val_loss: 44.2205\n",
      "Epoch 973/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 52.9234 - val_loss: 45.3828\n",
      "Epoch 974/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 57.8848 - val_loss: 42.1198\n",
      "Epoch 975/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 51.7901 - val_loss: 54.3768\n",
      "Epoch 976/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 57.2371 - val_loss: 51.2567\n",
      "Epoch 977/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 58.1831 - val_loss: 55.9937\n",
      "Epoch 978/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 66.3120 - val_loss: 52.2542\n",
      "Epoch 979/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 58.0824 - val_loss: 46.8913\n",
      "Epoch 980/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 58.9223 - val_loss: 47.7244\n",
      "Epoch 981/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 49.4955 - val_loss: 49.4876\n",
      "Epoch 982/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 55.6918 - val_loss: 53.5200\n",
      "Epoch 983/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 50.6895 - val_loss: 41.4966\n",
      "Epoch 984/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 46.5841 - val_loss: 40.8884\n",
      "Epoch 985/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 48.4909 - val_loss: 43.7563\n",
      "Epoch 986/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 47.1062 - val_loss: 47.4079\n",
      "Epoch 987/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 46.0358 - val_loss: 40.4284\n",
      "Epoch 988/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 43.6522 - val_loss: 58.1405\n",
      "Epoch 989/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 55.0295 - val_loss: 40.2284\n",
      "Epoch 990/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 47.4755 - val_loss: 40.6714\n",
      "Epoch 991/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 44.2618 - val_loss: 39.7269\n",
      "Epoch 992/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 44.5538 - val_loss: 45.2964\n",
      "Epoch 993/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 52.8037 - val_loss: 39.7313\n",
      "Epoch 994/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 69.2087 - val_loss: 44.6220\n",
      "Epoch 995/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 65.8913 - val_loss: 124.4718\n",
      "Epoch 996/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 73.3235 - val_loss: 50.8576\n",
      "Epoch 997/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 59.3452 - val_loss: 56.2141\n",
      "Epoch 998/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 51.9629 - val_loss: 39.9938\n",
      "Epoch 999/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 42.0838 - val_loss: 39.2420\n",
      "Epoch 1000/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 41.4116 - val_loss: 43.5603\n",
      "Epoch 1001/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 49.1921 - val_loss: 66.3909\n",
      "Epoch 1002/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 53.2203 - val_loss: 47.8191\n",
      "Epoch 1003/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 43.5331 - val_loss: 38.0438\n",
      "Epoch 1004/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 51.0766 - val_loss: 41.1250\n",
      "Epoch 1005/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 45.9413 - val_loss: 40.0222\n",
      "Epoch 1006/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 39.8601 - val_loss: 37.6899\n",
      "Epoch 1007/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 45.4515 - val_loss: 37.4188\n",
      "Epoch 1008/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 41.9960 - val_loss: 37.1370\n",
      "Epoch 1009/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 37.3668 - val_loss: 36.0428\n",
      "Epoch 1010/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 39.1874 - val_loss: 36.0807\n",
      "Epoch 1011/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 38.7074 - val_loss: 38.8100\n",
      "Epoch 1012/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 40.0848 - val_loss: 51.0615\n",
      "Epoch 1013/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 45.3306 - val_loss: 36.1568\n",
      "Epoch 1014/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 38.0800 - val_loss: 36.2956\n",
      "Epoch 1015/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 36.5512 - val_loss: 38.1472\n",
      "Epoch 1016/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 36.3884 - val_loss: 45.7996\n",
      "Epoch 1017/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 40.7738 - val_loss: 35.6510\n",
      "Epoch 1018/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 38.2798 - val_loss: 83.7046\n",
      "Epoch 1019/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 55.1932 - val_loss: 37.9718\n",
      "Epoch 1020/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 48.3200 - val_loss: 131.7114\n",
      "Epoch 1021/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 72.0302 - val_loss: 45.7136\n",
      "Epoch 1022/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 43.9993 - val_loss: 44.9975\n",
      "Epoch 1023/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 42.1920 - val_loss: 35.0631\n",
      "Epoch 1024/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 35.8255 - val_loss: 33.8321\n",
      "Epoch 1025/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 38.0477 - val_loss: 37.1136\n",
      "Epoch 1026/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 32.5751 - val_loss: 35.5445\n",
      "Epoch 1027/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 35.0754 - val_loss: 34.5463\n",
      "Epoch 1028/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 32.5808 - val_loss: 36.6062\n",
      "Epoch 1029/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 38.0008 - val_loss: 32.8420\n",
      "Epoch 1030/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 35.3618 - val_loss: 32.4615\n",
      "Epoch 1031/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 32.6596 - val_loss: 42.4698\n",
      "Epoch 1032/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 34.8824 - val_loss: 42.8542\n",
      "Epoch 1033/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 41.2043 - val_loss: 38.4987\n",
      "Epoch 1034/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 33.0110 - val_loss: 32.6461\n",
      "Epoch 1035/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 33.2407 - val_loss: 32.9134\n",
      "Epoch 1036/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 32.3556 - val_loss: 31.3061\n",
      "Epoch 1037/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 30.8469 - val_loss: 35.2335\n",
      "Epoch 1038/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 34.4528 - val_loss: 36.8461\n",
      "Epoch 1039/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 35.8831 - val_loss: 37.8259\n",
      "Epoch 1040/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 32.3496 - val_loss: 31.5267\n",
      "Epoch 1041/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 32.8010 - val_loss: 31.6230\n",
      "Epoch 1042/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 36.5014 - val_loss: 30.3954\n",
      "Epoch 1043/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 29.2168 - val_loss: 41.3504\n",
      "Epoch 1044/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 43.2440 - val_loss: 42.6465\n",
      "Epoch 1045/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 37.7583 - val_loss: 35.4244\n",
      "Epoch 1046/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 37.3042 - val_loss: 30.2475\n",
      "Epoch 1047/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 28.8095 - val_loss: 31.7813\n",
      "Epoch 1048/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 36.6139 - val_loss: 35.0003\n",
      "Epoch 1049/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 32.9608 - val_loss: 59.4483\n",
      "Epoch 1050/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 39.4845 - val_loss: 30.7723\n",
      "Epoch 1051/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 36.3297 - val_loss: 37.2166\n",
      "Epoch 1052/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 41.2453 - val_loss: 32.6773\n",
      "Epoch 1053/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 41.5627 - val_loss: 29.1103\n",
      "Epoch 1054/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 32.3377 - val_loss: 41.4734\n",
      "Epoch 1055/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 33.6728 - val_loss: 45.4225\n",
      "Epoch 1056/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 33.8409 - val_loss: 34.3151\n",
      "Epoch 1057/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 30.4236 - val_loss: 34.1822\n",
      "Epoch 1058/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 29.3296 - val_loss: 42.2216\n",
      "Epoch 1059/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 38.6243 - val_loss: 68.7371\n",
      "Epoch 1060/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 50.4616 - val_loss: 28.9335\n",
      "Epoch 1061/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 35.6440 - val_loss: 29.6113\n",
      "Epoch 1062/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 37.1744 - val_loss: 39.6559\n",
      "Epoch 1063/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 29.9263 - val_loss: 32.9480\n",
      "Epoch 1064/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26.0836 - val_loss: 30.4720\n",
      "Epoch 1065/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 27.2066 - val_loss: 35.9549\n",
      "Epoch 1066/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 27.4142 - val_loss: 26.8776\n",
      "Epoch 1067/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 25.0723 - val_loss: 28.5053\n",
      "Epoch 1068/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 25.0337 - val_loss: 26.7402\n",
      "Epoch 1069/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 25.8001 - val_loss: 29.6645\n",
      "Epoch 1070/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 31.1709 - val_loss: 32.0066\n",
      "Epoch 1071/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 22.9537 - val_loss: 26.0536\n",
      "Epoch 1072/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 29.7232 - val_loss: 27.2276\n",
      "Epoch 1073/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 27.6708 - val_loss: 27.8849\n",
      "Epoch 1074/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 24.5504 - val_loss: 25.1942\n",
      "Epoch 1075/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 28.9984 - val_loss: 33.9816\n",
      "Epoch 1076/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 38.7490 - val_loss: 38.8815\n",
      "Epoch 1077/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 31.5669 - val_loss: 27.7683\n",
      "Epoch 1078/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 26.6589 - val_loss: 24.7512\n",
      "Epoch 1079/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26.0168 - val_loss: 27.4399\n",
      "Epoch 1080/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 28.3429 - val_loss: 31.8882\n",
      "Epoch 1081/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 28.1773 - val_loss: 27.4089\n",
      "Epoch 1082/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 25.5582 - val_loss: 28.3182\n",
      "Epoch 1083/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 24.7281 - val_loss: 24.4472\n",
      "Epoch 1084/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 21.7695 - val_loss: 23.5473\n",
      "Epoch 1085/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21.9755 - val_loss: 23.5915\n",
      "Epoch 1086/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 24.1729 - val_loss: 28.2668\n",
      "Epoch 1087/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 41.0109 - val_loss: 26.6682\n",
      "Epoch 1088/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 40.3992 - val_loss: 24.4057\n",
      "Epoch 1089/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 40.6329 - val_loss: 31.2883\n",
      "Epoch 1090/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26.7368 - val_loss: 28.8085\n",
      "Epoch 1091/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 24.1997 - val_loss: 24.3699\n",
      "Epoch 1092/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 24.4650 - val_loss: 23.3750\n",
      "Epoch 1093/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 26.3075 - val_loss: 32.8494\n",
      "Epoch 1094/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 24.3540 - val_loss: 27.1127\n",
      "Epoch 1095/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 23.1921 - val_loss: 21.9917\n",
      "Epoch 1096/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21.5896 - val_loss: 26.0601\n",
      "Epoch 1097/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 22.1984 - val_loss: 23.4543\n",
      "Epoch 1098/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 22.5129 - val_loss: 23.4169\n",
      "Epoch 1099/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21.4029 - val_loss: 22.1107\n",
      "Epoch 1100/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 23.6098 - val_loss: 56.1029\n",
      "Epoch 1101/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 31.4535 - val_loss: 24.3851\n",
      "Epoch 1102/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 20.9054 - val_loss: 21.0981\n",
      "Epoch 1103/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.4257 - val_loss: 21.1853\n",
      "Epoch 1104/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.0753 - val_loss: 22.5826\n",
      "Epoch 1105/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 22.9080 - val_loss: 31.7033\n",
      "Epoch 1106/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 22.3078 - val_loss: 21.6776\n",
      "Epoch 1107/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19.6152 - val_loss: 20.1506\n",
      "Epoch 1108/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21.1344 - val_loss: 20.0415\n",
      "Epoch 1109/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 20.0234 - val_loss: 20.4648\n",
      "Epoch 1110/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 19.2942 - val_loss: 20.6682\n",
      "Epoch 1111/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 22.5801 - val_loss: 28.0653\n",
      "Epoch 1112/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.2817 - val_loss: 19.4959\n",
      "Epoch 1113/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 18.7648 - val_loss: 21.6820\n",
      "Epoch 1114/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.9487 - val_loss: 23.9623\n",
      "Epoch 1115/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 25.6047 - val_loss: 39.4545\n",
      "Epoch 1116/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 34.9928 - val_loss: 32.4175\n",
      "Epoch 1117/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 46.3296 - val_loss: 21.4896\n",
      "Epoch 1118/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 26.4447 - val_loss: 22.5497\n",
      "Epoch 1119/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.9857 - val_loss: 25.0153\n",
      "Epoch 1120/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 24.8051 - val_loss: 25.4440\n",
      "Epoch 1121/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 23.8653 - val_loss: 21.0958\n",
      "Epoch 1122/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 30.1527 - val_loss: 21.7474\n",
      "Epoch 1123/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19.5701 - val_loss: 21.4524\n",
      "Epoch 1124/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 19.6863 - val_loss: 17.5803\n",
      "Epoch 1125/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16.4415 - val_loss: 26.6795\n",
      "Epoch 1126/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 21.3006 - val_loss: 35.3381\n",
      "Epoch 1127/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 27.0801 - val_loss: 17.6735\n",
      "Epoch 1128/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 23.2125 - val_loss: 25.8284\n",
      "Epoch 1129/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17.7382 - val_loss: 19.7197\n",
      "Epoch 1130/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.6208 - val_loss: 18.6271\n",
      "Epoch 1131/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.5451 - val_loss: 18.4061\n",
      "Epoch 1132/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.3807 - val_loss: 46.9582\n",
      "Epoch 1133/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 27.5402 - val_loss: 22.3936\n",
      "Epoch 1134/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.6278 - val_loss: 17.7794\n",
      "Epoch 1135/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.7531 - val_loss: 19.5391\n",
      "Epoch 1136/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17.3891 - val_loss: 21.0677\n",
      "Epoch 1137/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 21.1540 - val_loss: 28.6896\n",
      "Epoch 1138/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 23.8545 - val_loss: 16.7500\n",
      "Epoch 1139/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15.8601 - val_loss: 16.6860\n",
      "Epoch 1140/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14.3112 - val_loss: 16.3213\n",
      "Epoch 1141/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.4873 - val_loss: 18.2729\n",
      "Epoch 1142/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.6980 - val_loss: 20.2778\n",
      "Epoch 1143/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 25.7493 - val_loss: 17.5916\n",
      "Epoch 1144/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 23.3039 - val_loss: 19.8613\n",
      "Epoch 1145/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19.5892 - val_loss: 14.8035\n",
      "Epoch 1146/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.3309 - val_loss: 15.1170\n",
      "Epoch 1147/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15.5885 - val_loss: 24.6386\n",
      "Epoch 1148/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.6078 - val_loss: 16.6382\n",
      "Epoch 1149/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.3858 - val_loss: 14.5097\n",
      "Epoch 1150/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.0553 - val_loss: 17.7303\n",
      "Epoch 1151/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15.6101 - val_loss: 14.2411\n",
      "Epoch 1152/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 14.1026 - val_loss: 17.0338\n",
      "Epoch 1153/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 17.7677 - val_loss: 14.1191\n",
      "Epoch 1154/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19.4598 - val_loss: 15.5345\n",
      "Epoch 1155/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 26.1367 - val_loss: 55.7663\n",
      "Epoch 1156/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 24.2950 - val_loss: 24.3677\n",
      "Epoch 1157/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26.5125 - val_loss: 26.5022\n",
      "Epoch 1158/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 26.5824 - val_loss: 21.1653\n",
      "Epoch 1159/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 176us/sample - loss: 21.7204 - val_loss: 23.4012\n",
      "Epoch 1160/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.1043 - val_loss: 15.2401\n",
      "Epoch 1161/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.6610 - val_loss: 16.9654\n",
      "Epoch 1162/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 12.5885 - val_loss: 14.7152\n",
      "Epoch 1163/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 13.5441 - val_loss: 24.1516\n",
      "Epoch 1164/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 30.5023 - val_loss: 19.7576\n",
      "Epoch 1165/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 22.5369 - val_loss: 14.0185\n",
      "Epoch 1166/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.7890 - val_loss: 17.1784\n",
      "Epoch 1167/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14.9462 - val_loss: 13.1297\n",
      "Epoch 1168/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.6828 - val_loss: 25.2087\n",
      "Epoch 1169/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 15.9499 - val_loss: 12.5641\n",
      "Epoch 1170/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.4957 - val_loss: 12.7092\n",
      "Epoch 1171/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16.7727 - val_loss: 17.2009\n",
      "Epoch 1172/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.0886 - val_loss: 14.6955\n",
      "Epoch 1173/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.5113 - val_loss: 16.2747\n",
      "Epoch 1174/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21.9219 - val_loss: 14.4715\n",
      "Epoch 1175/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 15.0950 - val_loss: 14.9995\n",
      "Epoch 1176/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 17.4010 - val_loss: 20.8563\n",
      "Epoch 1177/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.8577 - val_loss: 11.4393\n",
      "Epoch 1178/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.3884 - val_loss: 11.6849\n",
      "Epoch 1179/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.6969 - val_loss: 13.3300\n",
      "Epoch 1180/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.6204 - val_loss: 10.8517\n",
      "Epoch 1181/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.9044 - val_loss: 12.6959\n",
      "Epoch 1182/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.4333 - val_loss: 11.7365\n",
      "Epoch 1183/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 10.5662 - val_loss: 11.8792\n",
      "Epoch 1184/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11.4475 - val_loss: 10.8286\n",
      "Epoch 1185/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.2913 - val_loss: 14.9645\n",
      "Epoch 1186/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.9176 - val_loss: 21.4554\n",
      "Epoch 1187/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.1984 - val_loss: 16.4593\n",
      "Epoch 1188/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.9792 - val_loss: 10.1918\n",
      "Epoch 1189/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.4190 - val_loss: 10.7714\n",
      "Epoch 1190/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.2113 - val_loss: 11.2123\n",
      "Epoch 1191/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.0061 - val_loss: 10.0064\n",
      "Epoch 1192/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.8443 - val_loss: 11.8536\n",
      "Epoch 1193/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.4806 - val_loss: 18.9559\n",
      "Epoch 1194/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 19.4893 - val_loss: 11.2374\n",
      "Epoch 1195/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.0925 - val_loss: 13.5414\n",
      "Epoch 1196/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.5468 - val_loss: 25.4378\n",
      "Epoch 1197/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16.1119 - val_loss: 18.5360\n",
      "Epoch 1198/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.4119 - val_loss: 11.4142\n",
      "Epoch 1199/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.3877 - val_loss: 10.6438\n",
      "Epoch 1200/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.9828 - val_loss: 16.8530\n",
      "Epoch 1201/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12.3762 - val_loss: 10.4057\n",
      "Epoch 1202/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.1137 - val_loss: 19.2191\n",
      "Epoch 1203/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 20.6261 - val_loss: 18.1540\n",
      "Epoch 1204/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.6086 - val_loss: 8.6697\n",
      "Epoch 1205/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11.5732 - val_loss: 23.5339\n",
      "Epoch 1206/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 19.3689 - val_loss: 14.4194\n",
      "Epoch 1207/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.6589 - val_loss: 10.8167\n",
      "Epoch 1208/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.5703 - val_loss: 10.6839\n",
      "Epoch 1209/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.1615 - val_loss: 8.2003\n",
      "Epoch 1210/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.9008 - val_loss: 11.8439\n",
      "Epoch 1211/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.5137 - val_loss: 10.6034\n",
      "Epoch 1212/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.3382 - val_loss: 11.3527\n",
      "Epoch 1213/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.5070 - val_loss: 13.3961\n",
      "Epoch 1214/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.3850 - val_loss: 8.0380\n",
      "Epoch 1215/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9.2177 - val_loss: 12.2409\n",
      "Epoch 1216/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.5072 - val_loss: 7.8416\n",
      "Epoch 1217/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9.9341 - val_loss: 9.2037\n",
      "Epoch 1218/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.6713 - val_loss: 11.3038\n",
      "Epoch 1219/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 11.6228 - val_loss: 7.3700\n",
      "Epoch 1220/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.6092 - val_loss: 13.0002\n",
      "Epoch 1221/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.2867 - val_loss: 10.6818\n",
      "Epoch 1222/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.1069 - val_loss: 8.4061\n",
      "Epoch 1223/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.9993 - val_loss: 8.3504\n",
      "Epoch 1224/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.0000 - val_loss: 13.4640\n",
      "Epoch 1225/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 15.1016 - val_loss: 9.1280\n",
      "Epoch 1226/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.2586 - val_loss: 9.7248\n",
      "Epoch 1227/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.2543 - val_loss: 13.3328\n",
      "Epoch 1228/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.7544 - val_loss: 8.0350\n",
      "Epoch 1229/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.1883 - val_loss: 7.0946\n",
      "Epoch 1230/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.6886 - val_loss: 7.5343\n",
      "Epoch 1231/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.8829 - val_loss: 23.1157\n",
      "Epoch 1232/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26.6717 - val_loss: 7.8587\n",
      "Epoch 1233/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19.0424 - val_loss: 36.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1234/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.7647 - val_loss: 13.0347\n",
      "Epoch 1235/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 17.3775 - val_loss: 7.5492\n",
      "Epoch 1236/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.9973 - val_loss: 7.0476\n",
      "Epoch 1237/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.8262 - val_loss: 6.8894\n",
      "Epoch 1238/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.8044 - val_loss: 10.1647\n",
      "Epoch 1239/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.9839 - val_loss: 11.5816\n",
      "Epoch 1240/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.1440 - val_loss: 7.9061\n",
      "Epoch 1241/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.9089 - val_loss: 5.9472\n",
      "Epoch 1242/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.4710 - val_loss: 8.1370\n",
      "Epoch 1243/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.5129 - val_loss: 8.0702\n",
      "Epoch 1244/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12.2899 - val_loss: 27.8701\n",
      "Epoch 1245/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.8440 - val_loss: 10.8256\n",
      "Epoch 1246/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.3452 - val_loss: 7.7790\n",
      "Epoch 1247/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.8540 - val_loss: 6.5742\n",
      "Epoch 1248/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.6421 - val_loss: 7.9707\n",
      "Epoch 1249/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.0751 - val_loss: 6.7456\n",
      "Epoch 1250/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 6.9584 - val_loss: 10.1180\n",
      "Epoch 1251/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 8.8327 - val_loss: 5.7811\n",
      "Epoch 1252/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.5647 - val_loss: 9.7379\n",
      "Epoch 1253/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.2706 - val_loss: 5.5902\n",
      "Epoch 1254/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.2231 - val_loss: 5.5120\n",
      "Epoch 1255/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5.9067 - val_loss: 5.7227\n",
      "Epoch 1256/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 8.5783 - val_loss: 5.4139\n",
      "Epoch 1257/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.7491 - val_loss: 6.8569\n",
      "Epoch 1258/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.3655 - val_loss: 4.8369\n",
      "Epoch 1259/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.5469 - val_loss: 7.9401\n",
      "Epoch 1260/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.1346 - val_loss: 5.4899\n",
      "Epoch 1261/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.3222 - val_loss: 15.4551\n",
      "Epoch 1262/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.5638 - val_loss: 7.1252\n",
      "Epoch 1263/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.9500 - val_loss: 4.4076\n",
      "Epoch 1264/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.3637 - val_loss: 7.5817\n",
      "Epoch 1265/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 9.3699 - val_loss: 4.9911\n",
      "Epoch 1266/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.9621 - val_loss: 5.1684\n",
      "Epoch 1267/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.1385 - val_loss: 7.5490\n",
      "Epoch 1268/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.0361 - val_loss: 4.9771\n",
      "Epoch 1269/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.3271 - val_loss: 7.7698\n",
      "Epoch 1270/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.5418 - val_loss: 5.4727\n",
      "Epoch 1271/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.6736 - val_loss: 5.4095\n",
      "Epoch 1272/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.5858 - val_loss: 8.1820\n",
      "Epoch 1273/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.1976 - val_loss: 10.2211\n",
      "Epoch 1274/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.7754 - val_loss: 5.4057\n",
      "Epoch 1275/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.6801 - val_loss: 5.7380\n",
      "Epoch 1276/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4449 - val_loss: 5.5546\n",
      "Epoch 1277/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.0832 - val_loss: 6.6776\n",
      "Epoch 1278/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.6603 - val_loss: 4.2686\n",
      "Epoch 1279/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 15.5851 - val_loss: 10.2462\n",
      "Epoch 1280/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 27.6414 - val_loss: 5.4498\n",
      "Epoch 1281/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 20.4735 - val_loss: 5.7628\n",
      "Epoch 1282/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.6359 - val_loss: 7.1439\n",
      "Epoch 1283/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.5989 - val_loss: 43.7913\n",
      "Epoch 1284/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 33.8959 - val_loss: 48.9646\n",
      "Epoch 1285/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 26.3849 - val_loss: 44.1152\n",
      "Epoch 1286/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 21.6787 - val_loss: 15.1196\n",
      "Epoch 1287/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 15.6149 - val_loss: 10.6428\n",
      "Epoch 1288/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.0139 - val_loss: 5.8207\n",
      "Epoch 1289/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.4268 - val_loss: 13.9744\n",
      "Epoch 1290/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 5.6546 - val_loss: 5.3042\n",
      "Epoch 1291/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5.3483 - val_loss: 9.2898\n",
      "Epoch 1292/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 5.5763 - val_loss: 3.5566\n",
      "Epoch 1293/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 7.6028 - val_loss: 14.5658\n",
      "Epoch 1294/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 11.6195 - val_loss: 11.2208\n",
      "Epoch 1295/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.1682 - val_loss: 9.3227\n",
      "Epoch 1296/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.7248 - val_loss: 7.6545\n",
      "Epoch 1297/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 10.8583 - val_loss: 6.2428\n",
      "Epoch 1298/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 5.5109 - val_loss: 2.8714\n",
      "Epoch 1299/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.9039 - val_loss: 7.7061\n",
      "Epoch 1300/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.3339 - val_loss: 10.8605\n",
      "Epoch 1301/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 15.7130 - val_loss: 18.5184\n",
      "Epoch 1302/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 17.8729 - val_loss: 29.1574\n",
      "Epoch 1303/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 65.1310 - val_loss: 37.3394\n",
      "Epoch 1304/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 37.9133 - val_loss: 24.2805\n",
      "Epoch 1305/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 27.8443 - val_loss: 18.8742\n",
      "Epoch 1306/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 28.9108 - val_loss: 9.6408\n",
      "Epoch 1307/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 34.5836 - val_loss: 30.9577\n",
      "Epoch 1308/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 41.1320 - val_loss: 6.7417\n",
      "Epoch 1309/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 12.3999 - val_loss: 22.3527\n",
      "Epoch 1310/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 13.8634 - val_loss: 9.0387\n",
      "Epoch 1311/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 6.7217 - val_loss: 6.4668\n",
      "Epoch 1312/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 13.0028 - val_loss: 6.3849\n",
      "Epoch 1313/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 20.8165 - val_loss: 9.3238\n",
      "Epoch 1314/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 20.7209 - val_loss: 9.7836\n",
      "Epoch 1315/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.0698 - val_loss: 2.5683\n",
      "Epoch 1316/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.9741 - val_loss: 2.8590\n",
      "Epoch 1317/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.6527 - val_loss: 2.3187\n",
      "Epoch 1318/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.5242 - val_loss: 2.1446\n",
      "Epoch 1319/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.6467 - val_loss: 3.3370\n",
      "Epoch 1320/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 7.6102 - val_loss: 3.9105\n",
      "Epoch 1321/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.6982 - val_loss: 2.3711\n",
      "Epoch 1322/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.5268 - val_loss: 3.4019\n",
      "Epoch 1323/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.4057 - val_loss: 2.9019\n",
      "Epoch 1324/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.2451 - val_loss: 2.1259\n",
      "Epoch 1325/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 3.6644 - val_loss: 2.4080\n",
      "Epoch 1326/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 6.1955 - val_loss: 4.6013\n",
      "Epoch 1327/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.3264 - val_loss: 2.3039\n",
      "Epoch 1328/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.7430 - val_loss: 2.6166\n",
      "Epoch 1329/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.6552 - val_loss: 4.4079\n",
      "Epoch 1330/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.8803 - val_loss: 3.7408\n",
      "Epoch 1331/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 2.8509 - val_loss: 6.0186\n",
      "Epoch 1332/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.1459 - val_loss: 11.1707\n",
      "Epoch 1333/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.9839 - val_loss: 5.2474\n",
      "Epoch 1334/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.0933 - val_loss: 2.3253\n",
      "Epoch 1335/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.3519 - val_loss: 3.4764\n",
      "Epoch 1336/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.0536 - val_loss: 4.1493\n",
      "Epoch 1337/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.5750 - val_loss: 7.6307\n",
      "Epoch 1338/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 10.0153 - val_loss: 8.7377\n",
      "Epoch 1339/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 15.4548 - val_loss: 2.8598\n",
      "Epoch 1340/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 20.1441 - val_loss: 10.0624\n",
      "Epoch 1341/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 37.2977 - val_loss: 33.9435\n",
      "Epoch 1342/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 31.7765 - val_loss: 34.2363\n",
      "Epoch 1343/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 24.4079 - val_loss: 88.1673\n",
      "Epoch 1344/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 37.6693 - val_loss: 7.0332\n",
      "Epoch 1345/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.9283 - val_loss: 18.0369\n",
      "Epoch 1346/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.4971 - val_loss: 4.6400\n",
      "Epoch 1347/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6704 - val_loss: 1.7821\n",
      "Epoch 1348/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.1928 - val_loss: 1.5522\n",
      "Epoch 1349/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.3679 - val_loss: 7.9027\n",
      "Epoch 1350/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 4.3586 - val_loss: 3.8647\n",
      "Epoch 1351/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.0721 - val_loss: 2.3514\n",
      "Epoch 1352/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.0444 - val_loss: 15.5089\n",
      "Epoch 1353/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.4939 - val_loss: 6.2185\n",
      "Epoch 1354/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.6860 - val_loss: 2.6416\n",
      "Epoch 1355/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.2010 - val_loss: 5.0202\n",
      "Epoch 1356/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.8606 - val_loss: 1.9252\n",
      "Epoch 1357/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.4581 - val_loss: 1.5218\n",
      "Epoch 1358/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.6814 - val_loss: 1.7456\n",
      "Epoch 1359/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.1320 - val_loss: 2.2309\n",
      "Epoch 1360/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.4375 - val_loss: 1.2374\n",
      "Epoch 1361/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.4972 - val_loss: 2.9053\n",
      "Epoch 1362/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.6234 - val_loss: 1.7663\n",
      "Epoch 1363/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.3141 - val_loss: 1.3813\n",
      "Epoch 1364/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.1358 - val_loss: 1.1206\n",
      "Epoch 1365/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.3206 - val_loss: 1.6510\n",
      "Epoch 1366/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.9083 - val_loss: 2.4333\n",
      "Epoch 1367/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.1669 - val_loss: 1.1477\n",
      "Epoch 1368/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.3108 - val_loss: 1.7327\n",
      "Epoch 1369/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.4047 - val_loss: 3.3324\n",
      "Epoch 1370/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.8335 - val_loss: 1.3386\n",
      "Epoch 1371/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.6292 - val_loss: 4.2338\n",
      "Epoch 1372/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.3937 - val_loss: 5.1958\n",
      "Epoch 1373/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.3087 - val_loss: 1.4984\n",
      "Epoch 1374/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.7185 - val_loss: 5.8717\n",
      "Epoch 1375/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.5047 - val_loss: 1.3790\n",
      "Epoch 1376/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8461 - val_loss: 1.6589\n",
      "Epoch 1377/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2392 - val_loss: 2.2853\n",
      "Epoch 1378/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7221 - val_loss: 2.9104\n",
      "Epoch 1379/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4684 - val_loss: 2.5182\n",
      "Epoch 1380/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.8723 - val_loss: 2.3191\n",
      "Epoch 1381/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8088 - val_loss: 2.5550\n",
      "Epoch 1382/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.2052 - val_loss: 1.0440\n",
      "Epoch 1383/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 1.1748 - val_loss: 0.8545\n",
      "Epoch 1384/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8099 - val_loss: 2.1709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1385/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0585 - val_loss: 1.1237\n",
      "Epoch 1386/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.4345 - val_loss: 1.4816\n",
      "Epoch 1387/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.3623 - val_loss: 3.9702\n",
      "Epoch 1388/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.3370 - val_loss: 5.3476\n",
      "Epoch 1389/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.8168 - val_loss: 2.7392\n",
      "Epoch 1390/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.2160 - val_loss: 4.3131\n",
      "Epoch 1391/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.2069 - val_loss: 3.3617\n",
      "Epoch 1392/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.2852 - val_loss: 26.3753\n",
      "Epoch 1393/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.7394 - val_loss: 3.7127\n",
      "Epoch 1394/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.1521 - val_loss: 11.3326\n",
      "Epoch 1395/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.9783 - val_loss: 0.8180\n",
      "Epoch 1396/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 5.6533 - val_loss: 26.1318\n",
      "Epoch 1397/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.8130 - val_loss: 0.9075\n",
      "Epoch 1398/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.7579 - val_loss: 9.0439\n",
      "Epoch 1399/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.6517 - val_loss: 9.5896\n",
      "Epoch 1400/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.7395 - val_loss: 4.6543\n",
      "Epoch 1401/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.4438 - val_loss: 14.2753\n",
      "Epoch 1402/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.2077 - val_loss: 2.3043\n",
      "Epoch 1403/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.2055 - val_loss: 7.2092\n",
      "Epoch 1404/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9.0047 - val_loss: 7.2358\n",
      "Epoch 1405/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 19.2492 - val_loss: 3.7576\n",
      "Epoch 1406/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 16.6626 - val_loss: 63.5446\n",
      "Epoch 1407/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 27.6106 - val_loss: 24.3737\n",
      "Epoch 1408/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.3482 - val_loss: 4.5320\n",
      "Epoch 1409/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.4047 - val_loss: 6.1166\n",
      "Epoch 1410/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.8827 - val_loss: 0.4871\n",
      "Epoch 1411/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.7248 - val_loss: 7.6959\n",
      "Epoch 1412/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 4.4601 - val_loss: 2.9146\n",
      "Epoch 1413/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.4826 - val_loss: 0.5724\n",
      "Epoch 1414/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.2168 - val_loss: 0.7238\n",
      "Epoch 1415/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1510 - val_loss: 7.8399\n",
      "Epoch 1416/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.5027 - val_loss: 1.5164\n",
      "Epoch 1417/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2779 - val_loss: 0.5201\n",
      "Epoch 1418/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2863 - val_loss: 3.1757\n",
      "Epoch 1419/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 3.582 - 0s 147us/sample - loss: 2.0268 - val_loss: 4.0835\n",
      "Epoch 1420/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.1467 - val_loss: 3.6078\n",
      "Epoch 1421/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 4.5067 - val_loss: 8.3486\n",
      "Epoch 1422/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 20.9314 - val_loss: 14.1211\n",
      "Epoch 1423/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19.4544 - val_loss: 0.9717\n",
      "Epoch 1424/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.9712 - val_loss: 2.4440\n",
      "Epoch 1425/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.9477 - val_loss: 1.9622\n",
      "Epoch 1426/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0885 - val_loss: 0.6169\n",
      "Epoch 1427/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9642 - val_loss: 2.1650\n",
      "Epoch 1428/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.8702 - val_loss: 0.7419\n",
      "Epoch 1429/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9275 - val_loss: 0.8105\n",
      "Epoch 1430/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7533 - val_loss: 0.7285\n",
      "Epoch 1431/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8880 - val_loss: 1.8012\n",
      "Epoch 1432/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8255 - val_loss: 0.6598\n",
      "Epoch 1433/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4193 - val_loss: 0.3152\n",
      "Epoch 1434/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.2041 - val_loss: 0.9789\n",
      "Epoch 1435/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2822 - val_loss: 0.3172\n",
      "Epoch 1436/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1883 - val_loss: 1.2342\n",
      "Epoch 1437/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0067 - val_loss: 0.5723\n",
      "Epoch 1438/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4104 - val_loss: 0.4299\n",
      "Epoch 1439/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5182 - val_loss: 0.3627\n",
      "Epoch 1440/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4214 - val_loss: 0.5782\n",
      "Epoch 1441/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4722 - val_loss: 0.2990\n",
      "Epoch 1442/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5824 - val_loss: 0.9332\n",
      "Epoch 1443/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.5184 - val_loss: 0.2684\n",
      "Epoch 1444/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3324 - val_loss: 0.2512\n",
      "Epoch 1445/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4637 - val_loss: 0.5746\n",
      "Epoch 1446/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3929 - val_loss: 0.7106\n",
      "Epoch 1447/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3077 - val_loss: 0.7140\n",
      "Epoch 1448/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1570 - val_loss: 0.2114\n",
      "Epoch 1449/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3503 - val_loss: 0.2703\n",
      "Epoch 1450/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3110 - val_loss: 1.6121\n",
      "Epoch 1451/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2437 - val_loss: 1.1210\n",
      "Epoch 1452/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2518 - val_loss: 1.7823\n",
      "Epoch 1453/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9940 - val_loss: 1.2146\n",
      "Epoch 1454/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3863 - val_loss: 1.5354\n",
      "Epoch 1455/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2578 - val_loss: 0.2560\n",
      "Epoch 1456/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4584 - val_loss: 0.3653\n",
      "Epoch 1457/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.4555 - val_loss: 5.3585\n",
      "Epoch 1458/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.4522 - val_loss: 4.2733\n",
      "Epoch 1459/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.7133 - val_loss: 9.0605\n",
      "Epoch 1460/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 15.7128 - val_loss: 1.6240\n",
      "Epoch 1461/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.8356 - val_loss: 0.8820\n",
      "Epoch 1462/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.5741 - val_loss: 3.8205\n",
      "Epoch 1463/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.7505 - val_loss: 17.7172\n",
      "Epoch 1464/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.0737 - val_loss: 40.7020\n",
      "Epoch 1465/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 103.7773 - val_loss: 0.9221\n",
      "Epoch 1466/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 117.7773 - val_loss: 0.9935\n",
      "Epoch 1467/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 87.0452 - val_loss: 60.0222\n",
      "Epoch 1468/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 103.9431 - val_loss: 157.5986\n",
      "Epoch 1469/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 258.1039 - val_loss: 545.8410\n",
      "Epoch 1470/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 646.8884 - val_loss: 1303.8591\n",
      "Epoch 1471/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 1584.37 - 0s 118us/sample - loss: 1034.4849 - val_loss: 424.6236\n",
      "Epoch 1472/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 645.3137 - val_loss: 441.1643\n",
      "Epoch 1473/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 775.3068 - val_loss: 961.1623\n",
      "Epoch 1474/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 755.6344 - val_loss: 151.1273\n",
      "Epoch 1475/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 551.2727 - val_loss: 809.3583\n",
      "Epoch 1476/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 550.5918 - val_loss: 539.2421\n",
      "Epoch 1477/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 439.7974 - val_loss: 542.0435\n",
      "Epoch 1478/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 227.1808 - val_loss: 45.5825\n",
      "Epoch 1479/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 52.6930 - val_loss: 10.4544\n",
      "Epoch 1480/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 37.6258 - val_loss: 39.2004\n",
      "Epoch 1481/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 29.8938 - val_loss: 8.5340\n",
      "Epoch 1482/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.2604 - val_loss: 4.1278\n",
      "Epoch 1483/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.8158 - val_loss: 0.2466\n",
      "Epoch 1484/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.5767 - val_loss: 2.7679\n",
      "Epoch 1485/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9394 - val_loss: 1.0098\n",
      "Epoch 1486/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0401 - val_loss: 1.7782\n",
      "Epoch 1487/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1279 - val_loss: 0.8253\n",
      "Epoch 1488/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8770 - val_loss: 0.3413\n",
      "Epoch 1489/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1865 - val_loss: 2.3159\n",
      "Epoch 1490/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.7827 - val_loss: 1.7442\n",
      "Epoch 1491/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.5308 - val_loss: 4.3472\n",
      "Epoch 1492/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.5623 - val_loss: 3.5196\n",
      "Epoch 1493/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.6374 - val_loss: 29.5036\n",
      "Epoch 1494/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.0385 - val_loss: 1.4600\n",
      "Epoch 1495/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.7299 - val_loss: 6.7803\n",
      "Epoch 1496/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.5832 - val_loss: 7.7156\n",
      "Epoch 1497/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.4837 - val_loss: 2.4271\n",
      "Epoch 1498/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.6923 - val_loss: 4.8143\n",
      "Epoch 1499/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.0604 - val_loss: 1.4579\n",
      "Epoch 1500/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.3635 - val_loss: 0.0705\n",
      "Epoch 1501/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.6093 - val_loss: 3.8106\n",
      "Epoch 1502/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.8401 - val_loss: 2.4926\n",
      "Epoch 1503/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.3300 - val_loss: 0.9550\n",
      "Epoch 1504/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5725 - val_loss: 0.5627\n",
      "Epoch 1505/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3800 - val_loss: 0.2302\n",
      "Epoch 1506/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.3073 - val_loss: 0.2145\n",
      "Epoch 1507/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2260 - val_loss: 0.0827\n",
      "Epoch 1508/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1015 - val_loss: 0.0954\n",
      "Epoch 1509/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0770 - val_loss: 0.0624\n",
      "Epoch 1510/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0935 - val_loss: 0.0582\n",
      "Epoch 1511/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1350 - val_loss: 0.2093\n",
      "Epoch 1512/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1244 - val_loss: 0.1064\n",
      "Epoch 1513/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1675 - val_loss: 0.2588\n",
      "Epoch 1514/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2888 - val_loss: 0.0998\n",
      "Epoch 1515/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3676 - val_loss: 1.3524\n",
      "Epoch 1516/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0429 - val_loss: 3.5687\n",
      "Epoch 1517/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.6207 - val_loss: 0.3029\n",
      "Epoch 1518/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.3081 - val_loss: 4.6130\n",
      "Epoch 1519/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.3206 - val_loss: 3.3310\n",
      "Epoch 1520/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7559 - val_loss: 2.1641\n",
      "Epoch 1521/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.0529 - val_loss: 0.3194\n",
      "Epoch 1522/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6647 - val_loss: 2.0922\n",
      "Epoch 1523/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.4583 - val_loss: 10.0630\n",
      "Epoch 1524/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.0926 - val_loss: 2.2748\n",
      "Epoch 1525/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1351 - val_loss: 1.1638\n",
      "Epoch 1526/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.4465 - val_loss: 0.1856\n",
      "Epoch 1527/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1871 - val_loss: 0.0954\n",
      "Epoch 1528/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1552 - val_loss: 0.0447\n",
      "Epoch 1529/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3198 - val_loss: 0.0212\n",
      "Epoch 1530/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3368 - val_loss: 0.1241\n",
      "Epoch 1531/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2735 - val_loss: 0.0252\n",
      "Epoch 1532/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1550 - val_loss: 0.0335\n",
      "Epoch 1533/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1364 - val_loss: 0.1195\n",
      "Epoch 1534/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1300 - val_loss: 1.4053\n",
      "Epoch 1535/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 1.3058 - val_loss: 1.2373\n",
      "Epoch 1536/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.9896 - val_loss: 5.1145\n",
      "Epoch 1537/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21.0968 - val_loss: 78.1712\n",
      "Epoch 1538/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 36.9653 - val_loss: 10.3838\n",
      "Epoch 1539/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 118.9543 - val_loss: 20.4769\n",
      "Epoch 1540/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 76.8421 - val_loss: 84.1093\n",
      "Epoch 1541/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 46.7732 - val_loss: 25.8391\n",
      "Epoch 1542/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.3321 - val_loss: 24.7708\n",
      "Epoch 1543/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 22.0412 - val_loss: 67.6564\n",
      "Epoch 1544/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 52.6840 - val_loss: 3.7031\n",
      "Epoch 1545/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 134.2063 - val_loss: 238.8263\n",
      "Epoch 1546/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 131.3633 - val_loss: 165.7900\n",
      "Epoch 1547/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 98.7807 - val_loss: 3.7902\n",
      "Epoch 1548/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 161.6833 - val_loss: 29.7344\n",
      "Epoch 1549/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 98.2628 - val_loss: 58.2878\n",
      "Epoch 1550/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 28.0111 - val_loss: 2.6825\n",
      "Epoch 1551/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 19.4945 - val_loss: 43.2124\n",
      "Epoch 1552/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 68.5248 - val_loss: 164.4293\n",
      "Epoch 1553/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 76.9647 - val_loss: 23.1415\n",
      "Epoch 1554/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 29.9962 - val_loss: 64.5662\n",
      "Epoch 1555/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 119.2436 - val_loss: 246.9485\n",
      "Epoch 1556/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 101.9791 - val_loss: 117.8026\n",
      "Epoch 1557/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 47.6491 - val_loss: 137.8666\n",
      "Epoch 1558/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 52.7808 - val_loss: 53.4494\n",
      "Epoch 1559/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 25.6572 - val_loss: 22.3373\n",
      "Epoch 1560/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 15.3826 - val_loss: 3.7870\n",
      "Epoch 1561/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.6112 - val_loss: 5.8296\n",
      "Epoch 1562/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.0852 - val_loss: 2.1431\n",
      "Epoch 1563/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7546 - val_loss: 0.9204\n",
      "Epoch 1564/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.6948 - val_loss: 0.7846\n",
      "Epoch 1565/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.7981 - val_loss: 2.0462\n",
      "Epoch 1566/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.7198 - val_loss: 2.9598\n",
      "Epoch 1567/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0606 - val_loss: 0.4188\n",
      "Epoch 1568/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7519 - val_loss: 5.8518\n",
      "Epoch 1569/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3.5397 - val_loss: 3.3717\n",
      "Epoch 1570/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1311 - val_loss: 0.4158\n",
      "Epoch 1571/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2535 - val_loss: 0.2529\n",
      "Epoch 1572/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4162 - val_loss: 0.0319\n",
      "Epoch 1573/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2018 - val_loss: 0.1308\n",
      "Epoch 1574/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0629 - val_loss: 0.0196\n",
      "Epoch 1575/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0241 - val_loss: 0.0299\n",
      "Epoch 1576/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0550 - val_loss: 0.0591\n",
      "Epoch 1577/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2582 - val_loss: 0.0588\n",
      "Epoch 1578/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1498 - val_loss: 0.2345\n",
      "Epoch 1579/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5735 - val_loss: 5.0847\n",
      "Epoch 1580/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.9909 - val_loss: 0.4000\n",
      "Epoch 1581/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.7639 - val_loss: 5.9397\n",
      "Epoch 1582/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.3869 - val_loss: 5.2267\n",
      "Epoch 1583/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15.9253 - val_loss: 9.7871\n",
      "Epoch 1584/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.7624 - val_loss: 3.6572\n",
      "Epoch 1585/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.5222 - val_loss: 0.5053\n",
      "Epoch 1586/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.7886 - val_loss: 33.8852\n",
      "Epoch 1587/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 192.2537 - val_loss: 157.2192\n",
      "Epoch 1588/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 751.7166 - val_loss: 2178.4755\n",
      "Epoch 1589/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1517.3221 - val_loss: 869.4987\n",
      "Epoch 1590/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3427.8294 - val_loss: 12688.2192\n",
      "Epoch 1591/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8145.6524 - val_loss: 3100.4057\n",
      "Epoch 1592/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 4993.3719 - val_loss: 2379.0946\n",
      "Epoch 1593/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2682.0188 - val_loss: 3339.0111\n",
      "Epoch 1594/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3422.3074 - val_loss: 4929.0313\n",
      "Epoch 1595/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3174.1938 - val_loss: 8223.0752\n",
      "Epoch 1596/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7378.1822 - val_loss: 3863.5445\n",
      "Epoch 1597/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2439.1987 - val_loss: 1302.4148\n",
      "Epoch 1598/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 715.6476 - val_loss: 2086.0313\n",
      "Epoch 1599/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1367.1378 - val_loss: 2143.7838\n",
      "Epoch 1600/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 690.5121 - val_loss: 243.9246\n",
      "Epoch 1601/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 381.6307 - val_loss: 32.0539\n",
      "Epoch 1602/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 201.1892 - val_loss: 27.2594\n",
      "Epoch 1603/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 135.7399 - val_loss: 88.2832\n",
      "Epoch 1604/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 72.3419 - val_loss: 96.3852\n",
      "Epoch 1605/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 38.9930 - val_loss: 45.9283\n",
      "Epoch 1606/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16.0152 - val_loss: 9.8691\n",
      "Epoch 1607/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 6.4268 - val_loss: 0.1641\n",
      "Epoch 1608/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7797 - val_loss: 0.5842\n",
      "Epoch 1609/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5071 - val_loss: 2.6991\n",
      "Epoch 1610/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.5622 - val_loss: 1.3986\n",
      "Epoch 1611/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.2126 - val_loss: 1.7386\n",
      "Epoch 1612/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2729 - val_loss: 1.4753\n",
      "Epoch 1613/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9910 - val_loss: 1.6227\n",
      "Epoch 1614/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8849 - val_loss: 0.1428\n",
      "Epoch 1615/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2790 - val_loss: 0.2469\n",
      "Epoch 1616/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2372 - val_loss: 0.0912\n",
      "Epoch 1617/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1082 - val_loss: 0.0415\n",
      "Epoch 1618/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0848 - val_loss: 0.0462\n",
      "Epoch 1619/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0770 - val_loss: 0.0319\n",
      "Epoch 1620/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0556 - val_loss: 0.0163\n",
      "Epoch 1621/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0455 - val_loss: 0.0187\n",
      "Epoch 1622/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0455 - val_loss: 0.0968\n",
      "Epoch 1623/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0746 - val_loss: 0.1159\n",
      "Epoch 1624/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0659 - val_loss: 0.0218\n",
      "Epoch 1625/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0401 - val_loss: 0.0155\n",
      "Epoch 1626/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.015 - 0s 147us/sample - loss: 0.0453 - val_loss: 0.0145\n",
      "Epoch 1627/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0624 - val_loss: 0.0487\n",
      "Epoch 1628/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0689 - val_loss: 0.0260\n",
      "Epoch 1629/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0450 - val_loss: 0.0140\n",
      "Epoch 1630/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0449 - val_loss: 0.0312\n",
      "Epoch 1631/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0656 - val_loss: 0.0760\n",
      "Epoch 1632/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0322 - val_loss: 0.0260\n",
      "Epoch 1633/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0245 - val_loss: 0.0328\n",
      "Epoch 1634/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0333 - val_loss: 0.0393\n",
      "Epoch 1635/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0634 - val_loss: 0.0366\n",
      "Epoch 1636/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0381 - val_loss: 0.0320\n",
      "Epoch 1637/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0545 - val_loss: 0.0306\n",
      "Epoch 1638/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0318 - val_loss: 0.0302\n",
      "Epoch 1639/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0351 - val_loss: 0.0467\n",
      "Epoch 1640/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0619 - val_loss: 0.0174\n",
      "Epoch 1641/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0697 - val_loss: 0.0780\n",
      "Epoch 1642/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1219 - val_loss: 0.2319\n",
      "Epoch 1643/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1288 - val_loss: 0.1362\n",
      "Epoch 1644/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0833 - val_loss: 0.2366\n",
      "Epoch 1645/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0910 - val_loss: 0.1575\n",
      "Epoch 1646/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1028 - val_loss: 0.0530\n",
      "Epoch 1647/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1586 - val_loss: 0.0373\n",
      "Epoch 1648/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0929 - val_loss: 0.0524\n",
      "Epoch 1649/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0425 - val_loss: 0.0493\n",
      "Epoch 1650/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0436 - val_loss: 0.0476\n",
      "Epoch 1651/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0268 - val_loss: 0.0181\n",
      "Epoch 1652/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0152 - val_loss: 0.0109\n",
      "Epoch 1653/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0172 - val_loss: 0.0105\n",
      "Epoch 1654/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0137 - val_loss: 0.0157\n",
      "Epoch 1655/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0142 - val_loss: 0.0138\n",
      "Epoch 1656/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0154 - val_loss: 0.0181\n",
      "Epoch 1657/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0350 - val_loss: 0.0734\n",
      "Epoch 1658/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0424 - val_loss: 0.0093\n",
      "Epoch 1659/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0418 - val_loss: 0.0394\n",
      "Epoch 1660/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0278 - val_loss: 0.0266\n",
      "Epoch 1661/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0124 - val_loss: 0.0173\n",
      "Epoch 1662/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0158 - val_loss: 0.0409\n",
      "Epoch 1663/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0247 - val_loss: 0.0072\n",
      "Epoch 1664/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 1665/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0132 - val_loss: 0.0156\n",
      "Epoch 1666/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0161 - val_loss: 0.0137\n",
      "Epoch 1667/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0152 - val_loss: 0.0073\n",
      "Epoch 1668/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0258 - val_loss: 0.0228\n",
      "Epoch 1669/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0136 - val_loss: 0.0302\n",
      "Epoch 1670/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0155 - val_loss: 0.0206\n",
      "Epoch 1671/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0169 - val_loss: 0.0540\n",
      "Epoch 1672/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0359 - val_loss: 0.0511\n",
      "Epoch 1673/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0295 - val_loss: 0.0330\n",
      "Epoch 1674/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0394 - val_loss: 0.0911\n",
      "Epoch 1675/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0453 - val_loss: 0.1015\n",
      "Epoch 1676/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0532 - val_loss: 0.0171\n",
      "Epoch 1677/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0131 - val_loss: 0.0100\n",
      "Epoch 1678/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0199 - val_loss: 0.0234\n",
      "Epoch 1679/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0361 - val_loss: 0.0837\n",
      "Epoch 1680/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0843 - val_loss: 0.0515\n",
      "Epoch 1681/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0552 - val_loss: 0.0078\n",
      "Epoch 1682/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0348 - val_loss: 0.0375\n",
      "Epoch 1683/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0235 - val_loss: 0.0047\n",
      "Epoch 1684/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0169 - val_loss: 0.0192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1685/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0191 - val_loss: 0.0067\n",
      "Epoch 1686/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0096 - val_loss: 0.0165\n",
      "Epoch 1687/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0077 - val_loss: 0.0190\n",
      "Epoch 1688/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0214 - val_loss: 0.0097\n",
      "Epoch 1689/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0513 - val_loss: 0.1595\n",
      "Epoch 1690/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1718 - val_loss: 0.0976\n",
      "Epoch 1691/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1790 - val_loss: 0.0750\n",
      "Epoch 1692/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0364 - val_loss: 0.0130\n",
      "Epoch 1693/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0365 - val_loss: 0.0143\n",
      "Epoch 1694/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0135 - val_loss: 0.0086\n",
      "Epoch 1695/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0212 - val_loss: 0.0189\n",
      "Epoch 1696/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0180 - val_loss: 0.0096\n",
      "Epoch 1697/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0077 - val_loss: 0.0044\n",
      "Epoch 1698/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0084 - val_loss: 0.0054\n",
      "Epoch 1699/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0692 - val_loss: 0.0619\n",
      "Epoch 1700/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1170 - val_loss: 0.0815\n",
      "Epoch 1701/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1249 - val_loss: 0.1098\n",
      "Epoch 1702/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2188 - val_loss: 0.3610\n",
      "Epoch 1703/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3111 - val_loss: 0.0678\n",
      "Epoch 1704/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0455 - val_loss: 0.0150\n",
      "Epoch 1705/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0107 - val_loss: 0.0087\n",
      "Epoch 1706/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0116 - val_loss: 0.0389\n",
      "Epoch 1707/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0340 - val_loss: 0.0494\n",
      "Epoch 1708/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0811 - val_loss: 0.2930\n",
      "Epoch 1709/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2468 - val_loss: 0.4504\n",
      "Epoch 1710/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2422 - val_loss: 0.1499\n",
      "Epoch 1711/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4728 - val_loss: 1.4344\n",
      "Epoch 1712/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4352 - val_loss: 0.4092\n",
      "Epoch 1713/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2720 - val_loss: 3.0061\n",
      "Epoch 1714/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.9943 - val_loss: 3.6195\n",
      "Epoch 1715/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.6766 - val_loss: 0.3483\n",
      "Epoch 1716/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5844 - val_loss: 2.3433\n",
      "Epoch 1717/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2977 - val_loss: 3.0081\n",
      "Epoch 1718/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4626 - val_loss: 0.0247\n",
      "Epoch 1719/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3221 - val_loss: 0.2615\n",
      "Epoch 1720/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1877 - val_loss: 0.0655\n",
      "Epoch 1721/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0887 - val_loss: 0.0879\n",
      "Epoch 1722/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0777 - val_loss: 0.0234\n",
      "Epoch 1723/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0957 - val_loss: 0.0302\n",
      "Epoch 1724/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0410 - val_loss: 0.2614\n",
      "Epoch 1725/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1540 - val_loss: 0.0436\n",
      "Epoch 1726/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0286 - val_loss: 0.0178\n",
      "Epoch 1727/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0316 - val_loss: 0.0542\n",
      "Epoch 1728/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0282 - val_loss: 0.0112\n",
      "Epoch 1729/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0088 - val_loss: 0.0061\n",
      "Epoch 1730/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0074 - val_loss: 0.0015\n",
      "Epoch 1731/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0043 - val_loss: 0.0041\n",
      "Epoch 1732/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0027 - val_loss: 0.0057\n",
      "Epoch 1733/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0032 - val_loss: 0.0025\n",
      "Epoch 1734/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 1735/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0064 - val_loss: 0.0021\n",
      "Epoch 1736/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 1737/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0117 - val_loss: 0.0061\n",
      "Epoch 1738/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0063 - val_loss: 0.0055\n",
      "Epoch 1739/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0075 - val_loss: 0.0140\n",
      "Epoch 1740/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0547 - val_loss: 0.0132\n",
      "Epoch 1741/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0284 - val_loss: 0.0122\n",
      "Epoch 1742/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0040 - val_loss: 0.0015\n",
      "Epoch 1743/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0085 - val_loss: 0.0138\n",
      "Epoch 1744/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0137 - val_loss: 0.0247\n",
      "Epoch 1745/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0234 - val_loss: 0.0130\n",
      "Epoch 1746/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0165 - val_loss: 0.0164\n",
      "Epoch 1747/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0090 - val_loss: 0.0113\n",
      "Epoch 1748/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0067 - val_loss: 0.0020\n",
      "Epoch 1749/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0069 - val_loss: 0.0013\n",
      "Epoch 1750/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0052 - val_loss: 0.0027\n",
      "Epoch 1751/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0071 - val_loss: 0.0047\n",
      "Epoch 1752/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0095 - val_loss: 0.0061\n",
      "Epoch 1753/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0128 - val_loss: 0.0100\n",
      "Epoch 1754/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0093 - val_loss: 0.0051\n",
      "Epoch 1755/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.002 - 0s 132us/sample - loss: 0.0027 - val_loss: 0.0054\n",
      "Epoch 1756/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0038 - val_loss: 0.0050\n",
      "Epoch 1757/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0051 - val_loss: 0.0107\n",
      "Epoch 1758/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0045 - val_loss: 0.0019\n",
      "Epoch 1759/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 1760/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0099 - val_loss: 0.0159\n",
      "Epoch 1761/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0103 - val_loss: 0.0619\n",
      "Epoch 1762/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0449 - val_loss: 0.0505\n",
      "Epoch 1763/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0292 - val_loss: 0.0438\n",
      "Epoch 1764/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0311 - val_loss: 0.1272\n",
      "Epoch 1765/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0505 - val_loss: 0.0697\n",
      "Epoch 1766/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0449 - val_loss: 0.0939\n",
      "Epoch 1767/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4650 - val_loss: 0.8389\n",
      "Epoch 1768/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.5152 - val_loss: 1.8233\n",
      "Epoch 1769/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 8.6413 - val_loss: 10.4334\n",
      "Epoch 1770/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.7524 - val_loss: 28.7835\n",
      "Epoch 1771/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 42.2905 - val_loss: 23.8798\n",
      "Epoch 1772/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 67.5201 - val_loss: 161.3747\n",
      "Epoch 1773/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 67.3041 - val_loss: 253.0407\n",
      "Epoch 1774/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 121.8352 - val_loss: 174.2630\n",
      "Epoch 1775/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 756.9258 - val_loss: 247.0834\n",
      "Epoch 1776/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1008.7983 - val_loss: 53.8547\n",
      "Epoch 1777/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 570.9649 - val_loss: 25.4193\n",
      "Epoch 1778/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 143.7629 - val_loss: 185.8208\n",
      "Epoch 1779/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 189.9188 - val_loss: 184.4415\n",
      "Epoch 1780/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 475.7723 - val_loss: 869.3843\n",
      "Epoch 1781/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 581.7845 - val_loss: 214.1572\n",
      "Epoch 1782/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 109.1975 - val_loss: 10.0325\n",
      "Epoch 1783/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 46.1224 - val_loss: 0.1753\n",
      "Epoch 1784/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 58.5460 - val_loss: 182.8649\n",
      "Epoch 1785/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 369.3273 - val_loss: 629.9933\n",
      "Epoch 1786/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 839.3540 - val_loss: 284.0841\n",
      "Epoch 1787/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 456.6470 - val_loss: 113.6825\n",
      "Epoch 1788/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 86.6853 - val_loss: 20.9430\n",
      "Epoch 1789/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 58.0585 - val_loss: 79.6317\n",
      "Epoch 1790/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 88.8563 - val_loss: 70.6814\n",
      "Epoch 1791/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 108.2069 - val_loss: 133.5151\n",
      "Epoch 1792/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 114.6491 - val_loss: 337.8661\n",
      "Epoch 1793/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 666.4561 - val_loss: 153.3019\n",
      "Epoch 1794/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 749.0042 - val_loss: 593.7804\n",
      "Epoch 1795/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 622.1388 - val_loss: 899.1926\n",
      "Epoch 1796/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 450.2670 - val_loss: 361.8884\n",
      "Epoch 1797/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 127.5279 - val_loss: 71.2076\n",
      "Epoch 1798/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 28.9429 - val_loss: 7.6702\n",
      "Epoch 1799/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.9340 - val_loss: 3.6536\n",
      "Epoch 1800/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 25.4783 - val_loss: 66.8766\n",
      "Epoch 1801/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 26.8640 - val_loss: 20.3701\n",
      "Epoch 1802/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.3046 - val_loss: 6.4808\n",
      "Epoch 1803/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15.7111 - val_loss: 0.1064\n",
      "Epoch 1804/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.3750 - val_loss: 1.6604\n",
      "Epoch 1805/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2717 - val_loss: 0.7478\n",
      "Epoch 1806/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7774 - val_loss: 0.4200\n",
      "Epoch 1807/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2683 - val_loss: 0.3266\n",
      "Epoch 1808/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2729 - val_loss: 0.5954\n",
      "Epoch 1809/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4847 - val_loss: 0.3080\n",
      "Epoch 1810/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8801 - val_loss: 0.2600\n",
      "Epoch 1811/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9644 - val_loss: 1.6941\n",
      "Epoch 1812/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3283 - val_loss: 2.6179\n",
      "Epoch 1813/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.5569 - val_loss: 1.4648\n",
      "Epoch 1814/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.9598 - val_loss: 7.9547\n",
      "Epoch 1815/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.0130 - val_loss: 6.4337\n",
      "Epoch 1816/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.4538 - val_loss: 9.1004\n",
      "Epoch 1817/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.9041 - val_loss: 10.4860\n",
      "Epoch 1818/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.6321 - val_loss: 6.1271\n",
      "Epoch 1819/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.5497 - val_loss: 7.1840\n",
      "Epoch 1820/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.4369 - val_loss: 7.0555\n",
      "Epoch 1821/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 62.2172 - val_loss: 67.4049\n",
      "Epoch 1822/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 58.2261 - val_loss: 8.3898\n",
      "Epoch 1823/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.9723 - val_loss: 13.8104\n",
      "Epoch 1824/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.3289 - val_loss: 4.2732\n",
      "Epoch 1825/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8686 - val_loss: 3.3331\n",
      "Epoch 1826/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7438 - val_loss: 1.2103\n",
      "Epoch 1827/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8186 - val_loss: 1.2313\n",
      "Epoch 1828/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.8314 - val_loss: 1.0508\n",
      "Epoch 1829/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1956 - val_loss: 1.4338\n",
      "Epoch 1830/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.4675 - val_loss: 7.6704\n",
      "Epoch 1831/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.1887 - val_loss: 6.3070\n",
      "Epoch 1832/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.0774 - val_loss: 9.7140\n",
      "Epoch 1833/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.4079 - val_loss: 7.7342\n",
      "Epoch 1834/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8783 - val_loss: 3.7770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1835/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0798 - val_loss: 0.8322\n",
      "Epoch 1836/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.2909 - val_loss: 0.7200\n",
      "Epoch 1837/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6480 - val_loss: 1.0459\n",
      "Epoch 1838/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0199 - val_loss: 2.9893\n",
      "Epoch 1839/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.4926 - val_loss: 0.4643\n",
      "Epoch 1840/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0067 - val_loss: 0.6378\n",
      "Epoch 1841/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6281 - val_loss: 1.8766\n",
      "Epoch 1842/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9853 - val_loss: 0.0802\n",
      "Epoch 1843/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5119 - val_loss: 0.9637\n",
      "Epoch 1844/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.4327 - val_loss: 0.5215\n",
      "Epoch 1845/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3172 - val_loss: 0.4526\n",
      "Epoch 1846/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1414 - val_loss: 0.0050\n",
      "Epoch 1847/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0411 - val_loss: 0.0313\n",
      "Epoch 1848/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1470 - val_loss: 0.0400\n",
      "Epoch 1849/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0642 - val_loss: 0.0385\n",
      "Epoch 1850/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0750 - val_loss: 0.1460\n",
      "Epoch 1851/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0417 - val_loss: 0.0051\n",
      "Epoch 1852/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0028 - val_loss: 0.0107\n",
      "Epoch 1853/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0058 - val_loss: 0.0039\n",
      "Epoch 1854/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0157 - val_loss: 0.0348\n",
      "Epoch 1855/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0400 - val_loss: 0.1183\n",
      "Epoch 1856/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0656 - val_loss: 0.0470\n",
      "Epoch 1857/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0579 - val_loss: 0.0802\n",
      "Epoch 1858/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0840 - val_loss: 0.0114\n",
      "Epoch 1859/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0990 - val_loss: 0.0994\n",
      "Epoch 1860/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0604 - val_loss: 0.0127\n",
      "Epoch 1861/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0776 - val_loss: 0.0777\n",
      "Epoch 1862/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0720 - val_loss: 0.1982\n",
      "Epoch 1863/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1903 - val_loss: 0.9899\n",
      "Epoch 1864/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6904 - val_loss: 1.7472\n",
      "Epoch 1865/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.4941 - val_loss: 1.0878\n",
      "Epoch 1866/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.1643 - val_loss: 1.5441\n",
      "Epoch 1867/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.2950 - val_loss: 1.5775\n",
      "Epoch 1868/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 27.4091 - val_loss: 95.7169\n",
      "Epoch 1869/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 89.4200 - val_loss: 328.8829\n",
      "Epoch 1870/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 605.1712 - val_loss: 236.0861\n",
      "Epoch 1871/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 948.1913 - val_loss: 2685.2799\n",
      "Epoch 1872/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1398.0538 - val_loss: 3106.4586\n",
      "Epoch 1873/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2254.6598 - val_loss: 3848.8030\n",
      "Epoch 1874/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1411.1723 - val_loss: 474.5164\n",
      "Epoch 1875/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 281.4397 - val_loss: 197.5348\n",
      "Epoch 1876/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 378.1806 - val_loss: 321.6493\n",
      "Epoch 1877/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 876.9384 - val_loss: 1289.4357\n",
      "Epoch 1878/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2425.5944 - val_loss: 10366.0973\n",
      "Epoch 1879/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 5744.3512 - val_loss: 3437.7179\n",
      "Epoch 1880/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1556.7905 - val_loss: 648.7000\n",
      "Epoch 1881/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1991.8350 - val_loss: 1047.4693\n",
      "Epoch 1882/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 735.6454 - val_loss: 659.0271\n",
      "Epoch 1883/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 419.9436 - val_loss: 883.5536\n",
      "Epoch 1884/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 562.3668 - val_loss: 383.5336\n",
      "Epoch 1885/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 134.5337 - val_loss: 25.1208\n",
      "Epoch 1886/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 26.2647 - val_loss: 32.1660\n",
      "Epoch 1887/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.6637 - val_loss: 12.4225\n",
      "Epoch 1888/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.0774 - val_loss: 12.7176\n",
      "Epoch 1889/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.9686 - val_loss: 8.9212\n",
      "Epoch 1890/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.1987 - val_loss: 5.0380\n",
      "Epoch 1891/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.2669 - val_loss: 4.4295\n",
      "Epoch 1892/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.5189 - val_loss: 1.5083\n",
      "Epoch 1893/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.2077 - val_loss: 2.9855\n",
      "Epoch 1894/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8671 - val_loss: 0.4556\n",
      "Epoch 1895/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3461 - val_loss: 0.3102\n",
      "Epoch 1896/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2764 - val_loss: 0.8906\n",
      "Epoch 1897/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6495 - val_loss: 0.0960\n",
      "Epoch 1898/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2775 - val_loss: 0.2431\n",
      "Epoch 1899/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2513 - val_loss: 0.0661\n",
      "Epoch 1900/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1025 - val_loss: 0.0198\n",
      "Epoch 1901/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0555 - val_loss: 0.0605\n",
      "Epoch 1902/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0519 - val_loss: 0.1117\n",
      "Epoch 1903/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1496 - val_loss: 0.0433\n",
      "Epoch 1904/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0524 - val_loss: 0.0169\n",
      "Epoch 1905/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0324 - val_loss: 0.0182\n",
      "Epoch 1906/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0244 - val_loss: 0.0044\n",
      "Epoch 1907/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0159 - val_loss: 0.0054\n",
      "Epoch 1908/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0124 - val_loss: 0.0012\n",
      "Epoch 1909/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0098 - val_loss: 0.0014\n",
      "Epoch 1910/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0087 - val_loss: 0.0029\n",
      "Epoch 1911/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0079 - val_loss: 0.0021\n",
      "Epoch 1912/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0134 - val_loss: 0.0217\n",
      "Epoch 1913/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0175 - val_loss: 0.0041\n",
      "Epoch 1914/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0076 - val_loss: 0.0019\n",
      "Epoch 1915/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 1916/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 1917/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0120 - val_loss: 0.0068\n",
      "Epoch 1918/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0109 - val_loss: 0.0134\n",
      "Epoch 1919/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0085 - val_loss: 0.0030\n",
      "Epoch 1920/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0035 - val_loss: 9.1240e-04\n",
      "Epoch 1921/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0019 - val_loss: 7.4079e-04\n",
      "Epoch 1922/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 1923/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0022 - val_loss: 0.0011\n",
      "Epoch 1924/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0017 - val_loss: 8.4309e-04\n",
      "Epoch 1925/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0017 - val_loss: 7.1880e-04\n",
      "Epoch 1926/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 0.0045\n",
      "Epoch 1927/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0046 - val_loss: 0.0025\n",
      "Epoch 1928/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0017 - val_loss: 9.4979e-04\n",
      "Epoch 1929/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0023 - val_loss: 0.0046\n",
      "Epoch 1930/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0033 - val_loss: 0.0013\n",
      "Epoch 1931/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0048 - val_loss: 0.0050\n",
      "Epoch 1932/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0047 - val_loss: 0.0066\n",
      "Epoch 1933/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0056 - val_loss: 0.0042\n",
      "Epoch 1934/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0027 - val_loss: 8.0226e-04\n",
      "Epoch 1935/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0011 - val_loss: 9.8531e-04\n",
      "Epoch 1936/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0017 - val_loss: 0.0048\n",
      "Epoch 1937/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0087 - val_loss: 0.0013\n",
      "Epoch 1938/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0021 - val_loss: 0.0018\n",
      "Epoch 1939/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 1940/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0012 - val_loss: 0.0020\n",
      "Epoch 1941/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 1942/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0014 - val_loss: 7.6736e-04\n",
      "Epoch 1943/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 7.1998e-04 - val_loss: 6.5987e-04\n",
      "Epoch 1944/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 0.0024\n",
      "Epoch 1945/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0015 - val_loss: 8.1769e-04\n",
      "Epoch 1946/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0017 - val_loss: 0.0050\n",
      "Epoch 1947/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0047 - val_loss: 0.0033\n",
      "Epoch 1948/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0198 - val_loss: 0.0806\n",
      "Epoch 1949/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0240 - val_loss: 0.0013\n",
      "Epoch 1950/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0058 - val_loss: 0.0066\n",
      "Epoch 1951/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0042 - val_loss: 0.0088\n",
      "Epoch 1952/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0055 - val_loss: 0.0075\n",
      "Epoch 1953/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0052 - val_loss: 0.0040\n",
      "Epoch 1954/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0042 - val_loss: 0.0063\n",
      "Epoch 1955/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0105 - val_loss: 0.0075\n",
      "Epoch 1956/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0186 - val_loss: 0.0093\n",
      "Epoch 1957/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0153 - val_loss: 0.0114\n",
      "Epoch 1958/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0090 - val_loss: 0.0055\n",
      "Epoch 1959/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0058 - val_loss: 0.0064\n",
      "Epoch 1960/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0047 - val_loss: 0.0046\n",
      "Epoch 1961/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0056 - val_loss: 0.0088\n",
      "Epoch 1962/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0084 - val_loss: 0.0210\n",
      "Epoch 1963/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0436 - val_loss: 0.1837\n",
      "Epoch 1964/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1295 - val_loss: 0.1826\n",
      "Epoch 1965/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0796 - val_loss: 0.1022\n",
      "Epoch 1966/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1165 - val_loss: 0.1535\n",
      "Epoch 1967/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1463 - val_loss: 0.2747\n",
      "Epoch 1968/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1665 - val_loss: 0.2876\n",
      "Epoch 1969/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1518 - val_loss: 0.0365\n",
      "Epoch 1970/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0220 - val_loss: 0.0225\n",
      "Epoch 1971/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0099 - val_loss: 0.0048\n",
      "Epoch 1972/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0190 - val_loss: 0.0141\n",
      "Epoch 1973/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0386 - val_loss: 0.0330\n",
      "Epoch 1974/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0838 - val_loss: 0.1625\n",
      "Epoch 1975/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0581 - val_loss: 0.0462\n",
      "Epoch 1976/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0321 - val_loss: 0.0278\n",
      "Epoch 1977/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0243 - val_loss: 0.0244\n",
      "Epoch 1978/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0859 - val_loss: 0.1537\n",
      "Epoch 1979/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2686 - val_loss: 0.0285\n",
      "Epoch 1980/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0500 - val_loss: 0.0457\n",
      "Epoch 1981/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1478 - val_loss: 0.1999\n",
      "Epoch 1982/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1400 - val_loss: 0.0448\n",
      "Epoch 1983/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.3087 - val_loss: 0.6035\n",
      "Epoch 1984/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3407 - val_loss: 0.6465\n",
      "Epoch 1985/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3541 - val_loss: 0.7254\n",
      "Epoch 1986/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3213 - val_loss: 0.3160\n",
      "Epoch 1987/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4649 - val_loss: 0.5283\n",
      "Epoch 1988/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.1778 - val_loss: 2.1021\n",
      "Epoch 1989/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.8849 - val_loss: 5.4058\n",
      "Epoch 1990/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.4528 - val_loss: 3.7040\n",
      "Epoch 1991/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.6021 - val_loss: 6.7492\n",
      "Epoch 1992/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.0141 - val_loss: 0.4797\n",
      "Epoch 1993/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2647 - val_loss: 0.3515\n",
      "Epoch 1994/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2592 - val_loss: 0.1510\n",
      "Epoch 1995/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1700 - val_loss: 0.5236\n",
      "Epoch 1996/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1805 - val_loss: 0.0978\n",
      "Epoch 1997/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0463 - val_loss: 0.0828\n",
      "Epoch 1998/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0748 - val_loss: 0.2086\n",
      "Epoch 1999/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1507 - val_loss: 0.0259\n",
      "Epoch 2000/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0764 - val_loss: 0.0157\n",
      "Epoch 2001/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0245 - val_loss: 0.0074\n",
      "Epoch 2002/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0111 - val_loss: 0.0026\n",
      "Epoch 2003/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0161 - val_loss: 0.0408\n",
      "Epoch 2004/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0245 - val_loss: 0.0336\n",
      "Epoch 2005/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0203 - val_loss: 0.0092\n",
      "Epoch 2006/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0320 - val_loss: 0.0418\n",
      "Epoch 2007/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0219 - val_loss: 0.0196\n",
      "Epoch 2008/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0202 - val_loss: 0.0274\n",
      "Epoch 2009/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0187 - val_loss: 0.0255\n",
      "Epoch 2010/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0201 - val_loss: 0.0120\n",
      "Epoch 2011/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0063 - val_loss: 0.0011\n",
      "Epoch 2012/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.2685e-04 - val_loss: 0.0017\n",
      "Epoch 2013/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0028 - val_loss: 0.0079\n",
      "Epoch 2014/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0048 - val_loss: 6.8672e-04\n",
      "Epoch 2015/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0053 - val_loss: 0.0043\n",
      "Epoch 2016/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0102 - val_loss: 0.0027\n",
      "Epoch 2017/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0104 - val_loss: 0.0210\n",
      "Epoch 2018/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0231 - val_loss: 0.2111\n",
      "Epoch 2019/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2208 - val_loss: 0.8274\n",
      "Epoch 2020/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5406 - val_loss: 3.9327\n",
      "Epoch 2021/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.3564 - val_loss: 14.3986\n",
      "Epoch 2022/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 34.6181 - val_loss: 69.9248\n",
      "Epoch 2023/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 23.6911 - val_loss: 10.1696\n",
      "Epoch 2024/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.3615 - val_loss: 3.2409\n",
      "Epoch 2025/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.3820 - val_loss: 16.6776\n",
      "Epoch 2026/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 20.7684 - val_loss: 6.1493\n",
      "Epoch 2027/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.2702 - val_loss: 2.0824\n",
      "Epoch 2028/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.5165 - val_loss: 3.1718\n",
      "Epoch 2029/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.1193 - val_loss: 15.3114\n",
      "Epoch 2030/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.8665 - val_loss: 0.0605\n",
      "Epoch 2031/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.7534 - val_loss: 8.3053\n",
      "Epoch 2032/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.2871 - val_loss: 7.5371\n",
      "Epoch 2033/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 11.3608 - val_loss: 64.9221\n",
      "Epoch 2034/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 25.7713 - val_loss: 28.0092\n",
      "Epoch 2035/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 33.6667 - val_loss: 16.3149\n",
      "Epoch 2036/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 32.8694 - val_loss: 38.6679\n",
      "Epoch 2037/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 100.0398 - val_loss: 17.0493\n",
      "Epoch 2038/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.2418 - val_loss: 18.1304\n",
      "Epoch 2039/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 11.2439 - val_loss: 18.6794\n",
      "Epoch 2040/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.8730 - val_loss: 3.7694\n",
      "Epoch 2041/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.7860 - val_loss: 8.4336\n",
      "Epoch 2042/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 27.5441 - val_loss: 51.5224\n",
      "Epoch 2043/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 47.8022 - val_loss: 54.7374\n",
      "Epoch 2044/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 63.7695 - val_loss: 46.2967\n",
      "Epoch 2045/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 63.1547 - val_loss: 183.7555\n",
      "Epoch 2046/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 102.6701 - val_loss: 150.7573\n",
      "Epoch 2047/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 64.4066 - val_loss: 110.9809\n",
      "Epoch 2048/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 57.3145 - val_loss: 28.3641\n",
      "Epoch 2049/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 35.0656 - val_loss: 65.8761\n",
      "Epoch 2050/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 138.8921 - val_loss: 263.6720\n",
      "Epoch 2051/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 144.9217 - val_loss: 606.7671\n",
      "Epoch 2052/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 863.2815 - val_loss: 62.9120\n",
      "Epoch 2053/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2079.2816 - val_loss: 3188.5760\n",
      "Epoch 2054/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6891.1319 - val_loss: 23585.6301\n",
      "Epoch 2055/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12085.6251 - val_loss: 979.5906\n",
      "Epoch 2056/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5063.6856 - val_loss: 1469.8713\n",
      "Epoch 2057/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3477.2108 - val_loss: 4982.7718\n",
      "Epoch 2058/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2739.3985 - val_loss: 2468.5656\n",
      "Epoch 2059/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1488.3701 - val_loss: 360.0897\n",
      "Epoch 2060/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 995.7703 - val_loss: 201.2387\n",
      "Epoch 2061/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 598.7296 - val_loss: 129.2474\n",
      "Epoch 2062/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 503.2731 - val_loss: 442.4807\n",
      "Epoch 2063/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 399.2853 - val_loss: 225.8377\n",
      "Epoch 2064/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 377.2442 - val_loss: 277.7602\n",
      "Epoch 2065/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 619.5828 - val_loss: 865.8204\n",
      "Epoch 2066/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 360.6551 - val_loss: 215.6664\n",
      "Epoch 2067/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 200.4456 - val_loss: 52.2561\n",
      "Epoch 2068/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 131.1605 - val_loss: 59.7384\n",
      "Epoch 2069/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 88.9639 - val_loss: 37.8624\n",
      "Epoch 2070/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 50.8574 - val_loss: 7.5201\n",
      "Epoch 2071/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 31.8606 - val_loss: 7.0171\n",
      "Epoch 2072/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 20.1506 - val_loss: 0.6418\n",
      "Epoch 2073/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.7551 - val_loss: 11.2146\n",
      "Epoch 2074/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 25.6841 - val_loss: 36.2891\n",
      "Epoch 2075/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 29.7060 - val_loss: 30.2435\n",
      "Epoch 2076/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 17.0689 - val_loss: 42.8423\n",
      "Epoch 2077/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 29.3338 - val_loss: 39.7899\n",
      "Epoch 2078/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 17.2586 - val_loss: 14.6207\n",
      "Epoch 2079/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.2400 - val_loss: 4.0720\n",
      "Epoch 2080/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.1623 - val_loss: 2.4838\n",
      "Epoch 2081/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7071 - val_loss: 1.2258\n",
      "Epoch 2082/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3174 - val_loss: 1.6562\n",
      "Epoch 2083/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3434 - val_loss: 1.0875\n",
      "Epoch 2084/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7447 - val_loss: 0.6403\n",
      "Epoch 2085/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5560 - val_loss: 0.2108\n",
      "Epoch 2086/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4013 - val_loss: 0.1882\n",
      "Epoch 2087/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.218 - 0s 147us/sample - loss: 0.7102 - val_loss: 0.1598\n",
      "Epoch 2088/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4149 - val_loss: 0.1180\n",
      "Epoch 2089/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2310 - val_loss: 0.0574\n",
      "Epoch 2090/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1611 - val_loss: 0.0635\n",
      "Epoch 2091/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1653 - val_loss: 0.3031\n",
      "Epoch 2092/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2033 - val_loss: 0.2898\n",
      "Epoch 2093/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1541 - val_loss: 0.1095\n",
      "Epoch 2094/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1073 - val_loss: 0.0677\n",
      "Epoch 2095/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0648 - val_loss: 0.0568\n",
      "Epoch 2096/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0570 - val_loss: 0.0486\n",
      "Epoch 2097/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0615 - val_loss: 0.1309\n",
      "Epoch 2098/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0971 - val_loss: 0.0774\n",
      "Epoch 2099/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0810 - val_loss: 0.0965\n",
      "Epoch 2100/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0742 - val_loss: 0.0477\n",
      "Epoch 2101/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0497 - val_loss: 0.0493\n",
      "Epoch 2102/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0919 - val_loss: 0.0438\n",
      "Epoch 2103/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1564 - val_loss: 0.3665\n",
      "Epoch 2104/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2586 - val_loss: 0.0955\n",
      "Epoch 2105/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2334 - val_loss: 0.5596\n",
      "Epoch 2106/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5011 - val_loss: 0.0694\n",
      "Epoch 2107/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3724 - val_loss: 0.0700\n",
      "Epoch 2108/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3648 - val_loss: 0.3965\n",
      "Epoch 2109/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2564 - val_loss: 0.4106\n",
      "Epoch 2110/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3714 - val_loss: 0.3830\n",
      "Epoch 2111/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1628 - val_loss: 0.2758\n",
      "Epoch 2112/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0766 - val_loss: 0.0800\n",
      "Epoch 2113/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0522 - val_loss: 0.0580\n",
      "Epoch 2114/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0469 - val_loss: 0.0408\n",
      "Epoch 2115/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0460 - val_loss: 0.0492\n",
      "Epoch 2116/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0813 - val_loss: 0.0414\n",
      "Epoch 2117/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0817 - val_loss: 0.0565\n",
      "Epoch 2118/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0804 - val_loss: 0.0404\n",
      "Epoch 2119/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0389 - val_loss: 0.0369\n",
      "Epoch 2120/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0320 - val_loss: 0.0337\n",
      "Epoch 2121/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0494 - val_loss: 0.0476\n",
      "Epoch 2122/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0509 - val_loss: 0.0430\n",
      "Epoch 2123/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0476 - val_loss: 0.0356\n",
      "Epoch 2124/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0491 - val_loss: 0.0314\n",
      "Epoch 2125/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0303 - val_loss: 0.0390\n",
      "Epoch 2126/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0368 - val_loss: 0.0353\n",
      "Epoch 2127/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0343 - val_loss: 0.0297\n",
      "Epoch 2128/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0731 - val_loss: 0.1537\n",
      "Epoch 2129/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1347 - val_loss: 0.0797\n",
      "Epoch 2130/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0933 - val_loss: 0.0693\n",
      "Epoch 2131/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0636 - val_loss: 0.0593\n",
      "Epoch 2132/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0528 - val_loss: 0.0485\n",
      "Epoch 2133/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0647 - val_loss: 0.0402\n",
      "Epoch 2134/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0826 - val_loss: 0.0704\n",
      "Epoch 2135/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2078 - val_loss: 0.5655\n",
      "Epoch 2136/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2673 - val_loss: 0.0678\n",
      "Epoch 2137/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0606 - val_loss: 0.0654\n",
      "Epoch 2138/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0405 - val_loss: 0.0438\n",
      "Epoch 2139/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0396 - val_loss: 0.0362\n",
      "Epoch 2140/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0344 - val_loss: 0.0227\n",
      "Epoch 2141/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0595 - val_loss: 0.1345\n",
      "Epoch 2142/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0750 - val_loss: 0.0238\n",
      "Epoch 2143/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0214 - val_loss: 0.0260\n",
      "Epoch 2144/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0346 - val_loss: 0.0219\n",
      "Epoch 2145/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0374 - val_loss: 0.0327\n",
      "Epoch 2146/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0312 - val_loss: 0.0333\n",
      "Epoch 2147/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0402 - val_loss: 0.0205\n",
      "Epoch 2148/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0346 - val_loss: 0.0270\n",
      "Epoch 2149/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0376 - val_loss: 0.0834\n",
      "Epoch 2150/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0393 - val_loss: 0.0274\n",
      "Epoch 2151/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0209 - val_loss: 0.0288\n",
      "Epoch 2152/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0453 - val_loss: 0.0193\n",
      "Epoch 2153/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.005 - 0s 147us/sample - loss: 0.0188 - val_loss: 0.0292\n",
      "Epoch 2154/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0242 - val_loss: 0.0173\n",
      "Epoch 2155/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0247 - val_loss: 0.0218\n",
      "Epoch 2156/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0184 - val_loss: 0.0184\n",
      "Epoch 2157/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0180 - val_loss: 0.0157\n",
      "Epoch 2158/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0268 - val_loss: 0.0817\n",
      "Epoch 2159/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0966 - val_loss: 0.0769\n",
      "Epoch 2160/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0445 - val_loss: 0.0141\n",
      "Epoch 2161/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0165 - val_loss: 0.0151\n",
      "Epoch 2162/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0224 - val_loss: 0.0195\n",
      "Epoch 2163/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0340 - val_loss: 0.0453\n",
      "Epoch 2164/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0670 - val_loss: 0.0301\n",
      "Epoch 2165/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0184 - val_loss: 0.0156\n",
      "Epoch 2166/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0498 - val_loss: 0.0308\n",
      "Epoch 2167/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0228 - val_loss: 0.0575\n",
      "Epoch 2168/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0256 - val_loss: 0.0306\n",
      "Epoch 2169/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0275 - val_loss: 0.0132\n",
      "Epoch 2170/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0441 - val_loss: 0.0264\n",
      "Epoch 2171/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0579 - val_loss: 0.0739\n",
      "Epoch 2172/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0320 - val_loss: 0.0820\n",
      "Epoch 2173/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0608 - val_loss: 0.0138\n",
      "Epoch 2174/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0176 - val_loss: 0.0101\n",
      "Epoch 2175/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0099 - val_loss: 0.0108\n",
      "Epoch 2176/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0096 - val_loss: 0.0148\n",
      "Epoch 2177/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0241 - val_loss: 0.0628\n",
      "Epoch 2178/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0302 - val_loss: 0.0425\n",
      "Epoch 2179/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0352 - val_loss: 0.0125\n",
      "Epoch 2180/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0230 - val_loss: 0.0146\n",
      "Epoch 2181/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0140 - val_loss: 0.0209\n",
      "Epoch 2182/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0316 - val_loss: 0.0138\n",
      "Epoch 2183/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0187 - val_loss: 0.0146\n",
      "Epoch 2184/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0229 - val_loss: 0.0165\n",
      "Epoch 2185/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0152 - val_loss: 0.0095\n",
      "Epoch 2186/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0159 - val_loss: 0.0626\n",
      "Epoch 2187/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0494 - val_loss: 0.0349\n",
      "Epoch 2188/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0274 - val_loss: 0.0466\n",
      "Epoch 2189/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0820 - val_loss: 0.1059\n",
      "Epoch 2190/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0653 - val_loss: 0.0631\n",
      "Epoch 2191/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0545 - val_loss: 0.0312\n",
      "Epoch 2192/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0879 - val_loss: 0.0425\n",
      "Epoch 2193/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0732 - val_loss: 0.0899\n",
      "Epoch 2194/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0943 - val_loss: 0.0336\n",
      "Epoch 2195/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0274 - val_loss: 0.0089\n",
      "Epoch 2196/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0192 - val_loss: 0.0161\n",
      "Epoch 2197/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 2198/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0082 - val_loss: 0.0186\n",
      "Epoch 2199/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0215 - val_loss: 0.0262\n",
      "Epoch 2200/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0323 - val_loss: 0.0148\n",
      "Epoch 2201/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0367 - val_loss: 0.0132\n",
      "Epoch 2202/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0189 - val_loss: 0.0114\n",
      "Epoch 2203/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0442 - val_loss: 0.0144\n",
      "Epoch 2204/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0244 - val_loss: 0.0150\n",
      "Epoch 2205/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0143 - val_loss: 0.0092\n",
      "Epoch 2206/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0190 - val_loss: 0.0321\n",
      "Epoch 2207/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0241 - val_loss: 0.0829\n",
      "Epoch 2208/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0429 - val_loss: 0.2083\n",
      "Epoch 2209/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1108 - val_loss: 0.4802\n",
      "Epoch 2210/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7342 - val_loss: 0.8452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2211/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9639 - val_loss: 1.3718\n",
      "Epoch 2212/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0788 - val_loss: 0.4329\n",
      "Epoch 2213/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3768 - val_loss: 1.2163\n",
      "Epoch 2214/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7196 - val_loss: 1.7510\n",
      "Epoch 2215/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8998 - val_loss: 6.4853\n",
      "Epoch 2216/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.5452 - val_loss: 6.6472\n",
      "Epoch 2217/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.4289 - val_loss: 9.8539\n",
      "Epoch 2218/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21.5967 - val_loss: 9.2597\n",
      "Epoch 2219/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 10.7615 - val_loss: 15.2168\n",
      "Epoch 2220/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 64.3996 - val_loss: 270.0742\n",
      "Epoch 2221/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 144.4544 - val_loss: 64.1530\n",
      "Epoch 2222/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 82.6728 - val_loss: 157.5451\n",
      "Epoch 2223/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 112.1816 - val_loss: 507.7896\n",
      "Epoch 2224/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 609.4720 - val_loss: 701.6328\n",
      "Epoch 2225/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 686.4345 - val_loss: 826.7618\n",
      "Epoch 2226/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 728.7486 - val_loss: 213.2266\n",
      "Epoch 2227/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 258.8808 - val_loss: 200.9041\n",
      "Epoch 2228/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 278.9652 - val_loss: 458.9744\n",
      "Epoch 2229/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 336.1754 - val_loss: 375.3700\n",
      "Epoch 2230/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 578.9472 - val_loss: 462.2193\n",
      "Epoch 2231/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 416.5579 - val_loss: 951.6149\n",
      "Epoch 2232/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1420.4468 - val_loss: 4838.5575\n",
      "Epoch 2233/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7378.7866 - val_loss: 3566.3280\n",
      "Epoch 2234/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1816.6601 - val_loss: 1508.4024\n",
      "Epoch 2235/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1131.1880 - val_loss: 3588.9671\n",
      "Epoch 2236/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2733.8393 - val_loss: 2797.2796\n",
      "Epoch 2237/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1370.6389 - val_loss: 1632.7105\n",
      "Epoch 2238/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1154.8330 - val_loss: 650.5372\n",
      "Epoch 2239/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 369.8172 - val_loss: 171.6622\n",
      "Epoch 2240/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 140.6151 - val_loss: 34.2565\n",
      "Epoch 2241/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 52.0006 - val_loss: 12.0926\n",
      "Epoch 2242/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 51.0022 - val_loss: 33.6890\n",
      "Epoch 2243/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 76.9138 - val_loss: 125.7022\n",
      "Epoch 2244/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 135.9498 - val_loss: 340.8016\n",
      "Epoch 2245/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 261.9972 - val_loss: 274.6308\n",
      "Epoch 2246/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 211.1764 - val_loss: 20.2910\n",
      "Epoch 2247/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 82.2822 - val_loss: 25.1867\n",
      "Epoch 2248/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 27.4652 - val_loss: 19.3100\n",
      "Epoch 2249/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 20.8018 - val_loss: 11.6945\n",
      "Epoch 2250/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14.9035 - val_loss: 6.1759\n",
      "Epoch 2251/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15.7285 - val_loss: 16.9108\n",
      "Epoch 2252/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 20.1051 - val_loss: 96.0140\n",
      "Epoch 2253/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 41.8114 - val_loss: 42.3425\n",
      "Epoch 2254/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 27.9332 - val_loss: 0.3428\n",
      "Epoch 2255/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.1460 - val_loss: 8.2733\n",
      "Epoch 2256/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11.7027 - val_loss: 44.6577\n",
      "Epoch 2257/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 47.4185 - val_loss: 51.2875\n",
      "Epoch 2258/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 89.2396 - val_loss: 43.1058\n",
      "Epoch 2259/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 31.6074 - val_loss: 99.7132\n",
      "Epoch 2260/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 32.5485 - val_loss: 20.2929\n",
      "Epoch 2261/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 10.6520 - val_loss: 6.0374\n",
      "Epoch 2262/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.7030 - val_loss: 3.8151\n",
      "Epoch 2263/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.8567 - val_loss: 7.9604\n",
      "Epoch 2264/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.9690 - val_loss: 0.4587\n",
      "Epoch 2265/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.7387 - val_loss: 0.3711\n",
      "Epoch 2266/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2604 - val_loss: 0.0236\n",
      "Epoch 2267/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1582 - val_loss: 0.1008\n",
      "Epoch 2268/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1327 - val_loss: 0.0872\n",
      "Epoch 2269/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0945 - val_loss: 0.0284\n",
      "Epoch 2270/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1038 - val_loss: 0.1018\n",
      "Epoch 2271/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0606 - val_loss: 0.0296\n",
      "Epoch 2272/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0697 - val_loss: 0.0278\n",
      "Epoch 2273/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0857 - val_loss: 0.0338\n",
      "Epoch 2274/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0672 - val_loss: 0.0309\n",
      "Epoch 2275/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0439 - val_loss: 0.0884\n",
      "Epoch 2276/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0577 - val_loss: 0.0133\n",
      "Epoch 2277/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0254 - val_loss: 0.0254\n",
      "Epoch 2278/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0203 - val_loss: 0.0124\n",
      "Epoch 2279/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0190 - val_loss: 0.0317\n",
      "Epoch 2280/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0357 - val_loss: 0.0179\n",
      "Epoch 2281/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0414 - val_loss: 0.0183\n",
      "Epoch 2282/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0433 - val_loss: 0.2735\n",
      "Epoch 2283/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1894 - val_loss: 0.2897\n",
      "Epoch 2284/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3394 - val_loss: 0.3916\n",
      "Epoch 2285/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2179 - val_loss: 0.0839\n",
      "Epoch 2286/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1908 - val_loss: 0.3044\n",
      "Epoch 2287/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2436 - val_loss: 0.1581\n",
      "Epoch 2288/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0965 - val_loss: 0.0543\n",
      "Epoch 2289/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0297 - val_loss: 0.0104\n",
      "Epoch 2290/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0101 - val_loss: 0.0098\n",
      "Epoch 2291/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0179 - val_loss: 0.0251\n",
      "Epoch 2292/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0341 - val_loss: 0.0226\n",
      "Epoch 2293/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0553 - val_loss: 0.0537\n",
      "Epoch 2294/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0445 - val_loss: 0.0901\n",
      "Epoch 2295/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0802 - val_loss: 0.0157\n",
      "Epoch 2296/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0254 - val_loss: 0.0410\n",
      "Epoch 2297/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0430 - val_loss: 0.0825\n",
      "Epoch 2298/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0389 - val_loss: 0.0199\n",
      "Epoch 2299/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0587 - val_loss: 0.2989\n",
      "Epoch 2300/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1471 - val_loss: 0.0701\n",
      "Epoch 2301/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3429 - val_loss: 0.6523\n",
      "Epoch 2302/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6403 - val_loss: 0.2679\n",
      "Epoch 2303/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.6099 - val_loss: 0.6079\n",
      "Epoch 2304/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2055 - val_loss: 0.0652\n",
      "Epoch 2305/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0968 - val_loss: 0.0226\n",
      "Epoch 2306/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0921 - val_loss: 0.2291\n",
      "Epoch 2307/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1051 - val_loss: 0.0759\n",
      "Epoch 2308/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0832 - val_loss: 0.1099\n",
      "Epoch 2309/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0389 - val_loss: 0.0202\n",
      "Epoch 2310/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0317 - val_loss: 0.0205\n",
      "Epoch 2311/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0329 - val_loss: 0.0176\n",
      "Epoch 2312/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0378 - val_loss: 0.0849\n",
      "Epoch 2313/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0803 - val_loss: 0.3271\n",
      "Epoch 2314/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1485 - val_loss: 0.0590\n",
      "Epoch 2315/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0915 - val_loss: 0.0836\n",
      "Epoch 2316/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0749 - val_loss: 0.0864\n",
      "Epoch 2317/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0450 - val_loss: 0.0950\n",
      "Epoch 2318/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2174 - val_loss: 0.0100\n",
      "Epoch 2319/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1536 - val_loss: 0.1351\n",
      "Epoch 2320/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4006 - val_loss: 0.4597\n",
      "Epoch 2321/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3006 - val_loss: 0.3352\n",
      "Epoch 2322/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5668 - val_loss: 1.2768\n",
      "Epoch 2323/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8457 - val_loss: 0.8711\n",
      "Epoch 2324/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4429 - val_loss: 0.1791\n",
      "Epoch 2325/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3226 - val_loss: 0.1997\n",
      "Epoch 2326/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2586 - val_loss: 0.0950\n",
      "Epoch 2327/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0892 - val_loss: 0.0792\n",
      "Epoch 2328/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0619 - val_loss: 0.0881\n",
      "Epoch 2329/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0848 - val_loss: 0.0783\n",
      "Epoch 2330/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1837 - val_loss: 0.3008\n",
      "Epoch 2331/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1962 - val_loss: 0.2114\n",
      "Epoch 2332/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2254 - val_loss: 0.4227\n",
      "Epoch 2333/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2063 - val_loss: 0.0469\n",
      "Epoch 2334/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1499 - val_loss: 0.1741\n",
      "Epoch 2335/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2753 - val_loss: 0.1094\n",
      "Epoch 2336/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0572 - val_loss: 0.0308\n",
      "Epoch 2337/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0190 - val_loss: 0.0113\n",
      "Epoch 2338/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0300 - val_loss: 0.0317\n",
      "Epoch 2339/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0250 - val_loss: 0.0301\n",
      "Epoch 2340/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0237 - val_loss: 0.0858\n",
      "Epoch 2341/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0809 - val_loss: 0.2861\n",
      "Epoch 2342/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2385 - val_loss: 0.1251\n",
      "Epoch 2343/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2698 - val_loss: 0.2355\n",
      "Epoch 2344/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4262 - val_loss: 1.5847\n",
      "Epoch 2345/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5745 - val_loss: 0.0184\n",
      "Epoch 2346/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0787 - val_loss: 0.1912\n",
      "Epoch 2347/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9016 - val_loss: 0.5601\n",
      "Epoch 2348/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6265 - val_loss: 0.9760\n",
      "Epoch 2349/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.7121 - val_loss: 0.3332\n",
      "Epoch 2350/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.8217 - val_loss: 1.2019\n",
      "Epoch 2351/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0499 - val_loss: 3.0721\n",
      "Epoch 2352/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6322 - val_loss: 2.0103\n",
      "Epoch 2353/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6634 - val_loss: 5.6022\n",
      "Epoch 2354/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.5966 - val_loss: 8.3462\n",
      "Epoch 2355/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.9269 - val_loss: 1.8498\n",
      "Epoch 2356/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7790 - val_loss: 0.2187\n",
      "Epoch 2357/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.3199 - val_loss: 0.8308\n",
      "Epoch 2358/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8121 - val_loss: 0.6959\n",
      "Epoch 2359/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0530 - val_loss: 1.0053\n",
      "Epoch 2360/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7145 - val_loss: 1.8579\n",
      "Epoch 2361/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 176us/sample - loss: 1.4473 - val_loss: 0.0041\n",
      "Epoch 2362/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.2771 - val_loss: 0.7996\n",
      "Epoch 2363/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.6185 - val_loss: 4.5778\n",
      "Epoch 2364/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.9699 - val_loss: 2.2530\n",
      "Epoch 2365/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.0449 - val_loss: 3.3457\n",
      "Epoch 2366/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.9114 - val_loss: 26.3830\n",
      "Epoch 2367/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 29.4047 - val_loss: 135.0783\n",
      "Epoch 2368/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 403.6705 - val_loss: 1181.0002\n",
      "Epoch 2369/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2339.6327 - val_loss: 2062.2543\n",
      "Epoch 2370/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1086.2126 - val_loss: 253.2853\n",
      "Epoch 2371/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1124.1927 - val_loss: 1050.5166\n",
      "Epoch 2372/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2042.5926 - val_loss: 3835.7290\n",
      "Epoch 2373/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2902.3805 - val_loss: 442.5567\n",
      "Epoch 2374/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1295.3555 - val_loss: 708.8787\n",
      "Epoch 2375/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 843.9452 - val_loss: 703.0943\n",
      "Epoch 2376/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 588.0401 - val_loss: 224.9912\n",
      "Epoch 2377/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 249.8803 - val_loss: 175.3431\n",
      "Epoch 2378/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 137.0587 - val_loss: 134.4007\n",
      "Epoch 2379/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 103.3906 - val_loss: 145.7278\n",
      "Epoch 2380/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 77.4536 - val_loss: 14.5308\n",
      "Epoch 2381/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 67.4096 - val_loss: 40.3014\n",
      "Epoch 2382/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 79.5303 - val_loss: 13.7194\n",
      "Epoch 2383/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 38.3568 - val_loss: 39.2717\n",
      "Epoch 2384/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 29.7673 - val_loss: 12.1553\n",
      "Epoch 2385/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 19.0663 - val_loss: 4.5894\n",
      "Epoch 2386/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 20.7453 - val_loss: 32.6994\n",
      "Epoch 2387/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 35.4356 - val_loss: 12.8835\n",
      "Epoch 2388/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 22.9101 - val_loss: 96.7316\n",
      "Epoch 2389/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 38.6955 - val_loss: 15.3552\n",
      "Epoch 2390/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 15.43 - 0s 147us/sample - loss: 16.9904 - val_loss: 20.0191\n",
      "Epoch 2391/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 16.5158 - val_loss: 5.4499\n",
      "Epoch 2392/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.8223 - val_loss: 1.6796\n",
      "Epoch 2393/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4111 - val_loss: 1.9114\n",
      "Epoch 2394/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.6757 - val_loss: 0.6266\n",
      "Epoch 2395/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1763 - val_loss: 0.2696\n",
      "Epoch 2396/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5210 - val_loss: 0.6384\n",
      "Epoch 2397/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5909 - val_loss: 0.3968\n",
      "Epoch 2398/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.5775 - val_loss: 1.9862\n",
      "Epoch 2399/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8446 - val_loss: 0.0830\n",
      "Epoch 2400/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1828 - val_loss: 0.1137\n",
      "Epoch 2401/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0550 - val_loss: 0.0435\n",
      "Epoch 2402/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0488 - val_loss: 0.0127\n",
      "Epoch 2403/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0353 - val_loss: 0.0514\n",
      "Epoch 2404/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0463 - val_loss: 0.0144\n",
      "Epoch 2405/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0701 - val_loss: 0.0240\n",
      "Epoch 2406/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0519 - val_loss: 0.1014\n",
      "Epoch 2407/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1096 - val_loss: 0.0715\n",
      "Epoch 2408/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0784 - val_loss: 0.0090\n",
      "Epoch 2409/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0460 - val_loss: 0.1017\n",
      "Epoch 2410/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0757 - val_loss: 0.0484\n",
      "Epoch 2411/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0275 - val_loss: 0.0262\n",
      "Epoch 2412/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0118 - val_loss: 0.0082\n",
      "Epoch 2413/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0125 - val_loss: 0.0011\n",
      "Epoch 2414/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0022 - val_loss: 5.0838e-04\n",
      "Epoch 2415/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 9.8449e-04\n",
      "Epoch 2416/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.9750e-04 - val_loss: 0.0017\n",
      "Epoch 2417/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.4118e-04 - val_loss: 1.2531e-04\n",
      "Epoch 2418/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 7.6782e-04 - val_loss: 2.1064e-04\n",
      "Epoch 2419/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.4543e-04 - val_loss: 3.3209e-04\n",
      "Epoch 2420/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.9262e-04 - val_loss: 5.3337e-04\n",
      "Epoch 2421/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.7820e-04 - val_loss: 4.5739e-04\n",
      "Epoch 2422/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.8019e-04 - val_loss: 1.5114e-04\n",
      "Epoch 2423/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.0362e-04 - val_loss: 3.0943e-04\n",
      "Epoch 2424/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0011 - val_loss: 2.8539e-04\n",
      "Epoch 2425/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 4.0424e-0 - 0s 147us/sample - loss: 7.1900e-04 - val_loss: 6.1491e-04\n",
      "Epoch 2426/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 2427/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.4009e-04 - val_loss: 0.0019\n",
      "Epoch 2428/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0020 - val_loss: 9.7261e-05\n",
      "Epoch 2429/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0050 - val_loss: 5.0877e-04\n",
      "Epoch 2430/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0023 - val_loss: 9.4926e-04\n",
      "Epoch 2431/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 2432/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 4.7099e-04\n",
      "Epoch 2433/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.5984e-04 - val_loss: 2.6623e-04\n",
      "Epoch 2434/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.1942e-04 - val_loss: 2.7683e-04\n",
      "Epoch 2435/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 2.5024e-04 - val_loss: 2.6693e-04\n",
      "Epoch 2436/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.6743e-04 - val_loss: 9.9505e-05\n",
      "Epoch 2437/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3151e-04 - val_loss: 1.0966e-04\n",
      "Epoch 2438/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3957e-04 - val_loss: 2.3239e-05\n",
      "Epoch 2439/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1114e-04 - val_loss: 0.0020\n",
      "Epoch 2440/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 2441/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 2442/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0037 - val_loss: 0.0026\n",
      "Epoch 2443/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 2444/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0014 - val_loss: 2.3918e-04\n",
      "Epoch 2445/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.8748e-04 - val_loss: 8.4266e-04\n",
      "Epoch 2446/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.0781e-04 - val_loss: 5.9330e-04\n",
      "Epoch 2447/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2922e-04 - val_loss: 2.0569e-04\n",
      "Epoch 2448/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.2771e-04 - val_loss: 3.0596e-04\n",
      "Epoch 2449/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.6602e-04 - val_loss: 0.0024\n",
      "Epoch 2450/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 2451/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.4149e-04 - val_loss: 3.8585e-04\n",
      "Epoch 2452/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9.5366e-04 - val_loss: 2.4812e-04\n",
      "Epoch 2453/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2415e-04 - val_loss: 6.1147e-05\n",
      "Epoch 2454/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.6158e-04 - val_loss: 3.6714e-04\n",
      "Epoch 2455/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1984e-04 - val_loss: 2.4686e-04\n",
      "Epoch 2456/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.1255e-04 - val_loss: 2.4986e-04\n",
      "Epoch 2457/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.9714e-04 - val_loss: 2.6787e-04\n",
      "Epoch 2458/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.7021e-04 - val_loss: 2.6731e-05\n",
      "Epoch 2459/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2631e-04 - val_loss: 0.0011\n",
      "Epoch 2460/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0018 - val_loss: 0.0100\n",
      "Epoch 2461/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0042 - val_loss: 0.0019\n",
      "Epoch 2462/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0017 - val_loss: 9.2150e-04\n",
      "Epoch 2463/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.3686e-04 - val_loss: 1.4796e-04\n",
      "Epoch 2464/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 6.7411e-04 - val_loss: 6.2662e-04\n",
      "Epoch 2465/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.3886e-04 - val_loss: 6.4611e-04\n",
      "Epoch 2466/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0012 - val_loss: 4.6410e-04\n",
      "Epoch 2467/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.4906e-04 - val_loss: 0.0011\n",
      "Epoch 2468/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0023 - val_loss: 0.0059\n",
      "Epoch 2469/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0030 - val_loss: 0.0018\n",
      "Epoch 2470/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 2.4181e-04\n",
      "Epoch 2471/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.5587e-04 - val_loss: 4.7406e-04\n",
      "Epoch 2472/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.7415e-04 - val_loss: 1.5556e-04\n",
      "Epoch 2473/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.5375e-04 - val_loss: 2.9895e-04\n",
      "Epoch 2474/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0762e-04 - val_loss: 3.3799e-05\n",
      "Epoch 2475/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 4.2076e-0 - 0s 147us/sample - loss: 1.2227e-04 - val_loss: 1.3994e-04\n",
      "Epoch 2476/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 4.1691e-04 - val_loss: 2.9143e-05\n",
      "Epoch 2477/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0014 - val_loss: 9.3592e-04\n",
      "Epoch 2478/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 1.7668e-04\n",
      "Epoch 2479/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 2480/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.0406e-04 - val_loss: 0.0011\n",
      "Epoch 2481/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0018 - val_loss: 0.0044\n",
      "Epoch 2482/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0051 - val_loss: 0.0024\n",
      "Epoch 2483/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 2484/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 2485/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.5713e-04 - val_loss: 8.8533e-04\n",
      "Epoch 2486/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0031 - val_loss: 0.0092\n",
      "Epoch 2487/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0054 - val_loss: 0.0030\n",
      "Epoch 2488/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0024 - val_loss: 0.0012\n",
      "Epoch 2489/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.2595e-04 - val_loss: 5.1426e-04\n",
      "Epoch 2490/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0029 - val_loss: 0.0015\n",
      "Epoch 2491/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0024 - val_loss: 0.0056\n",
      "Epoch 2492/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0045 - val_loss: 0.0032\n",
      "Epoch 2493/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0237 - val_loss: 0.0203\n",
      "Epoch 2494/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0101 - val_loss: 2.6777e-04\n",
      "Epoch 2495/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0520 - val_loss: 0.1195\n",
      "Epoch 2496/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1927 - val_loss: 0.3717\n",
      "Epoch 2497/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1869 - val_loss: 0.0021\n",
      "Epoch 2498/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2297 - val_loss: 0.3468\n",
      "Epoch 2499/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0901 - val_loss: 0.0142\n",
      "Epoch 2500/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0720 - val_loss: 0.0450\n",
      "Epoch 2501/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0140 - val_loss: 0.0099\n",
      "Epoch 2502/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1548 - val_loss: 0.4186\n",
      "Epoch 2503/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6618 - val_loss: 0.6290\n",
      "Epoch 2504/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4725 - val_loss: 0.3344\n",
      "Epoch 2505/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1566 - val_loss: 0.2686\n",
      "Epoch 2506/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3847 - val_loss: 0.1674\n",
      "Epoch 2507/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1119 - val_loss: 0.1127\n",
      "Epoch 2508/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0521 - val_loss: 0.0088\n",
      "Epoch 2509/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0854 - val_loss: 0.4514\n",
      "Epoch 2510/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7933 - val_loss: 0.4006\n",
      "Epoch 2511/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1574 - val_loss: 0.9336\n",
      "Epoch 2512/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.6233 - val_loss: 0.1250\n",
      "Epoch 2513/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3238 - val_loss: 0.1281\n",
      "Epoch 2514/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1227 - val_loss: 6.2254e-04\n",
      "Epoch 2515/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0562 - val_loss: 0.0444\n",
      "Epoch 2516/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0682 - val_loss: 0.0097\n",
      "Epoch 2517/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0110 - val_loss: 0.0128\n",
      "Epoch 2518/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0128 - val_loss: 0.0056\n",
      "Epoch 2519/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0369 - val_loss: 0.3602\n",
      "Epoch 2520/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1563 - val_loss: 0.0113\n",
      "Epoch 2521/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0199 - val_loss: 0.0078\n",
      "Epoch 2522/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0340 - val_loss: 0.5322\n",
      "Epoch 2523/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.7104 - val_loss: 0.2356\n",
      "Epoch 2524/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.8941 - val_loss: 0.2320\n",
      "Epoch 2525/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.5486 - val_loss: 0.6146\n",
      "Epoch 2526/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.9081 - val_loss: 0.2958\n",
      "Epoch 2527/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 11.8851 - val_loss: 18.4279\n",
      "Epoch 2528/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.0029 - val_loss: 0.5824\n",
      "Epoch 2529/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2991 - val_loss: 2.4062\n",
      "Epoch 2530/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9651 - val_loss: 6.4653\n",
      "Epoch 2531/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.8082 - val_loss: 44.1468\n",
      "Epoch 2532/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.6228 - val_loss: 6.6393\n",
      "Epoch 2533/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16.9714 - val_loss: 19.0965\n",
      "Epoch 2534/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 99.6678 - val_loss: 144.5653\n",
      "Epoch 2535/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 176.5057 - val_loss: 1903.0727\n",
      "Epoch 2536/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4147.7704 - val_loss: 3701.5140\n",
      "Epoch 2537/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4762.6356 - val_loss: 3734.8032\n",
      "Epoch 2538/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5968.9898 - val_loss: 24.1122\n",
      "Epoch 2539/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2981.2410 - val_loss: 4099.3660\n",
      "Epoch 2540/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1396.0127 - val_loss: 278.5442\n",
      "Epoch 2541/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 836.1967 - val_loss: 466.2975\n",
      "Epoch 2542/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 163.2296 - val_loss: 54.7407\n",
      "Epoch 2543/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 109.4386 - val_loss: 44.3264\n",
      "Epoch 2544/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 62.5758 - val_loss: 60.7525\n",
      "Epoch 2545/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 53.1499 - val_loss: 0.3615\n",
      "Epoch 2546/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 32.5293 - val_loss: 12.2563\n",
      "Epoch 2547/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 28.9861 - val_loss: 39.1024\n",
      "Epoch 2548/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 49.7773 - val_loss: 72.9252\n",
      "Epoch 2549/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 42.6768 - val_loss: 5.0114\n",
      "Epoch 2550/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 33.6550 - val_loss: 26.6240\n",
      "Epoch 2551/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 40.3254 - val_loss: 92.8873\n",
      "Epoch 2552/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 69.2331 - val_loss: 11.0441\n",
      "Epoch 2553/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 164.0778 - val_loss: 318.0347\n",
      "Epoch 2554/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 374.4070 - val_loss: 438.0721\n",
      "Epoch 2555/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 213.5139 - val_loss: 27.2165\n",
      "Epoch 2556/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 98.0330 - val_loss: 8.8188\n",
      "Epoch 2557/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 76.6478 - val_loss: 77.3400\n",
      "Epoch 2558/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 57.9109 - val_loss: 47.1779\n",
      "Epoch 2559/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 41.2489 - val_loss: 3.1709\n",
      "Epoch 2560/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 22.7641 - val_loss: 19.9444\n",
      "Epoch 2561/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14.6681 - val_loss: 21.3880\n",
      "Epoch 2562/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.9464 - val_loss: 2.9337\n",
      "Epoch 2563/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 54.0251 - val_loss: 121.8862\n",
      "Epoch 2564/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 72.7803 - val_loss: 87.0403\n",
      "Epoch 2565/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 36.2509 - val_loss: 10.3378\n",
      "Epoch 2566/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 21.6254 - val_loss: 17.9456\n",
      "Epoch 2567/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 14.8796 - val_loss: 4.5408\n",
      "Epoch 2568/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.8930 - val_loss: 1.2543\n",
      "Epoch 2569/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.0013 - val_loss: 3.3717\n",
      "Epoch 2570/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6788 - val_loss: 0.0843\n",
      "Epoch 2571/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2349 - val_loss: 5.0978\n",
      "Epoch 2572/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8236 - val_loss: 0.2875\n",
      "Epoch 2573/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5859 - val_loss: 1.7097\n",
      "Epoch 2574/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4852 - val_loss: 1.6274\n",
      "Epoch 2575/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.4713 - val_loss: 1.6755\n",
      "Epoch 2576/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9724 - val_loss: 0.0182\n",
      "Epoch 2577/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5277 - val_loss: 1.4145\n",
      "Epoch 2578/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5422 - val_loss: 0.3853\n",
      "Epoch 2579/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1629 - val_loss: 0.1407\n",
      "Epoch 2580/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0702 - val_loss: 0.0554\n",
      "Epoch 2581/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0390 - val_loss: 0.0450\n",
      "Epoch 2582/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0753 - val_loss: 0.0049\n",
      "Epoch 2583/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0148 - val_loss: 0.0056\n",
      "Epoch 2584/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0055 - val_loss: 0.0065\n",
      "Epoch 2585/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0097 - val_loss: 0.0106\n",
      "Epoch 2586/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0218 - val_loss: 0.0036\n",
      "Epoch 2587/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0120 - val_loss: 0.0119\n",
      "Epoch 2588/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0061 - val_loss: 0.0027\n",
      "Epoch 2589/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0053 - val_loss: 0.0055\n",
      "Epoch 2590/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0050 - val_loss: 0.0072\n",
      "Epoch 2591/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0102 - val_loss: 0.0152\n",
      "Epoch 2592/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0092 - val_loss: 0.0126\n",
      "Epoch 2593/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0130 - val_loss: 0.0232\n",
      "Epoch 2594/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0301 - val_loss: 0.0115\n",
      "Epoch 2595/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0265 - val_loss: 0.0570\n",
      "Epoch 2596/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0475 - val_loss: 0.0204\n",
      "Epoch 2597/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0137 - val_loss: 0.0092\n",
      "Epoch 2598/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0356 - val_loss: 0.0020\n",
      "Epoch 2599/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0483 - val_loss: 0.0079\n",
      "Epoch 2600/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0154 - val_loss: 0.0124\n",
      "Epoch 2601/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0091 - val_loss: 0.0095\n",
      "Epoch 2602/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0065 - val_loss: 0.0093\n",
      "Epoch 2603/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0091 - val_loss: 0.0239\n",
      "Epoch 2604/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0112 - val_loss: 0.0246\n",
      "Epoch 2605/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0098 - val_loss: 0.0054\n",
      "Epoch 2606/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0087 - val_loss: 0.0020\n",
      "Epoch 2607/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0087 - val_loss: 0.0186\n",
      "Epoch 2608/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0162 - val_loss: 0.0229\n",
      "Epoch 2609/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0287 - val_loss: 0.0405\n",
      "Epoch 2610/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0824 - val_loss: 0.0229\n",
      "Epoch 2611/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0932 - val_loss: 0.0488\n",
      "Epoch 2612/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0385 - val_loss: 0.0624\n",
      "Epoch 2613/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0582 - val_loss: 0.1233\n",
      "Epoch 2614/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1126 - val_loss: 0.0387\n",
      "Epoch 2615/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2907 - val_loss: 0.2727\n",
      "Epoch 2616/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2283 - val_loss: 0.2659\n",
      "Epoch 2617/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1703 - val_loss: 0.0023\n",
      "Epoch 2618/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1274 - val_loss: 0.0731\n",
      "Epoch 2619/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0296 - val_loss: 0.0242\n",
      "Epoch 2620/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0498 - val_loss: 0.0900\n",
      "Epoch 2621/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0976 - val_loss: 0.0897\n",
      "Epoch 2622/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0476 - val_loss: 0.1102\n",
      "Epoch 2623/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0747 - val_loss: 0.0637\n",
      "Epoch 2624/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0383 - val_loss: 0.0330\n",
      "Epoch 2625/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0318 - val_loss: 0.0182\n",
      "Epoch 2626/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0302 - val_loss: 0.1114\n",
      "Epoch 2627/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0690 - val_loss: 0.1556\n",
      "Epoch 2628/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1419 - val_loss: 0.1469\n",
      "Epoch 2629/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0575 - val_loss: 0.0058\n",
      "Epoch 2630/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0049 - val_loss: 0.0057\n",
      "Epoch 2631/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0051 - val_loss: 0.0070\n",
      "Epoch 2632/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0062 - val_loss: 0.0066\n",
      "Epoch 2633/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0072 - val_loss: 0.0143\n",
      "Epoch 2634/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0074 - val_loss: 0.0076\n",
      "Epoch 2635/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0096 - val_loss: 0.0218\n",
      "Epoch 2636/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0230 - val_loss: 0.0030\n",
      "Epoch 2637/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0102 - val_loss: 0.0046\n",
      "Epoch 2638/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0031 - val_loss: 8.7378e-04\n",
      "Epoch 2639/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0045 - val_loss: 0.0013\n",
      "Epoch 2640/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0115 - val_loss: 0.0013\n",
      "Epoch 2641/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0152 - val_loss: 0.0140\n",
      "Epoch 2642/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0081 - val_loss: 0.0165\n",
      "Epoch 2643/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0060 - val_loss: 0.0081\n",
      "Epoch 2644/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0057 - val_loss: 0.0123\n",
      "Epoch 2645/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0057 - val_loss: 0.0042\n",
      "Epoch 2646/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0046 - val_loss: 8.4132e-04\n",
      "Epoch 2647/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0035 - val_loss: 0.0070\n",
      "Epoch 2648/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0045 - val_loss: 0.0039\n",
      "Epoch 2649/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0017 - val_loss: 8.4100e-04\n",
      "Epoch 2650/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.0291e-04 - val_loss: 7.7099e-04\n",
      "Epoch 2651/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0050 - val_loss: 0.0127\n",
      "Epoch 2652/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0132 - val_loss: 0.0308\n",
      "Epoch 2653/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0274 - val_loss: 0.2202\n",
      "Epoch 2654/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2277 - val_loss: 0.0414\n",
      "Epoch 2655/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1349 - val_loss: 0.0598\n",
      "Epoch 2656/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1595 - val_loss: 0.5948\n",
      "Epoch 2657/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4152 - val_loss: 1.3157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2658/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8256 - val_loss: 0.0264\n",
      "Epoch 2659/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9810 - val_loss: 2.0923\n",
      "Epoch 2660/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.1403 - val_loss: 5.5511\n",
      "Epoch 2661/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 19.9975 - val_loss: 8.4927\n",
      "Epoch 2662/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 46.7320 - val_loss: 21.5646\n",
      "Epoch 2663/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.4688 - val_loss: 2.6529\n",
      "Epoch 2664/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.0746 - val_loss: 4.3668\n",
      "Epoch 2665/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.8932 - val_loss: 4.3627\n",
      "Epoch 2666/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.9990 - val_loss: 2.9369\n",
      "Epoch 2667/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 3.234 - 0s 147us/sample - loss: 12.0545 - val_loss: 8.4482\n",
      "Epoch 2668/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 38.5585 - val_loss: 9.3219\n",
      "Epoch 2669/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 55.9636 - val_loss: 44.8899\n",
      "Epoch 2670/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 42.8698 - val_loss: 0.9439\n",
      "Epoch 2671/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.7943 - val_loss: 16.9072\n",
      "Epoch 2672/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.2307 - val_loss: 1.4187\n",
      "Epoch 2673/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 1.846 - 0s 132us/sample - loss: 24.2580 - val_loss: 82.8510\n",
      "Epoch 2674/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 459.1409 - val_loss: 25.0618\n",
      "Epoch 2675/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 274.7059 - val_loss: 296.8685\n",
      "Epoch 2676/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 181.2806 - val_loss: 330.5981\n",
      "Epoch 2677/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 649.8693 - val_loss: 846.6157\n",
      "Epoch 2678/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2868.7632 - val_loss: 2156.0782\n",
      "Epoch 2679/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 6410.9407 - val_loss: 2616.2698\n",
      "Epoch 2680/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2265.5009 - val_loss: 953.0199\n",
      "Epoch 2681/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 722.1518 - val_loss: 567.6030\n",
      "Epoch 2682/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 453.1490 - val_loss: 326.0777\n",
      "Epoch 2683/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 479.2329 - val_loss: 277.7329\n",
      "Epoch 2684/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 425.0070 - val_loss: 624.0875\n",
      "Epoch 2685/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 316.8959 - val_loss: 57.0547\n",
      "Epoch 2686/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 318.2295 - val_loss: 369.5683\n",
      "Epoch 2687/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 140.9258 - val_loss: 9.3893\n",
      "Epoch 2688/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.9136 - val_loss: 3.2462\n",
      "Epoch 2689/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.2618 - val_loss: 6.5606\n",
      "Epoch 2690/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.2409 - val_loss: 2.0446\n",
      "Epoch 2691/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.5537 - val_loss: 1.7153\n",
      "Epoch 2692/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2058 - val_loss: 1.5548\n",
      "Epoch 2693/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.6323 - val_loss: 3.4246\n",
      "Epoch 2694/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2290 - val_loss: 0.9439\n",
      "Epoch 2695/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3490 - val_loss: 1.7111\n",
      "Epoch 2696/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.9728 - val_loss: 0.8887\n",
      "Epoch 2697/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7794 - val_loss: 1.8019\n",
      "Epoch 2698/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.9799 - val_loss: 0.0497\n",
      "Epoch 2699/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4555 - val_loss: 0.2394\n",
      "Epoch 2700/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3630 - val_loss: 0.2781\n",
      "Epoch 2701/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1736 - val_loss: 0.1086\n",
      "Epoch 2702/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1245 - val_loss: 0.0409\n",
      "Epoch 2703/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2079 - val_loss: 0.5407\n",
      "Epoch 2704/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3966 - val_loss: 0.4150\n",
      "Epoch 2705/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2654 - val_loss: 0.0216\n",
      "Epoch 2706/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3434 - val_loss: 0.3418\n",
      "Epoch 2707/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3252 - val_loss: 0.2585\n",
      "Epoch 2708/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1902 - val_loss: 0.1732\n",
      "Epoch 2709/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0059 - val_loss: 0.2669\n",
      "Epoch 2710/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.6795 - val_loss: 0.8240\n",
      "Epoch 2711/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7152 - val_loss: 0.0400\n",
      "Epoch 2712/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1937 - val_loss: 0.2569\n",
      "Epoch 2713/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1334 - val_loss: 0.0905\n",
      "Epoch 2714/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1774 - val_loss: 0.3953\n",
      "Epoch 2715/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2320 - val_loss: 0.3511\n",
      "Epoch 2716/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.2276 - val_loss: 0.0302\n",
      "Epoch 2717/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0818 - val_loss: 0.3278\n",
      "Epoch 2718/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1393 - val_loss: 0.1621\n",
      "Epoch 2719/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1824 - val_loss: 0.2577\n",
      "Epoch 2720/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2106 - val_loss: 0.0325\n",
      "Epoch 2721/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0797 - val_loss: 0.0238\n",
      "Epoch 2722/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0911 - val_loss: 0.1499\n",
      "Epoch 2723/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0901 - val_loss: 0.0967\n",
      "Epoch 2724/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0683 - val_loss: 0.0079\n",
      "Epoch 2725/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0971 - val_loss: 0.4580\n",
      "Epoch 2726/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3190 - val_loss: 0.4105\n",
      "Epoch 2727/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1610 - val_loss: 0.1111\n",
      "Epoch 2728/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0573 - val_loss: 0.0045\n",
      "Epoch 2729/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0296 - val_loss: 0.0287\n",
      "Epoch 2730/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.017 - 0s 132us/sample - loss: 0.0177 - val_loss: 0.0176\n",
      "Epoch 2731/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 2732/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0558 - val_loss: 0.0066\n",
      "Epoch 2733/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1069 - val_loss: 0.0069\n",
      "Epoch 2734/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0884 - val_loss: 0.0499\n",
      "Epoch 2735/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0685 - val_loss: 0.0281\n",
      "Epoch 2736/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1366 - val_loss: 0.2921\n",
      "Epoch 2737/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2667 - val_loss: 1.0397\n",
      "Epoch 2738/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.5576 - val_loss: 2.6992\n",
      "Epoch 2739/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.7990 - val_loss: 8.0525\n",
      "Epoch 2740/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.6643 - val_loss: 2.0719\n",
      "Epoch 2741/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.4301 - val_loss: 1.5835\n",
      "Epoch 2742/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7449 - val_loss: 5.2999\n",
      "Epoch 2743/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5154 - val_loss: 0.8718\n",
      "Epoch 2744/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4272 - val_loss: 0.2862\n",
      "Epoch 2745/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2329 - val_loss: 0.5317\n",
      "Epoch 2746/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3691 - val_loss: 0.4038\n",
      "Epoch 2747/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.8792 - val_loss: 0.5386\n",
      "Epoch 2748/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0267 - val_loss: 2.5719\n",
      "Epoch 2749/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.4289 - val_loss: 0.8863\n",
      "Epoch 2750/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3864 - val_loss: 0.3495\n",
      "Epoch 2751/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5831 - val_loss: 1.1726\n",
      "Epoch 2752/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.5930 - val_loss: 2.9555\n",
      "Epoch 2753/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.3253 - val_loss: 4.1918\n",
      "Epoch 2754/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.7170 - val_loss: 9.5596\n",
      "Epoch 2755/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.9686 - val_loss: 3.5139\n",
      "Epoch 2756/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1039 - val_loss: 1.0668\n",
      "Epoch 2757/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5578 - val_loss: 0.0578\n",
      "Epoch 2758/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5773 - val_loss: 0.2348\n",
      "Epoch 2759/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9935 - val_loss: 0.6133\n",
      "Epoch 2760/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8368 - val_loss: 0.8726\n",
      "Epoch 2761/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1198 - val_loss: 3.3593\n",
      "Epoch 2762/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.5944 - val_loss: 1.7605\n",
      "Epoch 2763/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9589 - val_loss: 6.7481\n",
      "Epoch 2764/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.8441 - val_loss: 6.3938\n",
      "Epoch 2765/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.3922 - val_loss: 2.4703\n",
      "Epoch 2766/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.7307 - val_loss: 15.5962\n",
      "Epoch 2767/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.2769 - val_loss: 17.5754\n",
      "Epoch 2768/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.8921 - val_loss: 9.9948\n",
      "Epoch 2769/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 47.2911 - val_loss: 78.6806\n",
      "Epoch 2770/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 63.0435 - val_loss: 9.5356\n",
      "Epoch 2771/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 61.1962 - val_loss: 262.6537\n",
      "Epoch 2772/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 319.3686 - val_loss: 1629.8816\n",
      "Epoch 2773/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1174.7789 - val_loss: 1068.2896\n",
      "Epoch 2774/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 441.2318 - val_loss: 648.0086\n",
      "Epoch 2775/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 253.0873 - val_loss: 75.9226\n",
      "Epoch 2776/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 46.6555 - val_loss: 3.0836\n",
      "Epoch 2777/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 31.9532 - val_loss: 15.1367\n",
      "Epoch 2778/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.4716 - val_loss: 15.8520\n",
      "Epoch 2779/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14.2585 - val_loss: 1.0135\n",
      "Epoch 2780/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2507 - val_loss: 3.0271\n",
      "Epoch 2781/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.3796 - val_loss: 0.4196\n",
      "Epoch 2782/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5482 - val_loss: 1.5648\n",
      "Epoch 2783/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0946 - val_loss: 1.9212\n",
      "Epoch 2784/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.5470 - val_loss: 3.0825\n",
      "Epoch 2785/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 5.4143 - val_loss: 6.0709\n",
      "Epoch 2786/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.3825 - val_loss: 21.7791\n",
      "Epoch 2787/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17.6405 - val_loss: 29.0912\n",
      "Epoch 2788/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17.3317 - val_loss: 13.5371\n",
      "Epoch 2789/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12.8497 - val_loss: 0.5549\n",
      "Epoch 2790/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9394 - val_loss: 0.8783\n",
      "Epoch 2791/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.1767 - val_loss: 0.4549\n",
      "Epoch 2792/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.6402 - val_loss: 16.9748\n",
      "Epoch 2793/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.0945 - val_loss: 22.7892\n",
      "Epoch 2794/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.9754 - val_loss: 3.5943\n",
      "Epoch 2795/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.3765 - val_loss: 1.1918\n",
      "Epoch 2796/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.7003 - val_loss: 3.4303\n",
      "Epoch 2797/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1274 - val_loss: 1.0362\n",
      "Epoch 2798/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.6424 - val_loss: 5.8814\n",
      "Epoch 2799/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.9337 - val_loss: 2.1927\n",
      "Epoch 2800/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.1358 - val_loss: 6.3923\n",
      "Epoch 2801/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21.3128 - val_loss: 9.5064\n",
      "Epoch 2802/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.4301 - val_loss: 18.9510\n",
      "Epoch 2803/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.7793 - val_loss: 33.1986\n",
      "Epoch 2804/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.6676 - val_loss: 14.3913\n",
      "Epoch 2805/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 29.0910 - val_loss: 3.9602\n",
      "Epoch 2806/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 85.8051 - val_loss: 648.0100\n",
      "Epoch 2807/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 297.8055 - val_loss: 156.6437\n",
      "Epoch 2808/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 340.5727 - val_loss: 51.9371\n",
      "Epoch 2809/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 132.0021 - val_loss: 113.9945\n",
      "Epoch 2810/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 297.4998 - val_loss: 1863.7731\n",
      "Epoch 2811/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2509.2690 - val_loss: 3532.3578\n",
      "Epoch 2812/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1507.7974 - val_loss: 2706.2102\n",
      "Epoch 2813/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1348.8225 - val_loss: 764.1896\n",
      "Epoch 2814/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 381.3250 - val_loss: 153.7375\n",
      "Epoch 2815/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 96.8452 - val_loss: 33.7640\n",
      "Epoch 2816/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 105.2933 - val_loss: 126.7210\n",
      "Epoch 2817/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 98.47 - 0s 147us/sample - loss: 67.8887 - val_loss: 0.5155\n",
      "Epoch 2818/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 79.8465 - val_loss: 30.1203\n",
      "Epoch 2819/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 93.6478 - val_loss: 149.3303\n",
      "Epoch 2820/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 55.0642 - val_loss: 63.9001\n",
      "Epoch 2821/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 52.5345 - val_loss: 25.2767\n",
      "Epoch 2822/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 40.9567 - val_loss: 37.1888\n",
      "Epoch 2823/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17.0911 - val_loss: 1.0913\n",
      "Epoch 2824/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 4.7291 - val_loss: 3.1609\n",
      "Epoch 2825/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.3842 - val_loss: 7.3649\n",
      "Epoch 2826/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.4872 - val_loss: 20.1410\n",
      "Epoch 2827/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15.9955 - val_loss: 2.3048\n",
      "Epoch 2828/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.8010 - val_loss: 1.9044\n",
      "Epoch 2829/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3183 - val_loss: 0.6395\n",
      "Epoch 2830/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2728 - val_loss: 0.1210\n",
      "Epoch 2831/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1811 - val_loss: 0.3200\n",
      "Epoch 2832/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2083 - val_loss: 0.7219\n",
      "Epoch 2833/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6032 - val_loss: 0.2654\n",
      "Epoch 2834/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1054 - val_loss: 0.0930\n",
      "Epoch 2835/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0933 - val_loss: 0.1280\n",
      "Epoch 2836/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1987 - val_loss: 0.2798\n",
      "Epoch 2837/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6430 - val_loss: 1.0086\n",
      "Epoch 2838/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4316 - val_loss: 0.3572\n",
      "Epoch 2839/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2054 - val_loss: 0.1238\n",
      "Epoch 2840/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0734 - val_loss: 0.1475\n",
      "Epoch 2841/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2761 - val_loss: 0.0456\n",
      "Epoch 2842/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4437 - val_loss: 0.1972\n",
      "Epoch 2843/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1977 - val_loss: 0.0446\n",
      "Epoch 2844/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1383 - val_loss: 0.1433\n",
      "Epoch 2845/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2279 - val_loss: 0.3509\n",
      "Epoch 2846/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5485 - val_loss: 0.0964\n",
      "Epoch 2847/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9701 - val_loss: 0.6280\n",
      "Epoch 2848/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3333 - val_loss: 0.9480\n",
      "Epoch 2849/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.0160 - val_loss: 0.4611\n",
      "Epoch 2850/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8153 - val_loss: 0.3854\n",
      "Epoch 2851/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4590 - val_loss: 0.1719\n",
      "Epoch 2852/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.8769 - val_loss: 2.2799\n",
      "Epoch 2853/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.9496 - val_loss: 12.3159\n",
      "Epoch 2854/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12.7822 - val_loss: 20.9465\n",
      "Epoch 2855/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 19.1006 - val_loss: 2.3628\n",
      "Epoch 2856/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.8452 - val_loss: 10.2886\n",
      "Epoch 2857/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.7008 - val_loss: 111.2967\n",
      "Epoch 2858/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 41.8948 - val_loss: 30.3026\n",
      "Epoch 2859/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 38.9093 - val_loss: 0.3222\n",
      "Epoch 2860/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 56.6279 - val_loss: 182.4630\n",
      "Epoch 2861/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 175.4359 - val_loss: 155.9719\n",
      "Epoch 2862/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 174.5174 - val_loss: 47.7968\n",
      "Epoch 2863/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 35.1124 - val_loss: 71.2992\n",
      "Epoch 2864/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 58.0284 - val_loss: 109.0374\n",
      "Epoch 2865/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 114.5829 - val_loss: 77.5362\n",
      "Epoch 2866/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 34.8495 - val_loss: 30.0264\n",
      "Epoch 2867/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 69.2074 - val_loss: 131.1489\n",
      "Epoch 2868/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 941.4559 - val_loss: 203.3976\n",
      "Epoch 2869/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 285.3963 - val_loss: 211.3187\n",
      "Epoch 2870/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 179.1019 - val_loss: 243.7229\n",
      "Epoch 2871/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 103.1810 - val_loss: 203.7254\n",
      "Epoch 2872/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 229.7188 - val_loss: 624.9179\n",
      "Epoch 2873/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1441.4242 - val_loss: 1995.7942\n",
      "Epoch 2874/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 878.8895 - val_loss: 1499.4416\n",
      "Epoch 2875/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1310.8363 - val_loss: 917.9621\n",
      "Epoch 2876/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 323.9839 - val_loss: 431.2946\n",
      "Epoch 2877/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 231.7120 - val_loss: 168.4121\n",
      "Epoch 2878/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 110.8643 - val_loss: 27.8551\n",
      "Epoch 2879/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 199.3857 - val_loss: 398.4065\n",
      "Epoch 2880/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 184.1576 - val_loss: 112.1317\n",
      "Epoch 2881/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 170.1571 - val_loss: 114.5547\n",
      "Epoch 2882/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 244.2144 - val_loss: 30.7204\n",
      "Epoch 2883/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 142.2828 - val_loss: 76.4688\n",
      "Epoch 2884/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 151.8553 - val_loss: 374.2282\n",
      "Epoch 2885/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 180.7627 - val_loss: 47.0707\n",
      "Epoch 2886/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 317.5809 - val_loss: 392.3111\n",
      "Epoch 2887/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 386.0278 - val_loss: 554.4147\n",
      "Epoch 2888/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 465.9269 - val_loss: 290.3016\n",
      "Epoch 2889/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 312.2684 - val_loss: 131.6061\n",
      "Epoch 2890/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 312.3862 - val_loss: 170.7008\n",
      "Epoch 2891/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 183.7121 - val_loss: 275.4706\n",
      "Epoch 2892/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 363.0735 - val_loss: 403.6843\n",
      "Epoch 2893/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 709.0789 - val_loss: 1527.6587\n",
      "Epoch 2894/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 726.7279 - val_loss: 472.4066\n",
      "Epoch 2895/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 788.2610 - val_loss: 163.7244\n",
      "Epoch 2896/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 744.0905 - val_loss: 748.6920\n",
      "Epoch 2897/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 466.7014 - val_loss: 16.3579\n",
      "Epoch 2898/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 18.61 - 0s 147us/sample - loss: 158.0482 - val_loss: 221.5560\n",
      "Epoch 2899/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 180.1522 - val_loss: 67.8369\n",
      "Epoch 2900/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 154.7496 - val_loss: 294.3294\n",
      "Epoch 2901/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 145.0856 - val_loss: 48.2481\n",
      "Epoch 2902/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 88.1911 - val_loss: 2.0865\n",
      "Epoch 2903/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 36.2767 - val_loss: 117.1840\n",
      "Epoch 2904/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 64.9138 - val_loss: 35.4152\n",
      "Epoch 2905/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 50.5113 - val_loss: 94.2928\n",
      "Epoch 2906/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 57.7823 - val_loss: 42.1351\n",
      "Epoch 2907/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 152.4822 - val_loss: 258.0366\n",
      "Epoch 2908/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 615.1706 - val_loss: 1525.5679\n",
      "Epoch 2909/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1056.9023 - val_loss: 1108.3020\n",
      "Epoch 2910/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 762.3072 - val_loss: 693.6701\n",
      "Epoch 2911/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 776.3767 - val_loss: 114.1834\n",
      "Epoch 2912/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 619.3272 - val_loss: 1008.9941\n",
      "Epoch 2913/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 941.9956 - val_loss: 117.5159\n",
      "Epoch 2914/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 856.5773 - val_loss: 1141.6625\n",
      "Epoch 2915/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1123.1746 - val_loss: 3060.0565\n",
      "Epoch 2916/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1548.5407 - val_loss: 349.9800\n",
      "Epoch 2917/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1036.0164 - val_loss: 458.2203\n",
      "Epoch 2918/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 410.0592 - val_loss: 288.8313\n",
      "Epoch 2919/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 403.2200 - val_loss: 455.7340\n",
      "Epoch 2920/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 234.1759 - val_loss: 35.6273\n",
      "Epoch 2921/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 57.1647 - val_loss: 30.7981\n",
      "Epoch 2922/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 30.7678 - val_loss: 24.5705\n",
      "Epoch 2923/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14.5533 - val_loss: 23.0544\n",
      "Epoch 2924/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9.0156 - val_loss: 7.7103\n",
      "Epoch 2925/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.9833 - val_loss: 4.8290\n",
      "Epoch 2926/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.2039 - val_loss: 2.8861\n",
      "Epoch 2927/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7692 - val_loss: 0.0055\n",
      "Epoch 2928/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7921 - val_loss: 1.3883\n",
      "Epoch 2929/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8037 - val_loss: 1.6143\n",
      "Epoch 2930/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.0588 - val_loss: 1.3987\n",
      "Epoch 2931/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0865 - val_loss: 0.6131\n",
      "Epoch 2932/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4783 - val_loss: 0.4118\n",
      "Epoch 2933/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2149 - val_loss: 0.1615\n",
      "Epoch 2934/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1658 - val_loss: 0.2224\n",
      "Epoch 2935/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1310 - val_loss: 0.0053\n",
      "Epoch 2936/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0567 - val_loss: 0.0721\n",
      "Epoch 2937/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0653 - val_loss: 0.0148\n",
      "Epoch 2938/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0418 - val_loss: 0.0081\n",
      "Epoch 2939/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0448 - val_loss: 0.0429\n",
      "Epoch 2940/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0342 - val_loss: 0.0174\n",
      "Epoch 2941/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0189 - val_loss: 0.0180\n",
      "Epoch 2942/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0176 - val_loss: 0.0051\n",
      "Epoch 2943/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0091 - val_loss: 0.0032\n",
      "Epoch 2944/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0069 - val_loss: 0.0040\n",
      "Epoch 2945/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0094 - val_loss: 0.0031\n",
      "Epoch 2946/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0059 - val_loss: 0.0034\n",
      "Epoch 2947/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0059 - val_loss: 0.0030\n",
      "Epoch 2948/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0053 - val_loss: 0.0038\n",
      "Epoch 2949/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0062 - val_loss: 0.0036\n",
      "Epoch 2950/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0096 - val_loss: 0.0243\n",
      "Epoch 2951/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0123 - val_loss: 0.0102\n",
      "Epoch 2952/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0133 - val_loss: 0.0057\n",
      "Epoch 2953/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0188 - val_loss: 0.0160\n",
      "Epoch 2954/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0139 - val_loss: 0.0136\n",
      "Epoch 2955/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0084 - val_loss: 0.0025\n",
      "Epoch 2956/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0050 - val_loss: 0.0110\n",
      "Epoch 2957/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0096 - val_loss: 0.0210\n",
      "Epoch 2958/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0166 - val_loss: 0.0189\n",
      "Epoch 2959/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0074 - val_loss: 0.0032\n",
      "Epoch 2960/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0079 - val_loss: 0.0121\n",
      "Epoch 2961/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0230 - val_loss: 0.0156\n",
      "Epoch 2962/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0125 - val_loss: 0.0263\n",
      "Epoch 2963/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0185 - val_loss: 0.0070\n",
      "Epoch 2964/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0121 - val_loss: 0.0202\n",
      "Epoch 2965/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0076 - val_loss: 0.0101\n",
      "Epoch 2966/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0086 - val_loss: 0.0027\n",
      "Epoch 2967/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 2968/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0071 - val_loss: 0.0044\n",
      "Epoch 2969/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0045 - val_loss: 0.0033\n",
      "Epoch 2970/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0041 - val_loss: 0.0226\n",
      "Epoch 2971/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0162 - val_loss: 0.0071\n",
      "Epoch 2972/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0106 - val_loss: 0.0241\n",
      "Epoch 2973/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0149 - val_loss: 0.0061\n",
      "Epoch 2974/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0061 - val_loss: 0.0084\n",
      "Epoch 2975/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0029 - val_loss: 0.0015\n",
      "Epoch 2976/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0020 - val_loss: 0.0258\n",
      "Epoch 2977/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0158 - val_loss: 0.0120\n",
      "Epoch 2978/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0308 - val_loss: 0.0539\n",
      "Epoch 2979/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0248 - val_loss: 0.0363\n",
      "Epoch 2980/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0184 - val_loss: 0.0155\n",
      "Epoch 2981/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0105 - val_loss: 0.0037\n",
      "Epoch 2982/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0053 - val_loss: 0.0015\n",
      "Epoch 2983/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0101 - val_loss: 0.0084\n",
      "Epoch 2984/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0104 - val_loss: 0.0320\n",
      "Epoch 2985/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0170 - val_loss: 0.0258\n",
      "Epoch 2986/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0249 - val_loss: 0.0223\n",
      "Epoch 2987/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0155 - val_loss: 0.0047\n",
      "Epoch 2988/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0056 - val_loss: 0.0040\n",
      "Epoch 2989/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0037 - val_loss: 0.0020\n",
      "Epoch 2990/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0033 - val_loss: 0.0016\n",
      "Epoch 2991/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 2992/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 2993/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 2994/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 0.0030\n",
      "Epoch 2995/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 2996/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0053 - val_loss: 0.0047\n",
      "Epoch 2997/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0041 - val_loss: 0.0013\n",
      "Epoch 2998/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0049 - val_loss: 0.0012\n",
      "Epoch 2999/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0036 - val_loss: 0.0014\n",
      "Epoch 3000/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0072 - val_loss: 0.0023\n",
      "Epoch 3001/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 3002/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0029 - val_loss: 9.7099e-04\n",
      "Epoch 3003/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 3004/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0015 - val_loss: 7.2802e-04\n",
      "Epoch 3005/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0100 - val_loss: 0.0021\n",
      "Epoch 3006/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0170 - val_loss: 0.0131\n",
      "Epoch 3007/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0521 - val_loss: 0.1636\n",
      "Epoch 3008/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1777 - val_loss: 0.3281\n",
      "Epoch 3009/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3041 - val_loss: 0.1133\n",
      "Epoch 3010/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2388 - val_loss: 0.0592\n",
      "Epoch 3011/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3146 - val_loss: 0.1372\n",
      "Epoch 3012/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.4066 - val_loss: 0.0485\n",
      "Epoch 3013/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0422 - val_loss: 0.0229\n",
      "Epoch 3014/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0730 - val_loss: 0.2059\n",
      "Epoch 3015/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0888 - val_loss: 0.1168\n",
      "Epoch 3016/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0882 - val_loss: 0.0307\n",
      "Epoch 3017/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0438 - val_loss: 0.0697\n",
      "Epoch 3018/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0793 - val_loss: 0.1179\n",
      "Epoch 3019/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.128 - 0s 147us/sample - loss: 0.2120 - val_loss: 0.1193\n",
      "Epoch 3020/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0746 - val_loss: 0.0145\n",
      "Epoch 3021/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0261 - val_loss: 0.0060\n",
      "Epoch 3022/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0105 - val_loss: 0.0137\n",
      "Epoch 3023/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0138 - val_loss: 0.0455\n",
      "Epoch 3024/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0135 - val_loss: 0.0057\n",
      "Epoch 3025/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0108 - val_loss: 0.0058\n",
      "Epoch 3026/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0096 - val_loss: 0.0125\n",
      "Epoch 3027/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0054 - val_loss: 0.0171\n",
      "Epoch 3028/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0166 - val_loss: 0.0180\n",
      "Epoch 3029/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0172 - val_loss: 0.0092\n",
      "Epoch 3030/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0327 - val_loss: 0.0079\n",
      "Epoch 3031/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0351 - val_loss: 0.0067\n",
      "Epoch 3032/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0119 - val_loss: 0.0061\n",
      "Epoch 3033/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0349 - val_loss: 0.0349\n",
      "Epoch 3034/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0402 - val_loss: 0.2802\n",
      "Epoch 3035/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2248 - val_loss: 0.0217\n",
      "Epoch 3036/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2534 - val_loss: 0.3999\n",
      "Epoch 3037/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2136 - val_loss: 0.1044\n",
      "Epoch 3038/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3292 - val_loss: 0.0200\n",
      "Epoch 3039/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4125 - val_loss: 0.8379\n",
      "Epoch 3040/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1594 - val_loss: 4.7313\n",
      "Epoch 3041/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 7.1073 - val_loss: 8.6825\n",
      "Epoch 3042/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 19.9605 - val_loss: 7.8525\n",
      "Epoch 3043/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.0387 - val_loss: 9.4534\n",
      "Epoch 3044/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 17.2210 - val_loss: 5.1272\n",
      "Epoch 3045/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 75.8121 - val_loss: 186.8159\n",
      "Epoch 3046/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 565.6627 - val_loss: 310.1637\n",
      "Epoch 3047/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 306.5241 - val_loss: 84.1186\n",
      "Epoch 3048/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 101.3632 - val_loss: 5.8521\n",
      "Epoch 3049/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 78.8410 - val_loss: 30.3942\n",
      "Epoch 3050/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 31.8257 - val_loss: 67.2724\n",
      "Epoch 3051/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 31.8862 - val_loss: 0.5253\n",
      "Epoch 3052/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 69.5584 - val_loss: 59.8903\n",
      "Epoch 3053/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 49.8693 - val_loss: 106.2417\n",
      "Epoch 3054/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 86.7861 - val_loss: 22.9876\n",
      "Epoch 3055/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 171.1816 - val_loss: 3.8633\n",
      "Epoch 3056/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 561.3128 - val_loss: 2240.5693\n",
      "Epoch 3057/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2229.4665 - val_loss: 2649.3235\n",
      "Epoch 3058/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1031.3394 - val_loss: 596.4801\n",
      "Epoch 3059/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 6053.5896 - val_loss: 6397.4323\n",
      "Epoch 3060/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7265.3347 - val_loss: 7807.1452\n",
      "Epoch 3061/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3710.3955 - val_loss: 3675.1504\n",
      "Epoch 3062/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2556.5666 - val_loss: 3410.6558\n",
      "Epoch 3063/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1588.0834 - val_loss: 1473.6857\n",
      "Epoch 3064/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1679.5157 - val_loss: 488.0034\n",
      "Epoch 3065/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 865.8147 - val_loss: 1369.6479\n",
      "Epoch 3066/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 523.1464 - val_loss: 169.0216\n",
      "Epoch 3067/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 120.6075 - val_loss: 268.1938\n",
      "Epoch 3068/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 198.8445 - val_loss: 524.7515\n",
      "Epoch 3069/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 193.8345 - val_loss: 187.7768\n",
      "Epoch 3070/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 78.9149 - val_loss: 55.4564\n",
      "Epoch 3071/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 41.9044 - val_loss: 67.8255\n",
      "Epoch 3072/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 49.2089 - val_loss: 32.0148\n",
      "Epoch 3073/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 23.6267 - val_loss: 25.3862\n",
      "Epoch 3074/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 13.0680 - val_loss: 19.1903\n",
      "Epoch 3075/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.9391 - val_loss: 12.2113\n",
      "Epoch 3076/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.1971 - val_loss: 19.3508\n",
      "Epoch 3077/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.8076 - val_loss: 6.5032\n",
      "Epoch 3078/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.4771 - val_loss: 1.5880\n",
      "Epoch 3079/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8175 - val_loss: 0.5033\n",
      "Epoch 3080/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4479 - val_loss: 0.4501\n",
      "Epoch 3081/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4195 - val_loss: 0.1447\n",
      "Epoch 3082/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2754 - val_loss: 0.2417\n",
      "Epoch 3083/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1777 - val_loss: 0.1067\n",
      "Epoch 3084/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1124 - val_loss: 0.1652\n",
      "Epoch 3085/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1667 - val_loss: 0.1843\n",
      "Epoch 3086/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1368 - val_loss: 0.0597\n",
      "Epoch 3087/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0637 - val_loss: 0.0382\n",
      "Epoch 3088/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0396 - val_loss: 0.0267\n",
      "Epoch 3089/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0344 - val_loss: 0.0221\n",
      "Epoch 3090/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0322 - val_loss: 0.0235\n",
      "Epoch 3091/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0295 - val_loss: 0.0273\n",
      "Epoch 3092/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0374 - val_loss: 0.0225\n",
      "Epoch 3093/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0382 - val_loss: 0.0186\n",
      "Epoch 3094/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0583 - val_loss: 0.0799\n",
      "Epoch 3095/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0417 - val_loss: 0.0238\n",
      "Epoch 3096/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0397 - val_loss: 0.0543\n",
      "Epoch 3097/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0341 - val_loss: 0.0628\n",
      "Epoch 3098/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0780 - val_loss: 0.0201\n",
      "Epoch 3099/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0520 - val_loss: 0.0717\n",
      "Epoch 3100/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0480 - val_loss: 0.0169\n",
      "Epoch 3101/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0247 - val_loss: 0.0272\n",
      "Epoch 3102/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0565 - val_loss: 0.0621\n",
      "Epoch 3103/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0636 - val_loss: 0.0187\n",
      "Epoch 3104/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0211 - val_loss: 0.0237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3105/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0330 - val_loss: 0.0648\n",
      "Epoch 3106/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0425 - val_loss: 0.0585\n",
      "Epoch 3107/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0797 - val_loss: 0.0500\n",
      "Epoch 3108/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2455 - val_loss: 0.4878\n",
      "Epoch 3109/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6380 - val_loss: 0.0258\n",
      "Epoch 3110/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.5377 - val_loss: 0.3242\n",
      "Epoch 3111/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.6453 - val_loss: 0.4724\n",
      "Epoch 3112/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1929 - val_loss: 0.1883\n",
      "Epoch 3113/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0951 - val_loss: 0.1201\n",
      "Epoch 3114/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0651 - val_loss: 0.0431\n",
      "Epoch 3115/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0814 - val_loss: 0.1273\n",
      "Epoch 3116/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0678 - val_loss: 0.0160\n",
      "Epoch 3117/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0431 - val_loss: 0.0311\n",
      "Epoch 3118/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0496 - val_loss: 0.0844\n",
      "Epoch 3119/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0665 - val_loss: 0.0850\n",
      "Epoch 3120/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0961 - val_loss: 0.1372\n",
      "Epoch 3121/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0891 - val_loss: 0.0706\n",
      "Epoch 3122/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0464 - val_loss: 0.0543\n",
      "Epoch 3123/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0242 - val_loss: 0.0289\n",
      "Epoch 3124/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0307 - val_loss: 0.0180\n",
      "Epoch 3125/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0140 - val_loss: 0.0210\n",
      "Epoch 3126/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0511 - val_loss: 0.0767\n",
      "Epoch 3127/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1043 - val_loss: 0.0312\n",
      "Epoch 3128/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2065 - val_loss: 0.2741\n",
      "Epoch 3129/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1979 - val_loss: 0.1759\n",
      "Epoch 3130/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2267 - val_loss: 0.6005\n",
      "Epoch 3131/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9739 - val_loss: 1.2694\n",
      "Epoch 3132/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.0859 - val_loss: 0.4172\n",
      "Epoch 3133/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.2964 - val_loss: 3.9596\n",
      "Epoch 3134/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.5276 - val_loss: 1.1551\n",
      "Epoch 3135/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.1057 - val_loss: 3.8524\n",
      "Epoch 3136/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6158 - val_loss: 0.8647\n",
      "Epoch 3137/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5563 - val_loss: 1.0678\n",
      "Epoch 3138/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6832 - val_loss: 0.4324\n",
      "Epoch 3139/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3353 - val_loss: 0.7410\n",
      "Epoch 3140/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4877 - val_loss: 0.2628\n",
      "Epoch 3141/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6737 - val_loss: 0.7582\n",
      "Epoch 3142/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4951 - val_loss: 0.0758\n",
      "Epoch 3143/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2015 - val_loss: 0.4171\n",
      "Epoch 3144/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6592 - val_loss: 0.4200\n",
      "Epoch 3145/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.5437 - val_loss: 0.2986\n",
      "Epoch 3146/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5016 - val_loss: 0.3397\n",
      "Epoch 3147/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2318 - val_loss: 0.0116\n",
      "Epoch 3148/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5342 - val_loss: 1.7546\n",
      "Epoch 3149/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3972 - val_loss: 0.9813\n",
      "Epoch 3150/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1267 - val_loss: 3.3327\n",
      "Epoch 3151/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2988 - val_loss: 2.6959\n",
      "Epoch 3152/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.9800 - val_loss: 0.3363\n",
      "Epoch 3153/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3046 - val_loss: 1.0670\n",
      "Epoch 3154/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7982 - val_loss: 1.0208\n",
      "Epoch 3155/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.8113 - val_loss: 0.6736\n",
      "Epoch 3156/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.4673 - val_loss: 0.0345\n",
      "Epoch 3157/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1160 - val_loss: 0.2134\n",
      "Epoch 3158/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2331 - val_loss: 0.4030\n",
      "Epoch 3159/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5488 - val_loss: 0.1550\n",
      "Epoch 3160/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6966 - val_loss: 0.8338\n",
      "Epoch 3161/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3673 - val_loss: 0.8091\n",
      "Epoch 3162/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4038 - val_loss: 0.0810\n",
      "Epoch 3163/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3758 - val_loss: 0.2331\n",
      "Epoch 3164/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4679 - val_loss: 0.6313\n",
      "Epoch 3165/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4324 - val_loss: 0.0670\n",
      "Epoch 3166/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1070 - val_loss: 0.0680\n",
      "Epoch 3167/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0588 - val_loss: 0.0691\n",
      "Epoch 3168/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0586 - val_loss: 0.0311\n",
      "Epoch 3169/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0245 - val_loss: 0.0064\n",
      "Epoch 3170/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0163 - val_loss: 0.0193\n",
      "Epoch 3171/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0176 - val_loss: 0.0142\n",
      "Epoch 3172/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0110 - val_loss: 0.0430\n",
      "Epoch 3173/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0363 - val_loss: 0.0091\n",
      "Epoch 3174/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0329 - val_loss: 0.0331\n",
      "Epoch 3175/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0287 - val_loss: 0.0124\n",
      "Epoch 3176/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0092 - val_loss: 0.0047\n",
      "Epoch 3177/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0051 - val_loss: 0.0064\n",
      "Epoch 3178/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0052 - val_loss: 0.0060\n",
      "Epoch 3179/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0041 - val_loss: 0.0061\n",
      "Epoch 3180/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0032 - val_loss: 0.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3181/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0031 - val_loss: 0.0128\n",
      "Epoch 3182/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0059 - val_loss: 0.0040\n",
      "Epoch 3183/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0031 - val_loss: 0.0050\n",
      "Epoch 3184/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0035 - val_loss: 0.0058\n",
      "Epoch 3185/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 3186/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0036 - val_loss: 0.0032\n",
      "Epoch 3187/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0027 - val_loss: 0.0038\n",
      "Epoch 3188/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 3189/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0038 - val_loss: 0.0077\n",
      "Epoch 3190/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0038 - val_loss: 0.0072\n",
      "Epoch 3191/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0112 - val_loss: 0.0070\n",
      "Epoch 3192/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0069 - val_loss: 0.0093\n",
      "Epoch 3193/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0236 - val_loss: 0.0599\n",
      "Epoch 3194/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0331 - val_loss: 0.0563\n",
      "Epoch 3195/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0744 - val_loss: 0.2284\n",
      "Epoch 3196/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0887 - val_loss: 0.0498\n",
      "Epoch 3197/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0475 - val_loss: 0.0228\n",
      "Epoch 3198/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0174 - val_loss: 0.0482\n",
      "Epoch 3199/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0220 - val_loss: 0.0743\n",
      "Epoch 3200/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0500 - val_loss: 0.0049\n",
      "Epoch 3201/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0327 - val_loss: 0.0206\n",
      "Epoch 3202/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0361 - val_loss: 0.0817\n",
      "Epoch 3203/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0902 - val_loss: 0.0757\n",
      "Epoch 3204/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0430 - val_loss: 0.0427\n",
      "Epoch 3205/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0820 - val_loss: 0.0832\n",
      "Epoch 3206/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0792 - val_loss: 0.0427\n",
      "Epoch 3207/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0530 - val_loss: 0.0439\n",
      "Epoch 3208/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0329 - val_loss: 0.0799\n",
      "Epoch 3209/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0548 - val_loss: 0.1417\n",
      "Epoch 3210/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0789 - val_loss: 0.0313\n",
      "Epoch 3211/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1330 - val_loss: 0.5352\n",
      "Epoch 3212/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2238 - val_loss: 0.9094\n",
      "Epoch 3213/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.7860 - val_loss: 13.8396\n",
      "Epoch 3214/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 31.6427 - val_loss: 14.5316\n",
      "Epoch 3215/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 83.3729 - val_loss: 16.7287\n",
      "Epoch 3216/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 200.4501 - val_loss: 654.7136\n",
      "Epoch 3217/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 950.1712 - val_loss: 672.7642\n",
      "Epoch 3218/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1025.0885 - val_loss: 69.1344\n",
      "Epoch 3219/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 440.9323 - val_loss: 210.5730\n",
      "Epoch 3220/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 344.9521 - val_loss: 390.6034\n",
      "Epoch 3221/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 2571.8862 - val_loss: 2401.0218\n",
      "Epoch 3222/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1536.5381 - val_loss: 1017.8940\n",
      "Epoch 3223/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 431.3060 - val_loss: 146.2431\n",
      "Epoch 3224/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 173.0913 - val_loss: 106.5974\n",
      "Epoch 3225/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 183.1449 - val_loss: 849.6977\n",
      "Epoch 3226/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 560.7849 - val_loss: 4.1068\n",
      "Epoch 3227/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 855.4396 - val_loss: 594.8898\n",
      "Epoch 3228/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 588.0508 - val_loss: 426.1111\n",
      "Epoch 3229/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 658.1551 - val_loss: 1028.3866\n",
      "Epoch 3230/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 544.4565 - val_loss: 60.5260\n",
      "Epoch 3231/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 110.3624 - val_loss: 42.1352\n",
      "Epoch 3232/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 51.5082 - val_loss: 21.0209\n",
      "Epoch 3233/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 44.5255 - val_loss: 25.1216\n",
      "Epoch 3234/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11.1188 - val_loss: 8.1474\n",
      "Epoch 3235/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.6877 - val_loss: 14.6795\n",
      "Epoch 3236/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 17.20 - 0s 132us/sample - loss: 11.7285 - val_loss: 12.1881\n",
      "Epoch 3237/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16.0602 - val_loss: 3.5569\n",
      "Epoch 3238/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 25.2105 - val_loss: 50.8169\n",
      "Epoch 3239/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 39.0376 - val_loss: 35.7683\n",
      "Epoch 3240/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 33.1879 - val_loss: 11.7240\n",
      "Epoch 3241/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19.4986 - val_loss: 4.3821\n",
      "Epoch 3242/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26.8495 - val_loss: 22.0421\n",
      "Epoch 3243/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 16.1290 - val_loss: 4.7591\n",
      "Epoch 3244/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9.2954 - val_loss: 11.1705\n",
      "Epoch 3245/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.9344 - val_loss: 0.7681\n",
      "Epoch 3246/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.4218 - val_loss: 0.6211\n",
      "Epoch 3247/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.9437 - val_loss: 9.3613\n",
      "Epoch 3248/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 23.3428 - val_loss: 27.7714\n",
      "Epoch 3249/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 12.2370 - val_loss: 23.2630\n",
      "Epoch 3250/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 32.9630 - val_loss: 43.3512\n",
      "Epoch 3251/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 33.4131 - val_loss: 20.4837\n",
      "Epoch 3252/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 46.1673 - val_loss: 66.5643\n",
      "Epoch 3253/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 33.2409 - val_loss: 3.9646\n",
      "Epoch 3254/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 27.8718 - val_loss: 74.0002\n",
      "Epoch 3255/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 46.8739 - val_loss: 64.7589\n",
      "Epoch 3256/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 58.4654 - val_loss: 2.5407\n",
      "Epoch 3257/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 13.8609 - val_loss: 4.6930\n",
      "Epoch 3258/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.5135 - val_loss: 13.1505\n",
      "Epoch 3259/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.6985 - val_loss: 0.8464\n",
      "Epoch 3260/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2944 - val_loss: 4.1055\n",
      "Epoch 3261/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.5777 - val_loss: 4.0439\n",
      "Epoch 3262/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.3518 - val_loss: 0.0155\n",
      "Epoch 3263/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6895 - val_loss: 1.5489\n",
      "Epoch 3264/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4463 - val_loss: 1.5701\n",
      "Epoch 3265/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5336 - val_loss: 0.6379\n",
      "Epoch 3266/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3035 - val_loss: 0.1702\n",
      "Epoch 3267/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1426 - val_loss: 0.0407\n",
      "Epoch 3268/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1534 - val_loss: 0.1774\n",
      "Epoch 3269/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0653 - val_loss: 0.0532\n",
      "Epoch 3270/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0600 - val_loss: 0.1088\n",
      "Epoch 3271/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0545 - val_loss: 0.0177\n",
      "Epoch 3272/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0142 - val_loss: 0.0236\n",
      "Epoch 3273/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0123 - val_loss: 0.0103\n",
      "Epoch 3274/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0295 - val_loss: 0.0618\n",
      "Epoch 3275/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0333 - val_loss: 0.0216\n",
      "Epoch 3276/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0182 - val_loss: 0.0121\n",
      "Epoch 3277/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0116 - val_loss: 0.0109\n",
      "Epoch 3278/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0090 - val_loss: 0.0093\n",
      "Epoch 3279/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0088 - val_loss: 0.0079\n",
      "Epoch 3280/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0132 - val_loss: 0.0166\n",
      "Epoch 3281/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0135 - val_loss: 0.0610\n",
      "Epoch 3282/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0437 - val_loss: 0.0593\n",
      "Epoch 3283/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0269 - val_loss: 0.1023\n",
      "Epoch 3284/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1350 - val_loss: 0.5975\n",
      "Epoch 3285/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4384 - val_loss: 0.0736\n",
      "Epoch 3286/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1154 - val_loss: 0.3474\n",
      "Epoch 3287/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2675 - val_loss: 0.5094\n",
      "Epoch 3288/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5229 - val_loss: 0.7797\n",
      "Epoch 3289/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4421 - val_loss: 0.7235\n",
      "Epoch 3290/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4443 - val_loss: 0.0799\n",
      "Epoch 3291/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1101 - val_loss: 0.0837\n",
      "Epoch 3292/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2110 - val_loss: 0.2158\n",
      "Epoch 3293/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1419 - val_loss: 0.4645\n",
      "Epoch 3294/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2199 - val_loss: 0.1691\n",
      "Epoch 3295/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0989 - val_loss: 0.0712\n",
      "Epoch 3296/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0459 - val_loss: 0.0207\n",
      "Epoch 3297/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0198 - val_loss: 0.0120\n",
      "Epoch 3298/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0159 - val_loss: 0.0376\n",
      "Epoch 3299/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0364 - val_loss: 0.0268\n",
      "Epoch 3300/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0482 - val_loss: 0.0185\n",
      "Epoch 3301/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0259 - val_loss: 0.0082\n",
      "Epoch 3302/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0074 - val_loss: 0.0079\n",
      "Epoch 3303/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0093 - val_loss: 0.0095\n",
      "Epoch 3304/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0146 - val_loss: 0.0379\n",
      "Epoch 3305/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.1170 - val_loss: 1.4318\n",
      "Epoch 3306/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.5210 - val_loss: 0.5171\n",
      "Epoch 3307/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1687 - val_loss: 0.1444\n",
      "Epoch 3308/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1505 - val_loss: 0.0721\n",
      "Epoch 3309/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0494 - val_loss: 0.0093\n",
      "Epoch 3310/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0086 - val_loss: 0.0128\n",
      "Epoch 3311/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0177 - val_loss: 0.0043\n",
      "Epoch 3312/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0508 - val_loss: 0.0108\n",
      "Epoch 3313/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0492 - val_loss: 0.0379\n",
      "Epoch 3314/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1218 - val_loss: 0.4240\n",
      "Epoch 3315/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3238 - val_loss: 0.2844\n",
      "Epoch 3316/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3028 - val_loss: 0.3732\n",
      "Epoch 3317/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3217 - val_loss: 0.3260\n",
      "Epoch 3318/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.5010 - val_loss: 0.2360\n",
      "Epoch 3319/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2930 - val_loss: 0.4595\n",
      "Epoch 3320/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4251 - val_loss: 0.0355\n",
      "Epoch 3321/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0960 - val_loss: 0.0625\n",
      "Epoch 3322/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1721 - val_loss: 0.1912\n",
      "Epoch 3323/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2577 - val_loss: 0.0190\n",
      "Epoch 3324/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1228 - val_loss: 0.0840\n",
      "Epoch 3325/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.115 - 0s 132us/sample - loss: 0.0520 - val_loss: 0.0185\n",
      "Epoch 3326/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0560 - val_loss: 0.1338\n",
      "Epoch 3327/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7602 - val_loss: 2.0682\n",
      "Epoch 3328/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.0986 - val_loss: 2.7533\n",
      "Epoch 3329/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 6.2248 - val_loss: 6.1143\n",
      "Epoch 3330/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 206us/sample - loss: 15.1371 - val_loss: 113.2452\n",
      "Epoch 3331/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 89.2385 - val_loss: 37.9075\n",
      "Epoch 3332/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 249.7021 - val_loss: 349.0422\n",
      "Epoch 3333/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 182.2557 - val_loss: 1334.1885\n",
      "Epoch 3334/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 382.2763 - val_loss: 87.4220\n",
      "Epoch 3335/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 143.7422 - val_loss: 44.5926\n",
      "Epoch 3336/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 98.6466 - val_loss: 374.6494\n",
      "Epoch 3337/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 151.5895 - val_loss: 248.6874\n",
      "Epoch 3338/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 119.9809 - val_loss: 96.9573\n",
      "Epoch 3339/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 50.5899 - val_loss: 4.4914\n",
      "Epoch 3340/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 36.1884 - val_loss: 56.3205\n",
      "Epoch 3341/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 71.5333 - val_loss: 109.8594\n",
      "Epoch 3342/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 160.2535 - val_loss: 404.1151\n",
      "Epoch 3343/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 147.8288 - val_loss: 2.9867\n",
      "Epoch 3344/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 30.9391 - val_loss: 72.4876\n",
      "Epoch 3345/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 207.5225 - val_loss: 112.6093\n",
      "Epoch 3346/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 382.2620 - val_loss: 676.4951\n",
      "Epoch 3347/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 712.9062 - val_loss: 3976.9748\n",
      "Epoch 3348/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1550.5948 - val_loss: 1222.0049\n",
      "Epoch 3349/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 793.7748 - val_loss: 271.6542\n",
      "Epoch 3350/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2890.0779 - val_loss: 1249.5257\n",
      "Epoch 3351/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5260.4513 - val_loss: 9933.0177\n",
      "Epoch 3352/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4538.0777 - val_loss: 1770.1699\n",
      "Epoch 3353/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6529.9082 - val_loss: 9642.2631\n",
      "Epoch 3354/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6317.8660 - val_loss: 13991.9054\n",
      "Epoch 3355/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12564.1184 - val_loss: 2477.5740\n",
      "Epoch 3356/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14200.7958 - val_loss: 35159.3058\n",
      "Epoch 3357/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 20874.2392 - val_loss: 5251.8536\n",
      "Epoch 3358/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16395.5551 - val_loss: 11182.9914\n",
      "Epoch 3359/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4240.2220 - val_loss: 2110.9818\n",
      "Epoch 3360/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1174.9143 - val_loss: 662.4709\n",
      "Epoch 3361/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 793.1902 - val_loss: 256.0246\n",
      "Epoch 3362/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 673.2740 - val_loss: 252.9280\n",
      "Epoch 3363/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 407.9913 - val_loss: 290.1797\n",
      "Epoch 3364/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 819.8028 - val_loss: 2198.7941\n",
      "Epoch 3365/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 876.5621 - val_loss: 541.9567\n",
      "Epoch 3366/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 361.3593 - val_loss: 34.1226\n",
      "Epoch 3367/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 354.8254 - val_loss: 250.5060\n",
      "Epoch 3368/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 154.5207 - val_loss: 124.6684\n",
      "Epoch 3369/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 116.5114 - val_loss: 13.6849\n",
      "Epoch 3370/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 74.0170 - val_loss: 101.9921\n",
      "Epoch 3371/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 70.7728 - val_loss: 59.5174\n",
      "Epoch 3372/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 35.9195 - val_loss: 0.8019\n",
      "Epoch 3373/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.9776 - val_loss: 20.7272\n",
      "Epoch 3374/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17.1134 - val_loss: 5.5559\n",
      "Epoch 3375/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 11.7293 - val_loss: 2.2327\n",
      "Epoch 3376/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.1642 - val_loss: 4.9145\n",
      "Epoch 3377/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.2419 - val_loss: 0.2705\n",
      "Epoch 3378/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1108 - val_loss: 0.2923\n",
      "Epoch 3379/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8850 - val_loss: 0.2617\n",
      "Epoch 3380/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0751 - val_loss: 1.0776\n",
      "Epoch 3381/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2563 - val_loss: 0.3084\n",
      "Epoch 3382/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2085 - val_loss: 0.5333\n",
      "Epoch 3383/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0499 - val_loss: 0.5051\n",
      "Epoch 3384/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6761 - val_loss: 0.2260\n",
      "Epoch 3385/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6302 - val_loss: 0.3958\n",
      "Epoch 3386/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6302 - val_loss: 0.2474\n",
      "Epoch 3387/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5001 - val_loss: 0.1841\n",
      "Epoch 3388/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5627 - val_loss: 0.2527\n",
      "Epoch 3389/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4491 - val_loss: 0.1641\n",
      "Epoch 3390/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4182 - val_loss: 0.3128\n",
      "Epoch 3391/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4210 - val_loss: 0.1265\n",
      "Epoch 3392/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3521 - val_loss: 0.1320\n",
      "Epoch 3393/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3440 - val_loss: 0.2255\n",
      "Epoch 3394/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3085 - val_loss: 0.1266\n",
      "Epoch 3395/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.3453 - val_loss: 0.2006\n",
      "Epoch 3396/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3499 - val_loss: 0.1828\n",
      "Epoch 3397/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3002 - val_loss: 0.1312\n",
      "Epoch 3398/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2585 - val_loss: 0.1937\n",
      "Epoch 3399/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2854 - val_loss: 0.2004\n",
      "Epoch 3400/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2654 - val_loss: 0.1420\n",
      "Epoch 3401/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.2300 - val_loss: 0.1287\n",
      "Epoch 3402/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.2069 - val_loss: 0.1110\n",
      "Epoch 3403/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2063 - val_loss: 0.2156\n",
      "Epoch 3404/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.2609 - val_loss: 0.2600\n",
      "Epoch 3405/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.3035 - val_loss: 0.2634\n",
      "Epoch 3406/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2188 - val_loss: 0.1801\n",
      "Epoch 3407/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2537 - val_loss: 0.1189\n",
      "Epoch 3408/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2490 - val_loss: 0.2051\n",
      "Epoch 3409/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1903 - val_loss: 0.1273\n",
      "Epoch 3410/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1999 - val_loss: 0.1093\n",
      "Epoch 3411/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1763 - val_loss: 0.2235\n",
      "Epoch 3412/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2327 - val_loss: 0.1470\n",
      "Epoch 3413/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2290 - val_loss: 0.2596\n",
      "Epoch 3414/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2194 - val_loss: 0.1239\n",
      "Epoch 3415/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1616 - val_loss: 0.1679\n",
      "Epoch 3416/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1711 - val_loss: 0.3010\n",
      "Epoch 3417/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3584 - val_loss: 0.0938\n",
      "Epoch 3418/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2754 - val_loss: 0.2759\n",
      "Epoch 3419/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2787 - val_loss: 0.1635\n",
      "Epoch 3420/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1314 - val_loss: 0.1208\n",
      "Epoch 3421/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1383 - val_loss: 0.1325\n",
      "Epoch 3422/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1171 - val_loss: 0.1142\n",
      "Epoch 3423/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1262 - val_loss: 0.1395\n",
      "Epoch 3424/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1428 - val_loss: 0.1322\n",
      "Epoch 3425/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0972 - val_loss: 0.0836\n",
      "Epoch 3426/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0901 - val_loss: 0.0787\n",
      "Epoch 3427/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1056 - val_loss: 0.1032\n",
      "Epoch 3428/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1293 - val_loss: 0.1177\n",
      "Epoch 3429/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1336 - val_loss: 0.1000\n",
      "Epoch 3430/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1066 - val_loss: 0.1037\n",
      "Epoch 3431/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0870 - val_loss: 0.1282\n",
      "Epoch 3432/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1047 - val_loss: 0.0792\n",
      "Epoch 3433/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1078 - val_loss: 0.1570\n",
      "Epoch 3434/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1283 - val_loss: 0.1493\n",
      "Epoch 3435/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1238 - val_loss: 0.1898\n",
      "Epoch 3436/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1687 - val_loss: 0.0870\n",
      "Epoch 3437/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1040 - val_loss: 0.1080\n",
      "Epoch 3438/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1012 - val_loss: 0.1249\n",
      "Epoch 3439/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1436 - val_loss: 0.1713\n",
      "Epoch 3440/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2360 - val_loss: 0.3129\n",
      "Epoch 3441/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3249 - val_loss: 0.0738\n",
      "Epoch 3442/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1360 - val_loss: 0.1121\n",
      "Epoch 3443/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1373 - val_loss: 0.2975\n",
      "Epoch 3444/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2202 - val_loss: 0.0788\n",
      "Epoch 3445/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1121 - val_loss: 0.0629\n",
      "Epoch 3446/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0697 - val_loss: 0.0631\n",
      "Epoch 3447/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0700 - val_loss: 0.0618\n",
      "Epoch 3448/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0625 - val_loss: 0.0636\n",
      "Epoch 3449/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0635 - val_loss: 0.0612\n",
      "Epoch 3450/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0766 - val_loss: 0.1096\n",
      "Epoch 3451/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0785 - val_loss: 0.0678\n",
      "Epoch 3452/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0618 - val_loss: 0.0576\n",
      "Epoch 3453/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0896 - val_loss: 0.0853\n",
      "Epoch 3454/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1016 - val_loss: 0.1628\n",
      "Epoch 3455/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1254 - val_loss: 0.0762\n",
      "Epoch 3456/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0747 - val_loss: 0.0549\n",
      "Epoch 3457/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0495 - val_loss: 0.0548\n",
      "Epoch 3458/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0620 - val_loss: 0.1048\n",
      "Epoch 3459/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0795 - val_loss: 0.1404\n",
      "Epoch 3460/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1013 - val_loss: 0.0887\n",
      "Epoch 3461/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2955 - val_loss: 0.5597\n",
      "Epoch 3462/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6074 - val_loss: 0.7051\n",
      "Epoch 3463/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7700 - val_loss: 0.0488\n",
      "Epoch 3464/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3937 - val_loss: 0.0667\n",
      "Epoch 3465/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1431 - val_loss: 0.0447\n",
      "Epoch 3466/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1162 - val_loss: 0.1935\n",
      "Epoch 3467/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1288 - val_loss: 0.1132\n",
      "Epoch 3468/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0684 - val_loss: 0.0511\n",
      "Epoch 3469/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0665 - val_loss: 0.0442\n",
      "Epoch 3470/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0691 - val_loss: 0.0835\n",
      "Epoch 3471/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0759 - val_loss: 0.1099\n",
      "Epoch 3472/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1869 - val_loss: 0.1060\n",
      "Epoch 3473/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2701 - val_loss: 0.1216\n",
      "Epoch 3474/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1799 - val_loss: 0.1302\n",
      "Epoch 3475/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1002 - val_loss: 0.1095\n",
      "Epoch 3476/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0900 - val_loss: 0.0600\n",
      "Epoch 3477/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1964 - val_loss: 0.0383\n",
      "Epoch 3478/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1345 - val_loss: 0.0534\n",
      "Epoch 3479/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0968 - val_loss: 0.0720\n",
      "Epoch 3480/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0686 - val_loss: 0.1130\n",
      "Epoch 3481/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0783 - val_loss: 0.1378\n",
      "Epoch 3482/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1226 - val_loss: 0.3217\n",
      "Epoch 3483/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2605 - val_loss: 0.1481\n",
      "Epoch 3484/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1439 - val_loss: 0.0717\n",
      "Epoch 3485/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0694 - val_loss: 0.0362\n",
      "Epoch 3486/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0701 - val_loss: 0.0436\n",
      "Epoch 3487/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0491 - val_loss: 0.0320\n",
      "Epoch 3488/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0361 - val_loss: 0.0332\n",
      "Epoch 3489/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0534 - val_loss: 0.0447\n",
      "Epoch 3490/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0394 - val_loss: 0.0340\n",
      "Epoch 3491/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0498 - val_loss: 0.0392\n",
      "Epoch 3492/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0388 - val_loss: 0.0282\n",
      "Epoch 3493/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0317 - val_loss: 0.0364\n",
      "Epoch 3494/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0324 - val_loss: 0.0345\n",
      "Epoch 3495/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0316 - val_loss: 0.0530\n",
      "Epoch 3496/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0287 - val_loss: 0.0333\n",
      "Epoch 3497/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0382 - val_loss: 0.0268\n",
      "Epoch 3498/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0366 - val_loss: 0.0375\n",
      "Epoch 3499/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0477 - val_loss: 0.0307\n",
      "Epoch 3500/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0466 - val_loss: 0.0319\n",
      "Epoch 3501/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0719 - val_loss: 0.0274\n",
      "Epoch 3502/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0953 - val_loss: 0.0300\n",
      "Epoch 3503/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0945 - val_loss: 0.1607\n",
      "Epoch 3504/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.168 - 0s 132us/sample - loss: 0.1979 - val_loss: 0.1619\n",
      "Epoch 3505/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3051 - val_loss: 0.6326\n",
      "Epoch 3506/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3438 - val_loss: 0.7428\n",
      "Epoch 3507/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4323 - val_loss: 0.3803\n",
      "Epoch 3508/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3344 - val_loss: 0.3511\n",
      "Epoch 3509/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1550 - val_loss: 0.0856\n",
      "Epoch 3510/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0720 - val_loss: 0.1383\n",
      "Epoch 3511/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0813 - val_loss: 0.0866\n",
      "Epoch 3512/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1171 - val_loss: 0.0480\n",
      "Epoch 3513/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0626 - val_loss: 0.0535\n",
      "Epoch 3514/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0435 - val_loss: 0.0232\n",
      "Epoch 3515/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0294 - val_loss: 0.0229\n",
      "Epoch 3516/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0230 - val_loss: 0.0182\n",
      "Epoch 3517/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0192 - val_loss: 0.0222\n",
      "Epoch 3518/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0347 - val_loss: 0.0448\n",
      "Epoch 3519/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0342 - val_loss: 0.0269\n",
      "Epoch 3520/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0297 - val_loss: 0.0174\n",
      "Epoch 3521/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0244 - val_loss: 0.0470\n",
      "Epoch 3522/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0410 - val_loss: 0.0195\n",
      "Epoch 3523/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0264 - val_loss: 0.0158\n",
      "Epoch 3524/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0296 - val_loss: 0.0308\n",
      "Epoch 3525/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0361 - val_loss: 0.0693\n",
      "Epoch 3526/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0763 - val_loss: 0.0458\n",
      "Epoch 3527/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0914 - val_loss: 0.0349\n",
      "Epoch 3528/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0615 - val_loss: 0.0552\n",
      "Epoch 3529/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0485 - val_loss: 0.0348\n",
      "Epoch 3530/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0285 - val_loss: 0.0289\n",
      "Epoch 3531/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0222 - val_loss: 0.0198\n",
      "Epoch 3532/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0536 - val_loss: 0.0380\n",
      "Epoch 3533/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0465 - val_loss: 0.0362\n",
      "Epoch 3534/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0265 - val_loss: 0.0182\n",
      "Epoch 3535/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0237 - val_loss: 0.0378\n",
      "Epoch 3536/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0600 - val_loss: 0.0675\n",
      "Epoch 3537/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0434 - val_loss: 0.0196\n",
      "Epoch 3538/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0515 - val_loss: 0.1098\n",
      "Epoch 3539/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0529 - val_loss: 0.0149\n",
      "Epoch 3540/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0136 - val_loss: 0.0212\n",
      "Epoch 3541/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0149 - val_loss: 0.0112\n",
      "Epoch 3542/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0110 - val_loss: 0.0131\n",
      "Epoch 3543/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0173 - val_loss: 0.0209\n",
      "Epoch 3544/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0124 - val_loss: 0.0121\n",
      "Epoch 3545/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0140 - val_loss: 0.0144\n",
      "Epoch 3546/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0152 - val_loss: 0.0121\n",
      "Epoch 3547/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0128 - val_loss: 0.0145\n",
      "Epoch 3548/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0139 - val_loss: 0.0159\n",
      "Epoch 3549/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0132 - val_loss: 0.0135\n",
      "Epoch 3550/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0228 - val_loss: 0.0347\n",
      "Epoch 3551/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0170 - val_loss: 0.0136\n",
      "Epoch 3552/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0126 - val_loss: 0.0121\n",
      "Epoch 3553/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0140 - val_loss: 0.0118\n",
      "Epoch 3554/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0163 - val_loss: 0.0101\n",
      "Epoch 3555/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0321 - val_loss: 0.0573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3556/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0320 - val_loss: 0.0182\n",
      "Epoch 3557/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0285 - val_loss: 0.0129\n",
      "Epoch 3558/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0170 - val_loss: 0.0114\n",
      "Epoch 3559/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0112 - val_loss: 0.0135\n",
      "Epoch 3560/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0124 - val_loss: 0.0116\n",
      "Epoch 3561/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0138 - val_loss: 0.0258\n",
      "Epoch 3562/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0165 - val_loss: 0.0161\n",
      "Epoch 3563/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0226 - val_loss: 0.0281\n",
      "Epoch 3564/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0259 - val_loss: 0.0188\n",
      "Epoch 3565/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0212 - val_loss: 0.0219\n",
      "Epoch 3566/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0369 - val_loss: 0.0182\n",
      "Epoch 3567/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0168 - val_loss: 0.0107\n",
      "Epoch 3568/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0080 - val_loss: 0.0086\n",
      "Epoch 3569/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0147 - val_loss: 0.0143\n",
      "Epoch 3570/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0076 - val_loss: 0.0062\n",
      "Epoch 3571/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0061 - val_loss: 0.0054\n",
      "Epoch 3572/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0065 - val_loss: 0.0067\n",
      "Epoch 3573/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0071 - val_loss: 0.0059\n",
      "Epoch 3574/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0064 - val_loss: 0.0120\n",
      "Epoch 3575/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0104 - val_loss: 0.0198\n",
      "Epoch 3576/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0150 - val_loss: 0.0099\n",
      "Epoch 3577/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0172 - val_loss: 0.0087\n",
      "Epoch 3578/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0285 - val_loss: 0.0061\n",
      "Epoch 3579/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0298 - val_loss: 0.0214\n",
      "Epoch 3580/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0175 - val_loss: 0.0110\n",
      "Epoch 3581/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0106 - val_loss: 0.0161\n",
      "Epoch 3582/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0297 - val_loss: 0.0172\n",
      "Epoch 3583/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0173 - val_loss: 0.0280\n",
      "Epoch 3584/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0320 - val_loss: 0.0073\n",
      "Epoch 3585/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0471 - val_loss: 0.0585\n",
      "Epoch 3586/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1159 - val_loss: 0.1748\n",
      "Epoch 3587/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1090 - val_loss: 0.0518\n",
      "Epoch 3588/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0448 - val_loss: 0.0441\n",
      "Epoch 3589/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0259 - val_loss: 0.0145\n",
      "Epoch 3590/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0503 - val_loss: 0.0057\n",
      "Epoch 3591/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0233 - val_loss: 0.0240\n",
      "Epoch 3592/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0280 - val_loss: 0.0149\n",
      "Epoch 3593/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0198 - val_loss: 0.0483\n",
      "Epoch 3594/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0246 - val_loss: 0.0080\n",
      "Epoch 3595/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0112 - val_loss: 0.0053\n",
      "Epoch 3596/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0091 - val_loss: 0.0186\n",
      "Epoch 3597/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0258 - val_loss: 0.0352\n",
      "Epoch 3598/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0928 - val_loss: 0.0168\n",
      "Epoch 3599/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0617 - val_loss: 0.0764\n",
      "Epoch 3600/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0442 - val_loss: 0.0222\n",
      "Epoch 3601/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0102 - val_loss: 0.0042\n",
      "Epoch 3602/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0056 - val_loss: 0.0064\n",
      "Epoch 3603/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0085 - val_loss: 0.0088\n",
      "Epoch 3604/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0085 - val_loss: 0.0050\n",
      "Epoch 3605/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0061 - val_loss: 0.0050\n",
      "Epoch 3606/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0043 - val_loss: 0.0028\n",
      "Epoch 3607/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0101 - val_loss: 0.0029\n",
      "Epoch 3608/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0163 - val_loss: 0.0148\n",
      "Epoch 3609/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0146 - val_loss: 0.0031\n",
      "Epoch 3610/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0045 - val_loss: 0.0022\n",
      "Epoch 3611/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0021 - val_loss: 0.0029\n",
      "Epoch 3612/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0038 - val_loss: 0.0085\n",
      "Epoch 3613/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0136 - val_loss: 0.0570\n",
      "Epoch 3614/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0335 - val_loss: 0.0339\n",
      "Epoch 3615/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0329 - val_loss: 0.0116\n",
      "Epoch 3616/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0602 - val_loss: 0.0022\n",
      "Epoch 3617/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0739 - val_loss: 0.0530\n",
      "Epoch 3618/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2418 - val_loss: 0.5301\n",
      "Epoch 3619/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.4323 - val_loss: 0.4519\n",
      "Epoch 3620/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2788 - val_loss: 1.3077\n",
      "Epoch 3621/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6281 - val_loss: 1.4145\n",
      "Epoch 3622/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6031 - val_loss: 0.5417\n",
      "Epoch 3623/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3979 - val_loss: 0.7964\n",
      "Epoch 3624/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3225 - val_loss: 0.8400\n",
      "Epoch 3625/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7876 - val_loss: 1.9791\n",
      "Epoch 3626/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.4380 - val_loss: 0.8846\n",
      "Epoch 3627/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.4429 - val_loss: 12.3162\n",
      "Epoch 3628/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 19.6146 - val_loss: 11.8040\n",
      "Epoch 3629/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.6007 - val_loss: 11.3635\n",
      "Epoch 3630/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.3046 - val_loss: 6.2669\n",
      "Epoch 3631/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 27.8823 - val_loss: 45.2853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3632/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 51.8783 - val_loss: 1.2579\n",
      "Epoch 3633/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 18.5076 - val_loss: 18.7936\n",
      "Epoch 3634/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.1829 - val_loss: 16.7347\n",
      "Epoch 3635/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.1180 - val_loss: 17.7577\n",
      "Epoch 3636/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 22.1179 - val_loss: 150.8205\n",
      "Epoch 3637/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 125.8557 - val_loss: 208.3861\n",
      "Epoch 3638/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 104.4954 - val_loss: 4.4798\n",
      "Epoch 3639/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.8077 - val_loss: 1.3964\n",
      "Epoch 3640/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.7947 - val_loss: 3.6240\n",
      "Epoch 3641/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2513 - val_loss: 0.1699\n",
      "Epoch 3642/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6125 - val_loss: 0.4691\n",
      "Epoch 3643/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5290 - val_loss: 0.4704\n",
      "Epoch 3644/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2670 - val_loss: 0.2179\n",
      "Epoch 3645/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9655 - val_loss: 0.0698\n",
      "Epoch 3646/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.2747 - val_loss: 2.6766\n",
      "Epoch 3647/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8011 - val_loss: 7.2907\n",
      "Epoch 3648/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.8818 - val_loss: 2.7697\n",
      "Epoch 3649/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2736 - val_loss: 0.0155\n",
      "Epoch 3650/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2022 - val_loss: 0.1622\n",
      "Epoch 3651/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1749 - val_loss: 0.3711\n",
      "Epoch 3652/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2741 - val_loss: 0.3974\n",
      "Epoch 3653/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5165 - val_loss: 1.6269\n",
      "Epoch 3654/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8991 - val_loss: 0.6287\n",
      "Epoch 3655/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2606 - val_loss: 0.3011\n",
      "Epoch 3656/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1768 - val_loss: 0.0637\n",
      "Epoch 3657/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0568 - val_loss: 0.0593\n",
      "Epoch 3658/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0679 - val_loss: 0.0650\n",
      "Epoch 3659/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1482 - val_loss: 0.1931\n",
      "Epoch 3660/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7527 - val_loss: 1.4343\n",
      "Epoch 3661/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.4915 - val_loss: 7.2871\n",
      "Epoch 3662/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.3021 - val_loss: 6.2755\n",
      "Epoch 3663/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.2658 - val_loss: 8.4222\n",
      "Epoch 3664/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.4988 - val_loss: 7.1193\n",
      "Epoch 3665/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.7434 - val_loss: 10.0019\n",
      "Epoch 3666/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.1716 - val_loss: 0.0235\n",
      "Epoch 3667/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.0115 - val_loss: 15.4854\n",
      "Epoch 3668/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 25.9920 - val_loss: 10.7393\n",
      "Epoch 3669/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 21.2946 - val_loss: 7.9780\n",
      "Epoch 3670/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.9198 - val_loss: 2.8461\n",
      "Epoch 3671/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.3970 - val_loss: 1.6710\n",
      "Epoch 3672/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2997 - val_loss: 0.3370\n",
      "Epoch 3673/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4372 - val_loss: 3.0240\n",
      "Epoch 3674/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.6888 - val_loss: 0.2228\n",
      "Epoch 3675/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.2017 - val_loss: 2.3557\n",
      "Epoch 3676/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1211 - val_loss: 4.5383\n",
      "Epoch 3677/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.7792 - val_loss: 7.0644\n",
      "Epoch 3678/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.6540 - val_loss: 2.9673\n",
      "Epoch 3679/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8747 - val_loss: 0.1937\n",
      "Epoch 3680/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0987 - val_loss: 0.1076\n",
      "Epoch 3681/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3277 - val_loss: 0.0050\n",
      "Epoch 3682/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.5252 - val_loss: 4.4940\n",
      "Epoch 3683/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.0713 - val_loss: 1.4867\n",
      "Epoch 3684/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1783 - val_loss: 0.6268\n",
      "Epoch 3685/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0556 - val_loss: 1.4197\n",
      "Epoch 3686/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.0668 - val_loss: 7.8537\n",
      "Epoch 3687/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1336 - val_loss: 0.4424\n",
      "Epoch 3688/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.7147 - val_loss: 5.0276\n",
      "Epoch 3689/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.0077 - val_loss: 27.4507\n",
      "Epoch 3690/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 23.3406 - val_loss: 1.0528\n",
      "Epoch 3691/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 59.3727 - val_loss: 88.3333\n",
      "Epoch 3692/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 225.9207 - val_loss: 754.0229\n",
      "Epoch 3693/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 503.8796 - val_loss: 39.1750\n",
      "Epoch 3694/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 689.2086 - val_loss: 1194.4040\n",
      "Epoch 3695/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5688.1851 - val_loss: 6349.5160\n",
      "Epoch 3696/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12040.0075 - val_loss: 9150.6137\n",
      "Epoch 3697/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9217.4163 - val_loss: 4221.2591\n",
      "Epoch 3698/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7834.7577 - val_loss: 476.1967\n",
      "Epoch 3699/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8053.9268 - val_loss: 703.8606\n",
      "Epoch 3700/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3529.8312 - val_loss: 2134.2741\n",
      "Epoch 3701/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1500.7655 - val_loss: 1935.6629\n",
      "Epoch 3702/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1256.9009 - val_loss: 827.1056\n",
      "Epoch 3703/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 889.9343 - val_loss: 26.0923\n",
      "Epoch 3704/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 470.0358 - val_loss: 579.5628\n",
      "Epoch 3705/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 306.4527 - val_loss: 233.7300\n",
      "Epoch 3706/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 187.3763 - val_loss: 57.7257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3707/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 69.2475 - val_loss: 75.3605\n",
      "Epoch 3708/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 50.4573 - val_loss: 39.1792\n",
      "Epoch 3709/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 50.5015 - val_loss: 13.7369\n",
      "Epoch 3710/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 42.3025 - val_loss: 37.9760\n",
      "Epoch 3711/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 23.1237 - val_loss: 24.5201\n",
      "Epoch 3712/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.6427 - val_loss: 3.3777\n",
      "Epoch 3713/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4189 - val_loss: 0.6460\n",
      "Epoch 3714/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8287 - val_loss: 2.0170\n",
      "Epoch 3715/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3660 - val_loss: 1.5034\n",
      "Epoch 3716/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2434 - val_loss: 0.3006\n",
      "Epoch 3717/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0812 - val_loss: 0.9182\n",
      "Epoch 3718/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5913 - val_loss: 0.6186\n",
      "Epoch 3719/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.5645 - val_loss: 0.2377\n",
      "Epoch 3720/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7274 - val_loss: 0.1482\n",
      "Epoch 3721/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3965 - val_loss: 0.0457\n",
      "Epoch 3722/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1510 - val_loss: 0.0334\n",
      "Epoch 3723/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1213 - val_loss: 0.1036\n",
      "Epoch 3724/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1605 - val_loss: 0.2339\n",
      "Epoch 3725/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3339 - val_loss: 0.2974\n",
      "Epoch 3726/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2409 - val_loss: 0.0284\n",
      "Epoch 3727/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1988 - val_loss: 0.0321\n",
      "Epoch 3728/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1124 - val_loss: 0.1076\n",
      "Epoch 3729/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0835 - val_loss: 0.0355\n",
      "Epoch 3730/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1020 - val_loss: 0.1117\n",
      "Epoch 3731/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0755 - val_loss: 0.0987\n",
      "Epoch 3732/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0778 - val_loss: 0.0263\n",
      "Epoch 3733/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0808 - val_loss: 0.0307\n",
      "Epoch 3734/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0573 - val_loss: 0.0385\n",
      "Epoch 3735/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0475 - val_loss: 0.0474\n",
      "Epoch 3736/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0647 - val_loss: 0.0305\n",
      "Epoch 3737/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0551 - val_loss: 0.0307\n",
      "Epoch 3738/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0392 - val_loss: 0.0246\n",
      "Epoch 3739/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0415 - val_loss: 0.0231\n",
      "Epoch 3740/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0437 - val_loss: 0.0374\n",
      "Epoch 3741/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0375 - val_loss: 0.0232\n",
      "Epoch 3742/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0600 - val_loss: 0.0289\n",
      "Epoch 3743/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0702 - val_loss: 0.0798\n",
      "Epoch 3744/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0565 - val_loss: 0.0526\n",
      "Epoch 3745/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0420 - val_loss: 0.0891\n",
      "Epoch 3746/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0724 - val_loss: 0.0391\n",
      "Epoch 3747/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0944 - val_loss: 0.0862\n",
      "Epoch 3748/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.085 - 0s 147us/sample - loss: 0.0557 - val_loss: 0.0358\n",
      "Epoch 3749/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0438 - val_loss: 0.0301\n",
      "Epoch 3750/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0340 - val_loss: 0.0879\n",
      "Epoch 3751/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0621 - val_loss: 0.0949\n",
      "Epoch 3752/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0394 - val_loss: 0.0261\n",
      "Epoch 3753/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0309 - val_loss: 0.0206\n",
      "Epoch 3754/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0353 - val_loss: 0.0293\n",
      "Epoch 3755/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0245 - val_loss: 0.0561\n",
      "Epoch 3756/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0279 - val_loss: 0.0238\n",
      "Epoch 3757/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0209 - val_loss: 0.0514\n",
      "Epoch 3758/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0273 - val_loss: 0.0260\n",
      "Epoch 3759/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0217 - val_loss: 0.0241\n",
      "Epoch 3760/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0200 - val_loss: 0.0180\n",
      "Epoch 3761/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0395 - val_loss: 0.0157\n",
      "Epoch 3762/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0322 - val_loss: 0.0544\n",
      "Epoch 3763/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0651 - val_loss: 0.0360\n",
      "Epoch 3764/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0242 - val_loss: 0.0211\n",
      "Epoch 3765/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0190 - val_loss: 0.0207\n",
      "Epoch 3766/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0167 - val_loss: 0.0292\n",
      "Epoch 3767/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0296 - val_loss: 0.0422\n",
      "Epoch 3768/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0343 - val_loss: 0.0177\n",
      "Epoch 3769/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0255 - val_loss: 0.0265\n",
      "Epoch 3770/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0717 - val_loss: 0.0868\n",
      "Epoch 3771/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0449 - val_loss: 0.0348\n",
      "Epoch 3772/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0200 - val_loss: 0.0136\n",
      "Epoch 3773/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0138 - val_loss: 0.0141\n",
      "Epoch 3774/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0225 - val_loss: 0.0322\n",
      "Epoch 3775/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0292 - val_loss: 0.0161\n",
      "Epoch 3776/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0195 - val_loss: 0.0120\n",
      "Epoch 3777/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0127 - val_loss: 0.0116\n",
      "Epoch 3778/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0124 - val_loss: 0.0342\n",
      "Epoch 3779/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0196 - val_loss: 0.0150\n",
      "Epoch 3780/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0142 - val_loss: 0.0114\n",
      "Epoch 3781/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0152 - val_loss: 0.0118\n",
      "Epoch 3782/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0127 - val_loss: 0.0110\n",
      "Epoch 3783/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0158 - val_loss: 0.0137\n",
      "Epoch 3784/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0290 - val_loss: 0.0208\n",
      "Epoch 3785/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0341 - val_loss: 0.0672\n",
      "Epoch 3786/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0518 - val_loss: 0.0752\n",
      "Epoch 3787/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0427 - val_loss: 0.0437\n",
      "Epoch 3788/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0240 - val_loss: 0.0295\n",
      "Epoch 3789/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.032 - 0s 132us/sample - loss: 0.0241 - val_loss: 0.0190\n",
      "Epoch 3790/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0142 - val_loss: 0.0157\n",
      "Epoch 3791/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0150 - val_loss: 0.0251\n",
      "Epoch 3792/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0458 - val_loss: 0.0783\n",
      "Epoch 3793/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0698 - val_loss: 0.0163\n",
      "Epoch 3794/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0573 - val_loss: 0.0362\n",
      "Epoch 3795/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0477 - val_loss: 0.0761\n",
      "Epoch 3796/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0527 - val_loss: 0.0317\n",
      "Epoch 3797/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0261 - val_loss: 0.0279\n",
      "Epoch 3798/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0354 - val_loss: 0.0177\n",
      "Epoch 3799/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0458 - val_loss: 0.0819\n",
      "Epoch 3800/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0535 - val_loss: 0.0076\n",
      "Epoch 3801/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0164 - val_loss: 0.0132\n",
      "Epoch 3802/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0118 - val_loss: 0.0277\n",
      "Epoch 3803/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0306 - val_loss: 0.0089\n",
      "Epoch 3804/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0173 - val_loss: 0.0124\n",
      "Epoch 3805/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0280 - val_loss: 0.1475\n",
      "Epoch 3806/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0840 - val_loss: 0.0300\n",
      "Epoch 3807/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0703 - val_loss: 0.0553\n",
      "Epoch 3808/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0564 - val_loss: 0.0131\n",
      "Epoch 3809/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0179 - val_loss: 0.0306\n",
      "Epoch 3810/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0323 - val_loss: 0.0251\n",
      "Epoch 3811/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0262 - val_loss: 0.0124\n",
      "Epoch 3812/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0183 - val_loss: 0.0071\n",
      "Epoch 3813/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0097 - val_loss: 0.0136\n",
      "Epoch 3814/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0095 - val_loss: 0.0080\n",
      "Epoch 3815/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 3816/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0144 - val_loss: 0.0059\n",
      "Epoch 3817/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0179 - val_loss: 0.0302\n",
      "Epoch 3818/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0283 - val_loss: 0.0130\n",
      "Epoch 3819/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0396 - val_loss: 0.0243\n",
      "Epoch 3820/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.038 - 0s 132us/sample - loss: 0.0224 - val_loss: 0.0289\n",
      "Epoch 3821/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0181 - val_loss: 0.0051\n",
      "Epoch 3822/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0118 - val_loss: 0.0069\n",
      "Epoch 3823/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0135 - val_loss: 0.0206\n",
      "Epoch 3824/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0085 - val_loss: 0.0091\n",
      "Epoch 3825/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0095 - val_loss: 0.0077\n",
      "Epoch 3826/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0127 - val_loss: 0.0052\n",
      "Epoch 3827/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0068 - val_loss: 0.0141\n",
      "Epoch 3828/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0084 - val_loss: 0.0065\n",
      "Epoch 3829/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0059 - val_loss: 0.0068\n",
      "Epoch 3830/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0085 - val_loss: 0.0074\n",
      "Epoch 3831/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0102 - val_loss: 0.0144\n",
      "Epoch 3832/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0130 - val_loss: 0.0365\n",
      "Epoch 3833/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0378 - val_loss: 0.0283\n",
      "Epoch 3834/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0222 - val_loss: 0.0137\n",
      "Epoch 3835/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0167 - val_loss: 0.0049\n",
      "Epoch 3836/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0082 - val_loss: 0.0058\n",
      "Epoch 3837/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0048 - val_loss: 0.0038\n",
      "Epoch 3838/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0050 - val_loss: 0.0092\n",
      "Epoch 3839/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0121 - val_loss: 0.0105\n",
      "Epoch 3840/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0116 - val_loss: 0.0088\n",
      "Epoch 3841/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0177 - val_loss: 0.0088\n",
      "Epoch 3842/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0441 - val_loss: 0.0517\n",
      "Epoch 3843/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0231 - val_loss: 0.0058\n",
      "Epoch 3844/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0293 - val_loss: 0.0544\n",
      "Epoch 3845/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0247 - val_loss: 0.0081\n",
      "Epoch 3846/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0094 - val_loss: 0.0182\n",
      "Epoch 3847/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0095 - val_loss: 0.0035\n",
      "Epoch 3848/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0088 - val_loss: 0.0168\n",
      "Epoch 3849/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0353 - val_loss: 0.0998\n",
      "Epoch 3850/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0768 - val_loss: 0.0685\n",
      "Epoch 3851/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1270 - val_loss: 0.1557\n",
      "Epoch 3852/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1535 - val_loss: 0.1167\n",
      "Epoch 3853/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0934 - val_loss: 0.0283\n",
      "Epoch 3854/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0804 - val_loss: 0.1221\n",
      "Epoch 3855/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0880 - val_loss: 0.1280\n",
      "Epoch 3856/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0632 - val_loss: 0.0434\n",
      "Epoch 3857/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0154 - val_loss: 0.0114\n",
      "Epoch 3858/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0055 - val_loss: 0.0021\n",
      "Epoch 3859/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0054 - val_loss: 0.0019\n",
      "Epoch 3860/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0040 - val_loss: 0.0111\n",
      "Epoch 3861/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0057 - val_loss: 0.0039\n",
      "Epoch 3862/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0098 - val_loss: 0.0024\n",
      "Epoch 3863/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0083 - val_loss: 0.0146\n",
      "Epoch 3864/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0056 - val_loss: 0.0075\n",
      "Epoch 3865/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0484 - val_loss: 0.1329\n",
      "Epoch 3866/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1198 - val_loss: 0.1269\n",
      "Epoch 3867/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1321 - val_loss: 0.0207\n",
      "Epoch 3868/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0997 - val_loss: 0.0246\n",
      "Epoch 3869/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0439 - val_loss: 0.0031\n",
      "Epoch 3870/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0121 - val_loss: 0.0123\n",
      "Epoch 3871/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0060 - val_loss: 0.0063\n",
      "Epoch 3872/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0030 - val_loss: 0.0014\n",
      "Epoch 3873/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0057 - val_loss: 0.0126\n",
      "Epoch 3874/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0052 - val_loss: 0.0037\n",
      "Epoch 3875/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0028 - val_loss: 0.0014\n",
      "Epoch 3876/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 3877/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0033 - val_loss: 0.0048\n",
      "Epoch 3878/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0046 - val_loss: 0.0024\n",
      "Epoch 3879/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 3880/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0032 - val_loss: 0.0086\n",
      "Epoch 3881/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0079 - val_loss: 0.0283\n",
      "Epoch 3882/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0091 - val_loss: 0.0057\n",
      "Epoch 3883/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 3884/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0027 - val_loss: 0.0018\n",
      "Epoch 3885/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0036 - val_loss: 0.0017\n",
      "Epoch 3886/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0029 - val_loss: 0.0131\n",
      "Epoch 3887/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0114 - val_loss: 0.0230\n",
      "Epoch 3888/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0111 - val_loss: 0.0097\n",
      "Epoch 3889/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0147 - val_loss: 0.0865\n",
      "Epoch 3890/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0434 - val_loss: 0.0315\n",
      "Epoch 3891/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0359 - val_loss: 0.1754\n",
      "Epoch 3892/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0755 - val_loss: 0.0584\n",
      "Epoch 3893/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0733 - val_loss: 0.0897\n",
      "Epoch 3894/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2006 - val_loss: 0.4462\n",
      "Epoch 3895/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3367 - val_loss: 0.2367\n",
      "Epoch 3896/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1014 - val_loss: 0.1946\n",
      "Epoch 3897/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1666 - val_loss: 0.0488\n",
      "Epoch 3898/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2539 - val_loss: 0.3451\n",
      "Epoch 3899/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5516 - val_loss: 0.2073\n",
      "Epoch 3900/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3092 - val_loss: 0.1189\n",
      "Epoch 3901/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3155 - val_loss: 0.3610\n",
      "Epoch 3902/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6080 - val_loss: 0.0544\n",
      "Epoch 3903/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2626 - val_loss: 0.4387\n",
      "Epoch 3904/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3661 - val_loss: 0.6349\n",
      "Epoch 3905/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1527 - val_loss: 6.8515\n",
      "Epoch 3906/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 13.5243 - val_loss: 17.4302\n",
      "Epoch 3907/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.2692 - val_loss: 1.2559\n",
      "Epoch 3908/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.5827 - val_loss: 1.5575\n",
      "Epoch 3909/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.6762 - val_loss: 0.2867\n",
      "Epoch 3910/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4068 - val_loss: 1.1694\n",
      "Epoch 3911/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.1736 - val_loss: 11.1088\n",
      "Epoch 3912/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.7060 - val_loss: 1.6866\n",
      "Epoch 3913/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3896 - val_loss: 0.1024\n",
      "Epoch 3914/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7568 - val_loss: 0.7365\n",
      "Epoch 3915/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5559 - val_loss: 1.4748\n",
      "Epoch 3916/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.2901 - val_loss: 1.9527\n",
      "Epoch 3917/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16.0268 - val_loss: 3.7111\n",
      "Epoch 3918/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 36.2992 - val_loss: 63.6659\n",
      "Epoch 3919/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 23.0820 - val_loss: 11.3394\n",
      "Epoch 3920/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15.2350 - val_loss: 12.7051\n",
      "Epoch 3921/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.9389 - val_loss: 2.8373\n",
      "Epoch 3922/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.6359 - val_loss: 3.9117\n",
      "Epoch 3923/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 67.8763 - val_loss: 37.8639\n",
      "Epoch 3924/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 141.2090 - val_loss: 205.9073\n",
      "Epoch 3925/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 200.2175 - val_loss: 63.7149\n",
      "Epoch 3926/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 267.8016 - val_loss: 131.6248\n",
      "Epoch 3927/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 141.5679 - val_loss: 64.5284\n",
      "Epoch 3928/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 462.0876 - val_loss: 890.0398\n",
      "Epoch 3929/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 897.0638 - val_loss: 244.2845\n",
      "Epoch 3930/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 301.799 - 0s 118us/sample - loss: 481.3264 - val_loss: 31.2330\n",
      "Epoch 3931/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 32.1865 - val_loss: 5.1846\n",
      "Epoch 3932/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 10.0585 - val_loss: 8.1409\n",
      "Epoch 3933/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 18.3247 - val_loss: 0.3180\n",
      "Epoch 3934/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9638 - val_loss: 0.2504\n",
      "Epoch 3935/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3965 - val_loss: 0.5216\n",
      "Epoch 3936/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9915 - val_loss: 1.0055\n",
      "Epoch 3937/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3.3190 - val_loss: 0.7364\n",
      "Epoch 3938/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.1814 - val_loss: 0.7693\n",
      "Epoch 3939/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8613 - val_loss: 0.0997\n",
      "Epoch 3940/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.8788 - val_loss: 0.5187\n",
      "Epoch 3941/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.0789 - val_loss: 6.3548\n",
      "Epoch 3942/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.8146 - val_loss: 3.5093\n",
      "Epoch 3943/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.9773 - val_loss: 11.7063\n",
      "Epoch 3944/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.5021 - val_loss: 5.2164\n",
      "Epoch 3945/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.9675 - val_loss: 1.5486\n",
      "Epoch 3946/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7190 - val_loss: 0.0638\n",
      "Epoch 3947/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2437 - val_loss: 0.3443\n",
      "Epoch 3948/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2152 - val_loss: 0.0422\n",
      "Epoch 3949/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2072 - val_loss: 0.2498\n",
      "Epoch 3950/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2370 - val_loss: 0.0607\n",
      "Epoch 3951/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1093 - val_loss: 0.2707\n",
      "Epoch 3952/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2135 - val_loss: 0.5183\n",
      "Epoch 3953/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9987 - val_loss: 0.0897\n",
      "Epoch 3954/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4697 - val_loss: 0.6427\n",
      "Epoch 3955/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4265 - val_loss: 0.8610\n",
      "Epoch 3956/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.7557 - val_loss: 7.6440\n",
      "Epoch 3957/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.1013 - val_loss: 31.3268\n",
      "Epoch 3958/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16.6374 - val_loss: 3.5743\n",
      "Epoch 3959/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.1101 - val_loss: 70.1352\n",
      "Epoch 3960/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 108.2223 - val_loss: 323.0239\n",
      "Epoch 3961/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 198.6180 - val_loss: 163.5224\n",
      "Epoch 3962/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 83.6379 - val_loss: 30.6690\n",
      "Epoch 3963/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 206.5995 - val_loss: 169.2725\n",
      "Epoch 3964/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 366.5888 - val_loss: 841.0217\n",
      "Epoch 3965/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1697.4132 - val_loss: 2719.1872\n",
      "Epoch 3966/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4169.0599 - val_loss: 6108.6606\n",
      "Epoch 3967/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4692.4975 - val_loss: 2869.9396\n",
      "Epoch 3968/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2350.8788 - val_loss: 2548.1247\n",
      "Epoch 3969/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4527.9577 - val_loss: 250.8177\n",
      "Epoch 3970/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1806.4775 - val_loss: 673.2041\n",
      "Epoch 3971/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 484.9184 - val_loss: 95.9590\n",
      "Epoch 3972/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 146.8767 - val_loss: 86.5286\n",
      "Epoch 3973/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 63.5687 - val_loss: 63.9550\n",
      "Epoch 3974/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 56.1898 - val_loss: 79.3495\n",
      "Epoch 3975/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 65.0438 - val_loss: 68.6094\n",
      "Epoch 3976/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 99.0836 - val_loss: 237.1552\n",
      "Epoch 3977/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 167.1223 - val_loss: 88.4100\n",
      "Epoch 3978/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 40.6293 - val_loss: 37.3316\n",
      "Epoch 3979/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 28.3617 - val_loss: 29.4107\n",
      "Epoch 3980/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 34.5489 - val_loss: 13.7594\n",
      "Epoch 3981/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 27.3542 - val_loss: 2.5613\n",
      "Epoch 3982/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.8649 - val_loss: 14.5254\n",
      "Epoch 3983/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 27.8564 - val_loss: 99.7304\n",
      "Epoch 3984/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 74.9146 - val_loss: 76.9629\n",
      "Epoch 3985/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 96.0861 - val_loss: 12.1231\n",
      "Epoch 3986/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 27.0707 - val_loss: 33.5904\n",
      "Epoch 3987/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 38.1362 - val_loss: 68.1330\n",
      "Epoch 3988/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 46.9487 - val_loss: 11.6860\n",
      "Epoch 3989/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17.1532 - val_loss: 55.7820\n",
      "Epoch 3990/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 77.5860 - val_loss: 19.2394\n",
      "Epoch 3991/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 103.4210 - val_loss: 121.8625\n",
      "Epoch 3992/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 57.4747 - val_loss: 87.6839\n",
      "Epoch 3993/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 191.7122 - val_loss: 131.4561\n",
      "Epoch 3994/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 154.1326 - val_loss: 35.1656\n",
      "Epoch 3995/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 66.9212 - val_loss: 99.1882\n",
      "Epoch 3996/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 55.0588 - val_loss: 17.1475\n",
      "Epoch 3997/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 27.6664 - val_loss: 7.5043\n",
      "Epoch 3998/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.0471 - val_loss: 6.2666\n",
      "Epoch 3999/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.2841 - val_loss: 0.8548\n",
      "Epoch 4000/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.1968 - val_loss: 0.0987\n",
      "Epoch 4001/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7006 - val_loss: 1.5976\n",
      "Epoch 4002/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7919 - val_loss: 0.1816\n",
      "Epoch 4003/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1966 - val_loss: 0.0876\n",
      "Epoch 4004/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0634 - val_loss: 0.0168\n",
      "Epoch 4005/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0270 - val_loss: 0.0157\n",
      "Epoch 4006/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0187 - val_loss: 0.0028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4007/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0148 - val_loss: 0.0040\n",
      "Epoch 4008/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0125 - val_loss: 0.0067\n",
      "Epoch 4009/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0083 - val_loss: 0.0036\n",
      "Epoch 4010/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0126 - val_loss: 0.0023\n",
      "Epoch 4011/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0117 - val_loss: 0.0050\n",
      "Epoch 4012/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0083 - val_loss: 0.0038\n",
      "Epoch 4013/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 4014/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0095 - val_loss: 0.0039\n",
      "Epoch 4015/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0044 - val_loss: 8.9307e-04\n",
      "Epoch 4016/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0029 - val_loss: 4.1381e-04\n",
      "Epoch 4017/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0025 - val_loss: 6.8460e-04\n",
      "Epoch 4018/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0026 - val_loss: 9.4201e-04\n",
      "Epoch 4019/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0021 - val_loss: 5.6609e-04\n",
      "Epoch 4020/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 4021/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0022 - val_loss: 3.1489e-04\n",
      "Epoch 4022/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0014 - val_loss: 6.7991e-04\n",
      "Epoch 4023/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 4024/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 4025/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0017 - val_loss: 6.9975e-04\n",
      "Epoch 4026/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0010 - val_loss: 0.0019\n",
      "Epoch 4027/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0030 - val_loss: 0.0020\n",
      "Epoch 4028/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0089 - val_loss: 0.0095\n",
      "Epoch 4029/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0318 - val_loss: 0.0536\n",
      "Epoch 4030/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0306 - val_loss: 0.0291\n",
      "Epoch 4031/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0403 - val_loss: 0.0028\n",
      "Epoch 4032/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0332 - val_loss: 0.0627\n",
      "Epoch 4033/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0485 - val_loss: 0.0117\n",
      "Epoch 4034/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0270 - val_loss: 0.0087\n",
      "Epoch 4035/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 4036/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0041 - val_loss: 6.2311e-04\n",
      "Epoch 4037/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 4038/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.6381e-04 - val_loss: 0.0026\n",
      "Epoch 4039/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0018 - val_loss: 5.1309e-04\n",
      "Epoch 4040/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.4409e-04 - val_loss: 0.0024\n",
      "Epoch 4041/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 4042/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0031 - val_loss: 7.9108e-04\n",
      "Epoch 4043/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0036 - val_loss: 0.0082\n",
      "Epoch 4044/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0083 - val_loss: 0.0149\n",
      "Epoch 4045/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0229 - val_loss: 0.0241\n",
      "Epoch 4046/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0135 - val_loss: 0.0253\n",
      "Epoch 4047/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0134 - val_loss: 0.0013\n",
      "Epoch 4048/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0090 - val_loss: 0.0067\n",
      "Epoch 4049/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0141 - val_loss: 0.0249\n",
      "Epoch 4050/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0158 - val_loss: 0.0350\n",
      "Epoch 4051/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0252 - val_loss: 0.0076\n",
      "Epoch 4052/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0092 - val_loss: 0.0164\n",
      "Epoch 4053/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0139 - val_loss: 0.0128\n",
      "Epoch 4054/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0094 - val_loss: 0.0070\n",
      "Epoch 4055/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0083 - val_loss: 0.0153\n",
      "Epoch 4056/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0273 - val_loss: 0.1477\n",
      "Epoch 4057/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0578 - val_loss: 0.0779\n",
      "Epoch 4058/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1303 - val_loss: 0.0177\n",
      "Epoch 4059/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0832 - val_loss: 0.0248\n",
      "Epoch 4060/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0150 - val_loss: 0.0181\n",
      "Epoch 4061/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0182 - val_loss: 0.0116\n",
      "Epoch 4062/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0085 - val_loss: 0.0028\n",
      "Epoch 4063/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0067 - val_loss: 0.0077\n",
      "Epoch 4064/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0079 - val_loss: 0.0013\n",
      "Epoch 4065/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0101 - val_loss: 0.0024\n",
      "Epoch 4066/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0033 - val_loss: 0.0071\n",
      "Epoch 4067/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0074 - val_loss: 0.0208\n",
      "Epoch 4068/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0137 - val_loss: 0.0011\n",
      "Epoch 4069/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0076 - val_loss: 0.0079\n",
      "Epoch 4070/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0145 - val_loss: 0.0166\n",
      "Epoch 4071/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0080 - val_loss: 0.0011\n",
      "Epoch 4072/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0091 - val_loss: 1.1200e-04\n",
      "Epoch 4073/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0054 - val_loss: 0.0246\n",
      "Epoch 4074/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0210 - val_loss: 0.0315\n",
      "Epoch 4075/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0448 - val_loss: 0.0769\n",
      "Epoch 4076/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1467 - val_loss: 0.2907\n",
      "Epoch 4077/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3501 - val_loss: 0.4046\n",
      "Epoch 4078/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3255 - val_loss: 0.2956\n",
      "Epoch 4079/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5918 - val_loss: 1.7432\n",
      "Epoch 4080/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.3616 - val_loss: 0.7197\n",
      "Epoch 4081/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4996 - val_loss: 0.1832\n",
      "Epoch 4082/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0629 - val_loss: 0.0348\n",
      "Epoch 4083/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0548 - val_loss: 0.0219\n",
      "Epoch 4084/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0115 - val_loss: 0.0032\n",
      "Epoch 4085/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0123 - val_loss: 0.0062\n",
      "Epoch 4086/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0076 - val_loss: 0.0159\n",
      "Epoch 4087/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0240 - val_loss: 0.0266\n",
      "Epoch 4088/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1234 - val_loss: 0.1263\n",
      "Epoch 4089/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1344 - val_loss: 0.0828\n",
      "Epoch 4090/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0642 - val_loss: 0.2978\n",
      "Epoch 4091/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6360 - val_loss: 1.3471\n",
      "Epoch 4092/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7656 - val_loss: 1.7930\n",
      "Epoch 4093/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.5511 - val_loss: 1.0259\n",
      "Epoch 4094/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.5111 - val_loss: 0.1557\n",
      "Epoch 4095/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1649 - val_loss: 0.0659\n",
      "Epoch 4096/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1295 - val_loss: 0.3075\n",
      "Epoch 4097/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1148 - val_loss: 0.1042\n",
      "Epoch 4098/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7395 - val_loss: 0.2802\n",
      "Epoch 4099/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4691 - val_loss: 0.1928\n",
      "Epoch 4100/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1032 - val_loss: 0.0270\n",
      "Epoch 4101/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0724 - val_loss: 0.1627\n",
      "Epoch 4102/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5074 - val_loss: 0.2077\n",
      "Epoch 4103/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1514 - val_loss: 0.0433\n",
      "Epoch 4104/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0238 - val_loss: 0.0154\n",
      "Epoch 4105/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0299 - val_loss: 0.0326\n",
      "Epoch 4106/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0203 - val_loss: 0.0429\n",
      "Epoch 4107/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0603 - val_loss: 0.0522\n",
      "Epoch 4108/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1909 - val_loss: 0.1627\n",
      "Epoch 4109/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1558 - val_loss: 0.1770\n",
      "Epoch 4110/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3618 - val_loss: 0.0653\n",
      "Epoch 4111/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1300 - val_loss: 0.0963\n",
      "Epoch 4112/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2392 - val_loss: 0.1205\n",
      "Epoch 4113/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0838 - val_loss: 0.2599\n",
      "Epoch 4114/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5109 - val_loss: 0.2666\n",
      "Epoch 4115/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.0677 - val_loss: 6.6473\n",
      "Epoch 4116/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.8290 - val_loss: 3.8674\n",
      "Epoch 4117/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.4755 - val_loss: 2.4008\n",
      "Epoch 4118/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5044 - val_loss: 0.1647\n",
      "Epoch 4119/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2689 - val_loss: 0.4551\n",
      "Epoch 4120/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3645 - val_loss: 0.0303\n",
      "Epoch 4121/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2574 - val_loss: 0.7606\n",
      "Epoch 4122/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4181 - val_loss: 0.4288\n",
      "Epoch 4123/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7025 - val_loss: 2.6749\n",
      "Epoch 4124/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8954 - val_loss: 0.2533\n",
      "Epoch 4125/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5459 - val_loss: 0.8478\n",
      "Epoch 4126/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.5177 - val_loss: 0.1996\n",
      "Epoch 4127/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4799 - val_loss: 0.2706\n",
      "Epoch 4128/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1427 - val_loss: 0.0986\n",
      "Epoch 4129/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2136 - val_loss: 0.7408\n",
      "Epoch 4130/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.5916 - val_loss: 8.6691\n",
      "Epoch 4131/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.7682 - val_loss: 6.6419\n",
      "Epoch 4132/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.3337 - val_loss: 0.6284\n",
      "Epoch 4133/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6039 - val_loss: 0.3869\n",
      "Epoch 4134/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4939 - val_loss: 0.0776\n",
      "Epoch 4135/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.8812 - val_loss: 13.4379\n",
      "Epoch 4136/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.7742 - val_loss: 14.9813\n",
      "Epoch 4137/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.4840 - val_loss: 24.1340\n",
      "Epoch 4138/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 179.6513 - val_loss: 24.5089\n",
      "Epoch 4139/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 72.8973 - val_loss: 96.2343\n",
      "Epoch 4140/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 49.5248 - val_loss: 149.2431\n",
      "Epoch 4141/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 165.3003 - val_loss: 592.3103\n",
      "Epoch 4142/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 973.2265 - val_loss: 748.0170\n",
      "Epoch 4143/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 894.2708 - val_loss: 176.6507\n",
      "Epoch 4144/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 542.0664 - val_loss: 1090.2932\n",
      "Epoch 4145/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1244.8322 - val_loss: 136.8268\n",
      "Epoch 4146/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 437.2738 - val_loss: 215.9336\n",
      "Epoch 4147/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 281.4112 - val_loss: 264.6082\n",
      "Epoch 4148/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 197.1979 - val_loss: 296.7962\n",
      "Epoch 4149/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 236.2665 - val_loss: 0.7486\n",
      "Epoch 4150/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 252.9010 - val_loss: 82.8582\n",
      "Epoch 4151/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 423.9979 - val_loss: 72.9254\n",
      "Epoch 4152/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1260.8361 - val_loss: 38.3575\n",
      "Epoch 4153/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 294.7514 - val_loss: 283.3803\n",
      "Epoch 4154/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 921.1901 - val_loss: 578.2216\n",
      "Epoch 4155/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 496.8158 - val_loss: 6.7046\n",
      "Epoch 4156/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 381.4256 - val_loss: 144.2914\n",
      "Epoch 4157/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 102.1443 - val_loss: 139.5811\n",
      "Epoch 4158/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 155.7319 - val_loss: 198.7309\n",
      "Epoch 4159/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 209.0799 - val_loss: 353.6131\n",
      "Epoch 4160/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 221.5074 - val_loss: 272.0248\n",
      "Epoch 4161/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 145.4186 - val_loss: 303.2147\n",
      "Epoch 4162/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 496.6357 - val_loss: 564.1103\n",
      "Epoch 4163/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 672.7624 - val_loss: 238.6976\n",
      "Epoch 4164/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 359.3382 - val_loss: 678.5587\n",
      "Epoch 4165/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 419.3081 - val_loss: 466.9415\n",
      "Epoch 4166/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 271.9448 - val_loss: 124.4924\n",
      "Epoch 4167/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 255.1553 - val_loss: 323.4018\n",
      "Epoch 4168/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 330.0569 - val_loss: 67.5714\n",
      "Epoch 4169/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 96.0986 - val_loss: 9.9963\n",
      "Epoch 4170/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 47.0840 - val_loss: 12.2976\n",
      "Epoch 4171/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 28.5102 - val_loss: 27.2116\n",
      "Epoch 4172/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 42.6683 - val_loss: 174.9904\n",
      "Epoch 4173/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 85.0952 - val_loss: 14.1881\n",
      "Epoch 4174/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 30.5939 - val_loss: 151.5643\n",
      "Epoch 4175/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 118.0641 - val_loss: 202.1434\n",
      "Epoch 4176/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 436.8043 - val_loss: 314.1998\n",
      "Epoch 4177/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 734.6951 - val_loss: 1101.0548\n",
      "Epoch 4178/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1510.0441 - val_loss: 5528.6420\n",
      "Epoch 4179/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4933.8401 - val_loss: 2623.0837\n",
      "Epoch 4180/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1401.8390 - val_loss: 456.7276\n",
      "Epoch 4181/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 760.5361 - val_loss: 696.2943\n",
      "Epoch 4182/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 957.3586 - val_loss: 2803.7717\n",
      "Epoch 4183/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2994.3157 - val_loss: 9193.8377\n",
      "Epoch 4184/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4525.3491 - val_loss: 1840.9597\n",
      "Epoch 4185/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1808.6289 - val_loss: 1292.7325\n",
      "Epoch 4186/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1429.8375 - val_loss: 171.8771\n",
      "Epoch 4187/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 418.7111 - val_loss: 81.5435\n",
      "Epoch 4188/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 114.4996 - val_loss: 50.2655\n",
      "Epoch 4189/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 115.3006 - val_loss: 45.4768\n",
      "Epoch 4190/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 93.3036 - val_loss: 130.1530\n",
      "Epoch 4191/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 228.0709 - val_loss: 158.4247\n",
      "Epoch 4192/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 180.0003 - val_loss: 131.4995\n",
      "Epoch 4193/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 202.6931 - val_loss: 248.7682\n",
      "Epoch 4194/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 278.0707 - val_loss: 678.9066\n",
      "Epoch 4195/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 340.2254 - val_loss: 492.0461\n",
      "Epoch 4196/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 384.0347 - val_loss: 812.1748\n",
      "Epoch 4197/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 333.5982 - val_loss: 264.1919\n",
      "Epoch 4198/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 179.1550 - val_loss: 210.1129\n",
      "Epoch 4199/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 187.8719 - val_loss: 298.6993\n",
      "Epoch 4200/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 326.7547 - val_loss: 1117.0947\n",
      "Epoch 4201/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 1311.12 - 0s 132us/sample - loss: 704.3732 - val_loss: 1532.6141\n",
      "Epoch 4202/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1960.1777 - val_loss: 2428.7450\n",
      "Epoch 4203/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 916.1284 - val_loss: 194.1445\n",
      "Epoch 4204/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 126.1983 - val_loss: 107.5725\n",
      "Epoch 4205/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 67.5223 - val_loss: 93.6757\n",
      "Epoch 4206/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 53.3342 - val_loss: 171.6785\n",
      "Epoch 4207/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 52.0272 - val_loss: 59.6883\n",
      "Epoch 4208/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.2885 - val_loss: 22.6881\n",
      "Epoch 4209/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 22.5159 - val_loss: 6.3530\n",
      "Epoch 4210/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19.8504 - val_loss: 23.2612\n",
      "Epoch 4211/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 21.5824 - val_loss: 20.8072\n",
      "Epoch 4212/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 33.9566 - val_loss: 42.0112\n",
      "Epoch 4213/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 22.2321 - val_loss: 39.9182\n",
      "Epoch 4214/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 28.5818 - val_loss: 28.0441\n",
      "Epoch 4215/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.0078 - val_loss: 11.0133\n",
      "Epoch 4216/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14.2527 - val_loss: 8.2671\n",
      "Epoch 4217/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.9984 - val_loss: 2.1655\n",
      "Epoch 4218/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6680 - val_loss: 0.5109\n",
      "Epoch 4219/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2606 - val_loss: 0.6802\n",
      "Epoch 4220/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.8952 - val_loss: 2.1296\n",
      "Epoch 4221/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0041 - val_loss: 0.6076\n",
      "Epoch 4222/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4172 - val_loss: 0.0790\n",
      "Epoch 4223/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2173 - val_loss: 0.0401\n",
      "Epoch 4224/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1985 - val_loss: 0.1563\n",
      "Epoch 4225/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6582 - val_loss: 1.9432\n",
      "Epoch 4226/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7546 - val_loss: 0.5714\n",
      "Epoch 4227/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2973 - val_loss: 0.2643\n",
      "Epoch 4228/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1781 - val_loss: 0.0370\n",
      "Epoch 4229/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0489 - val_loss: 0.0282\n",
      "Epoch 4230/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0365 - val_loss: 0.0295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4231/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0412 - val_loss: 0.0315\n",
      "Epoch 4232/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0649 - val_loss: 0.0317\n",
      "Epoch 4233/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0403 - val_loss: 0.0259\n",
      "Epoch 4234/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0293 - val_loss: 0.0271\n",
      "Epoch 4235/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0348 - val_loss: 0.0587\n",
      "Epoch 4236/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0462 - val_loss: 0.0747\n",
      "Epoch 4237/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0397 - val_loss: 0.0238\n",
      "Epoch 4238/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0304 - val_loss: 0.0377\n",
      "Epoch 4239/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0271 - val_loss: 0.0259\n",
      "Epoch 4240/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0270 - val_loss: 0.0309\n",
      "Epoch 4241/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0252 - val_loss: 0.0287\n",
      "Epoch 4242/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.030 - 0s 132us/sample - loss: 0.0314 - val_loss: 0.0223\n",
      "Epoch 4243/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0253 - val_loss: 0.0397\n",
      "Epoch 4244/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0372 - val_loss: 0.0835\n",
      "Epoch 4245/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0560 - val_loss: 0.2001\n",
      "Epoch 4246/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1588 - val_loss: 0.1634\n",
      "Epoch 4247/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1515 - val_loss: 0.1653\n",
      "Epoch 4248/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1700 - val_loss: 0.0400\n",
      "Epoch 4249/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0621 - val_loss: 0.0335\n",
      "Epoch 4250/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0798 - val_loss: 0.1347\n",
      "Epoch 4251/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0811 - val_loss: 0.0456\n",
      "Epoch 4252/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0378 - val_loss: 0.0410\n",
      "Epoch 4253/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0641 - val_loss: 0.0392\n",
      "Epoch 4254/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0670 - val_loss: 0.1188\n",
      "Epoch 4255/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0667 - val_loss: 0.0308\n",
      "Epoch 4256/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0506 - val_loss: 0.0445\n",
      "Epoch 4257/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0379 - val_loss: 0.0538\n",
      "Epoch 4258/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0339 - val_loss: 0.0428\n",
      "Epoch 4259/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0367 - val_loss: 0.1525\n",
      "Epoch 4260/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0629 - val_loss: 0.0217\n",
      "Epoch 4261/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0654 - val_loss: 0.0412\n",
      "Epoch 4262/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0352 - val_loss: 0.0145\n",
      "Epoch 4263/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0152 - val_loss: 0.0164\n",
      "Epoch 4264/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0248 - val_loss: 0.0275\n",
      "Epoch 4265/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0238 - val_loss: 0.0177\n",
      "Epoch 4266/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0195 - val_loss: 0.0133\n",
      "Epoch 4267/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0192 - val_loss: 0.0247\n",
      "Epoch 4268/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0446 - val_loss: 0.0166\n",
      "Epoch 4269/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0627 - val_loss: 0.0343\n",
      "Epoch 4270/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0564 - val_loss: 0.0763\n",
      "Epoch 4271/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0485 - val_loss: 0.0473\n",
      "Epoch 4272/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0791 - val_loss: 0.1257\n",
      "Epoch 4273/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1297 - val_loss: 0.2113\n",
      "Epoch 4274/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1229 - val_loss: 0.0110\n",
      "Epoch 4275/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1106 - val_loss: 0.2789\n",
      "Epoch 4276/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2523 - val_loss: 0.1985\n",
      "Epoch 4277/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5512 - val_loss: 0.3427\n",
      "Epoch 4278/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6013 - val_loss: 0.5394\n",
      "Epoch 4279/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9812 - val_loss: 1.6410\n",
      "Epoch 4280/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0486 - val_loss: 0.0641\n",
      "Epoch 4281/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8533 - val_loss: 2.0931\n",
      "Epoch 4282/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.5433 - val_loss: 0.6406\n",
      "Epoch 4283/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.8295 - val_loss: 1.0392\n",
      "Epoch 4284/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5253 - val_loss: 3.0205\n",
      "Epoch 4285/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.0561 - val_loss: 0.4031\n",
      "Epoch 4286/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.3684 - val_loss: 2.9684\n",
      "Epoch 4287/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5030 - val_loss: 0.1817\n",
      "Epoch 4288/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3406 - val_loss: 0.5981\n",
      "Epoch 4289/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3340 - val_loss: 0.1475\n",
      "Epoch 4290/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2519 - val_loss: 0.0687\n",
      "Epoch 4291/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0491 - val_loss: 0.0509\n",
      "Epoch 4292/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0275 - val_loss: 0.0104\n",
      "Epoch 4293/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0128 - val_loss: 0.0791\n",
      "Epoch 4294/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0417 - val_loss: 0.0255\n",
      "Epoch 4295/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0141 - val_loss: 0.0144\n",
      "Epoch 4296/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0249 - val_loss: 0.0148\n",
      "Epoch 4297/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0453 - val_loss: 0.0313\n",
      "Epoch 4298/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0163 - val_loss: 0.0128\n",
      "Epoch 4299/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0155 - val_loss: 0.0080\n",
      "Epoch 4300/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0157 - val_loss: 0.0082\n",
      "Epoch 4301/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0220 - val_loss: 0.0577\n",
      "Epoch 4302/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2306 - val_loss: 0.1719\n",
      "Epoch 4303/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3353 - val_loss: 0.2366\n",
      "Epoch 4304/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1692 - val_loss: 0.2551\n",
      "Epoch 4305/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2519 - val_loss: 0.1023\n",
      "Epoch 4306/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0718 - val_loss: 0.1321\n",
      "Epoch 4307/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0937 - val_loss: 0.0093\n",
      "Epoch 4308/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1975 - val_loss: 0.2871\n",
      "Epoch 4309/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4305 - val_loss: 0.2610\n",
      "Epoch 4310/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5364 - val_loss: 0.0447\n",
      "Epoch 4311/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3693 - val_loss: 0.1508\n",
      "Epoch 4312/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1885 - val_loss: 0.4574\n",
      "Epoch 4313/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5076 - val_loss: 0.0991\n",
      "Epoch 4314/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2283 - val_loss: 0.4556\n",
      "Epoch 4315/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5407 - val_loss: 1.6615\n",
      "Epoch 4316/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6491 - val_loss: 0.5576\n",
      "Epoch 4317/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.5152 - val_loss: 0.5904\n",
      "Epoch 4318/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5022 - val_loss: 0.6897\n",
      "Epoch 4319/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4085 - val_loss: 0.5816\n",
      "Epoch 4320/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.8927 - val_loss: 1.2285\n",
      "Epoch 4321/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6454 - val_loss: 0.4324\n",
      "Epoch 4322/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1938 - val_loss: 0.0825\n",
      "Epoch 4323/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0998 - val_loss: 0.0930\n",
      "Epoch 4324/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0846 - val_loss: 0.2611\n",
      "Epoch 4325/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1421 - val_loss: 0.0882\n",
      "Epoch 4326/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1553 - val_loss: 0.2863\n",
      "Epoch 4327/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4322 - val_loss: 0.0588\n",
      "Epoch 4328/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2074 - val_loss: 0.1925\n",
      "Epoch 4329/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2060 - val_loss: 0.2041\n",
      "Epoch 4330/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1659 - val_loss: 0.0398\n",
      "Epoch 4331/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1735 - val_loss: 0.0472\n",
      "Epoch 4332/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1019 - val_loss: 0.0581\n",
      "Epoch 4333/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1677 - val_loss: 0.2028\n",
      "Epoch 4334/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1155 - val_loss: 0.1228\n",
      "Epoch 4335/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1428 - val_loss: 0.0624\n",
      "Epoch 4336/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2804 - val_loss: 0.0980\n",
      "Epoch 4337/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3254 - val_loss: 0.0487\n",
      "Epoch 4338/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4499 - val_loss: 0.7078\n",
      "Epoch 4339/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5729 - val_loss: 1.0248\n",
      "Epoch 4340/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9243 - val_loss: 0.8569\n",
      "Epoch 4341/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4474 - val_loss: 0.8238\n",
      "Epoch 4342/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8614 - val_loss: 0.2175\n",
      "Epoch 4343/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4469 - val_loss: 0.3848\n",
      "Epoch 4344/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1281 - val_loss: 0.2272\n",
      "Epoch 4345/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1258 - val_loss: 0.0404\n",
      "Epoch 4346/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5062 - val_loss: 0.0475\n",
      "Epoch 4347/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7510 - val_loss: 0.3772\n",
      "Epoch 4348/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5265 - val_loss: 0.6521\n",
      "Epoch 4349/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4428 - val_loss: 0.2393\n",
      "Epoch 4350/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2615 - val_loss: 0.1707\n",
      "Epoch 4351/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0571 - val_loss: 0.0103\n",
      "Epoch 4352/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0851 - val_loss: 0.0578\n",
      "Epoch 4353/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1657 - val_loss: 0.4117\n",
      "Epoch 4354/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1607 - val_loss: 0.0880\n",
      "Epoch 4355/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0596 - val_loss: 0.0265\n",
      "Epoch 4356/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2434 - val_loss: 2.4152\n",
      "Epoch 4357/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4261 - val_loss: 2.9894\n",
      "Epoch 4358/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.8646 - val_loss: 1.0842\n",
      "Epoch 4359/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.0850 - val_loss: 1.2409\n",
      "Epoch 4360/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.0350 - val_loss: 15.4583\n",
      "Epoch 4361/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 13.7441 - val_loss: 2.0394\n",
      "Epoch 4362/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.7626 - val_loss: 0.8732\n",
      "Epoch 4363/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.1519 - val_loss: 9.3464\n",
      "Epoch 4364/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19.0555 - val_loss: 41.8003\n",
      "Epoch 4365/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 110.4227 - val_loss: 95.7538\n",
      "Epoch 4366/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 160.7777 - val_loss: 26.8800\n",
      "Epoch 4367/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 148.3229 - val_loss: 111.3981\n",
      "Epoch 4368/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 176.6550 - val_loss: 58.0024\n",
      "Epoch 4369/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 300.7919 - val_loss: 2243.9141\n",
      "Epoch 4370/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4378.0585 - val_loss: 7362.5967\n",
      "Epoch 4371/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4980.7914 - val_loss: 1491.0183\n",
      "Epoch 4372/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3030.0511 - val_loss: 226.9381\n",
      "Epoch 4373/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1251.9161 - val_loss: 324.0695\n",
      "Epoch 4374/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1132.7413 - val_loss: 864.5074\n",
      "Epoch 4375/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 748.9566 - val_loss: 906.2277\n",
      "Epoch 4376/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 396.5320 - val_loss: 97.4005\n",
      "Epoch 4377/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 277.2438 - val_loss: 415.3658\n",
      "Epoch 4378/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 166.6123 - val_loss: 130.7974\n",
      "Epoch 4379/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 179.8231 - val_loss: 107.6348\n",
      "Epoch 4380/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 153.4157 - val_loss: 216.9560\n",
      "Epoch 4381/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 191us/sample - loss: 106.8106 - val_loss: 57.9395\n",
      "Epoch 4382/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 41.6442 - val_loss: 34.3701\n",
      "Epoch 4383/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 32.8478 - val_loss: 21.9616\n",
      "Epoch 4384/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.4443 - val_loss: 11.6831\n",
      "Epoch 4385/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.9403 - val_loss: 2.3815\n",
      "Epoch 4386/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0302 - val_loss: 3.7699\n",
      "Epoch 4387/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.3690 - val_loss: 3.1598\n",
      "Epoch 4388/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3543 - val_loss: 0.7217\n",
      "Epoch 4389/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0927 - val_loss: 1.4935\n",
      "Epoch 4390/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1277 - val_loss: 1.4564\n",
      "Epoch 4391/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.2769 - val_loss: 0.0983\n",
      "Epoch 4392/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.3390 - val_loss: 0.9070\n",
      "Epoch 4393/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.4033 - val_loss: 3.2205\n",
      "Epoch 4394/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6242 - val_loss: 2.0963\n",
      "Epoch 4395/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.5071 - val_loss: 0.3974\n",
      "Epoch 4396/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.273 - 0s 132us/sample - loss: 0.5968 - val_loss: 0.8746\n",
      "Epoch 4397/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6604 - val_loss: 0.6715\n",
      "Epoch 4398/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0083 - val_loss: 0.2280\n",
      "Epoch 4399/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9459 - val_loss: 1.2131\n",
      "Epoch 4400/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6886 - val_loss: 0.8610\n",
      "Epoch 4401/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4128 - val_loss: 0.2175\n",
      "Epoch 4402/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1219 - val_loss: 0.0111\n",
      "Epoch 4403/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0246 - val_loss: 0.0204\n",
      "Epoch 4404/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0271 - val_loss: 0.0191\n",
      "Epoch 4405/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0232 - val_loss: 0.0311\n",
      "Epoch 4406/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0428 - val_loss: 0.0887\n",
      "Epoch 4407/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0562 - val_loss: 0.0196\n",
      "Epoch 4408/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0270 - val_loss: 0.0054\n",
      "Epoch 4409/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0294 - val_loss: 0.0450\n",
      "Epoch 4410/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0293 - val_loss: 0.0301\n",
      "Epoch 4411/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0132 - val_loss: 0.0194\n",
      "Epoch 4412/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0357 - val_loss: 0.1464\n",
      "Epoch 4413/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2097 - val_loss: 0.4532\n",
      "Epoch 4414/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1667 - val_loss: 0.0293\n",
      "Epoch 4415/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0569 - val_loss: 0.1002\n",
      "Epoch 4416/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0384 - val_loss: 0.0104\n",
      "Epoch 4417/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0065 - val_loss: 0.0102\n",
      "Epoch 4418/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0078 - val_loss: 0.0031\n",
      "Epoch 4419/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0172 - val_loss: 0.0063\n",
      "Epoch 4420/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0109 - val_loss: 0.0024\n",
      "Epoch 4421/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0039 - val_loss: 0.0057\n",
      "Epoch 4422/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0089 - val_loss: 0.0242\n",
      "Epoch 4423/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0426 - val_loss: 0.0867\n",
      "Epoch 4424/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0581 - val_loss: 0.0198\n",
      "Epoch 4425/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0349 - val_loss: 0.0099\n",
      "Epoch 4426/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0077 - val_loss: 0.0067\n",
      "Epoch 4427/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0040 - val_loss: 0.0024\n",
      "Epoch 4428/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0048 - val_loss: 0.0024\n",
      "Epoch 4429/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0270 - val_loss: 0.0257\n",
      "Epoch 4430/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0128 - val_loss: 0.0050\n",
      "Epoch 4431/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 4432/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0052 - val_loss: 0.0033\n",
      "Epoch 4433/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0170 - val_loss: 0.0027\n",
      "Epoch 4434/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0054 - val_loss: 0.0019\n",
      "Epoch 4435/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0076 - val_loss: 0.0056\n",
      "Epoch 4436/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0074 - val_loss: 0.0107\n",
      "Epoch 4437/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0086 - val_loss: 0.0255\n",
      "Epoch 4438/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0159 - val_loss: 0.0069\n",
      "Epoch 4439/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0085 - val_loss: 0.0039\n",
      "Epoch 4440/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0162 - val_loss: 0.0422\n",
      "Epoch 4441/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0754 - val_loss: 0.0110\n",
      "Epoch 4442/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0651 - val_loss: 0.0834\n",
      "Epoch 4443/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0550 - val_loss: 0.0435\n",
      "Epoch 4444/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0370 - val_loss: 0.0754\n",
      "Epoch 4445/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0546 - val_loss: 0.0119\n",
      "Epoch 4446/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0267 - val_loss: 0.0070\n",
      "Epoch 4447/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0469 - val_loss: 0.0331\n",
      "Epoch 4448/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0458 - val_loss: 0.0135\n",
      "Epoch 4449/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.012 - 0s 132us/sample - loss: 0.0889 - val_loss: 0.4266\n",
      "Epoch 4450/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0998 - val_loss: 0.0303\n",
      "Epoch 4451/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0132 - val_loss: 0.0021\n",
      "Epoch 4452/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0038 - val_loss: 0.0033\n",
      "Epoch 4453/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0062 - val_loss: 0.0050\n",
      "Epoch 4454/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0288 - val_loss: 0.0407\n",
      "Epoch 4455/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0223 - val_loss: 0.0868\n",
      "Epoch 4456/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0685 - val_loss: 0.2453\n",
      "Epoch 4457/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1550 - val_loss: 0.0012\n",
      "Epoch 4458/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0716 - val_loss: 0.0533\n",
      "Epoch 4459/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0442 - val_loss: 0.0117\n",
      "Epoch 4460/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0386 - val_loss: 0.2938\n",
      "Epoch 4461/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7255 - val_loss: 1.2191\n",
      "Epoch 4462/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6080 - val_loss: 0.0522\n",
      "Epoch 4463/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.4232 - val_loss: 15.3561\n",
      "Epoch 4464/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.6154 - val_loss: 22.2119\n",
      "Epoch 4465/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 31.7831 - val_loss: 14.7030\n",
      "Epoch 4466/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 36.7431 - val_loss: 32.7750\n",
      "Epoch 4467/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.7259 - val_loss: 0.0270\n",
      "Epoch 4468/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5417 - val_loss: 0.2133\n",
      "Epoch 4469/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.9264 - val_loss: 1.3745\n",
      "Epoch 4470/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9536 - val_loss: 1.6617\n",
      "Epoch 4471/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3822 - val_loss: 2.5984\n",
      "Epoch 4472/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4438 - val_loss: 0.2502\n",
      "Epoch 4473/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2096 - val_loss: 0.0211\n",
      "Epoch 4474/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.013 - 0s 132us/sample - loss: 0.1909 - val_loss: 0.5225\n",
      "Epoch 4475/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6601 - val_loss: 0.1673\n",
      "Epoch 4476/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6008 - val_loss: 1.1509\n",
      "Epoch 4477/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.7782 - val_loss: 1.6587\n",
      "Epoch 4478/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.3819 - val_loss: 3.6968\n",
      "Epoch 4479/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.9953 - val_loss: 0.7599\n",
      "Epoch 4480/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.7304 - val_loss: 34.4866\n",
      "Epoch 4481/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.6019 - val_loss: 0.0080\n",
      "Epoch 4482/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7788 - val_loss: 2.9253\n",
      "Epoch 4483/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2790 - val_loss: 0.8832\n",
      "Epoch 4484/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0940 - val_loss: 1.8141\n",
      "Epoch 4485/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0701 - val_loss: 0.2911\n",
      "Epoch 4486/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5696 - val_loss: 0.7785\n",
      "Epoch 4487/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2123 - val_loss: 0.1573\n",
      "Epoch 4488/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1849 - val_loss: 0.5349\n",
      "Epoch 4489/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3631 - val_loss: 2.8768\n",
      "Epoch 4490/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.7485 - val_loss: 9.9198\n",
      "Epoch 4491/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.3076 - val_loss: 60.4734\n",
      "Epoch 4492/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 23.7392 - val_loss: 27.6857\n",
      "Epoch 4493/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 116.5550 - val_loss: 72.8410\n",
      "Epoch 4494/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 138.4181 - val_loss: 92.8099\n",
      "Epoch 4495/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 55.5261 - val_loss: 3.6246\n",
      "Epoch 4496/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 19.3899 - val_loss: 41.9394\n",
      "Epoch 4497/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 62.6310 - val_loss: 25.0780\n",
      "Epoch 4498/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 350.2705 - val_loss: 8.9488\n",
      "Epoch 4499/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 38.4546 - val_loss: 41.2206\n",
      "Epoch 4500/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 23.2694 - val_loss: 18.2534\n",
      "Epoch 4501/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 39.8763 - val_loss: 13.4088\n",
      "Epoch 4502/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 30.3422 - val_loss: 22.8418\n",
      "Epoch 4503/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 33.1306 - val_loss: 24.8667\n",
      "Epoch 4504/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 11.3691 - val_loss: 6.8897\n",
      "Epoch 4505/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.4188 - val_loss: 11.0392\n",
      "Epoch 4506/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 11.25 - 0s 147us/sample - loss: 13.9239 - val_loss: 142.4204\n",
      "Epoch 4507/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 49.8170 - val_loss: 62.9823\n",
      "Epoch 4508/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 291.6433 - val_loss: 714.9578\n",
      "Epoch 4509/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 558.5616 - val_loss: 55.7650\n",
      "Epoch 4510/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 670.6554 - val_loss: 88.0142\n",
      "Epoch 4511/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 140.1214 - val_loss: 810.5978\n",
      "Epoch 4512/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 243.0000 - val_loss: 149.5808\n",
      "Epoch 4513/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 160.6167 - val_loss: 123.8695\n",
      "Epoch 4514/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 380.2311 - val_loss: 689.6018\n",
      "Epoch 4515/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 982.6657 - val_loss: 6708.3417\n",
      "Epoch 4516/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5181.0375 - val_loss: 1714.4721\n",
      "Epoch 4517/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5862.6330 - val_loss: 25536.7788\n",
      "Epoch 4518/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 15254.9462 - val_loss: 8773.4185\n",
      "Epoch 4519/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6213.8073 - val_loss: 4159.0162\n",
      "Epoch 4520/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2740.2669 - val_loss: 2912.6331\n",
      "Epoch 4521/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1600.7723 - val_loss: 1605.6652\n",
      "Epoch 4522/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 886.9235 - val_loss: 427.1327\n",
      "Epoch 4523/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 815.4649 - val_loss: 356.5843\n",
      "Epoch 4524/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 571.6022 - val_loss: 377.8107\n",
      "Epoch 4525/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 227.9264 - val_loss: 234.3318\n",
      "Epoch 4526/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 73.2813 - val_loss: 45.6771\n",
      "Epoch 4527/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 45.10 - 0s 147us/sample - loss: 25.9030 - val_loss: 3.6635\n",
      "Epoch 4528/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15.0679 - val_loss: 8.6464\n",
      "Epoch 4529/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.1509 - val_loss: 2.1803\n",
      "Epoch 4530/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 2.7469 - val_loss: 0.5172\n",
      "Epoch 4531/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0127 - val_loss: 0.4981\n",
      "Epoch 4532/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5832 - val_loss: 1.1320\n",
      "Epoch 4533/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.1996 - val_loss: 1.1475\n",
      "Epoch 4534/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7213 - val_loss: 0.3584\n",
      "Epoch 4535/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6196 - val_loss: 0.2365\n",
      "Epoch 4536/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4442 - val_loss: 0.2140\n",
      "Epoch 4537/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.3426 - val_loss: 0.1567\n",
      "Epoch 4538/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.3171 - val_loss: 0.4666\n",
      "Epoch 4539/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4686 - val_loss: 0.3949\n",
      "Epoch 4540/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4941 - val_loss: 0.8495\n",
      "Epoch 4541/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6526 - val_loss: 0.4326\n",
      "Epoch 4542/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7150 - val_loss: 0.2989\n",
      "Epoch 4543/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.3764 - val_loss: 0.1936\n",
      "Epoch 4544/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4734 - val_loss: 0.1375\n",
      "Epoch 4545/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2089 - val_loss: 0.2592\n",
      "Epoch 4546/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2075 - val_loss: 0.2455\n",
      "Epoch 4547/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1811 - val_loss: 0.1439\n",
      "Epoch 4548/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2627 - val_loss: 0.4188\n",
      "Epoch 4549/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3440 - val_loss: 0.4521\n",
      "Epoch 4550/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5151 - val_loss: 0.7992\n",
      "Epoch 4551/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4798 - val_loss: 0.5063\n",
      "Epoch 4552/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3896 - val_loss: 0.1440\n",
      "Epoch 4553/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2687 - val_loss: 0.1171\n",
      "Epoch 4554/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2012 - val_loss: 0.1814\n",
      "Epoch 4555/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3682 - val_loss: 0.1144\n",
      "Epoch 4556/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.021 - 0s 132us/sample - loss: 0.2407 - val_loss: 0.2758\n",
      "Epoch 4557/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3827 - val_loss: 1.0360\n",
      "Epoch 4558/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8383 - val_loss: 0.4399\n",
      "Epoch 4559/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6800 - val_loss: 0.1323\n",
      "Epoch 4560/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0180 - val_loss: 0.1399\n",
      "Epoch 4561/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9315 - val_loss: 0.1430\n",
      "Epoch 4562/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2885 - val_loss: 0.2110\n",
      "Epoch 4563/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2687 - val_loss: 0.1928\n",
      "Epoch 4564/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1480 - val_loss: 0.1010\n",
      "Epoch 4565/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0940 - val_loss: 0.1376\n",
      "Epoch 4566/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1876 - val_loss: 0.1974\n",
      "Epoch 4567/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1114 - val_loss: 0.1327\n",
      "Epoch 4568/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1323 - val_loss: 0.1524\n",
      "Epoch 4569/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1221 - val_loss: 0.1102\n",
      "Epoch 4570/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1221 - val_loss: 0.1234\n",
      "Epoch 4571/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1699 - val_loss: 0.1679\n",
      "Epoch 4572/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2428 - val_loss: 0.1012\n",
      "Epoch 4573/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0986 - val_loss: 0.0973\n",
      "Epoch 4574/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1565 - val_loss: 0.1721\n",
      "Epoch 4575/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1007 - val_loss: 0.0983\n",
      "Epoch 4576/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0927 - val_loss: 0.1530\n",
      "Epoch 4577/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2051 - val_loss: 0.5197\n",
      "Epoch 4578/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3061 - val_loss: 0.1343\n",
      "Epoch 4579/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2610 - val_loss: 0.0833\n",
      "Epoch 4580/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1295 - val_loss: 0.0909\n",
      "Epoch 4581/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1003 - val_loss: 0.0744\n",
      "Epoch 4582/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0921 - val_loss: 0.0745\n",
      "Epoch 4583/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0845 - val_loss: 0.1135\n",
      "Epoch 4584/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0807 - val_loss: 0.1166\n",
      "Epoch 4585/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0741 - val_loss: 0.0743\n",
      "Epoch 4586/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0804 - val_loss: 0.0783\n",
      "Epoch 4587/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2452 - val_loss: 0.7199\n",
      "Epoch 4588/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.3428 - val_loss: 0.6319\n",
      "Epoch 4589/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4187 - val_loss: 0.8761\n",
      "Epoch 4590/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5020 - val_loss: 0.0885\n",
      "Epoch 4591/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2253 - val_loss: 0.2441\n",
      "Epoch 4592/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3550 - val_loss: 0.7499\n",
      "Epoch 4593/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6364 - val_loss: 0.0980\n",
      "Epoch 4594/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3295 - val_loss: 0.0855\n",
      "Epoch 4595/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4369 - val_loss: 0.4008\n",
      "Epoch 4596/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1603 - val_loss: 0.1359\n",
      "Epoch 4597/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0976 - val_loss: 0.1605\n",
      "Epoch 4598/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1023 - val_loss: 0.0897\n",
      "Epoch 4599/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0663 - val_loss: 0.0714\n",
      "Epoch 4600/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0673 - val_loss: 0.1420\n",
      "Epoch 4601/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1172 - val_loss: 0.1568\n",
      "Epoch 4602/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1531 - val_loss: 0.0671\n",
      "Epoch 4603/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1787 - val_loss: 0.3712\n",
      "Epoch 4604/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2993 - val_loss: 0.3809\n",
      "Epoch 4605/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1239 - val_loss: 0.0648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4606/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0783 - val_loss: 0.0611\n",
      "Epoch 4607/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1001 - val_loss: 0.1114\n",
      "Epoch 4608/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2406 - val_loss: 0.2086\n",
      "Epoch 4609/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2985 - val_loss: 1.0497\n",
      "Epoch 4610/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8279 - val_loss: 0.5542\n",
      "Epoch 4611/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3831 - val_loss: 0.0760\n",
      "Epoch 4612/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3099 - val_loss: 0.6836\n",
      "Epoch 4613/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3515 - val_loss: 0.2000\n",
      "Epoch 4614/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4508 - val_loss: 0.7637\n",
      "Epoch 4615/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4090 - val_loss: 0.0745\n",
      "Epoch 4616/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6257 - val_loss: 1.1254\n",
      "Epoch 4617/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.0722 - val_loss: 0.3037\n",
      "Epoch 4618/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4134 - val_loss: 0.4260\n",
      "Epoch 4619/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.4200 - val_loss: 0.0506\n",
      "Epoch 4620/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2679 - val_loss: 0.4695\n",
      "Epoch 4621/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3775 - val_loss: 0.0653\n",
      "Epoch 4622/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5205 - val_loss: 0.2373\n",
      "Epoch 4623/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1235 - val_loss: 0.0705\n",
      "Epoch 4624/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1317 - val_loss: 0.0636\n",
      "Epoch 4625/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2224 - val_loss: 0.0903\n",
      "Epoch 4626/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1516 - val_loss: 0.2066\n",
      "Epoch 4627/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1316 - val_loss: 0.0493\n",
      "Epoch 4628/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1765 - val_loss: 0.0426\n",
      "Epoch 4629/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1515 - val_loss: 0.1945\n",
      "Epoch 4630/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1033 - val_loss: 0.0307\n",
      "Epoch 4631/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0749 - val_loss: 0.3158\n",
      "Epoch 4632/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2653 - val_loss: 0.1765\n",
      "Epoch 4633/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3909 - val_loss: 0.7408\n",
      "Epoch 4634/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4675 - val_loss: 0.9242\n",
      "Epoch 4635/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.6150 - val_loss: 0.1051\n",
      "Epoch 4636/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6050 - val_loss: 0.2480\n",
      "Epoch 4637/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1967 - val_loss: 0.2156\n",
      "Epoch 4638/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1383 - val_loss: 0.0413\n",
      "Epoch 4639/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0597 - val_loss: 0.0257\n",
      "Epoch 4640/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0690 - val_loss: 0.1826\n",
      "Epoch 4641/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1556 - val_loss: 0.0384\n",
      "Epoch 4642/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1489 - val_loss: 0.0335\n",
      "Epoch 4643/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2673 - val_loss: 0.2214\n",
      "Epoch 4644/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1928 - val_loss: 0.1949\n",
      "Epoch 4645/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0649 - val_loss: 0.0506\n",
      "Epoch 4646/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0379 - val_loss: 0.0333\n",
      "Epoch 4647/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0249 - val_loss: 0.0550\n",
      "Epoch 4648/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0462 - val_loss: 0.0604\n",
      "Epoch 4649/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0479 - val_loss: 0.0204\n",
      "Epoch 4650/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0831 - val_loss: 0.0284\n",
      "Epoch 4651/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1240 - val_loss: 0.1657\n",
      "Epoch 4652/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1046 - val_loss: 0.0595\n",
      "Epoch 4653/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2125 - val_loss: 0.0882\n",
      "Epoch 4654/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2345 - val_loss: 0.1592\n",
      "Epoch 4655/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1168 - val_loss: 0.0742\n",
      "Epoch 4656/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0564 - val_loss: 0.0899\n",
      "Epoch 4657/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0648 - val_loss: 0.0817\n",
      "Epoch 4658/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1357 - val_loss: 0.0286\n",
      "Epoch 4659/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0948 - val_loss: 0.0447\n",
      "Epoch 4660/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0576 - val_loss: 0.0414\n",
      "Epoch 4661/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0230 - val_loss: 0.0267\n",
      "Epoch 4662/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0229 - val_loss: 0.0541\n",
      "Epoch 4663/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0402 - val_loss: 0.0142\n",
      "Epoch 4664/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0226 - val_loss: 0.0981\n",
      "Epoch 4665/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0824 - val_loss: 0.0659\n",
      "Epoch 4666/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1051 - val_loss: 0.0338\n",
      "Epoch 4667/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0329 - val_loss: 0.0387\n",
      "Epoch 4668/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0412 - val_loss: 0.0239\n",
      "Epoch 4669/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0247 - val_loss: 0.0636\n",
      "Epoch 4670/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0472 - val_loss: 0.0394\n",
      "Epoch 4671/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1112 - val_loss: 0.3148\n",
      "Epoch 4672/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3564 - val_loss: 0.4391\n",
      "Epoch 4673/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6500 - val_loss: 0.4267\n",
      "Epoch 4674/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5741 - val_loss: 0.1124\n",
      "Epoch 4675/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0922 - val_loss: 0.0568\n",
      "Epoch 4676/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0541 - val_loss: 0.0961\n",
      "Epoch 4677/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0502 - val_loss: 0.0487\n",
      "Epoch 4678/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0423 - val_loss: 0.0565\n",
      "Epoch 4679/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0605 - val_loss: 0.2257\n",
      "Epoch 4680/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0658 - val_loss: 0.0396\n",
      "Epoch 4681/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0477 - val_loss: 0.1477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4682/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0750 - val_loss: 0.0574\n",
      "Epoch 4683/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0573 - val_loss: 0.0640\n",
      "Epoch 4684/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0306 - val_loss: 0.1193\n",
      "Epoch 4685/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0894 - val_loss: 0.0521\n",
      "Epoch 4686/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0661 - val_loss: 0.0229\n",
      "Epoch 4687/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0356 - val_loss: 0.0173\n",
      "Epoch 4688/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0869 - val_loss: 0.1717\n",
      "Epoch 4689/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1916 - val_loss: 0.0995\n",
      "Epoch 4690/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1190 - val_loss: 0.1849\n",
      "Epoch 4691/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0579 - val_loss: 0.0353\n",
      "Epoch 4692/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0353 - val_loss: 0.0689\n",
      "Epoch 4693/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0471 - val_loss: 0.1022\n",
      "Epoch 4694/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0930 - val_loss: 0.0356\n",
      "Epoch 4695/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0813 - val_loss: 0.0192\n",
      "Epoch 4696/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0633 - val_loss: 0.0831\n",
      "Epoch 4697/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0600 - val_loss: 0.0298\n",
      "Epoch 4698/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0686 - val_loss: 0.2592\n",
      "Epoch 4699/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1836 - val_loss: 0.3164\n",
      "Epoch 4700/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2490 - val_loss: 0.0117\n",
      "Epoch 4701/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1633 - val_loss: 0.1802\n",
      "Epoch 4702/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4865 - val_loss: 0.4816\n",
      "Epoch 4703/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3766 - val_loss: 0.2929\n",
      "Epoch 4704/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1919 - val_loss: 0.4201\n",
      "Epoch 4705/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4651 - val_loss: 1.6261\n",
      "Epoch 4706/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3.0078 - val_loss: 0.4870\n",
      "Epoch 4707/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.7854 - val_loss: 21.4832\n",
      "Epoch 4708/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 36.1747 - val_loss: 28.0632\n",
      "Epoch 4709/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 111.3871 - val_loss: 75.2431\n",
      "Epoch 4710/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 328.0606 - val_loss: 1016.7647\n",
      "Epoch 4711/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 850.6615 - val_loss: 51.8904\n",
      "Epoch 4712/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 105.3942 - val_loss: 108.3062\n",
      "Epoch 4713/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 52.6712 - val_loss: 171.0108\n",
      "Epoch 4714/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 301.5001 - val_loss: 9.1625\n",
      "Epoch 4715/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 88.1807 - val_loss: 18.6347\n",
      "Epoch 4716/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 26.5713 - val_loss: 6.5743\n",
      "Epoch 4717/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.0693 - val_loss: 5.4195\n",
      "Epoch 4718/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.7806 - val_loss: 4.4326\n",
      "Epoch 4719/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2997 - val_loss: 3.4769\n",
      "Epoch 4720/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.1574 - val_loss: 0.7842\n",
      "Epoch 4721/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.9446 - val_loss: 0.3688\n",
      "Epoch 4722/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.7574 - val_loss: 0.1463\n",
      "Epoch 4723/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7237 - val_loss: 1.8292\n",
      "Epoch 4724/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9100 - val_loss: 0.5871\n",
      "Epoch 4725/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5050 - val_loss: 0.4524\n",
      "Epoch 4726/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.5506 - val_loss: 2.7534\n",
      "Epoch 4727/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.1329 - val_loss: 0.4592\n",
      "Epoch 4728/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5438 - val_loss: 1.8963\n",
      "Epoch 4729/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.3772 - val_loss: 6.6333\n",
      "Epoch 4730/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.6106 - val_loss: 21.3349\n",
      "Epoch 4731/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 35.0169 - val_loss: 50.7745\n",
      "Epoch 4732/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 19.8933 - val_loss: 59.1344\n",
      "Epoch 4733/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 96.0883 - val_loss: 9.0509\n",
      "Epoch 4734/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 188.2335 - val_loss: 818.3847\n",
      "Epoch 4735/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 333.0340 - val_loss: 340.6572\n",
      "Epoch 4736/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 331.9280 - val_loss: 65.2288\n",
      "Epoch 4737/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 335.0275 - val_loss: 2007.2104\n",
      "Epoch 4738/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3533.7987 - val_loss: 18232.3079\n",
      "Epoch 4739/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11152.1588 - val_loss: 296.5002\n",
      "Epoch 4740/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4812.8259 - val_loss: 8232.3905\n",
      "Epoch 4741/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4297.9786 - val_loss: 3792.4243\n",
      "Epoch 4742/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3562.8189 - val_loss: 2211.3091\n",
      "Epoch 4743/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1222.4026 - val_loss: 301.6269\n",
      "Epoch 4744/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1003.2591 - val_loss: 236.4310\n",
      "Epoch 4745/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 727.2797 - val_loss: 29.2066\n",
      "Epoch 4746/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 270.0720 - val_loss: 29.7736\n",
      "Epoch 4747/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 85.7815 - val_loss: 23.5159\n",
      "Epoch 4748/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 54.6958 - val_loss: 24.3506\n",
      "Epoch 4749/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 49.1058 - val_loss: 42.1735\n",
      "Epoch 4750/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 24.0090 - val_loss: 19.6278\n",
      "Epoch 4751/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.2942 - val_loss: 16.2103\n",
      "Epoch 4752/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.9096 - val_loss: 15.9799\n",
      "Epoch 4753/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 13.9572 - val_loss: 9.5453\n",
      "Epoch 4754/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 4.3953 - val_loss: 2.6841\n",
      "Epoch 4755/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.0992 - val_loss: 0.8351\n",
      "Epoch 4756/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6953 - val_loss: 1.3992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4757/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0877 - val_loss: 5.7749\n",
      "Epoch 4758/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.5057 - val_loss: 0.4899\n",
      "Epoch 4759/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3974 - val_loss: 0.7984\n",
      "Epoch 4760/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4779 - val_loss: 0.3053\n",
      "Epoch 4761/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2335 - val_loss: 0.3190\n",
      "Epoch 4762/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2874 - val_loss: 0.5165\n",
      "Epoch 4763/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3818 - val_loss: 0.6961\n",
      "Epoch 4764/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4897 - val_loss: 0.4823\n",
      "Epoch 4765/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3640 - val_loss: 0.4169\n",
      "Epoch 4766/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2715 - val_loss: 0.1169\n",
      "Epoch 4767/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1549 - val_loss: 0.1357\n",
      "Epoch 4768/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1266 - val_loss: 0.0596\n",
      "Epoch 4769/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1098 - val_loss: 0.0582\n",
      "Epoch 4770/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1491 - val_loss: 0.0588\n",
      "Epoch 4771/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0702 - val_loss: 0.0651\n",
      "Epoch 4772/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0663 - val_loss: 0.0553\n",
      "Epoch 4773/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0739 - val_loss: 0.0719\n",
      "Epoch 4774/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0816 - val_loss: 0.0849\n",
      "Epoch 4775/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1051 - val_loss: 0.1909\n",
      "Epoch 4776/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2128 - val_loss: 0.2675\n",
      "Epoch 4777/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2744 - val_loss: 0.1405\n",
      "Epoch 4778/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2034 - val_loss: 0.0676\n",
      "Epoch 4779/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1670 - val_loss: 0.0937\n",
      "Epoch 4780/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1045 - val_loss: 0.4507\n",
      "Epoch 4781/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7533 - val_loss: 0.2294\n",
      "Epoch 4782/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4988 - val_loss: 0.4475\n",
      "Epoch 4783/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8656 - val_loss: 1.3363\n",
      "Epoch 4784/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8390 - val_loss: 0.2525\n",
      "Epoch 4785/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3383 - val_loss: 0.2443\n",
      "Epoch 4786/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3262 - val_loss: 0.3284\n",
      "Epoch 4787/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5654 - val_loss: 1.6723\n",
      "Epoch 4788/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6934 - val_loss: 1.0176\n",
      "Epoch 4789/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5758 - val_loss: 1.6937\n",
      "Epoch 4790/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0593 - val_loss: 1.4383\n",
      "Epoch 4791/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4377 - val_loss: 0.9710\n",
      "Epoch 4792/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.5921 - val_loss: 0.8425\n",
      "Epoch 4793/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7355 - val_loss: 2.2921\n",
      "Epoch 4794/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0046 - val_loss: 1.0333\n",
      "Epoch 4795/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7181 - val_loss: 0.3419\n",
      "Epoch 4796/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8922 - val_loss: 0.6917\n",
      "Epoch 4797/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3328 - val_loss: 1.2475\n",
      "Epoch 4798/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6401 - val_loss: 1.0995\n",
      "Epoch 4799/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6893 - val_loss: 0.2547\n",
      "Epoch 4800/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2824 - val_loss: 0.1448\n",
      "Epoch 4801/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1826 - val_loss: 0.1652\n",
      "Epoch 4802/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1126 - val_loss: 0.0689\n",
      "Epoch 4803/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1048 - val_loss: 0.0681\n",
      "Epoch 4804/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1432 - val_loss: 0.0739\n",
      "Epoch 4805/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0596 - val_loss: 0.0817\n",
      "Epoch 4806/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0515 - val_loss: 0.0369\n",
      "Epoch 4807/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0457 - val_loss: 0.0393\n",
      "Epoch 4808/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0430 - val_loss: 0.0464\n",
      "Epoch 4809/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0377 - val_loss: 0.0282\n",
      "Epoch 4810/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0535 - val_loss: 0.0372\n",
      "Epoch 4811/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0278 - val_loss: 0.0289\n",
      "Epoch 4812/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0276 - val_loss: 0.0553\n",
      "Epoch 4813/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0285 - val_loss: 0.0294\n",
      "Epoch 4814/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0368 - val_loss: 0.0501\n",
      "Epoch 4815/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0514 - val_loss: 0.0282\n",
      "Epoch 4816/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0435 - val_loss: 0.0396\n",
      "Epoch 4817/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0468 - val_loss: 0.0328\n",
      "Epoch 4818/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0664 - val_loss: 0.0324\n",
      "Epoch 4819/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0724 - val_loss: 0.1612\n",
      "Epoch 4820/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0893 - val_loss: 0.0408\n",
      "Epoch 4821/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0744 - val_loss: 0.1393\n",
      "Epoch 4822/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0655 - val_loss: 0.0732\n",
      "Epoch 4823/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0892 - val_loss: 0.0924\n",
      "Epoch 4824/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1050 - val_loss: 0.2910\n",
      "Epoch 4825/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2584 - val_loss: 0.0244\n",
      "Epoch 4826/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2095 - val_loss: 0.2838\n",
      "Epoch 4827/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2007 - val_loss: 0.1172\n",
      "Epoch 4828/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2735 - val_loss: 0.3576\n",
      "Epoch 4829/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2552 - val_loss: 0.1591\n",
      "Epoch 4830/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1798 - val_loss: 0.2597\n",
      "Epoch 4831/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2293 - val_loss: 0.3086\n",
      "Epoch 4832/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2876 - val_loss: 0.0351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4833/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0638 - val_loss: 0.0226\n",
      "Epoch 4834/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0742 - val_loss: 0.0982\n",
      "Epoch 4835/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1780 - val_loss: 0.1283\n",
      "Epoch 4836/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1308 - val_loss: 0.0434\n",
      "Epoch 4837/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0631 - val_loss: 0.0454\n",
      "Epoch 4838/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0938 - val_loss: 0.1369\n",
      "Epoch 4839/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1018 - val_loss: 0.0345\n",
      "Epoch 4840/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0643 - val_loss: 0.0438\n",
      "Epoch 4841/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0251 - val_loss: 0.0214\n",
      "Epoch 4842/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0169 - val_loss: 0.0251\n",
      "Epoch 4843/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0321 - val_loss: 0.0300\n",
      "Epoch 4844/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0430 - val_loss: 0.0799\n",
      "Epoch 4845/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1142 - val_loss: 0.0959\n",
      "Epoch 4846/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1109 - val_loss: 0.1791\n",
      "Epoch 4847/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1301 - val_loss: 0.0922\n",
      "Epoch 4848/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0700 - val_loss: 0.1359\n",
      "Epoch 4849/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1165 - val_loss: 0.0706\n",
      "Epoch 4850/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1019 - val_loss: 0.0235\n",
      "Epoch 4851/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0266 - val_loss: 0.0174\n",
      "Epoch 4852/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0205 - val_loss: 0.0207\n",
      "Epoch 4853/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0220 - val_loss: 0.0128\n",
      "Epoch 4854/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0183 - val_loss: 0.0363\n",
      "Epoch 4855/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0232 - val_loss: 0.0215\n",
      "Epoch 4856/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0417 - val_loss: 0.1822\n",
      "Epoch 4857/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0706 - val_loss: 0.0900\n",
      "Epoch 4858/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2743 - val_loss: 0.1973\n",
      "Epoch 4859/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4974 - val_loss: 0.4818\n",
      "Epoch 4860/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3372 - val_loss: 0.4718\n",
      "Epoch 4861/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4044 - val_loss: 0.1091\n",
      "Epoch 4862/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2867 - val_loss: 0.0998\n",
      "Epoch 4863/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1782 - val_loss: 0.2287\n",
      "Epoch 4864/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1728 - val_loss: 0.5012\n",
      "Epoch 4865/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3654 - val_loss: 0.0813\n",
      "Epoch 4866/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1852 - val_loss: 0.0268\n",
      "Epoch 4867/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2086 - val_loss: 0.1666\n",
      "Epoch 4868/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6837 - val_loss: 0.1598\n",
      "Epoch 4869/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4789 - val_loss: 0.0378\n",
      "Epoch 4870/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2465 - val_loss: 0.3817\n",
      "Epoch 4871/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1971 - val_loss: 0.1714\n",
      "Epoch 4872/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1186 - val_loss: 0.1394\n",
      "Epoch 4873/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2141 - val_loss: 0.1412\n",
      "Epoch 4874/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1288 - val_loss: 0.3107\n",
      "Epoch 4875/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1339 - val_loss: 0.2282\n",
      "Epoch 4876/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1078 - val_loss: 0.1851\n",
      "Epoch 4877/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1486 - val_loss: 0.1024\n",
      "Epoch 4878/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1080 - val_loss: 0.0489\n",
      "Epoch 4879/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1807 - val_loss: 0.0681\n",
      "Epoch 4880/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4499 - val_loss: 1.5888\n",
      "Epoch 4881/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5901 - val_loss: 2.7715\n",
      "Epoch 4882/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.1851 - val_loss: 5.6039\n",
      "Epoch 4883/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2253 - val_loss: 1.5631\n",
      "Epoch 4884/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6387 - val_loss: 1.9663\n",
      "Epoch 4885/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17.1443 - val_loss: 27.6978\n",
      "Epoch 4886/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 37.4189 - val_loss: 106.4619\n",
      "Epoch 4887/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 46.4388 - val_loss: 22.7994\n",
      "Epoch 4888/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.4125 - val_loss: 6.2636\n",
      "Epoch 4889/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.5592 - val_loss: 1.8556\n",
      "Epoch 4890/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.8103 - val_loss: 8.8863\n",
      "Epoch 4891/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.7763 - val_loss: 90.6220\n",
      "Epoch 4892/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 34.8081 - val_loss: 14.3237\n",
      "Epoch 4893/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.7260 - val_loss: 6.4742\n",
      "Epoch 4894/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.7663 - val_loss: 2.4084\n",
      "Epoch 4895/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.0368 - val_loss: 0.1251\n",
      "Epoch 4896/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2101 - val_loss: 0.2082\n",
      "Epoch 4897/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8105 - val_loss: 3.4341\n",
      "Epoch 4898/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.8778 - val_loss: 2.6361\n",
      "Epoch 4899/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7192 - val_loss: 0.9907\n",
      "Epoch 4900/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5360 - val_loss: 0.5013\n",
      "Epoch 4901/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4698 - val_loss: 0.1863\n",
      "Epoch 4902/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2345 - val_loss: 0.1041\n",
      "Epoch 4903/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4416 - val_loss: 0.5669\n",
      "Epoch 4904/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4382 - val_loss: 0.7234\n",
      "Epoch 4905/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2667 - val_loss: 6.2948\n",
      "Epoch 4906/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.0027 - val_loss: 16.2472\n",
      "Epoch 4907/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.6008 - val_loss: 13.8312\n",
      "Epoch 4908/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 20.5142 - val_loss: 49.1598\n",
      "Epoch 4909/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21.7922 - val_loss: 10.2061\n",
      "Epoch 4910/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 28.0488 - val_loss: 101.2172\n",
      "Epoch 4911/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 62.1334 - val_loss: 245.7664\n",
      "Epoch 4912/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 250.1004 - val_loss: 614.7745\n",
      "Epoch 4913/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 247.1159 - val_loss: 6.3551\n",
      "Epoch 4914/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 96.9269 - val_loss: 291.5727\n",
      "Epoch 4915/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 107.7764 - val_loss: 56.7060\n",
      "Epoch 4916/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 26.8594 - val_loss: 18.1427\n",
      "Epoch 4917/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 28.3729 - val_loss: 22.7387\n",
      "Epoch 4918/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11.8465 - val_loss: 0.0218\n",
      "Epoch 4919/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.0804 - val_loss: 22.4239\n",
      "Epoch 4920/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.6026 - val_loss: 40.6656\n",
      "Epoch 4921/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 98.5361 - val_loss: 17.7685\n",
      "Epoch 4922/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 52.7965 - val_loss: 78.2254\n",
      "Epoch 4923/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 77.7449 - val_loss: 35.0125\n",
      "Epoch 4924/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 144.7203 - val_loss: 15.5115\n",
      "Epoch 4925/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 275.9612 - val_loss: 109.1027\n",
      "Epoch 4926/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 201.5683 - val_loss: 278.1028\n",
      "Epoch 4927/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 97.2609 - val_loss: 59.8891\n",
      "Epoch 4928/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 70.6902 - val_loss: 146.0464\n",
      "Epoch 4929/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 158.2549 - val_loss: 41.5458\n",
      "Epoch 4930/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 530.7303 - val_loss: 245.9925\n",
      "Epoch 4931/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 665.0756 - val_loss: 1508.7207\n",
      "Epoch 4932/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1777.5887 - val_loss: 4906.4145\n",
      "Epoch 4933/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4413.6695 - val_loss: 158.5202\n",
      "Epoch 4934/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1973.2329 - val_loss: 4998.6858\n",
      "Epoch 4935/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2199.7539 - val_loss: 703.5864\n",
      "Epoch 4936/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 801.5895 - val_loss: 711.4346\n",
      "Epoch 4937/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 869.8389 - val_loss: 560.2716\n",
      "Epoch 4938/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1256.7668 - val_loss: 1009.0844\n",
      "Epoch 4939/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 468.8220 - val_loss: 271.5714\n",
      "Epoch 4940/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 592.2624 - val_loss: 430.9675\n",
      "Epoch 4941/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 428.8593 - val_loss: 163.0179\n",
      "Epoch 4942/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 314.0681 - val_loss: 502.8771\n",
      "Epoch 4943/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 250.4738 - val_loss: 110.4951\n",
      "Epoch 4944/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 38.5740 - val_loss: 12.9740\n",
      "Epoch 4945/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.2394 - val_loss: 5.6024\n",
      "Epoch 4946/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.4280 - val_loss: 29.1707\n",
      "Epoch 4947/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 24.1471 - val_loss: 42.9028\n",
      "Epoch 4948/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 20.9895 - val_loss: 1.3423\n",
      "Epoch 4949/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.4243 - val_loss: 4.6983\n",
      "Epoch 4950/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.0588 - val_loss: 2.2216\n",
      "Epoch 4951/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4352 - val_loss: 0.6920\n",
      "Epoch 4952/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4642 - val_loss: 0.9053\n",
      "Epoch 4953/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7187 - val_loss: 0.8680\n",
      "Epoch 4954/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3377 - val_loss: 1.5135\n",
      "Epoch 4955/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9620 - val_loss: 0.1911\n",
      "Epoch 4956/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.7612 - val_loss: 0.6203\n",
      "Epoch 4957/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7622 - val_loss: 0.2779\n",
      "Epoch 4958/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2329 - val_loss: 0.0684\n",
      "Epoch 4959/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1129 - val_loss: 0.0353\n",
      "Epoch 4960/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0469 - val_loss: 0.0170\n",
      "Epoch 4961/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0283 - val_loss: 0.0129\n",
      "Epoch 4962/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0130 - val_loss: 7.2672e-04\n",
      "Epoch 4963/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0073 - val_loss: 0.0020\n",
      "Epoch 4964/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0067 - val_loss: 0.0019\n",
      "Epoch 4965/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0054 - val_loss: 0.0015\n",
      "Epoch 4966/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0043 - val_loss: 0.0028\n",
      "Epoch 4967/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0047 - val_loss: 0.0011\n",
      "Epoch 4968/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0035 - val_loss: 4.7458e-04\n",
      "Epoch 4969/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0060 - val_loss: 0.0023\n",
      "Epoch 4970/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0050 - val_loss: 0.0069\n",
      "Epoch 4971/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0211 - val_loss: 0.0047\n",
      "Epoch 4972/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0113 - val_loss: 0.0261\n",
      "Epoch 4973/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0221 - val_loss: 0.0208\n",
      "Epoch 4974/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0135 - val_loss: 0.0232\n",
      "Epoch 4975/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0275 - val_loss: 0.0016\n",
      "Epoch 4976/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0236 - val_loss: 0.0472\n",
      "Epoch 4977/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0238 - val_loss: 0.0283\n",
      "Epoch 4978/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0563 - val_loss: 0.1092\n",
      "Epoch 4979/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0529 - val_loss: 0.0391\n",
      "Epoch 4980/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0759 - val_loss: 0.1149\n",
      "Epoch 4981/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1997 - val_loss: 0.0168\n",
      "Epoch 4982/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1160 - val_loss: 0.1275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4983/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1033 - val_loss: 0.0127\n",
      "Epoch 4984/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0147 - val_loss: 0.0236\n",
      "Epoch 4985/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0288 - val_loss: 0.0480\n",
      "Epoch 4986/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0395 - val_loss: 0.0341\n",
      "Epoch 4987/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0611 - val_loss: 0.0765\n",
      "Epoch 4988/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0325 - val_loss: 0.0294\n",
      "Epoch 4989/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0394 - val_loss: 0.0125\n",
      "Epoch 4990/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0435 - val_loss: 0.0394\n",
      "Epoch 4991/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0404 - val_loss: 0.0316\n",
      "Epoch 4992/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0308 - val_loss: 0.0290\n",
      "Epoch 4993/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0179 - val_loss: 0.0054\n",
      "Epoch 4994/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0100 - val_loss: 0.0143\n",
      "Epoch 4995/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0184 - val_loss: 0.0220\n",
      "Epoch 4996/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0124 - val_loss: 0.0069\n",
      "Epoch 4997/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0164 - val_loss: 0.0019\n",
      "Epoch 4998/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0190 - val_loss: 0.0018\n",
      "Epoch 4999/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0115 - val_loss: 0.0013\n",
      "Epoch 5000/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0031 - val_loss: 0.0034\n",
      "Epoch 5001/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0127 - val_loss: 0.0281\n",
      "Epoch 5002/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0143 - val_loss: 0.0306\n",
      "Epoch 5003/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0229 - val_loss: 0.0194\n",
      "Epoch 5004/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0096 - val_loss: 0.0086\n",
      "Epoch 5005/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 5006/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0702 - val_loss: 0.2208\n",
      "Epoch 5007/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1375 - val_loss: 0.1437\n",
      "Epoch 5008/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1755 - val_loss: 0.2725\n",
      "Epoch 5009/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1267 - val_loss: 0.0413\n",
      "Epoch 5010/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0244 - val_loss: 0.0110\n",
      "Epoch 5011/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0072 - val_loss: 0.0012\n",
      "Epoch 5012/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0027 - val_loss: 0.0128\n",
      "Epoch 5013/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0124 - val_loss: 0.0080\n",
      "Epoch 5014/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0121 - val_loss: 9.3873e-04\n",
      "Epoch 5015/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0090 - val_loss: 0.0011\n",
      "Epoch 5016/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0093 - val_loss: 0.0162\n",
      "Epoch 5017/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0160 - val_loss: 0.0176\n",
      "Epoch 5018/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0574 - val_loss: 0.0475\n",
      "Epoch 5019/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0462 - val_loss: 0.1058\n",
      "Epoch 5020/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0661 - val_loss: 0.0892\n",
      "Epoch 5021/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0497 - val_loss: 0.0426\n",
      "Epoch 5022/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0132 - val_loss: 0.0040\n",
      "Epoch 5023/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0048 - val_loss: 0.0010\n",
      "Epoch 5024/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 5025/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0067 - val_loss: 0.0126\n",
      "Epoch 5026/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0077 - val_loss: 0.0361\n",
      "Epoch 5027/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0136 - val_loss: 0.0155\n",
      "Epoch 5028/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0155 - val_loss: 0.0028\n",
      "Epoch 5029/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0053 - val_loss: 0.0083\n",
      "Epoch 5030/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0066 - val_loss: 0.0044\n",
      "Epoch 5031/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 5032/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0027 - val_loss: 7.5437e-04\n",
      "Epoch 5033/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 5034/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0033 - val_loss: 0.0035\n",
      "Epoch 5035/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0037 - val_loss: 0.0114\n",
      "Epoch 5036/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0163 - val_loss: 0.0113\n",
      "Epoch 5037/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0101 - val_loss: 0.0014\n",
      "Epoch 5038/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0023 - val_loss: 5.8176e-04\n",
      "Epoch 5039/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0013 - val_loss: 0.0033\n",
      "Epoch 5040/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0051 - val_loss: 0.0078\n",
      "Epoch 5041/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0088 - val_loss: 0.0021\n",
      "Epoch 5042/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0013 - val_loss: 6.2002e-04\n",
      "Epoch 5043/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.5743e-04 - val_loss: 3.0433e-04\n",
      "Epoch 5044/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0014 - val_loss: 4.4980e-04\n",
      "Epoch 5045/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 5046/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.2445e-04 - val_loss: 4.6724e-04\n",
      "Epoch 5047/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0027 - val_loss: 0.0065\n",
      "Epoch 5048/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0113 - val_loss: 0.0380\n",
      "Epoch 5049/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0411 - val_loss: 0.0120\n",
      "Epoch 5050/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0263 - val_loss: 0.0527\n",
      "Epoch 5051/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0266 - val_loss: 0.0389\n",
      "Epoch 5052/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0950 - val_loss: 0.3636\n",
      "Epoch 5053/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1373 - val_loss: 0.0498\n",
      "Epoch 5054/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3055 - val_loss: 0.0567\n",
      "Epoch 5055/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9237 - val_loss: 2.4578\n",
      "Epoch 5056/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4103 - val_loss: 0.8140\n",
      "Epoch 5057/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3839 - val_loss: 0.2808\n",
      "Epoch 5058/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.7057 - val_loss: 30.5233\n",
      "Epoch 5059/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 37.6675 - val_loss: 1.7716\n",
      "Epoch 5060/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.1149 - val_loss: 5.3156\n",
      "Epoch 5061/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.3473 - val_loss: 8.3020\n",
      "Epoch 5062/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 8.6404 - val_loss: 0.1895\n",
      "Epoch 5063/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.6618 - val_loss: 13.6202\n",
      "Epoch 5064/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 13.49 - 0s 147us/sample - loss: 32.2847 - val_loss: 136.6006\n",
      "Epoch 5065/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 62.0760 - val_loss: 2.5706\n",
      "Epoch 5066/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8935 - val_loss: 2.6416\n",
      "Epoch 5067/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.4676 - val_loss: 2.1440\n",
      "Epoch 5068/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1108 - val_loss: 0.7822\n",
      "Epoch 5069/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1888 - val_loss: 1.9131\n",
      "Epoch 5070/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6859 - val_loss: 4.4318\n",
      "Epoch 5071/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3.6000 - val_loss: 5.0710\n",
      "Epoch 5072/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 7.0990 - val_loss: 15.5765\n",
      "Epoch 5073/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.3598 - val_loss: 22.4768\n",
      "Epoch 5074/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 40.2895 - val_loss: 1.8988\n",
      "Epoch 5075/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 29.5893 - val_loss: 28.5173\n",
      "Epoch 5076/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 60.5715 - val_loss: 149.6702\n",
      "Epoch 5077/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 202.1391 - val_loss: 44.2136\n",
      "Epoch 5078/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 370.0271 - val_loss: 61.2401\n",
      "Epoch 5079/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 612.3561 - val_loss: 1031.5906\n",
      "Epoch 5080/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 544.6361 - val_loss: 614.5382\n",
      "Epoch 5081/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 735.2068 - val_loss: 208.5964\n",
      "Epoch 5082/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 413.8016 - val_loss: 830.0422\n",
      "Epoch 5083/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 621.7900 - val_loss: 703.1081\n",
      "Epoch 5084/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 904.8612 - val_loss: 133.9094\n",
      "Epoch 5085/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 154.9900 - val_loss: 74.6542\n",
      "Epoch 5086/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 52.9054 - val_loss: 115.1051\n",
      "Epoch 5087/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 79.8152 - val_loss: 14.0740\n",
      "Epoch 5088/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.0942 - val_loss: 4.2513\n",
      "Epoch 5089/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.1544 - val_loss: 28.1348\n",
      "Epoch 5090/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.2472 - val_loss: 29.1547\n",
      "Epoch 5091/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.5443 - val_loss: 5.4606\n",
      "Epoch 5092/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.1251 - val_loss: 0.1980\n",
      "Epoch 5093/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.0130 - val_loss: 2.7499\n",
      "Epoch 5094/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.5485 - val_loss: 29.8644\n",
      "Epoch 5095/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 100.6030 - val_loss: 155.2322\n",
      "Epoch 5096/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 605.6360 - val_loss: 997.9267\n",
      "Epoch 5097/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1404.4931 - val_loss: 2018.5029\n",
      "Epoch 5098/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1288.5539 - val_loss: 1851.0218\n",
      "Epoch 5099/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 672.6545 - val_loss: 747.6537\n",
      "Epoch 5100/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 544.1996 - val_loss: 397.8181\n",
      "Epoch 5101/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 289.1938 - val_loss: 375.3798\n",
      "Epoch 5102/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 573.281 - 0s 147us/sample - loss: 1655.4701 - val_loss: 3922.0520\n",
      "Epoch 5103/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9536.5011 - val_loss: 13821.4504\n",
      "Epoch 5104/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 23406.2629 - val_loss: 16561.2945\n",
      "Epoch 5105/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 25132.1593 - val_loss: 24028.3640\n",
      "Epoch 5106/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18831.9270 - val_loss: 15332.1137\n",
      "Epoch 5107/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16359.4610 - val_loss: 769.5173\n",
      "Epoch 5108/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11440.1931 - val_loss: 3937.4140\n",
      "Epoch 5109/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3676.8617 - val_loss: 931.8052\n",
      "Epoch 5110/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1655.3193 - val_loss: 1026.2786\n",
      "Epoch 5111/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 507.0851 - val_loss: 786.7499\n",
      "Epoch 5112/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 341.1576 - val_loss: 649.7424\n",
      "Epoch 5113/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 433.0568 - val_loss: 115.4459\n",
      "Epoch 5114/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 372.0798 - val_loss: 68.0694\n",
      "Epoch 5115/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 227.9579 - val_loss: 51.7533\n",
      "Epoch 5116/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 137.3453 - val_loss: 166.9938\n",
      "Epoch 5117/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 63.7549 - val_loss: 102.1885\n",
      "Epoch 5118/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 30.8028 - val_loss: 20.5775\n",
      "Epoch 5119/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.4674 - val_loss: 4.7913\n",
      "Epoch 5120/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.8015 - val_loss: 3.1414\n",
      "Epoch 5121/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.9166 - val_loss: 0.8273\n",
      "Epoch 5122/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.7965 - val_loss: 0.1794\n",
      "Epoch 5123/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3633 - val_loss: 1.3519\n",
      "Epoch 5124/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7689 - val_loss: 1.0174\n",
      "Epoch 5125/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3635 - val_loss: 0.0704\n",
      "Epoch 5126/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9128 - val_loss: 0.1083\n",
      "Epoch 5127/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6349 - val_loss: 0.0673\n",
      "Epoch 5128/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5268 - val_loss: 0.1070\n",
      "Epoch 5129/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5321 - val_loss: 0.0866\n",
      "Epoch 5130/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5061 - val_loss: 0.1336\n",
      "Epoch 5131/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4326 - val_loss: 0.0773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5132/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.4169 - val_loss: 0.0838\n",
      "Epoch 5133/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3748 - val_loss: 0.0659\n",
      "Epoch 5134/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3954 - val_loss: 0.2145\n",
      "Epoch 5135/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4799 - val_loss: 0.3502\n",
      "Epoch 5136/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3858 - val_loss: 0.0849\n",
      "Epoch 5137/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3130 - val_loss: 0.0629\n",
      "Epoch 5138/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2861 - val_loss: 0.0876\n",
      "Epoch 5139/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2580 - val_loss: 0.0602\n",
      "Epoch 5140/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2400 - val_loss: 0.0773\n",
      "Epoch 5141/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2302 - val_loss: 0.0592\n",
      "Epoch 5142/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2235 - val_loss: 0.0831\n",
      "Epoch 5143/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2294 - val_loss: 0.1198\n",
      "Epoch 5144/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2145 - val_loss: 0.0756\n",
      "Epoch 5145/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2130 - val_loss: 0.1373\n",
      "Epoch 5146/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2422 - val_loss: 0.0739\n",
      "Epoch 5147/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1672 - val_loss: 0.0559\n",
      "Epoch 5148/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1471 - val_loss: 0.0755\n",
      "Epoch 5149/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1666 - val_loss: 0.0592\n",
      "Epoch 5150/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1516 - val_loss: 0.0641\n",
      "Epoch 5151/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1304 - val_loss: 0.1246\n",
      "Epoch 5152/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1514 - val_loss: 0.0568\n",
      "Epoch 5153/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1362 - val_loss: 0.0529\n",
      "Epoch 5154/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1191 - val_loss: 0.0738\n",
      "Epoch 5155/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.077 - 0s 132us/sample - loss: 0.1156 - val_loss: 0.0648\n",
      "Epoch 5156/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1065 - val_loss: 0.0667\n",
      "Epoch 5157/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1296 - val_loss: 0.1220\n",
      "Epoch 5158/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1693 - val_loss: 0.0530\n",
      "Epoch 5159/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1757 - val_loss: 0.1834\n",
      "Epoch 5160/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1847 - val_loss: 0.0899\n",
      "Epoch 5161/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1051 - val_loss: 0.0601\n",
      "Epoch 5162/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0821 - val_loss: 0.0705\n",
      "Epoch 5163/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0923 - val_loss: 0.0514\n",
      "Epoch 5164/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0974 - val_loss: 0.1269\n",
      "Epoch 5165/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0902 - val_loss: 0.0871\n",
      "Epoch 5166/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1298 - val_loss: 0.0653\n",
      "Epoch 5167/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1506 - val_loss: 0.0988\n",
      "Epoch 5168/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1149 - val_loss: 0.1139\n",
      "Epoch 5169/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1209 - val_loss: 0.1738\n",
      "Epoch 5170/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2132 - val_loss: 0.0534\n",
      "Epoch 5171/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2634 - val_loss: 0.2561\n",
      "Epoch 5172/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1592 - val_loss: 0.0748\n",
      "Epoch 5173/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1293 - val_loss: 0.1140\n",
      "Epoch 5174/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1204 - val_loss: 0.0501\n",
      "Epoch 5175/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1085 - val_loss: 0.1849\n",
      "Epoch 5176/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1500 - val_loss: 0.0620\n",
      "Epoch 5177/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0741 - val_loss: 0.0605\n",
      "Epoch 5178/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0777 - val_loss: 0.1595\n",
      "Epoch 5179/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1379 - val_loss: 0.0729\n",
      "Epoch 5180/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1098 - val_loss: 0.1204\n",
      "Epoch 5181/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0869 - val_loss: 0.0574\n",
      "Epoch 5182/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0847 - val_loss: 0.0489\n",
      "Epoch 5183/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0650 - val_loss: 0.0572\n",
      "Epoch 5184/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.022 - 0s 147us/sample - loss: 0.0526 - val_loss: 0.0630\n",
      "Epoch 5185/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0540 - val_loss: 0.0682\n",
      "Epoch 5186/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0703 - val_loss: 0.0551\n",
      "Epoch 5187/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0645 - val_loss: 0.0462\n",
      "Epoch 5188/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0764 - val_loss: 0.1200\n",
      "Epoch 5189/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1054 - val_loss: 0.0521\n",
      "Epoch 5190/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0767 - val_loss: 0.0605\n",
      "Epoch 5191/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0441 - val_loss: 0.0504\n",
      "Epoch 5192/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0505 - val_loss: 0.1306\n",
      "Epoch 5193/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0606 - val_loss: 0.0386\n",
      "Epoch 5194/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0451 - val_loss: 0.0361\n",
      "Epoch 5195/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.036 - 0s 147us/sample - loss: 0.0637 - val_loss: 0.0665\n",
      "Epoch 5196/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0397 - val_loss: 0.0393\n",
      "Epoch 5197/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0354 - val_loss: 0.0623\n",
      "Epoch 5198/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0392 - val_loss: 0.0447\n",
      "Epoch 5199/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0519 - val_loss: 0.0385\n",
      "Epoch 5200/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0616 - val_loss: 0.0697\n",
      "Epoch 5201/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0708 - val_loss: 0.0480\n",
      "Epoch 5202/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0662 - val_loss: 0.0711\n",
      "Epoch 5203/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0651 - val_loss: 0.0957\n",
      "Epoch 5204/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0983 - val_loss: 0.0878\n",
      "Epoch 5205/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1253 - val_loss: 0.1489\n",
      "Epoch 5206/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1268 - val_loss: 0.0675\n",
      "Epoch 5207/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1319 - val_loss: 0.0585\n",
      "Epoch 5208/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1255 - val_loss: 0.2870\n",
      "Epoch 5209/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1957 - val_loss: 0.0329\n",
      "Epoch 5210/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1529 - val_loss: 0.2672\n",
      "Epoch 5211/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1729 - val_loss: 0.0796\n",
      "Epoch 5212/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0997 - val_loss: 0.0691\n",
      "Epoch 5213/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0460 - val_loss: 0.0322\n",
      "Epoch 5214/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0502 - val_loss: 0.0430\n",
      "Epoch 5215/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0322 - val_loss: 0.0322\n",
      "Epoch 5216/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0281 - val_loss: 0.0453\n",
      "Epoch 5217/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0507 - val_loss: 0.0462\n",
      "Epoch 5218/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0506 - val_loss: 0.0365\n",
      "Epoch 5219/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0321 - val_loss: 0.0395\n",
      "Epoch 5220/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0363 - val_loss: 0.0489\n",
      "Epoch 5221/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0307 - val_loss: 0.0316\n",
      "Epoch 5222/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0286 - val_loss: 0.0317\n",
      "Epoch 5223/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0327 - val_loss: 0.0295\n",
      "Epoch 5224/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0253 - val_loss: 0.0238\n",
      "Epoch 5225/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0268 - val_loss: 0.0231\n",
      "Epoch 5226/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0234 - val_loss: 0.0241\n",
      "Epoch 5227/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0223 - val_loss: 0.0234\n",
      "Epoch 5228/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0230 - val_loss: 0.0218\n",
      "Epoch 5229/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0225 - val_loss: 0.0327\n",
      "Epoch 5230/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0283 - val_loss: 0.0780\n",
      "Epoch 5231/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0679 - val_loss: 0.0730\n",
      "Epoch 5232/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0479 - val_loss: 0.0260\n",
      "Epoch 5233/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0401 - val_loss: 0.0249\n",
      "Epoch 5234/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0360 - val_loss: 0.0308\n",
      "Epoch 5235/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0281 - val_loss: 0.0482\n",
      "Epoch 5236/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0289 - val_loss: 0.0259\n",
      "Epoch 5237/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0336 - val_loss: 0.0198\n",
      "Epoch 5238/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0246 - val_loss: 0.0806\n",
      "Epoch 5239/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0318 - val_loss: 0.0340\n",
      "Epoch 5240/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0384 - val_loss: 0.0538\n",
      "Epoch 5241/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0620 - val_loss: 0.1657\n",
      "Epoch 5242/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0931 - val_loss: 0.1178\n",
      "Epoch 5243/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0635 - val_loss: 0.0220\n",
      "Epoch 5244/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0522 - val_loss: 0.0959\n",
      "Epoch 5245/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0493 - val_loss: 0.0192\n",
      "Epoch 5246/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0172 - val_loss: 0.0343\n",
      "Epoch 5247/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0243 - val_loss: 0.0251\n",
      "Epoch 5248/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0331 - val_loss: 0.0222\n",
      "Epoch 5249/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0247 - val_loss: 0.0211\n",
      "Epoch 5250/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0162 - val_loss: 0.0180\n",
      "Epoch 5251/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0297 - val_loss: 0.0174\n",
      "Epoch 5252/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0287 - val_loss: 0.0180\n",
      "Epoch 5253/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0213 - val_loss: 0.0174\n",
      "Epoch 5254/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0315 - val_loss: 0.0298\n",
      "Epoch 5255/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0258 - val_loss: 0.0158\n",
      "Epoch 5256/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0158 - val_loss: 0.0263\n",
      "Epoch 5257/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0226 - val_loss: 0.0248\n",
      "Epoch 5258/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0197 - val_loss: 0.0148\n",
      "Epoch 5259/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0188 - val_loss: 0.0158\n",
      "Epoch 5260/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0126 - val_loss: 0.0308\n",
      "Epoch 5261/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0264 - val_loss: 0.0189\n",
      "Epoch 5262/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0180 - val_loss: 0.0183\n",
      "Epoch 5263/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0136 - val_loss: 0.0130\n",
      "Epoch 5264/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0143 - val_loss: 0.0146\n",
      "Epoch 5265/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0137 - val_loss: 0.0132\n",
      "Epoch 5266/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0142 - val_loss: 0.0169\n",
      "Epoch 5267/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0154 - val_loss: 0.0136\n",
      "Epoch 5268/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0149 - val_loss: 0.0123\n",
      "Epoch 5269/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0208 - val_loss: 0.0350\n",
      "Epoch 5270/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0620 - val_loss: 0.0511\n",
      "Epoch 5271/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1139 - val_loss: 0.0716\n",
      "Epoch 5272/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0473 - val_loss: 0.0559\n",
      "Epoch 5273/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0330 - val_loss: 0.0317\n",
      "Epoch 5274/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0206 - val_loss: 0.0306\n",
      "Epoch 5275/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0163 - val_loss: 0.0166\n",
      "Epoch 5276/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0162 - val_loss: 0.0253\n",
      "Epoch 5277/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0195 - val_loss: 0.0120\n",
      "Epoch 5278/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0165 - val_loss: 0.0373\n",
      "Epoch 5279/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0495 - val_loss: 0.0776\n",
      "Epoch 5280/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0582 - val_loss: 0.0245\n",
      "Epoch 5281/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0244 - val_loss: 0.0259\n",
      "Epoch 5282/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0298 - val_loss: 0.0326\n",
      "Epoch 5283/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0185 - val_loss: 0.0211\n",
      "Epoch 5284/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0183 - val_loss: 0.0149\n",
      "Epoch 5285/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0203 - val_loss: 0.0217\n",
      "Epoch 5286/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0230 - val_loss: 0.0261\n",
      "Epoch 5287/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0131 - val_loss: 0.0103\n",
      "Epoch 5288/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0094 - val_loss: 0.0090\n",
      "Epoch 5289/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0085 - val_loss: 0.0129\n",
      "Epoch 5290/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0149 - val_loss: 0.0168\n",
      "Epoch 5291/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0248 - val_loss: 0.0218\n",
      "Epoch 5292/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0133 - val_loss: 0.0084\n",
      "Epoch 5293/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0074 - val_loss: 0.0077\n",
      "Epoch 5294/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0078 - val_loss: 0.0084\n",
      "Epoch 5295/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0106 - val_loss: 0.0139\n",
      "Epoch 5296/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0076 - val_loss: 0.0083\n",
      "Epoch 5297/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0088 - val_loss: 0.0157\n",
      "Epoch 5298/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0115 - val_loss: 0.0074\n",
      "Epoch 5299/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0082 - val_loss: 0.0138\n",
      "Epoch 5300/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0096 - val_loss: 0.0069\n",
      "Epoch 5301/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0081 - val_loss: 0.0087\n",
      "Epoch 5302/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0118 - val_loss: 0.0102\n",
      "Epoch 5303/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0113 - val_loss: 0.0075\n",
      "Epoch 5304/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0229 - val_loss: 0.0127\n",
      "Epoch 5305/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0171 - val_loss: 0.0074\n",
      "Epoch 5306/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0089 - val_loss: 0.0076\n",
      "Epoch 5307/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0113 - val_loss: 0.0119\n",
      "Epoch 5308/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0110 - val_loss: 0.0142\n",
      "Epoch 5309/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0074 - val_loss: 0.0059\n",
      "Epoch 5310/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0076 - val_loss: 0.0072\n",
      "Epoch 5311/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0056 - val_loss: 0.0060\n",
      "Epoch 5312/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0076 - val_loss: 0.0098\n",
      "Epoch 5313/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0065 - val_loss: 0.0058\n",
      "Epoch 5314/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0098 - val_loss: 0.0302\n",
      "Epoch 5315/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0333 - val_loss: 0.0071\n",
      "Epoch 5316/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0083 - val_loss: 0.0122\n",
      "Epoch 5317/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0109 - val_loss: 0.0130\n",
      "Epoch 5318/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0131 - val_loss: 0.0069\n",
      "Epoch 5319/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0143 - val_loss: 0.0255\n",
      "Epoch 5320/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0338 - val_loss: 0.0182\n",
      "Epoch 5321/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0182 - val_loss: 0.0086\n",
      "Epoch 5322/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0121 - val_loss: 0.0061\n",
      "Epoch 5323/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0390 - val_loss: 0.0556\n",
      "Epoch 5324/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0322 - val_loss: 0.0141\n",
      "Epoch 5325/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0102 - val_loss: 0.0092\n",
      "Epoch 5326/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0088 - val_loss: 0.0056\n",
      "Epoch 5327/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0076 - val_loss: 0.0050\n",
      "Epoch 5328/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0097 - val_loss: 0.0052\n",
      "Epoch 5329/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0077 - val_loss: 0.0084\n",
      "Epoch 5330/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0133 - val_loss: 0.0586\n",
      "Epoch 5331/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0258 - val_loss: 0.0096\n",
      "Epoch 5332/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0197 - val_loss: 0.0113\n",
      "Epoch 5333/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0120 - val_loss: 0.0037\n",
      "Epoch 5334/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0071 - val_loss: 0.0056\n",
      "Epoch 5335/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0091 - val_loss: 0.0084\n",
      "Epoch 5336/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0250 - val_loss: 0.1268\n",
      "Epoch 5337/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0633 - val_loss: 0.0343\n",
      "Epoch 5338/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0169 - val_loss: 0.0115\n",
      "Epoch 5339/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0327 - val_loss: 0.0645\n",
      "Epoch 5340/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0651 - val_loss: 0.0361\n",
      "Epoch 5341/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0271 - val_loss: 0.0151\n",
      "Epoch 5342/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0248 - val_loss: 0.0228\n",
      "Epoch 5343/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0223 - val_loss: 0.0545\n",
      "Epoch 5344/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0258 - val_loss: 0.0184\n",
      "Epoch 5345/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0198 - val_loss: 0.0317\n",
      "Epoch 5346/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0537 - val_loss: 0.0651\n",
      "Epoch 5347/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0597 - val_loss: 0.0625\n",
      "Epoch 5348/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0207 - val_loss: 0.0053\n",
      "Epoch 5349/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0056 - val_loss: 0.0060\n",
      "Epoch 5350/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 5351/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0046 - val_loss: 0.0048\n",
      "Epoch 5352/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 5353/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0031 - val_loss: 0.0036\n",
      "Epoch 5354/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0035 - val_loss: 0.0031\n",
      "Epoch 5355/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 5356/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 5357/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0052 - val_loss: 0.0046\n",
      "Epoch 5358/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0051 - val_loss: 0.0027\n",
      "Epoch 5359/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0060 - val_loss: 0.0071\n",
      "Epoch 5360/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0132 - val_loss: 0.0238\n",
      "Epoch 5361/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0163 - val_loss: 0.0057\n",
      "Epoch 5362/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0081 - val_loss: 0.0035\n",
      "Epoch 5363/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0085 - val_loss: 0.0025\n",
      "Epoch 5364/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0042 - val_loss: 0.0036\n",
      "Epoch 5365/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0041 - val_loss: 0.0030\n",
      "Epoch 5366/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0045 - val_loss: 0.0038\n",
      "Epoch 5367/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0078 - val_loss: 0.0129\n",
      "Epoch 5368/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.012 - 0s 118us/sample - loss: 0.0098 - val_loss: 0.0022\n",
      "Epoch 5369/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0048 - val_loss: 0.0069\n",
      "Epoch 5370/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0038 - val_loss: 0.0072\n",
      "Epoch 5371/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0077 - val_loss: 0.0114\n",
      "Epoch 5372/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0108 - val_loss: 0.0031\n",
      "Epoch 5373/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0085 - val_loss: 0.0045\n",
      "Epoch 5374/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 5375/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0021 - val_loss: 0.0044\n",
      "Epoch 5376/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0068 - val_loss: 0.0064\n",
      "Epoch 5377/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0034 - val_loss: 0.0024\n",
      "Epoch 5378/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0022 - val_loss: 0.0158\n",
      "Epoch 5379/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0145 - val_loss: 0.0335\n",
      "Epoch 5380/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0449 - val_loss: 0.0227\n",
      "Epoch 5381/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0487 - val_loss: 0.0816\n",
      "Epoch 5382/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1191 - val_loss: 0.0718\n",
      "Epoch 5383/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0509 - val_loss: 0.0261\n",
      "Epoch 5384/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1269 - val_loss: 0.3604\n",
      "Epoch 5385/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1959 - val_loss: 0.0360\n",
      "Epoch 5386/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0265 - val_loss: 0.0040\n",
      "Epoch 5387/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0217 - val_loss: 0.0343\n",
      "Epoch 5388/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0337 - val_loss: 0.0474\n",
      "Epoch 5389/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0344 - val_loss: 0.0458\n",
      "Epoch 5390/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0362 - val_loss: 0.0265\n",
      "Epoch 5391/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0132 - val_loss: 0.0229\n",
      "Epoch 5392/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0110 - val_loss: 0.0095\n",
      "Epoch 5393/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0090 - val_loss: 0.0028\n",
      "Epoch 5394/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0064 - val_loss: 0.0016\n",
      "Epoch 5395/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0033 - val_loss: 0.0012\n",
      "Epoch 5396/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0060 - val_loss: 0.0046\n",
      "Epoch 5397/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0074 - val_loss: 0.0033\n",
      "Epoch 5398/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 5399/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0156 - val_loss: 0.0176\n",
      "Epoch 5400/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0094 - val_loss: 0.0071\n",
      "Epoch 5401/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0249 - val_loss: 0.0027\n",
      "Epoch 5402/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0348 - val_loss: 0.0078\n",
      "Epoch 5403/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0132 - val_loss: 0.0144\n",
      "Epoch 5404/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0143 - val_loss: 0.0044\n",
      "Epoch 5405/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0052 - val_loss: 0.0025\n",
      "Epoch 5406/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0022 - val_loss: 0.0068\n",
      "Epoch 5407/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0027 - val_loss: 0.0013\n",
      "Epoch 5408/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0019 - val_loss: 0.0032\n",
      "Epoch 5409/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0026 - val_loss: 0.0060\n",
      "Epoch 5410/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0039 - val_loss: 0.0020\n",
      "Epoch 5411/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0108 - val_loss: 0.0163\n",
      "Epoch 5412/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0134 - val_loss: 0.0220\n",
      "Epoch 5413/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0114 - val_loss: 0.0080\n",
      "Epoch 5414/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0051 - val_loss: 0.0041\n",
      "Epoch 5415/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0042 - val_loss: 9.9412e-04\n",
      "Epoch 5416/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.6064e-04 - val_loss: 0.0032\n",
      "Epoch 5417/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0012 - val_loss: 6.5278e-04\n",
      "Epoch 5418/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0013 - val_loss: 5.1921e-04\n",
      "Epoch 5419/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.5860e-04 - val_loss: 0.0013\n",
      "Epoch 5420/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0026 - val_loss: 0.0016\n",
      "Epoch 5421/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0056 - val_loss: 0.0018\n",
      "Epoch 5422/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 5423/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0011 - val_loss: 7.2509e-04\n",
      "Epoch 5424/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 0.0028\n",
      "Epoch 5425/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0048 - val_loss: 0.0191\n",
      "Epoch 5426/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0172 - val_loss: 0.0178\n",
      "Epoch 5427/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0421 - val_loss: 0.0445\n",
      "Epoch 5428/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1042 - val_loss: 0.1871\n",
      "Epoch 5429/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1076 - val_loss: 0.1143\n",
      "Epoch 5430/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0892 - val_loss: 0.0204\n",
      "Epoch 5431/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0372 - val_loss: 0.0615\n",
      "Epoch 5432/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0659 - val_loss: 0.0993\n",
      "Epoch 5433/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1529 - val_loss: 0.0867\n",
      "Epoch 5434/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0446 - val_loss: 0.0947\n",
      "Epoch 5435/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0668 - val_loss: 0.0177\n",
      "Epoch 5436/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0579 - val_loss: 0.0032\n",
      "Epoch 5437/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0880 - val_loss: 0.2336\n",
      "Epoch 5438/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2829 - val_loss: 0.2689\n",
      "Epoch 5439/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5376 - val_loss: 1.6801\n",
      "Epoch 5440/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8058 - val_loss: 0.0427\n",
      "Epoch 5441/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1295 - val_loss: 0.1258\n",
      "Epoch 5442/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2829 - val_loss: 0.2016\n",
      "Epoch 5443/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0005 - val_loss: 0.1659\n",
      "Epoch 5444/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3676 - val_loss: 0.5026\n",
      "Epoch 5445/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2653 - val_loss: 0.4920\n",
      "Epoch 5446/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4012 - val_loss: 0.0975\n",
      "Epoch 5447/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5404 - val_loss: 1.5264\n",
      "Epoch 5448/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6328 - val_loss: 0.0992\n",
      "Epoch 5449/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3776 - val_loss: 0.9319\n",
      "Epoch 5450/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3468 - val_loss: 0.5157\n",
      "Epoch 5451/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2953 - val_loss: 0.7285\n",
      "Epoch 5452/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2302 - val_loss: 0.1096\n",
      "Epoch 5453/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0458 - val_loss: 0.0143\n",
      "Epoch 5454/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0248 - val_loss: 0.0155\n",
      "Epoch 5455/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0719 - val_loss: 0.4639\n",
      "Epoch 5456/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5031 - val_loss: 1.3880\n",
      "Epoch 5457/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.9359 - val_loss: 0.0032\n",
      "Epoch 5458/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3914 - val_loss: 1.4007\n",
      "Epoch 5459/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6217 - val_loss: 0.5430\n",
      "Epoch 5460/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0661 - val_loss: 0.6592\n",
      "Epoch 5461/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3583 - val_loss: 0.0196\n",
      "Epoch 5462/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0465 - val_loss: 0.0104\n",
      "Epoch 5463/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0244 - val_loss: 0.0188\n",
      "Epoch 5464/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0095 - val_loss: 0.0018\n",
      "Epoch 5465/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0573 - val_loss: 0.0113\n",
      "Epoch 5466/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.3126 - val_loss: 0.1004\n",
      "Epoch 5467/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.1038 - val_loss: 0.1363\n",
      "Epoch 5468/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1.5411 - val_loss: 1.1116\n",
      "Epoch 5469/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.9211 - val_loss: 0.0352\n",
      "Epoch 5470/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3830 - val_loss: 1.3694\n",
      "Epoch 5471/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.0928 - val_loss: 4.6543\n",
      "Epoch 5472/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.0107 - val_loss: 17.0317\n",
      "Epoch 5473/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.0528 - val_loss: 0.2352\n",
      "Epoch 5474/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.1386 - val_loss: 0.7074\n",
      "Epoch 5475/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 126.9903 - val_loss: 31.5057\n",
      "Epoch 5476/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 132.6584 - val_loss: 287.8556\n",
      "Epoch 5477/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3724.8919 - val_loss: 6924.2463\n",
      "Epoch 5478/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4421.5905 - val_loss: 7893.9421\n",
      "Epoch 5479/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8472.1059 - val_loss: 3201.3228\n",
      "Epoch 5480/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5360.5541 - val_loss: 6302.9751\n",
      "Epoch 5481/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4885.3888 - val_loss: 5907.4469\n",
      "Epoch 5482/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4448.0674 - val_loss: 6404.9804\n",
      "Epoch 5483/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 6096.24 - 0s 132us/sample - loss: 6161.2944 - val_loss: 23520.2487\n",
      "Epoch 5484/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12465.9815 - val_loss: 13198.4074\n",
      "Epoch 5485/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 15320.4000 - val_loss: 1144.3589\n",
      "Epoch 5486/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5427.8083 - val_loss: 6280.4145\n",
      "Epoch 5487/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5109.8719 - val_loss: 503.7075\n",
      "Epoch 5488/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3650.8497 - val_loss: 3936.1216\n",
      "Epoch 5489/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1794.6374 - val_loss: 1540.1059\n",
      "Epoch 5490/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1059.8257 - val_loss: 772.7848\n",
      "Epoch 5491/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 572.8858 - val_loss: 247.8047\n",
      "Epoch 5492/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 363.4028 - val_loss: 45.6069\n",
      "Epoch 5493/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 237.7895 - val_loss: 348.5133\n",
      "Epoch 5494/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 199.1665 - val_loss: 32.1696\n",
      "Epoch 5495/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 75.8499 - val_loss: 89.4588\n",
      "Epoch 5496/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 41.0135 - val_loss: 10.0742\n",
      "Epoch 5497/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.0950 - val_loss: 9.8061\n",
      "Epoch 5498/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 4.8195 - val_loss: 1.6819\n",
      "Epoch 5499/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9058 - val_loss: 0.3560\n",
      "Epoch 5500/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3198 - val_loss: 0.5131\n",
      "Epoch 5501/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1403 - val_loss: 0.4641\n",
      "Epoch 5502/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7548 - val_loss: 0.1628\n",
      "Epoch 5503/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4328 - val_loss: 0.0910\n",
      "Epoch 5504/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.4218 - val_loss: 0.0383\n",
      "Epoch 5505/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3671 - val_loss: 0.0796\n",
      "Epoch 5506/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2964 - val_loss: 0.0525\n",
      "Epoch 5507/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3323 - val_loss: 0.2692\n",
      "Epoch 5508/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3668 - val_loss: 0.0571\n",
      "Epoch 5509/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2884 - val_loss: 0.0746\n",
      "Epoch 5510/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2351 - val_loss: 0.0368\n",
      "Epoch 5511/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2140 - val_loss: 0.0417\n",
      "Epoch 5512/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1997 - val_loss: 0.0317\n",
      "Epoch 5513/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2122 - val_loss: 0.1471\n",
      "Epoch 5514/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1995 - val_loss: 0.0351\n",
      "Epoch 5515/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1412 - val_loss: 0.0522\n",
      "Epoch 5516/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1331 - val_loss: 0.0270\n",
      "Epoch 5517/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1177 - val_loss: 0.0313\n",
      "Epoch 5518/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1315 - val_loss: 0.0361\n",
      "Epoch 5519/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1310 - val_loss: 0.0541\n",
      "Epoch 5520/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1344 - val_loss: 0.0295\n",
      "Epoch 5521/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1576 - val_loss: 0.1295\n",
      "Epoch 5522/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1844 - val_loss: 0.1649\n",
      "Epoch 5523/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1807 - val_loss: 0.0438\n",
      "Epoch 5524/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1105 - val_loss: 0.1664\n",
      "Epoch 5525/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1346 - val_loss: 0.0252\n",
      "Epoch 5526/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0786 - val_loss: 0.0735\n",
      "Epoch 5527/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0822 - val_loss: 0.0430\n",
      "Epoch 5528/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0733 - val_loss: 0.0294\n",
      "Epoch 5529/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0628 - val_loss: 0.0700\n",
      "Epoch 5530/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0680 - val_loss: 0.0441\n",
      "Epoch 5531/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0637 - val_loss: 0.0238\n",
      "Epoch 5532/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0493 - val_loss: 0.0224\n",
      "Epoch 5533/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0399 - val_loss: 0.0233\n",
      "Epoch 5534/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0412 - val_loss: 0.0230\n",
      "Epoch 5535/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0366 - val_loss: 0.0318\n",
      "Epoch 5536/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0399 - val_loss: 0.0217\n",
      "Epoch 5537/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0440 - val_loss: 0.0587\n",
      "Epoch 5538/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0410 - val_loss: 0.0719\n",
      "Epoch 5539/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0671 - val_loss: 0.0219\n",
      "Epoch 5540/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0421 - val_loss: 0.0243\n",
      "Epoch 5541/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0439 - val_loss: 0.0302\n",
      "Epoch 5542/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0363 - val_loss: 0.0336\n",
      "Epoch 5543/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0328 - val_loss: 0.0384\n",
      "Epoch 5544/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0268 - val_loss: 0.0184\n",
      "Epoch 5545/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0236 - val_loss: 0.0182\n",
      "Epoch 5546/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0239 - val_loss: 0.0175\n",
      "Epoch 5547/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0280 - val_loss: 0.0482\n",
      "Epoch 5548/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0372 - val_loss: 0.0318\n",
      "Epoch 5549/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0478 - val_loss: 0.0214\n",
      "Epoch 5550/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0422 - val_loss: 0.0167\n",
      "Epoch 5551/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0502 - val_loss: 0.0381\n",
      "Epoch 5552/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0577 - val_loss: 0.2139\n",
      "Epoch 5553/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1018 - val_loss: 0.0261\n",
      "Epoch 5554/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0568 - val_loss: 0.0528\n",
      "Epoch 5555/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0768 - val_loss: 0.1226\n",
      "Epoch 5556/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0485 - val_loss: 0.0638\n",
      "Epoch 5557/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0466 - val_loss: 0.0289\n",
      "Epoch 5558/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0343 - val_loss: 0.0314\n",
      "Epoch 5559/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0230 - val_loss: 0.0144\n",
      "Epoch 5560/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0205 - val_loss: 0.0278\n",
      "Epoch 5561/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0201 - val_loss: 0.0140\n",
      "Epoch 5562/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0177 - val_loss: 0.0143\n",
      "Epoch 5563/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0170 - val_loss: 0.0195\n",
      "Epoch 5564/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0166 - val_loss: 0.0382\n",
      "Epoch 5565/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0209 - val_loss: 0.0143\n",
      "Epoch 5566/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0164 - val_loss: 0.0240\n",
      "Epoch 5567/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0194 - val_loss: 0.0333\n",
      "Epoch 5568/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0242 - val_loss: 0.0270\n",
      "Epoch 5569/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0184 - val_loss: 0.0155\n",
      "Epoch 5570/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0162 - val_loss: 0.0195\n",
      "Epoch 5571/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0227 - val_loss: 0.0179\n",
      "Epoch 5572/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0408 - val_loss: 0.0258\n",
      "Epoch 5573/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0149 - val_loss: 0.0136\n",
      "Epoch 5574/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0140 - val_loss: 0.0215\n",
      "Epoch 5575/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0239 - val_loss: 0.0198\n",
      "Epoch 5576/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0214 - val_loss: 0.0171\n",
      "Epoch 5577/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0168 - val_loss: 0.0160\n",
      "Epoch 5578/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0169 - val_loss: 0.0140\n",
      "Epoch 5579/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0136 - val_loss: 0.0132\n",
      "Epoch 5580/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0177 - val_loss: 0.0119\n",
      "Epoch 5581/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0133 - val_loss: 0.0151\n",
      "Epoch 5582/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0151 - val_loss: 0.0137\n",
      "Epoch 5583/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0174 - val_loss: 0.0155\n",
      "Epoch 5584/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0331 - val_loss: 0.0252\n",
      "Epoch 5585/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0178 - val_loss: 0.0343\n",
      "Epoch 5586/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0261 - val_loss: 0.0092\n",
      "Epoch 5587/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0194 - val_loss: 0.0099\n",
      "Epoch 5588/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0275 - val_loss: 0.0248\n",
      "Epoch 5589/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0202 - val_loss: 0.0227\n",
      "Epoch 5590/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0182 - val_loss: 0.0116\n",
      "Epoch 5591/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0237 - val_loss: 0.0127\n",
      "Epoch 5592/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0270 - val_loss: 0.0146\n",
      "Epoch 5593/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0188 - val_loss: 0.0216\n",
      "Epoch 5594/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0172 - val_loss: 0.0359\n",
      "Epoch 5595/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0257 - val_loss: 0.0274\n",
      "Epoch 5596/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0173 - val_loss: 0.0085\n",
      "Epoch 5597/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0201 - val_loss: 0.0118\n",
      "Epoch 5598/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0124 - val_loss: 0.0102\n",
      "Epoch 5599/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0109 - val_loss: 0.0135\n",
      "Epoch 5600/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0099 - val_loss: 0.0083\n",
      "Epoch 5601/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0077 - val_loss: 0.0073\n",
      "Epoch 5602/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0104 - val_loss: 0.0074\n",
      "Epoch 5603/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0087 - val_loss: 0.0139\n",
      "Epoch 5604/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0121 - val_loss: 0.0301\n",
      "Epoch 5605/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0293 - val_loss: 0.0625\n",
      "Epoch 5606/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0604 - val_loss: 0.0190\n",
      "Epoch 5607/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0183 - val_loss: 0.0131\n",
      "Epoch 5608/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0093 - val_loss: 0.0069\n",
      "Epoch 5609/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0098 - val_loss: 0.0083\n",
      "Epoch 5610/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0093 - val_loss: 0.0075\n",
      "Epoch 5611/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0093 - val_loss: 0.0081\n",
      "Epoch 5612/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0114 - val_loss: 0.0074\n",
      "Epoch 5613/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0088 - val_loss: 0.0077\n",
      "Epoch 5614/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0066 - val_loss: 0.0065\n",
      "Epoch 5615/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0066 - val_loss: 0.0058\n",
      "Epoch 5616/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 5617/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0107 - val_loss: 0.0074\n",
      "Epoch 5618/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0143 - val_loss: 0.0194\n",
      "Epoch 5619/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0590 - val_loss: 0.0171\n",
      "Epoch 5620/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0237 - val_loss: 0.0061\n",
      "Epoch 5621/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0101 - val_loss: 0.0187\n",
      "Epoch 5622/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0230 - val_loss: 0.0081\n",
      "Epoch 5623/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0140 - val_loss: 0.0106\n",
      "Epoch 5624/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0224 - val_loss: 0.0153\n",
      "Epoch 5625/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0121 - val_loss: 0.0106\n",
      "Epoch 5626/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0148 - val_loss: 0.0053\n",
      "Epoch 5627/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0156 - val_loss: 0.0148\n",
      "Epoch 5628/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0182 - val_loss: 0.0241\n",
      "Epoch 5629/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0136 - val_loss: 0.0209\n",
      "Epoch 5630/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0116 - val_loss: 0.0206\n",
      "Epoch 5631/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0120 - val_loss: 0.0124\n",
      "Epoch 5632/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0193 - val_loss: 0.0049\n",
      "Epoch 5633/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0092 - val_loss: 0.0165\n",
      "Epoch 5634/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0159 - val_loss: 0.0455\n",
      "Epoch 5635/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0245 - val_loss: 0.0078\n",
      "Epoch 5636/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0128 - val_loss: 0.0056\n",
      "Epoch 5637/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0077 - val_loss: 0.0053\n",
      "Epoch 5638/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0089 - val_loss: 0.0087\n",
      "Epoch 5639/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0072 - val_loss: 0.0057\n",
      "Epoch 5640/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0043 - val_loss: 0.0033\n",
      "Epoch 5641/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0042 - val_loss: 0.0049\n",
      "Epoch 5642/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0037 - val_loss: 0.0032\n",
      "Epoch 5643/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0045 - val_loss: 0.0091\n",
      "Epoch 5644/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0070 - val_loss: 0.0091\n",
      "Epoch 5645/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0114 - val_loss: 0.0058\n",
      "Epoch 5646/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0061 - val_loss: 0.0031\n",
      "Epoch 5647/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 5648/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0040 - val_loss: 0.0028\n",
      "Epoch 5649/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0080 - val_loss: 0.0134\n",
      "Epoch 5650/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0288 - val_loss: 0.0629\n",
      "Epoch 5651/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0728 - val_loss: 0.1250\n",
      "Epoch 5652/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1581 - val_loss: 0.1681\n",
      "Epoch 5653/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4136 - val_loss: 0.6873\n",
      "Epoch 5654/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3368 - val_loss: 0.3440\n",
      "Epoch 5655/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2210 - val_loss: 0.1319\n",
      "Epoch 5656/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0970 - val_loss: 0.3595\n",
      "Epoch 5657/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2003 - val_loss: 0.0199\n",
      "Epoch 5658/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0734 - val_loss: 0.0302\n",
      "Epoch 5659/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0257 - val_loss: 0.0049\n",
      "Epoch 5660/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0076 - val_loss: 0.0045\n",
      "Epoch 5661/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0045 - val_loss: 0.0039\n",
      "Epoch 5662/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0036 - val_loss: 0.0121\n",
      "Epoch 5663/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0110 - val_loss: 0.0368\n",
      "Epoch 5664/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0175 - val_loss: 0.0075\n",
      "Epoch 5665/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0062 - val_loss: 0.0025\n",
      "Epoch 5666/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0053 - val_loss: 0.0113\n",
      "Epoch 5667/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0063 - val_loss: 0.0035\n",
      "Epoch 5668/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0181 - val_loss: 0.0410\n",
      "Epoch 5669/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0328 - val_loss: 0.0436\n",
      "Epoch 5670/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0679 - val_loss: 0.0571\n",
      "Epoch 5671/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0602 - val_loss: 0.0204\n",
      "Epoch 5672/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0238 - val_loss: 0.0199\n",
      "Epoch 5673/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0143 - val_loss: 0.0022\n",
      "Epoch 5674/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0247 - val_loss: 0.0477\n",
      "Epoch 5675/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0171 - val_loss: 0.0126\n",
      "Epoch 5676/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0057 - val_loss: 0.0028\n",
      "Epoch 5677/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0082 - val_loss: 0.0085\n",
      "Epoch 5678/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0259 - val_loss: 0.0222\n",
      "Epoch 5679/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0347 - val_loss: 0.0836\n",
      "Epoch 5680/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0523 - val_loss: 0.0109\n",
      "Epoch 5681/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0169 - val_loss: 0.0038\n",
      "Epoch 5682/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0041 - val_loss: 0.0025\n",
      "Epoch 5683/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0033 - val_loss: 0.0038\n",
      "Epoch 5684/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0058 - val_loss: 0.0029\n",
      "Epoch 5685/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0044 - val_loss: 0.0070\n",
      "Epoch 5686/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0120 - val_loss: 0.0143\n",
      "Epoch 5687/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0252 - val_loss: 0.0047\n",
      "Epoch 5688/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0359 - val_loss: 0.0731\n",
      "Epoch 5689/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0294 - val_loss: 0.0095\n",
      "Epoch 5690/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0142 - val_loss: 0.0321\n",
      "Epoch 5691/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0586 - val_loss: 0.0110\n",
      "Epoch 5692/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0225 - val_loss: 0.0275\n",
      "Epoch 5693/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0211 - val_loss: 0.0025\n",
      "Epoch 5694/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0047 - val_loss: 0.0031\n",
      "Epoch 5695/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0060 - val_loss: 0.0027\n",
      "Epoch 5696/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0035 - val_loss: 0.0021\n",
      "Epoch 5697/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0031 - val_loss: 0.0020\n",
      "Epoch 5698/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0013 - val_loss: 0.0023\n",
      "Epoch 5699/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 5700/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 5701/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 5702/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0064 - val_loss: 0.0242\n",
      "Epoch 5703/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0184 - val_loss: 0.0159\n",
      "Epoch 5704/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0053 - val_loss: 0.0096\n",
      "Epoch 5705/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0100 - val_loss: 0.0097\n",
      "Epoch 5706/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.016 - 0s 132us/sample - loss: 0.0064 - val_loss: 0.0031\n",
      "Epoch 5707/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0028 - val_loss: 0.0019\n",
      "Epoch 5708/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0041 - val_loss: 0.0026\n",
      "Epoch 5709/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0050 - val_loss: 0.0049\n",
      "Epoch 5710/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0047 - val_loss: 0.0110\n",
      "Epoch 5711/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0083 - val_loss: 0.0028\n",
      "Epoch 5712/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0053 - val_loss: 0.0160\n",
      "Epoch 5713/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0114 - val_loss: 0.0255\n",
      "Epoch 5714/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0221 - val_loss: 0.0052\n",
      "Epoch 5715/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0102 - val_loss: 0.0015\n",
      "Epoch 5716/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0046 - val_loss: 0.0035\n",
      "Epoch 5717/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0087 - val_loss: 6.5530e-04\n",
      "Epoch 5718/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0063 - val_loss: 6.9715e-04\n",
      "Epoch 5719/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0040 - val_loss: 0.0010\n",
      "Epoch 5720/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0011 - val_loss: 6.5532e-04\n",
      "Epoch 5721/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0034 - val_loss: 0.0059\n",
      "Epoch 5722/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0053 - val_loss: 0.0038\n",
      "Epoch 5723/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.003 - 0s 147us/sample - loss: 0.0038 - val_loss: 0.0238\n",
      "Epoch 5724/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0106 - val_loss: 0.0153\n",
      "Epoch 5725/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0068 - val_loss: 0.0111\n",
      "Epoch 5726/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0081 - val_loss: 0.0019\n",
      "Epoch 5727/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0039 - val_loss: 0.0017\n",
      "Epoch 5728/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0078 - val_loss: 0.0049\n",
      "Epoch 5729/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0149 - val_loss: 0.0405\n",
      "Epoch 5730/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0611 - val_loss: 0.0067\n",
      "Epoch 5731/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0697 - val_loss: 0.0161\n",
      "Epoch 5732/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0801 - val_loss: 0.0153\n",
      "Epoch 5733/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0497 - val_loss: 0.0450\n",
      "Epoch 5734/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0311 - val_loss: 0.1040\n",
      "Epoch 5735/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2480 - val_loss: 0.9606\n",
      "Epoch 5736/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.9579 - val_loss: 2.7851\n",
      "Epoch 5737/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.0901 - val_loss: 5.5973\n",
      "Epoch 5738/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 5.852 - 0s 176us/sample - loss: 22.5798 - val_loss: 5.2480\n",
      "Epoch 5739/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 15.8685 - val_loss: 6.8291\n",
      "Epoch 5740/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.0674 - val_loss: 69.3251\n",
      "Epoch 5741/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 33.2204 - val_loss: 6.6970\n",
      "Epoch 5742/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 23.4141 - val_loss: 15.4841\n",
      "Epoch 5743/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 56.0698 - val_loss: 12.4099\n",
      "Epoch 5744/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 45.5148 - val_loss: 222.8102\n",
      "Epoch 5745/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 90.3888 - val_loss: 56.6095\n",
      "Epoch 5746/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 48.6179 - val_loss: 31.0206\n",
      "Epoch 5747/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 30.4069 - val_loss: 20.8924\n",
      "Epoch 5748/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.2527 - val_loss: 2.3307\n",
      "Epoch 5749/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.9615 - val_loss: 1.9830\n",
      "Epoch 5750/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.7164 - val_loss: 2.1469\n",
      "Epoch 5751/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.8498 - val_loss: 12.7535\n",
      "Epoch 5752/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.9583 - val_loss: 9.3448\n",
      "Epoch 5753/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.2805 - val_loss: 8.8056\n",
      "Epoch 5754/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.9140 - val_loss: 0.9567\n",
      "Epoch 5755/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 24.4275 - val_loss: 3.2956\n",
      "Epoch 5756/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 38.7097 - val_loss: 1.3816\n",
      "Epoch 5757/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 52.6403 - val_loss: 32.4675\n",
      "Epoch 5758/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 65.1001 - val_loss: 3.5218\n",
      "Epoch 5759/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 13.4383 - val_loss: 9.8719\n",
      "Epoch 5760/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.4534 - val_loss: 1.3005\n",
      "Epoch 5761/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.9475 - val_loss: 3.4585\n",
      "Epoch 5762/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.3273 - val_loss: 29.2826\n",
      "Epoch 5763/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 25.1958 - val_loss: 2.8734\n",
      "Epoch 5764/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16.8961 - val_loss: 0.4568\n",
      "Epoch 5765/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 12.9995 - val_loss: 3.6584\n",
      "Epoch 5766/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 12.8815 - val_loss: 19.5240\n",
      "Epoch 5767/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 53.7389 - val_loss: 160.7657\n",
      "Epoch 5768/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 173.437 - 0s 162us/sample - loss: 150.4241 - val_loss: 589.8139\n",
      "Epoch 5769/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1451.7111 - val_loss: 246.6838\n",
      "Epoch 5770/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2614.3575 - val_loss: 27.2296\n",
      "Epoch 5771/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3724.3922 - val_loss: 4080.4875\n",
      "Epoch 5772/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4321.4530 - val_loss: 3579.8466\n",
      "Epoch 5773/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3377.8010 - val_loss: 3117.1511\n",
      "Epoch 5774/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1600.8136 - val_loss: 24.8665\n",
      "Epoch 5775/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 455.0742 - val_loss: 551.6052\n",
      "Epoch 5776/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 320.3643 - val_loss: 102.4804\n",
      "Epoch 5777/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 119.1878 - val_loss: 150.6347\n",
      "Epoch 5778/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 61.6585 - val_loss: 61.2959\n",
      "Epoch 5779/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 31.4606 - val_loss: 16.8069\n",
      "Epoch 5780/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15.8935 - val_loss: 5.6471\n",
      "Epoch 5781/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11.1117 - val_loss: 0.0069\n",
      "Epoch 5782/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.2760 - val_loss: 2.0831\n",
      "Epoch 5783/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.8067 - val_loss: 0.5072\n",
      "Epoch 5784/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.2361 - val_loss: 1.8423\n",
      "Epoch 5785/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.8560 - val_loss: 6.1012\n",
      "Epoch 5786/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.2080 - val_loss: 7.0688\n",
      "Epoch 5787/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.0693 - val_loss: 0.2579\n",
      "Epoch 5788/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.3963 - val_loss: 4.0312\n",
      "Epoch 5789/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.0797 - val_loss: 3.6004\n",
      "Epoch 5790/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.4525 - val_loss: 0.9576\n",
      "Epoch 5791/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.4427 - val_loss: 5.6940\n",
      "Epoch 5792/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.1134 - val_loss: 5.0636\n",
      "Epoch 5793/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4809 - val_loss: 3.1617\n",
      "Epoch 5794/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.8810 - val_loss: 12.6503\n",
      "Epoch 5795/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.8204 - val_loss: 2.8435\n",
      "Epoch 5796/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.5712 - val_loss: 20.9529\n",
      "Epoch 5797/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 18.2085 - val_loss: 40.0469\n",
      "Epoch 5798/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26.3096 - val_loss: 32.3459\n",
      "Epoch 5799/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 28.8659 - val_loss: 36.1797\n",
      "Epoch 5800/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16.8339 - val_loss: 7.6476\n",
      "Epoch 5801/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.2470 - val_loss: 0.0379\n",
      "Epoch 5802/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.4830 - val_loss: 12.5023\n",
      "Epoch 5803/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.6980 - val_loss: 7.0593\n",
      "Epoch 5804/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.5252 - val_loss: 32.5931\n",
      "Epoch 5805/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.9050 - val_loss: 23.0523\n",
      "Epoch 5806/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 12.6547 - val_loss: 7.8283\n",
      "Epoch 5807/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.8569 - val_loss: 40.6397\n",
      "Epoch 5808/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.0929 - val_loss: 3.0702\n",
      "Epoch 5809/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 5.0580 - val_loss: 6.7080\n",
      "Epoch 5810/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.6692 - val_loss: 2.8243\n",
      "Epoch 5811/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.7401 - val_loss: 32.1893\n",
      "Epoch 5812/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15.8324 - val_loss: 13.8465\n",
      "Epoch 5813/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.4452 - val_loss: 1.4576\n",
      "Epoch 5814/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.1772 - val_loss: 10.7316\n",
      "Epoch 5815/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.0293 - val_loss: 1.0668\n",
      "Epoch 5816/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7455 - val_loss: 1.2585\n",
      "Epoch 5817/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.9091 - val_loss: 0.9581\n",
      "Epoch 5818/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.0128 - val_loss: 0.4565\n",
      "Epoch 5819/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4247 - val_loss: 0.3034\n",
      "Epoch 5820/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2155 - val_loss: 0.0988\n",
      "Epoch 5821/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2084 - val_loss: 0.1297\n",
      "Epoch 5822/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3681 - val_loss: 0.2439\n",
      "Epoch 5823/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5990 - val_loss: 0.1740\n",
      "Epoch 5824/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.8042 - val_loss: 2.4430\n",
      "Epoch 5825/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.0607 - val_loss: 2.5175\n",
      "Epoch 5826/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7041 - val_loss: 0.0484\n",
      "Epoch 5827/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.5300 - val_loss: 0.9203\n",
      "Epoch 5828/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7478 - val_loss: 0.4245\n",
      "Epoch 5829/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4925 - val_loss: 0.9359\n",
      "Epoch 5830/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.7556 - val_loss: 0.5406\n",
      "Epoch 5831/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3459 - val_loss: 0.8828\n",
      "Epoch 5832/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8408 - val_loss: 0.6456\n",
      "Epoch 5833/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.2478 - val_loss: 0.7771\n",
      "Epoch 5834/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4225 - val_loss: 0.8687\n",
      "Epoch 5835/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4133 - val_loss: 0.0868\n",
      "Epoch 5836/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2714 - val_loss: 0.1547\n",
      "Epoch 5837/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1193 - val_loss: 0.0190\n",
      "Epoch 5838/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0133 - val_loss: 0.0053\n",
      "Epoch 5839/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0149 - val_loss: 0.0168\n",
      "Epoch 5840/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0254 - val_loss: 0.0205\n",
      "Epoch 5841/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0353 - val_loss: 0.0090\n",
      "Epoch 5842/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0240 - val_loss: 0.0841\n",
      "Epoch 5843/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0379 - val_loss: 0.0081\n",
      "Epoch 5844/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0188 - val_loss: 0.0017\n",
      "Epoch 5845/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0060 - val_loss: 0.0222\n",
      "Epoch 5846/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0138 - val_loss: 0.0042\n",
      "Epoch 5847/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0094 - val_loss: 0.0166\n",
      "Epoch 5848/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0171 - val_loss: 0.0040\n",
      "Epoch 5849/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0166 - val_loss: 0.0014\n",
      "Epoch 5850/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0113 - val_loss: 0.0093\n",
      "Epoch 5851/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0372 - val_loss: 0.1269\n",
      "Epoch 5852/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0530 - val_loss: 0.0484\n",
      "Epoch 5853/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0647 - val_loss: 0.1272\n",
      "Epoch 5854/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1380 - val_loss: 0.0018\n",
      "Epoch 5855/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0922 - val_loss: 0.3763\n",
      "Epoch 5856/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7259 - val_loss: 1.2955\n",
      "Epoch 5857/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.3629 - val_loss: 2.3911\n",
      "Epoch 5858/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.7680 - val_loss: 0.1311\n",
      "Epoch 5859/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3132 - val_loss: 0.9723\n",
      "Epoch 5860/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1.4762 - val_loss: 0.3964\n",
      "Epoch 5861/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1240 - val_loss: 1.8727\n",
      "Epoch 5862/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.6731 - val_loss: 0.3588\n",
      "Epoch 5863/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.7155 - val_loss: 1.2965\n",
      "Epoch 5864/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.5488 - val_loss: 2.4409\n",
      "Epoch 5865/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 4.0349 - val_loss: 8.6443\n",
      "Epoch 5866/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.2110 - val_loss: 16.8203\n",
      "Epoch 5867/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 252.0929 - val_loss: 538.0102\n",
      "Epoch 5868/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 247.6986 - val_loss: 833.9417\n",
      "Epoch 5869/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 309.5668 - val_loss: 115.0712\n",
      "Epoch 5870/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 149.2101 - val_loss: 189.8059\n",
      "Epoch 5871/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 233.4883 - val_loss: 387.1660\n",
      "Epoch 5872/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 491.2236 - val_loss: 24.3786\n",
      "Epoch 5873/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 746.7020 - val_loss: 1136.2625\n",
      "Epoch 5874/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1287.5435 - val_loss: 38.9105\n",
      "Epoch 5875/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1898.9616 - val_loss: 1982.4561\n",
      "Epoch 5876/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1389.8283 - val_loss: 1758.4148\n",
      "Epoch 5877/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1123.1209 - val_loss: 1591.6282\n",
      "Epoch 5878/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2146.7697 - val_loss: 386.7322\n",
      "Epoch 5879/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 615.2334 - val_loss: 225.1124\n",
      "Epoch 5880/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 201.5572 - val_loss: 55.1369\n",
      "Epoch 5881/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 74.3073 - val_loss: 71.0984\n",
      "Epoch 5882/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 55.2787 - val_loss: 82.8616\n",
      "Epoch 5883/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 64.1829 - val_loss: 126.7769\n",
      "Epoch 5884/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 130.7100 - val_loss: 10.7918\n",
      "Epoch 5885/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 242.2631 - val_loss: 203.3727\n",
      "Epoch 5886/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 83.7167 - val_loss: 11.7960\n",
      "Epoch 5887/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 52.3122 - val_loss: 104.8912\n",
      "Epoch 5888/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 93.6952 - val_loss: 77.4719\n",
      "Epoch 5889/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 77.1309 - val_loss: 35.3494\n",
      "Epoch 5890/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 35.6280 - val_loss: 42.3246\n",
      "Epoch 5891/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 57.9589 - val_loss: 7.9575\n",
      "Epoch 5892/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 95.4964 - val_loss: 282.5223\n",
      "Epoch 5893/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 102.2068 - val_loss: 35.0942\n",
      "Epoch 5894/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 123.4229 - val_loss: 1.4282\n",
      "Epoch 5895/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 57.2521 - val_loss: 56.7697\n",
      "Epoch 5896/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 40.4068 - val_loss: 10.6178\n",
      "Epoch 5897/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.5147 - val_loss: 1.4664\n",
      "Epoch 5898/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.3511 - val_loss: 7.2055\n",
      "Epoch 5899/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.5477 - val_loss: 0.5166\n",
      "Epoch 5900/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3563 - val_loss: 5.1349\n",
      "Epoch 5901/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.0041 - val_loss: 0.9805\n",
      "Epoch 5902/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4077 - val_loss: 0.9214\n",
      "Epoch 5903/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2107 - val_loss: 3.2950\n",
      "Epoch 5904/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2158 - val_loss: 4.9499\n",
      "Epoch 5905/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.5777 - val_loss: 1.3844\n",
      "Epoch 5906/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6863 - val_loss: 0.1198\n",
      "Epoch 5907/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4037 - val_loss: 0.7176\n",
      "Epoch 5908/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3860 - val_loss: 0.3601\n",
      "Epoch 5909/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2173 - val_loss: 0.1299\n",
      "Epoch 5910/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1378 - val_loss: 0.3907\n",
      "Epoch 5911/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2658 - val_loss: 0.2087\n",
      "Epoch 5912/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1272 - val_loss: 0.0312\n",
      "Epoch 5913/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1341 - val_loss: 0.1511\n",
      "Epoch 5914/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0967 - val_loss: 0.0618\n",
      "Epoch 5915/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0511 - val_loss: 0.0175\n",
      "Epoch 5916/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0301 - val_loss: 0.0175\n",
      "Epoch 5917/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0303 - val_loss: 0.0266\n",
      "Epoch 5918/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0298 - val_loss: 0.0686\n",
      "Epoch 5919/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0428 - val_loss: 0.0192\n",
      "Epoch 5920/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0398 - val_loss: 0.1655\n",
      "Epoch 5921/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0920 - val_loss: 0.0483\n",
      "Epoch 5922/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0536 - val_loss: 0.1369\n",
      "Epoch 5923/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2739 - val_loss: 0.1661\n",
      "Epoch 5924/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0747 - val_loss: 0.0157\n",
      "Epoch 5925/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0277 - val_loss: 0.0286\n",
      "Epoch 5926/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0276 - val_loss: 0.0278\n",
      "Epoch 5927/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0252 - val_loss: 0.0131\n",
      "Epoch 5928/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0229 - val_loss: 0.0366\n",
      "Epoch 5929/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0227 - val_loss: 0.0568\n",
      "Epoch 5930/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0301 - val_loss: 0.0190\n",
      "Epoch 5931/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0208 - val_loss: 0.0734\n",
      "Epoch 5932/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0476 - val_loss: 0.0847\n",
      "Epoch 5933/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0479 - val_loss: 0.0272\n",
      "Epoch 5934/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0272 - val_loss: 0.0557\n",
      "Epoch 5935/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0253 - val_loss: 0.0194\n",
      "Epoch 5936/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0267 - val_loss: 0.0181\n",
      "Epoch 5937/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0129 - val_loss: 0.0102\n",
      "Epoch 5938/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0084 - val_loss: 0.0234\n",
      "Epoch 5939/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0139 - val_loss: 0.0125\n",
      "Epoch 5940/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0151 - val_loss: 0.0134\n",
      "Epoch 5941/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0164 - val_loss: 0.0365\n",
      "Epoch 5942/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0191 - val_loss: 0.0682\n",
      "Epoch 5943/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0379 - val_loss: 0.0324\n",
      "Epoch 5944/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0159 - val_loss: 0.0101\n",
      "Epoch 5945/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0114 - val_loss: 0.0290\n",
      "Epoch 5946/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0438 - val_loss: 0.1056\n",
      "Epoch 5947/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2073 - val_loss: 0.3071\n",
      "Epoch 5948/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2282 - val_loss: 0.6641\n",
      "Epoch 5949/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.0827 - val_loss: 6.2200\n",
      "Epoch 5950/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.6517 - val_loss: 0.3491\n",
      "Epoch 5951/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5585 - val_loss: 0.0720\n",
      "Epoch 5952/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5279 - val_loss: 0.7794\n",
      "Epoch 5953/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2615 - val_loss: 0.0214\n",
      "Epoch 5954/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2582 - val_loss: 0.0404\n",
      "Epoch 5955/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3598 - val_loss: 0.1517\n",
      "Epoch 5956/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1184 - val_loss: 0.0350\n",
      "Epoch 5957/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2816 - val_loss: 0.9119\n",
      "Epoch 5958/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4719 - val_loss: 0.1984\n",
      "Epoch 5959/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0785 - val_loss: 0.0106\n",
      "Epoch 5960/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0689 - val_loss: 0.0369\n",
      "Epoch 5961/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0174 - val_loss: 0.0057\n",
      "Epoch 5962/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0093 - val_loss: 0.0408\n",
      "Epoch 5963/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0468 - val_loss: 0.0384\n",
      "Epoch 5964/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0486 - val_loss: 0.0077\n",
      "Epoch 5965/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1399 - val_loss: 0.1788\n",
      "Epoch 5966/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1626 - val_loss: 0.1418\n",
      "Epoch 5967/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1792 - val_loss: 0.4154\n",
      "Epoch 5968/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4186 - val_loss: 0.0584\n",
      "Epoch 5969/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2808 - val_loss: 0.3347\n",
      "Epoch 5970/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0860 - val_loss: 0.0314\n",
      "Epoch 5971/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0698 - val_loss: 0.4586\n",
      "Epoch 5972/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1302 - val_loss: 0.0132\n",
      "Epoch 5973/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0693 - val_loss: 0.0618\n",
      "Epoch 5974/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0911 - val_loss: 0.3092\n",
      "Epoch 5975/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0809 - val_loss: 0.0576\n",
      "Epoch 5976/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0308 - val_loss: 0.0869\n",
      "Epoch 5977/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1852 - val_loss: 0.3980\n",
      "Epoch 5978/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4050 - val_loss: 0.2956\n",
      "Epoch 5979/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5360 - val_loss: 1.9147\n",
      "Epoch 5980/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7916 - val_loss: 0.3125\n",
      "Epoch 5981/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6536 - val_loss: 1.2134\n",
      "Epoch 5982/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14.7428 - val_loss: 3.5593\n",
      "Epoch 5983/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 51.5561 - val_loss: 102.3957\n",
      "Epoch 5984/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 318.1730 - val_loss: 469.6523\n",
      "Epoch 5985/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 323.1533 - val_loss: 392.4734\n",
      "Epoch 5986/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 170.8369 - val_loss: 82.1125\n",
      "Epoch 5987/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 70.4295 - val_loss: 20.8073\n",
      "Epoch 5988/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 366.0998 - val_loss: 1869.5526\n",
      "Epoch 5989/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1208.2704 - val_loss: 315.7772\n",
      "Epoch 5990/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 663.7781 - val_loss: 947.6463\n",
      "Epoch 5991/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 599.7530 - val_loss: 601.7694\n",
      "Epoch 5992/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 584.5737 - val_loss: 2098.9080\n",
      "Epoch 5993/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2652.7748 - val_loss: 2340.6711\n",
      "Epoch 5994/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1498.5093 - val_loss: 2739.6729\n",
      "Epoch 5995/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1809.4560 - val_loss: 3867.7274\n",
      "Epoch 5996/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2234.2148 - val_loss: 156.7906\n",
      "Epoch 5997/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1302.1087 - val_loss: 2649.7593\n",
      "Epoch 5998/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1412.8775 - val_loss: 165.3751\n",
      "Epoch 5999/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2869.0319 - val_loss: 6784.4744\n",
      "Epoch 6000/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4672.7090 - val_loss: 7724.7983\n",
      "Epoch 6001/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4094.6141 - val_loss: 3469.2425\n",
      "Epoch 6002/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3135.6485 - val_loss: 1765.7647\n",
      "Epoch 6003/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2270.0018 - val_loss: 3063.4881\n",
      "Epoch 6004/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3511.6934 - val_loss: 1914.6575\n",
      "Epoch 6005/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2358.0026 - val_loss: 3159.3415\n",
      "Epoch 6006/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1454.1509 - val_loss: 597.7567\n",
      "Epoch 6007/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 713.0168 - val_loss: 1166.7128\n",
      "Epoch 6008/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 903.6874 - val_loss: 326.4536\n",
      "Epoch 6009/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 722.7684 - val_loss: 103.3214\n",
      "Epoch 6010/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 359.4213 - val_loss: 216.9813\n",
      "Epoch 6011/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 415.2320 - val_loss: 394.2398\n",
      "Epoch 6012/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 549.2170 - val_loss: 41.5329\n",
      "Epoch 6013/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 409.3072 - val_loss: 36.9823\n",
      "Epoch 6014/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 357.1611 - val_loss: 209.1130\n",
      "Epoch 6015/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 211.1539 - val_loss: 29.4863\n",
      "Epoch 6016/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 103.0935 - val_loss: 150.2916\n",
      "Epoch 6017/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 195.2466 - val_loss: 174.8121\n",
      "Epoch 6018/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 278.2249 - val_loss: 49.5281\n",
      "Epoch 6019/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 229.0550 - val_loss: 22.5759\n",
      "Epoch 6020/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 123.5520 - val_loss: 23.8801\n",
      "Epoch 6021/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 114.6522 - val_loss: 47.9078\n",
      "Epoch 6022/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 57.2992 - val_loss: 96.8725\n",
      "Epoch 6023/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 45.6883 - val_loss: 66.0809\n",
      "Epoch 6024/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 37.0482 - val_loss: 57.1956\n",
      "Epoch 6025/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 47.1530 - val_loss: 18.8794\n",
      "Epoch 6026/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 26.7608 - val_loss: 3.1882\n",
      "Epoch 6027/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 15.7940 - val_loss: 0.1090\n",
      "Epoch 6028/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.4986 - val_loss: 18.8398\n",
      "Epoch 6029/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12.9906 - val_loss: 12.9600\n",
      "Epoch 6030/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.5649 - val_loss: 5.9689\n",
      "Epoch 6031/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.3012 - val_loss: 0.2138\n",
      "Epoch 6032/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8564 - val_loss: 0.5866\n",
      "Epoch 6033/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8677 - val_loss: 0.4331\n",
      "Epoch 6034/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3677 - val_loss: 0.3302\n",
      "Epoch 6035/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4838 - val_loss: 0.2009\n",
      "Epoch 6036/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2228 - val_loss: 0.1734\n",
      "Epoch 6037/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2054 - val_loss: 0.0996\n",
      "Epoch 6038/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2058 - val_loss: 0.0861\n",
      "Epoch 6039/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1677 - val_loss: 0.0713\n",
      "Epoch 6040/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0914 - val_loss: 0.0739\n",
      "Epoch 6041/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1855 - val_loss: 0.3805\n",
      "Epoch 6042/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2032 - val_loss: 0.0863\n",
      "Epoch 6043/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0857 - val_loss: 0.0374\n",
      "Epoch 6044/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0343 - val_loss: 0.0290\n",
      "Epoch 6045/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0393 - val_loss: 0.0509\n",
      "Epoch 6046/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0571 - val_loss: 0.0768\n",
      "Epoch 6047/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0696 - val_loss: 0.0320\n",
      "Epoch 6048/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0342 - val_loss: 0.0263\n",
      "Epoch 6049/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0309 - val_loss: 0.0463\n",
      "Epoch 6050/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0663 - val_loss: 0.0883\n",
      "Epoch 6051/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0914 - val_loss: 0.2584\n",
      "Epoch 6052/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1614 - val_loss: 0.1126\n",
      "Epoch 6053/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1220 - val_loss: 0.2511\n",
      "Epoch 6054/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1678 - val_loss: 0.1014\n",
      "Epoch 6055/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1285 - val_loss: 0.0725\n",
      "Epoch 6056/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1805 - val_loss: 0.1530\n",
      "Epoch 6057/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2560 - val_loss: 0.4231\n",
      "Epoch 6058/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3101 - val_loss: 0.2149\n",
      "Epoch 6059/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1661 - val_loss: 0.1707\n",
      "Epoch 6060/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1240 - val_loss: 0.1291\n",
      "Epoch 6061/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1029 - val_loss: 0.0499\n",
      "Epoch 6062/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1679 - val_loss: 0.0430\n",
      "Epoch 6063/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1187 - val_loss: 0.0228\n",
      "Epoch 6064/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0776 - val_loss: 0.0343\n",
      "Epoch 6065/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0951 - val_loss: 0.1122\n",
      "Epoch 6066/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0808 - val_loss: 0.0716\n",
      "Epoch 6067/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0411 - val_loss: 0.0229\n",
      "Epoch 6068/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0365 - val_loss: 0.0196\n",
      "Epoch 6069/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0299 - val_loss: 0.0216\n",
      "Epoch 6070/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0217 - val_loss: 0.0182\n",
      "Epoch 6071/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0308 - val_loss: 0.0219\n",
      "Epoch 6072/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0191 - val_loss: 0.0441\n",
      "Epoch 6073/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0274 - val_loss: 0.1058\n",
      "Epoch 6074/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1036 - val_loss: 0.0551\n",
      "Epoch 6075/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0421 - val_loss: 0.0263\n",
      "Epoch 6076/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0282 - val_loss: 0.0301\n",
      "Epoch 6077/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0252 - val_loss: 0.0478\n",
      "Epoch 6078/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0243 - val_loss: 0.0200\n",
      "Epoch 6079/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0198 - val_loss: 0.0375\n",
      "Epoch 6080/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0272 - val_loss: 0.0270\n",
      "Epoch 6081/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0288 - val_loss: 0.0163\n",
      "Epoch 6082/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0193 - val_loss: 0.0188\n",
      "Epoch 6083/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0190 - val_loss: 0.0244\n",
      "Epoch 6084/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0225 - val_loss: 0.0280\n",
      "Epoch 6085/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0241 - val_loss: 0.0143\n",
      "Epoch 6086/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0172 - val_loss: 0.0220\n",
      "Epoch 6087/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0293 - val_loss: 0.0388\n",
      "Epoch 6088/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0334 - val_loss: 0.0157\n",
      "Epoch 6089/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0238 - val_loss: 0.0365\n",
      "Epoch 6090/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0220 - val_loss: 0.0153\n",
      "Epoch 6091/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0173 - val_loss: 0.0134\n",
      "Epoch 6092/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0183 - val_loss: 0.0143\n",
      "Epoch 6093/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0239 - val_loss: 0.0367\n",
      "Epoch 6094/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0410 - val_loss: 0.0135\n",
      "Epoch 6095/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0424 - val_loss: 0.0600\n",
      "Epoch 6096/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0759 - val_loss: 0.0402\n",
      "Epoch 6097/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0470 - val_loss: 0.0364\n",
      "Epoch 6098/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0461 - val_loss: 0.0893\n",
      "Epoch 6099/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0390 - val_loss: 0.0357\n",
      "Epoch 6100/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0494 - val_loss: 0.0545\n",
      "Epoch 6101/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0653 - val_loss: 0.0831\n",
      "Epoch 6102/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3243 - val_loss: 0.2886\n",
      "Epoch 6103/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8892 - val_loss: 0.1550\n",
      "Epoch 6104/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5648 - val_loss: 0.6040\n",
      "Epoch 6105/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4560 - val_loss: 0.6442\n",
      "Epoch 6106/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3446 - val_loss: 0.4268\n",
      "Epoch 6107/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4969 - val_loss: 0.1484\n",
      "Epoch 6108/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9912 - val_loss: 1.7106\n",
      "Epoch 6109/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8862 - val_loss: 0.8342\n",
      "Epoch 6110/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.5188 - val_loss: 0.0661\n",
      "Epoch 6111/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1736 - val_loss: 0.4775\n",
      "Epoch 6112/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1748 - val_loss: 0.0152\n",
      "Epoch 6113/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0486 - val_loss: 0.0358\n",
      "Epoch 6114/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0552 - val_loss: 0.0621\n",
      "Epoch 6115/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0615 - val_loss: 0.0132\n",
      "Epoch 6116/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1884 - val_loss: 0.2583\n",
      "Epoch 6117/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1772 - val_loss: 0.1870\n",
      "Epoch 6118/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1322 - val_loss: 0.0881\n",
      "Epoch 6119/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0624 - val_loss: 0.0783\n",
      "Epoch 6120/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0292 - val_loss: 0.0535\n",
      "Epoch 6121/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0608 - val_loss: 0.1088\n",
      "Epoch 6122/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0506 - val_loss: 0.0643\n",
      "Epoch 6123/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0455 - val_loss: 0.0127\n",
      "Epoch 6124/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0258 - val_loss: 0.0737\n",
      "Epoch 6125/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0735 - val_loss: 0.0061\n",
      "Epoch 6126/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0168 - val_loss: 0.0456\n",
      "Epoch 6127/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0176 - val_loss: 0.0071\n",
      "Epoch 6128/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0105 - val_loss: 0.0373\n",
      "Epoch 6129/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0150 - val_loss: 0.0090\n",
      "Epoch 6130/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0124 - val_loss: 0.0169\n",
      "Epoch 6131/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0215 - val_loss: 0.0633\n",
      "Epoch 6132/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0536 - val_loss: 0.0800\n",
      "Epoch 6133/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0343 - val_loss: 0.0069\n",
      "Epoch 6134/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0114 - val_loss: 0.0099\n",
      "Epoch 6135/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0139 - val_loss: 0.0140\n",
      "Epoch 6136/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0164 - val_loss: 0.0184\n",
      "Epoch 6137/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0283 - val_loss: 0.0342\n",
      "Epoch 6138/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0292 - val_loss: 0.0143\n",
      "Epoch 6139/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0181 - val_loss: 0.0185\n",
      "Epoch 6140/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0180 - val_loss: 0.0390\n",
      "Epoch 6141/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0304 - val_loss: 0.0140\n",
      "Epoch 6142/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0206 - val_loss: 0.0189\n",
      "Epoch 6143/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0095 - val_loss: 0.0153\n",
      "Epoch 6144/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0065 - val_loss: 0.0124\n",
      "Epoch 6145/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0165 - val_loss: 0.0363\n",
      "Epoch 6146/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0294 - val_loss: 0.0481\n",
      "Epoch 6147/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0994 - val_loss: 0.0612\n",
      "Epoch 6148/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0321 - val_loss: 0.0040\n",
      "Epoch 6149/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0709 - val_loss: 0.1247\n",
      "Epoch 6150/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2184 - val_loss: 0.4915\n",
      "Epoch 6151/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5340 - val_loss: 1.1568\n",
      "Epoch 6152/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5838 - val_loss: 0.4784\n",
      "Epoch 6153/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3401 - val_loss: 0.5716\n",
      "Epoch 6154/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5547 - val_loss: 0.0139\n",
      "Epoch 6155/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3148 - val_loss: 0.0345\n",
      "Epoch 6156/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2274 - val_loss: 0.0721\n",
      "Epoch 6157/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.3575 - val_loss: 1.0138\n",
      "Epoch 6158/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.6184 - val_loss: 2.3171\n",
      "Epoch 6159/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.9436 - val_loss: 4.6466\n",
      "Epoch 6160/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.5467 - val_loss: 5.2997\n",
      "Epoch 6161/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.3660 - val_loss: 4.7744\n",
      "Epoch 6162/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.9056 - val_loss: 11.7087\n",
      "Epoch 6163/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.2709 - val_loss: 21.5698\n",
      "Epoch 6164/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 29.6398 - val_loss: 56.8202\n",
      "Epoch 6165/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 23.8481 - val_loss: 30.3009\n",
      "Epoch 6166/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.9560 - val_loss: 6.1279\n",
      "Epoch 6167/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8563 - val_loss: 2.5405\n",
      "Epoch 6168/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.0555 - val_loss: 7.2445\n",
      "Epoch 6169/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 13.2151 - val_loss: 42.6718\n",
      "Epoch 6170/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 33.2792 - val_loss: 19.7173\n",
      "Epoch 6171/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 33.5833 - val_loss: 112.0031\n",
      "Epoch 6172/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 200.1189 - val_loss: 1103.6019\n",
      "Epoch 6173/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1748.5374 - val_loss: 2009.9307\n",
      "Epoch 6174/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 947.3377 - val_loss: 3.1728\n",
      "Epoch 6175/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 660.8614 - val_loss: 7.8607\n",
      "Epoch 6176/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 424.7849 - val_loss: 917.5112\n",
      "Epoch 6177/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1328.8642 - val_loss: 3685.6134\n",
      "Epoch 6178/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2496.5124 - val_loss: 75.7442\n",
      "Epoch 6179/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 485.3664 - val_loss: 99.9452\n",
      "Epoch 6180/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 322.6061 - val_loss: 327.1318\n",
      "Epoch 6181/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 357.3170 - val_loss: 694.2062\n",
      "Epoch 6182/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 307.9909 - val_loss: 283.8559\n",
      "Epoch 6183/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 215.3583 - val_loss: 276.4742\n",
      "Epoch 6184/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 358.0777 - val_loss: 54.2159\n",
      "Epoch 6185/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1200.0824 - val_loss: 174.8169\n",
      "Epoch 6186/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1490.8790 - val_loss: 3553.9021\n",
      "Epoch 6187/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1764.8822 - val_loss: 628.6148\n",
      "Epoch 6188/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 693.8802 - val_loss: 901.3670\n",
      "Epoch 6189/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 486.4483 - val_loss: 288.4907\n",
      "Epoch 6190/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 175.7092 - val_loss: 19.3794\n",
      "Epoch 6191/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 45.7699 - val_loss: 18.4226\n",
      "Epoch 6192/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 66.9320 - val_loss: 45.5305\n",
      "Epoch 6193/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 42.6533 - val_loss: 60.3170\n",
      "Epoch 6194/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 87.8375 - val_loss: 117.5239\n",
      "Epoch 6195/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 79.2554 - val_loss: 13.1371\n",
      "Epoch 6196/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 48.4019 - val_loss: 28.5997\n",
      "Epoch 6197/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.2698 - val_loss: 5.8219\n",
      "Epoch 6198/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.1619 - val_loss: 11.9726\n",
      "Epoch 6199/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.6680 - val_loss: 4.3571\n",
      "Epoch 6200/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.0739 - val_loss: 0.1692\n",
      "Epoch 6201/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3694 - val_loss: 0.4624\n",
      "Epoch 6202/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3114 - val_loss: 0.0991\n",
      "Epoch 6203/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2203 - val_loss: 0.5604\n",
      "Epoch 6204/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2217 - val_loss: 0.0667\n",
      "Epoch 6205/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0524 - val_loss: 0.1054\n",
      "Epoch 6206/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0684 - val_loss: 0.0074\n",
      "Epoch 6207/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0317 - val_loss: 0.0639\n",
      "Epoch 6208/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0767 - val_loss: 0.0408\n",
      "Epoch 6209/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0327 - val_loss: 0.0358\n",
      "Epoch 6210/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0225 - val_loss: 0.0080\n",
      "Epoch 6211/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0187 - val_loss: 0.0417\n",
      "Epoch 6212/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0542 - val_loss: 0.0076\n",
      "Epoch 6213/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0196 - val_loss: 0.0169\n",
      "Epoch 6214/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0235 - val_loss: 0.0077\n",
      "Epoch 6215/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0395 - val_loss: 0.1320\n",
      "Epoch 6216/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0768 - val_loss: 0.0667\n",
      "Epoch 6217/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0993 - val_loss: 0.0734\n",
      "Epoch 6218/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0552 - val_loss: 0.0140\n",
      "Epoch 6219/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0328 - val_loss: 0.0326\n",
      "Epoch 6220/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0273 - val_loss: 0.0289\n",
      "Epoch 6221/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0305 - val_loss: 0.0611\n",
      "Epoch 6222/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0620 - val_loss: 0.0086\n",
      "Epoch 6223/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0358 - val_loss: 0.0275\n",
      "Epoch 6224/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0244 - val_loss: 0.0201\n",
      "Epoch 6225/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0360 - val_loss: 0.0168\n",
      "Epoch 6226/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0093 - val_loss: 0.0060\n",
      "Epoch 6227/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0073 - val_loss: 0.0048\n",
      "Epoch 6228/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 6229/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0052 - val_loss: 0.0053\n",
      "Epoch 6230/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0117 - val_loss: 0.0050\n",
      "Epoch 6231/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0063 - val_loss: 0.0080\n",
      "Epoch 6232/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0118 - val_loss: 0.0070\n",
      "Epoch 6233/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0091 - val_loss: 0.0057\n",
      "Epoch 6234/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0116 - val_loss: 0.0043\n",
      "Epoch 6235/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0058 - val_loss: 0.0053\n",
      "Epoch 6236/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0048 - val_loss: 0.0042\n",
      "Epoch 6237/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0064 - val_loss: 0.0090\n",
      "Epoch 6238/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0096 - val_loss: 0.0043\n",
      "Epoch 6239/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 6240/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0038 - val_loss: 0.0101\n",
      "Epoch 6241/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0097 - val_loss: 0.0153\n",
      "Epoch 6242/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0058 - val_loss: 0.0062\n",
      "Epoch 6243/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0049 - val_loss: 0.0035\n",
      "Epoch 6244/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0054 - val_loss: 0.0033\n",
      "Epoch 6245/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 6246/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0041 - val_loss: 0.0046\n",
      "Epoch 6247/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0086 - val_loss: 0.0075\n",
      "Epoch 6248/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0074 - val_loss: 0.0113\n",
      "Epoch 6249/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0065 - val_loss: 0.0093\n",
      "Epoch 6250/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0083 - val_loss: 0.0043\n",
      "Epoch 6251/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0122 - val_loss: 0.0035\n",
      "Epoch 6252/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0211 - val_loss: 0.0373\n",
      "Epoch 6253/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0333 - val_loss: 0.1171\n",
      "Epoch 6254/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0679 - val_loss: 0.0565\n",
      "Epoch 6255/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.043 - 0s 147us/sample - loss: 0.0892 - val_loss: 0.0202\n",
      "Epoch 6256/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1398 - val_loss: 0.3350\n",
      "Epoch 6257/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0929 - val_loss: 0.0346\n",
      "Epoch 6258/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0195 - val_loss: 0.0231\n",
      "Epoch 6259/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0145 - val_loss: 0.0159\n",
      "Epoch 6260/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0111 - val_loss: 0.0025\n",
      "Epoch 6261/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0165 - val_loss: 0.0075\n",
      "Epoch 6262/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0248 - val_loss: 0.0051\n",
      "Epoch 6263/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0379 - val_loss: 0.0051\n",
      "Epoch 6264/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0763 - val_loss: 0.0099\n",
      "Epoch 6265/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1504 - val_loss: 0.1202\n",
      "Epoch 6266/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5067 - val_loss: 0.1367\n",
      "Epoch 6267/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1485 - val_loss: 0.0976\n",
      "Epoch 6268/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.1114 - val_loss: 0.1630\n",
      "Epoch 6269/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2990 - val_loss: 0.4904\n",
      "Epoch 6270/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2692 - val_loss: 0.8115\n",
      "Epoch 6271/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4763 - val_loss: 0.6097\n",
      "Epoch 6272/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6606 - val_loss: 0.4967\n",
      "Epoch 6273/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3434 - val_loss: 0.8547\n",
      "Epoch 6274/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3632 - val_loss: 1.1018\n",
      "Epoch 6275/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4136 - val_loss: 0.3530\n",
      "Epoch 6276/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3284 - val_loss: 0.2826\n",
      "Epoch 6277/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1832 - val_loss: 0.0688\n",
      "Epoch 6278/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1141 - val_loss: 0.2371\n",
      "Epoch 6279/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2824 - val_loss: 0.2193\n",
      "Epoch 6280/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2912 - val_loss: 0.0497\n",
      "Epoch 6281/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0638 - val_loss: 0.0169\n",
      "Epoch 6282/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0109 - val_loss: 0.0034\n",
      "Epoch 6283/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0050 - val_loss: 0.0028\n",
      "Epoch 6284/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0030 - val_loss: 0.0100\n",
      "Epoch 6285/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0141 - val_loss: 0.0200\n",
      "Epoch 6286/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2049 - val_loss: 0.1921\n",
      "Epoch 6287/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9791 - val_loss: 0.1709\n",
      "Epoch 6288/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4346 - val_loss: 0.0026\n",
      "Epoch 6289/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2333 - val_loss: 0.0383\n",
      "Epoch 6290/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3330 - val_loss: 0.2528\n",
      "Epoch 6291/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.9054 - val_loss: 3.6034\n",
      "Epoch 6292/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.4407 - val_loss: 0.6273\n",
      "Epoch 6293/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8910 - val_loss: 2.7206\n",
      "Epoch 6294/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0251 - val_loss: 1.1121\n",
      "Epoch 6295/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1386 - val_loss: 0.5460\n",
      "Epoch 6296/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9523 - val_loss: 0.2917\n",
      "Epoch 6297/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.2909 - val_loss: 1.7872\n",
      "Epoch 6298/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.8115 - val_loss: 2.1063\n",
      "Epoch 6299/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8664 - val_loss: 0.1219\n",
      "Epoch 6300/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3232 - val_loss: 0.1371\n",
      "Epoch 6301/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3288 - val_loss: 0.2202\n",
      "Epoch 6302/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4222 - val_loss: 0.0041\n",
      "Epoch 6303/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5090 - val_loss: 4.3383\n",
      "Epoch 6304/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.4828 - val_loss: 0.7170\n",
      "Epoch 6305/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4650 - val_loss: 0.0982\n",
      "Epoch 6306/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3808 - val_loss: 0.2751\n",
      "Epoch 6307/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1172 - val_loss: 0.0841\n",
      "Epoch 6308/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1826 - val_loss: 0.0979\n",
      "Epoch 6309/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2604 - val_loss: 0.4675\n",
      "Epoch 6310/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8505 - val_loss: 3.1045\n",
      "Epoch 6311/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9029 - val_loss: 6.0106\n",
      "Epoch 6312/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0520 - val_loss: 0.4566\n",
      "Epoch 6313/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1641 - val_loss: 5.3739\n",
      "Epoch 6314/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.0463 - val_loss: 1.7611\n",
      "Epoch 6315/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.9741 - val_loss: 28.9831\n",
      "Epoch 6316/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 37.0733 - val_loss: 5.1929\n",
      "Epoch 6317/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15.6343 - val_loss: 26.2023\n",
      "Epoch 6318/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19.1318 - val_loss: 2.9519\n",
      "Epoch 6319/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1422 - val_loss: 3.8250\n",
      "Epoch 6320/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.3224 - val_loss: 11.1369\n",
      "Epoch 6321/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.6267 - val_loss: 0.7862\n",
      "Epoch 6322/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 43.1550 - val_loss: 88.3776\n",
      "Epoch 6323/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 516.7798 - val_loss: 1118.3257\n",
      "Epoch 6324/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 797.9710 - val_loss: 92.5983\n",
      "Epoch 6325/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 181.5626 - val_loss: 129.7438\n",
      "Epoch 6326/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 85.4771 - val_loss: 304.7179\n",
      "Epoch 6327/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 260.0370 - val_loss: 21.4423\n",
      "Epoch 6328/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 160.0656 - val_loss: 49.9915\n",
      "Epoch 6329/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 32.9871 - val_loss: 135.4532\n",
      "Epoch 6330/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 71.8067 - val_loss: 180.7201\n",
      "Epoch 6331/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 247.6277 - val_loss: 444.3432\n",
      "Epoch 6332/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1762.9043 - val_loss: 5385.2975\n",
      "Epoch 6333/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2081.7193 - val_loss: 2059.3860\n",
      "Epoch 6334/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1823.4664 - val_loss: 598.5322\n",
      "Epoch 6335/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2081.1729 - val_loss: 2078.1257\n",
      "Epoch 6336/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2299.1257 - val_loss: 2362.6157\n",
      "Epoch 6337/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1279.1222 - val_loss: 411.0279\n",
      "Epoch 6338/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 757.2246 - val_loss: 769.8477\n",
      "Epoch 6339/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 221.3125 - val_loss: 124.2849\n",
      "Epoch 6340/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 196.6472 - val_loss: 61.9878\n",
      "Epoch 6341/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 57.9694 - val_loss: 25.8410\n",
      "Epoch 6342/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 40.5408 - val_loss: 55.0492\n",
      "Epoch 6343/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 45.2452 - val_loss: 5.5620\n",
      "Epoch 6344/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 25.5261 - val_loss: 53.7364\n",
      "Epoch 6345/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 50.2943 - val_loss: 9.5871\n",
      "Epoch 6346/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 23.7347 - val_loss: 9.3024\n",
      "Epoch 6347/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 26.6240 - val_loss: 60.4314\n",
      "Epoch 6348/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 32.3936 - val_loss: 1.7374\n",
      "Epoch 6349/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.9342 - val_loss: 11.1349\n",
      "Epoch 6350/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.8380 - val_loss: 0.6607\n",
      "Epoch 6351/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.8589 - val_loss: 2.9378\n",
      "Epoch 6352/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3979 - val_loss: 0.6659\n",
      "Epoch 6353/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5605 - val_loss: 0.0568\n",
      "Epoch 6354/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3858 - val_loss: 0.1507\n",
      "Epoch 6355/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1048 - val_loss: 0.0498\n",
      "Epoch 6356/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0813 - val_loss: 0.0419\n",
      "Epoch 6357/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0603 - val_loss: 0.0162\n",
      "Epoch 6358/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0460 - val_loss: 0.0204\n",
      "Epoch 6359/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0545 - val_loss: 0.0876\n",
      "Epoch 6360/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0799 - val_loss: 0.0568\n",
      "Epoch 6361/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0826 - val_loss: 0.0658\n",
      "Epoch 6362/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0483 - val_loss: 0.0414\n",
      "Epoch 6363/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0545 - val_loss: 0.0306\n",
      "Epoch 6364/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0417 - val_loss: 0.0232\n",
      "Epoch 6365/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0501 - val_loss: 0.0727\n",
      "Epoch 6366/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0908 - val_loss: 0.4809\n",
      "Epoch 6367/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4332 - val_loss: 1.0228\n",
      "Epoch 6368/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4728 - val_loss: 0.3093\n",
      "Epoch 6369/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9374 - val_loss: 3.7313\n",
      "Epoch 6370/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.4621 - val_loss: 7.6066\n",
      "Epoch 6371/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.3802 - val_loss: 1.1596\n",
      "Epoch 6372/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.4999 - val_loss: 5.2329\n",
      "Epoch 6373/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.9033 - val_loss: 6.6226\n",
      "Epoch 6374/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.5928 - val_loss: 0.3156\n",
      "Epoch 6375/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1686 - val_loss: 4.9336\n",
      "Epoch 6376/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.2641 - val_loss: 5.3585\n",
      "Epoch 6377/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2484 - val_loss: 3.7219\n",
      "Epoch 6378/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.4160 - val_loss: 1.6413\n",
      "Epoch 6379/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5696 - val_loss: 0.3352\n",
      "Epoch 6380/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1498 - val_loss: 0.1867\n",
      "Epoch 6381/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5945 - val_loss: 0.4197\n",
      "Epoch 6382/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1148 - val_loss: 0.5959\n",
      "Epoch 6383/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2678 - val_loss: 0.1214\n",
      "Epoch 6384/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3931 - val_loss: 0.4730\n",
      "Epoch 6385/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3797 - val_loss: 1.7893\n",
      "Epoch 6386/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7218 - val_loss: 2.9242\n",
      "Epoch 6387/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.3957 - val_loss: 0.8988\n",
      "Epoch 6388/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.5927 - val_loss: 0.0712\n",
      "Epoch 6389/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.3734 - val_loss: 1.0691\n",
      "Epoch 6390/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.6240 - val_loss: 2.3112\n",
      "Epoch 6391/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9620 - val_loss: 2.9890\n",
      "Epoch 6392/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8262 - val_loss: 1.6844\n",
      "Epoch 6393/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.3344 - val_loss: 1.7134\n",
      "Epoch 6394/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7543 - val_loss: 3.5682\n",
      "Epoch 6395/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6963 - val_loss: 1.1235\n",
      "Epoch 6396/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8744 - val_loss: 5.3882\n",
      "Epoch 6397/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.6727 - val_loss: 7.5956\n",
      "Epoch 6398/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.8514 - val_loss: 8.6736\n",
      "Epoch 6399/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.3578 - val_loss: 12.9591\n",
      "Epoch 6400/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.5527 - val_loss: 4.5656\n",
      "Epoch 6401/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 10.4290 - val_loss: 0.0766\n",
      "Epoch 6402/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.8674 - val_loss: 1.8523\n",
      "Epoch 6403/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.8428 - val_loss: 1.4305\n",
      "Epoch 6404/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7026 - val_loss: 0.3429\n",
      "Epoch 6405/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.3849 - val_loss: 0.0344\n",
      "Epoch 6406/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5979 - val_loss: 0.0965\n",
      "Epoch 6407/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2277 - val_loss: 0.1018\n",
      "Epoch 6408/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1442 - val_loss: 0.0303\n",
      "Epoch 6409/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0635 - val_loss: 0.0723\n",
      "Epoch 6410/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1060 - val_loss: 0.0595\n",
      "Epoch 6411/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1463 - val_loss: 0.0315\n",
      "Epoch 6412/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0915 - val_loss: 0.1181\n",
      "Epoch 6413/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1096 - val_loss: 0.0378\n",
      "Epoch 6414/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1354 - val_loss: 0.1003\n",
      "Epoch 6415/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1152 - val_loss: 0.2711\n",
      "Epoch 6416/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4014 - val_loss: 0.6809\n",
      "Epoch 6417/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5367 - val_loss: 0.1685\n",
      "Epoch 6418/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2897 - val_loss: 0.3505\n",
      "Epoch 6419/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1986 - val_loss: 0.2592\n",
      "Epoch 6420/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1475 - val_loss: 0.1265\n",
      "Epoch 6421/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0597 - val_loss: 0.0745\n",
      "Epoch 6422/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0186 - val_loss: 0.0101\n",
      "Epoch 6423/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0214 - val_loss: 0.0032\n",
      "Epoch 6424/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0219 - val_loss: 0.0300\n",
      "Epoch 6425/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0127 - val_loss: 0.0289\n",
      "Epoch 6426/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 6427/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0090 - val_loss: 0.0029\n",
      "Epoch 6428/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0265 - val_loss: 0.1003\n",
      "Epoch 6429/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0604 - val_loss: 0.0252\n",
      "Epoch 6430/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0203 - val_loss: 0.0703\n",
      "Epoch 6431/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2482 - val_loss: 0.1184\n",
      "Epoch 6432/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3614 - val_loss: 1.8426\n",
      "Epoch 6433/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8944 - val_loss: 0.0300\n",
      "Epoch 6434/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3275 - val_loss: 2.0435\n",
      "Epoch 6435/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.3935 - val_loss: 0.2162\n",
      "Epoch 6436/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6497 - val_loss: 0.8772\n",
      "Epoch 6437/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3389 - val_loss: 0.2483\n",
      "Epoch 6438/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3300 - val_loss: 0.0319\n",
      "Epoch 6439/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0907 - val_loss: 0.0783\n",
      "Epoch 6440/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2183 - val_loss: 0.1688\n",
      "Epoch 6441/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3557 - val_loss: 0.1240\n",
      "Epoch 6442/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0911 - val_loss: 0.0287\n",
      "Epoch 6443/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0418 - val_loss: 0.0230\n",
      "Epoch 6444/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0314 - val_loss: 0.0173\n",
      "Epoch 6445/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0143 - val_loss: 0.0822\n",
      "Epoch 6446/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0341 - val_loss: 0.0399\n",
      "Epoch 6447/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0344 - val_loss: 0.0218\n",
      "Epoch 6448/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0345 - val_loss: 0.0065\n",
      "Epoch 6449/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1011 - val_loss: 0.0919\n",
      "Epoch 6450/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1046 - val_loss: 0.3851\n",
      "Epoch 6451/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7173 - val_loss: 4.2166\n",
      "Epoch 6452/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.3039 - val_loss: 1.5210\n",
      "Epoch 6453/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.8567 - val_loss: 12.9192\n",
      "Epoch 6454/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 16.58 - 0s 132us/sample - loss: 8.4989 - val_loss: 11.1542\n",
      "Epoch 6455/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.1313 - val_loss: 0.7907\n",
      "Epoch 6456/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.8768 - val_loss: 52.6195\n",
      "Epoch 6457/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 33.7304 - val_loss: 35.9146\n",
      "Epoch 6458/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 35.3689 - val_loss: 12.8296\n",
      "Epoch 6459/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.7280 - val_loss: 11.5459\n",
      "Epoch 6460/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.6063 - val_loss: 8.9644\n",
      "Epoch 6461/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.8792 - val_loss: 1.2495\n",
      "Epoch 6462/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.8784 - val_loss: 13.0375\n",
      "Epoch 6463/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.8344 - val_loss: 4.6493\n",
      "Epoch 6464/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.0319 - val_loss: 16.2289\n",
      "Epoch 6465/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 24.4148 - val_loss: 151.3319\n",
      "Epoch 6466/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 759.2392 - val_loss: 249.9337\n",
      "Epoch 6467/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4312.7802 - val_loss: 5892.1672\n",
      "Epoch 6468/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3475.4329 - val_loss: 3471.6950\n",
      "Epoch 6469/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1901.5636 - val_loss: 1712.4089\n",
      "Epoch 6470/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1728.4369 - val_loss: 2636.3670\n",
      "Epoch 6471/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1625.7685 - val_loss: 858.5724\n",
      "Epoch 6472/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2970.4336 - val_loss: 7333.4425\n",
      "Epoch 6473/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3842.0721 - val_loss: 672.3340\n",
      "Epoch 6474/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2091.5055 - val_loss: 7516.9269\n",
      "Epoch 6475/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3338.4274 - val_loss: 6210.6114\n",
      "Epoch 6476/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3934.9908 - val_loss: 8346.3027\n",
      "Epoch 6477/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3408.6460 - val_loss: 5739.7414\n",
      "Epoch 6478/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4369.0728 - val_loss: 2288.3929\n",
      "Epoch 6479/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2262.8977 - val_loss: 3355.3854\n",
      "Epoch 6480/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2283.4882 - val_loss: 1345.4067\n",
      "Epoch 6481/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1218.0657 - val_loss: 716.2746\n",
      "Epoch 6482/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1303.4808 - val_loss: 604.1423\n",
      "Epoch 6483/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 629.4834 - val_loss: 23.6898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6484/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 374.2769 - val_loss: 122.8500\n",
      "Epoch 6485/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 349.0505 - val_loss: 6.9874\n",
      "Epoch 6486/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 298.9801 - val_loss: 158.6760\n",
      "Epoch 6487/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 310.5232 - val_loss: 733.1307\n",
      "Epoch 6488/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 254.3082 - val_loss: 170.5329\n",
      "Epoch 6489/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 70.9227 - val_loss: 12.0656\n",
      "Epoch 6490/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 20.4999 - val_loss: 3.9110\n",
      "Epoch 6491/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.5554 - val_loss: 1.4398\n",
      "Epoch 6492/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1070 - val_loss: 1.9953\n",
      "Epoch 6493/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.8386 - val_loss: 0.4175\n",
      "Epoch 6494/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4751 - val_loss: 0.1732\n",
      "Epoch 6495/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2257 - val_loss: 0.0759\n",
      "Epoch 6496/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1599 - val_loss: 0.0212\n",
      "Epoch 6497/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1215 - val_loss: 0.0195\n",
      "Epoch 6498/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1659 - val_loss: 0.1370\n",
      "Epoch 6499/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1935 - val_loss: 0.0569\n",
      "Epoch 6500/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1441 - val_loss: 0.1737\n",
      "Epoch 6501/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1962 - val_loss: 0.4505\n",
      "Epoch 6502/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2583 - val_loss: 0.0986\n",
      "Epoch 6503/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1086 - val_loss: 0.1314\n",
      "Epoch 6504/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1097 - val_loss: 0.1349\n",
      "Epoch 6505/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1204 - val_loss: 0.2216\n",
      "Epoch 6506/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1587 - val_loss: 0.1103\n",
      "Epoch 6507/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0957 - val_loss: 0.0192\n",
      "Epoch 6508/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0586 - val_loss: 0.0496\n",
      "Epoch 6509/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0598 - val_loss: 0.0365\n",
      "Epoch 6510/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0431 - val_loss: 0.0353\n",
      "Epoch 6511/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0370 - val_loss: 0.0130\n",
      "Epoch 6512/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0292 - val_loss: 0.0124\n",
      "Epoch 6513/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0203 - val_loss: 0.0061\n",
      "Epoch 6514/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0164 - val_loss: 0.0048\n",
      "Epoch 6515/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0138 - val_loss: 0.0036\n",
      "Epoch 6516/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0095 - val_loss: 1.2485e-04\n",
      "Epoch 6517/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0080 - val_loss: 0.0013\n",
      "Epoch 6518/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0081 - val_loss: 0.0016\n",
      "Epoch 6519/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0082 - val_loss: 1.3412e-04\n",
      "Epoch 6520/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0070 - val_loss: 0.0014\n",
      "Epoch 6521/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0056 - val_loss: 1.8993e-04\n",
      "Epoch 6522/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0043 - val_loss: 7.0764e-04\n",
      "Epoch 6523/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0042 - val_loss: 0.0011\n",
      "Epoch 6524/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0043 - val_loss: 5.2733e-05\n",
      "Epoch 6525/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0038 - val_loss: 5.2469e-04\n",
      "Epoch 6526/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0039 - val_loss: 2.1486e-04\n",
      "Epoch 6527/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0036 - val_loss: 1.4748e-04\n",
      "Epoch 6528/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0035 - val_loss: 7.2320e-04\n",
      "Epoch 6529/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0033 - val_loss: 0.0013\n",
      "Epoch 6530/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0024 - val_loss: 4.0915e-04\n",
      "Epoch 6531/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0018 - val_loss: 5.6507e-04\n",
      "Epoch 6532/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0016 - val_loss: 2.5938e-05\n",
      "Epoch 6533/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0012 - val_loss: 1.3730e-05\n",
      "Epoch 6534/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.3277e-04 - val_loss: 3.6730e-05\n",
      "Epoch 6535/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.6239e-04 - val_loss: 8.9435e-05\n",
      "Epoch 6536/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.8458e-04 - val_loss: 1.2907e-04\n",
      "Epoch 6537/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.7390e-04 - val_loss: 8.4625e-05\n",
      "Epoch 6538/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.2656e-04 - val_loss: 1.0322e-04\n",
      "Epoch 6539/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.5622e-04 - val_loss: 3.5735e-05\n",
      "Epoch 6540/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.4003e-04 - val_loss: 9.4918e-05\n",
      "Epoch 6541/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.9863e-04 - val_loss: 8.5157e-05\n",
      "Epoch 6542/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.2831e-04 - val_loss: 3.4458e-05\n",
      "Epoch 6543/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.3301e-04 - val_loss: 1.5174e-05\n",
      "Epoch 6544/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.4868e-04 - val_loss: 4.8086e-05\n",
      "Epoch 6545/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.4145e-04 - val_loss: 8.4508e-05\n",
      "Epoch 6546/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.3930e-04 - val_loss: 2.9864e-04\n",
      "Epoch 6547/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.5526e-04 - val_loss: 7.4728e-04\n",
      "Epoch 6548/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.5232e-04 - val_loss: 4.0667e-05\n",
      "Epoch 6549/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6510e-04 - val_loss: 2.2589e-05\n",
      "Epoch 6550/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4648e-04 - val_loss: 1.9097e-05\n",
      "Epoch 6551/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1141e-04 - val_loss: 5.3925e-05\n",
      "Epoch 6552/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.8523e-05 - val_loss: 2.7166e-05\n",
      "Epoch 6553/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.5197e-04 - val_loss: 8.8215e-06\n",
      "Epoch 6554/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3923e-04 - val_loss: 2.4417e-05\n",
      "Epoch 6555/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2218e-04 - val_loss: 2.8849e-05\n",
      "Epoch 6556/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.6214e-05 - val_loss: 1.6619e-05\n",
      "Epoch 6557/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9981e-04 - val_loss: 2.5178e-04\n",
      "Epoch 6558/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.3268e-04 - val_loss: 2.0006e-04\n",
      "Epoch 6559/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.1330e-04 - val_loss: 1.6058e-05\n",
      "Epoch 6560/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.5945e-04 - val_loss: 1.6444e-04\n",
      "Epoch 6561/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4867e-04 - val_loss: 1.4037e-04\n",
      "Epoch 6562/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.0222e-04 - val_loss: 1.2192e-04\n",
      "Epoch 6563/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3880e-04 - val_loss: 3.5141e-04\n",
      "Epoch 6564/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1127e-04 - val_loss: 1.3263e-04\n",
      "Epoch 6565/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2914e-04 - val_loss: 4.1868e-04\n",
      "Epoch 6566/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.6774e-04 - val_loss: 6.8946e-04\n",
      "Epoch 6567/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.6340e-04 - val_loss: 2.1801e-04\n",
      "Epoch 6568/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5176e-04 - val_loss: 4.3771e-05\n",
      "Epoch 6569/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.3259e-04 - val_loss: 2.2368e-04\n",
      "Epoch 6570/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.0574e-04 - val_loss: 4.9929e-04\n",
      "Epoch 6571/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.8541e-04 - val_loss: 0.0015\n",
      "Epoch 6572/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9.1212e-04 - val_loss: 1.5905e-04\n",
      "Epoch 6573/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.0518e-04 - val_loss: 7.7210e-04\n",
      "Epoch 6574/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.1093e-04 - val_loss: 0.0020\n",
      "Epoch 6575/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0017 - val_loss: 2.8382e-04\n",
      "Epoch 6576/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 6577/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0012 - val_loss: 6.4696e-05\n",
      "Epoch 6578/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.7700e-04 - val_loss: 4.5234e-04\n",
      "Epoch 6579/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.5515e-04 - val_loss: 3.1669e-04\n",
      "Epoch 6580/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.9279e-04 - val_loss: 9.2142e-06\n",
      "Epoch 6581/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.7496e-04 - val_loss: 6.3757e-04\n",
      "Epoch 6582/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.7498e-04 - val_loss: 5.7795e-05\n",
      "Epoch 6583/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.6661e-04 - val_loss: 1.9602e-04\n",
      "Epoch 6584/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9663e-04 - val_loss: 3.1054e-04\n",
      "Epoch 6585/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1883e-04 - val_loss: 3.4454e-05\n",
      "Epoch 6586/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.2838e-05 - val_loss: 5.2069e-05\n",
      "Epoch 6587/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.8952e-05 - val_loss: 1.0892e-04\n",
      "Epoch 6588/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.4965e-04 - val_loss: 7.7092e-06\n",
      "Epoch 6589/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.0372e-05 - val_loss: 2.0542e-05\n",
      "Epoch 6590/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9749e-05 - val_loss: 7.5268e-06\n",
      "Epoch 6591/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.1845e-06 - val_loss: 6.7274e-06\n",
      "Epoch 6592/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.9251e-06 - val_loss: 3.2785e-05\n",
      "Epoch 6593/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9861e-05 - val_loss: 8.8311e-06\n",
      "Epoch 6594/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0739e-05 - val_loss: 3.6642e-05\n",
      "Epoch 6595/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6774e-05 - val_loss: 9.7988e-06\n",
      "Epoch 6596/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.7208e-05 - val_loss: 3.0289e-05\n",
      "Epoch 6597/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7314e-04 - val_loss: 1.5916e-04\n",
      "Epoch 6598/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 6.1266e-05 - val_loss: 7.0315e-05\n",
      "Epoch 6599/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.3327e-05 - val_loss: 1.1201e-05\n",
      "Epoch 6600/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4617e-05 - val_loss: 1.0107e-05\n",
      "Epoch 6601/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0077e-05 - val_loss: 1.4273e-05\n",
      "Epoch 6602/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.0546e-05 - val_loss: 4.8913e-05\n",
      "Epoch 6603/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8175e-05 - val_loss: 7.4146e-06\n",
      "Epoch 6604/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.4520e-05 - val_loss: 5.1988e-06\n",
      "Epoch 6605/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.5912e-05 - val_loss: 1.7905e-05\n",
      "Epoch 6606/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.0152e-05 - val_loss: 6.6152e-06\n",
      "Epoch 6607/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1493e-05 - val_loss: 1.2646e-05\n",
      "Epoch 6608/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.7803e-05 - val_loss: 1.2001e-05\n",
      "Epoch 6609/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1.0126e-05 - val_loss: 9.6305e-06\n",
      "Epoch 6610/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.0869e-05 - val_loss: 5.6206e-05\n",
      "Epoch 6611/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.4599e-05 - val_loss: 4.5396e-06\n",
      "Epoch 6612/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0479e-05 - val_loss: 3.0191e-05\n",
      "Epoch 6613/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.0736e-05 - val_loss: 3.0948e-05\n",
      "Epoch 6614/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.0064e-05 - val_loss: 1.8710e-04\n",
      "Epoch 6615/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8642e-04 - val_loss: 6.8534e-05\n",
      "Epoch 6616/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1944e-04 - val_loss: 2.1986e-04\n",
      "Epoch 6617/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.4073e-04 - val_loss: 9.6740e-04\n",
      "Epoch 6618/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.8485e-04 - val_loss: 0.0022\n",
      "Epoch 6619/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0013 - val_loss: 7.9197e-04\n",
      "Epoch 6620/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0010 - val_loss: 1.4203e-04\n",
      "Epoch 6621/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.6986e-04 - val_loss: 2.4508e-04\n",
      "Epoch 6622/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.6483e-04 - val_loss: 5.9421e-04\n",
      "Epoch 6623/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.0414e-04 - val_loss: 1.8151e-04\n",
      "Epoch 6624/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.6655e-04 - val_loss: 9.3825e-04\n",
      "Epoch 6625/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 0.0089\n",
      "Epoch 6626/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0067 - val_loss: 0.0081\n",
      "Epoch 6627/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0041 - val_loss: 0.0011\n",
      "Epoch 6628/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 4.7861e-04 - val_loss: 1.9019e-04\n",
      "Epoch 6629/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4719e-04 - val_loss: 7.8775e-05\n",
      "Epoch 6630/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1791e-04 - val_loss: 1.0618e-04\n",
      "Epoch 6631/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.9363e-05 - val_loss: 1.4661e-04\n",
      "Epoch 6632/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.0961e-05 - val_loss: 4.3187e-06\n",
      "Epoch 6633/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.7047e-05 - val_loss: 2.6478e-05\n",
      "Epoch 6634/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.9073e-05 - val_loss: 3.0655e-04\n",
      "Epoch 6635/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6929e-04 - val_loss: 1.0541e-04\n",
      "Epoch 6636/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.7398e-05 - val_loss: 8.2857e-05\n",
      "Epoch 6637/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.6643e-05 - val_loss: 8.7681e-05\n",
      "Epoch 6638/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.8447e-05 - val_loss: 6.5592e-05\n",
      "Epoch 6639/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.6377e-05 - val_loss: 4.3433e-05\n",
      "Epoch 6640/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8674e-05 - val_loss: 1.0938e-05\n",
      "Epoch 6641/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.5368e-05 - val_loss: 2.6557e-04\n",
      "Epoch 6642/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2407e-04 - val_loss: 1.9260e-04\n",
      "Epoch 6643/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5.0236e-04 - val_loss: 0.0015\n",
      "Epoch 6644/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0018 - val_loss: 0.0042\n",
      "Epoch 6645/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 6646/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0013 - val_loss: 0.0029\n",
      "Epoch 6647/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0012 - val_loss: 0.0025\n",
      "Epoch 6648/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0023 - val_loss: 0.0064\n",
      "Epoch 6649/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0052 - val_loss: 0.0078\n",
      "Epoch 6650/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0129 - val_loss: 0.0297\n",
      "Epoch 6651/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0265 - val_loss: 0.0148\n",
      "Epoch 6652/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0208 - val_loss: 3.4945e-04\n",
      "Epoch 6653/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0144 - val_loss: 0.0031\n",
      "Epoch 6654/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0034 - val_loss: 0.0052\n",
      "Epoch 6655/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0188 - val_loss: 0.0038\n",
      "Epoch 6656/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0030 - val_loss: 0.0047\n",
      "Epoch 6657/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 6658/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.0182e-04 - val_loss: 0.0031\n",
      "Epoch 6659/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0013 - val_loss: 9.3884e-04\n",
      "Epoch 6660/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.9664e-04 - val_loss: 2.5976e-04\n",
      "Epoch 6661/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.8688e-04 - val_loss: 9.1668e-04\n",
      "Epoch 6662/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.6636e-04 - val_loss: 1.3879e-04\n",
      "Epoch 6663/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0132e-04 - val_loss: 5.7669e-05\n",
      "Epoch 6664/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0502e-04 - val_loss: 2.1920e-04\n",
      "Epoch 6665/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8567e-04 - val_loss: 2.6361e-04\n",
      "Epoch 6666/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.6103e-04 - val_loss: 4.8710e-04\n",
      "Epoch 6667/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.5937e-04 - val_loss: 0.0011\n",
      "Epoch 6668/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 3.7822e-04\n",
      "Epoch 6669/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.6178e-04 - val_loss: 5.3658e-05\n",
      "Epoch 6670/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.7278e-04 - val_loss: 1.5511e-04\n",
      "Epoch 6671/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.1308e-04 - val_loss: 3.3143e-04\n",
      "Epoch 6672/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.6651e-04 - val_loss: 2.4549e-04\n",
      "Epoch 6673/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8302e-04 - val_loss: 3.5965e-04\n",
      "Epoch 6674/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.4676e-04 - val_loss: 2.8947e-04\n",
      "Epoch 6675/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.0787e-05 - val_loss: 6.9310e-05\n",
      "Epoch 6676/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.7616e-05 - val_loss: 7.2739e-04\n",
      "Epoch 6677/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.6811e-04 - val_loss: 1.5133e-05\n",
      "Epoch 6678/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4832e-04 - val_loss: 0.0011\n",
      "Epoch 6679/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.8926e-04 - val_loss: 0.0013\n",
      "Epoch 6680/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.6193e-04 - val_loss: 1.1525e-04\n",
      "Epoch 6681/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6934e-04 - val_loss: 0.0017\n",
      "Epoch 6682/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.4575e-04 - val_loss: 0.0029\n",
      "Epoch 6683/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.1909e-04 - val_loss: 1.1388e-05\n",
      "Epoch 6684/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.3063e-04 - val_loss: 1.7270e-04\n",
      "Epoch 6685/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7732e-04 - val_loss: 1.5848e-05\n",
      "Epoch 6686/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4812e-05 - val_loss: 4.7880e-05\n",
      "Epoch 6687/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.9450e-05 - val_loss: 3.0490e-05\n",
      "Epoch 6688/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.1859e-05 - val_loss: 2.4571e-05\n",
      "Epoch 6689/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.9374e-05 - val_loss: 8.1146e-05\n",
      "Epoch 6690/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.5560e-04 - val_loss: 0.0011\n",
      "Epoch 6691/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.9087e-04 - val_loss: 2.7272e-04\n",
      "Epoch 6692/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.3890e-04 - val_loss: 0.0025\n",
      "Epoch 6693/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0063 - val_loss: 0.0013\n",
      "Epoch 6694/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0088 - val_loss: 0.0107\n",
      "Epoch 6695/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0117 - val_loss: 0.0022\n",
      "Epoch 6696/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0080 - val_loss: 0.0063\n",
      "Epoch 6697/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0558 - val_loss: 0.0054\n",
      "Epoch 6698/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0333 - val_loss: 0.2942\n",
      "Epoch 6699/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.3035 - val_loss: 0.0428\n",
      "Epoch 6700/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.2312 - val_loss: 0.7020\n",
      "Epoch 6701/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3247 - val_loss: 0.0763\n",
      "Epoch 6702/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0963 - val_loss: 0.0342\n",
      "Epoch 6703/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0277 - val_loss: 0.0069\n",
      "Epoch 6704/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0139 - val_loss: 0.0130\n",
      "Epoch 6705/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0215 - val_loss: 0.0079\n",
      "Epoch 6706/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0041 - val_loss: 0.0214\n",
      "Epoch 6707/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0160 - val_loss: 0.0229\n",
      "Epoch 6708/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0837 - val_loss: 0.1352\n",
      "Epoch 6709/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0753 - val_loss: 0.1080\n",
      "Epoch 6710/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5162 - val_loss: 0.1638\n",
      "Epoch 6711/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.1991 - val_loss: 2.8602\n",
      "Epoch 6712/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.9963 - val_loss: 57.9299\n",
      "Epoch 6713/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 78.2317 - val_loss: 106.4472\n",
      "Epoch 6714/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 473.8111 - val_loss: 498.3628\n",
      "Epoch 6715/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2338.1980 - val_loss: 2294.2594\n",
      "Epoch 6716/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2241.2957 - val_loss: 2609.4623\n",
      "Epoch 6717/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6758.7181 - val_loss: 5466.5627\n",
      "Epoch 6718/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 4417.2857 - val_loss: 5569.8473\n",
      "Epoch 6719/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2566.7163 - val_loss: 3357.5892\n",
      "Epoch 6720/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2005.2376 - val_loss: 607.0474\n",
      "Epoch 6721/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1921.3500 - val_loss: 1513.7707\n",
      "Epoch 6722/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1451.8230 - val_loss: 1536.3954\n",
      "Epoch 6723/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1403.3404 - val_loss: 2634.7969\n",
      "Epoch 6724/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2325.4859 - val_loss: 2921.5860\n",
      "Epoch 6725/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1071.3889 - val_loss: 988.5198\n",
      "Epoch 6726/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1690.4139 - val_loss: 2540.3995\n",
      "Epoch 6727/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2418.7289 - val_loss: 2453.2932\n",
      "Epoch 6728/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1770.5862 - val_loss: 1304.2814\n",
      "Epoch 6729/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 811.0254 - val_loss: 1020.4873\n",
      "Epoch 6730/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 458.4780 - val_loss: 497.6199\n",
      "Epoch 6731/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 532.7901 - val_loss: 932.8597\n",
      "Epoch 6732/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 532.0500 - val_loss: 332.1084\n",
      "Epoch 6733/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 583.3686 - val_loss: 307.6403\n",
      "Epoch 6734/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1410.9376 - val_loss: 924.3438\n",
      "Epoch 6735/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1285.5138 - val_loss: 1402.4812\n",
      "Epoch 6736/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1238.1557 - val_loss: 817.4729\n",
      "Epoch 6737/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 637.958 - 0s 147us/sample - loss: 1052.5476 - val_loss: 239.4533\n",
      "Epoch 6738/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 496.0201 - val_loss: 232.4612\n",
      "Epoch 6739/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 370.4260 - val_loss: 380.7676\n",
      "Epoch 6740/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 157.8126 - val_loss: 78.4948\n",
      "Epoch 6741/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 77.1784 - val_loss: 47.7930\n",
      "Epoch 6742/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 78.0879 - val_loss: 147.0961\n",
      "Epoch 6743/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 131.2637 - val_loss: 88.4746\n",
      "Epoch 6744/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 83.9811 - val_loss: 167.7413\n",
      "Epoch 6745/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 122.3375 - val_loss: 144.8771\n",
      "Epoch 6746/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 83.2464 - val_loss: 106.6752\n",
      "Epoch 6747/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 67.4920 - val_loss: 53.0778\n",
      "Epoch 6748/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 39.8702 - val_loss: 9.0392\n",
      "Epoch 6749/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.5807 - val_loss: 19.6908\n",
      "Epoch 6750/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.6590 - val_loss: 0.7163\n",
      "Epoch 6751/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.2230 - val_loss: 0.3938\n",
      "Epoch 6752/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2921 - val_loss: 1.0999\n",
      "Epoch 6753/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2095 - val_loss: 0.1703\n",
      "Epoch 6754/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6055 - val_loss: 0.3568\n",
      "Epoch 6755/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6891 - val_loss: 0.0970\n",
      "Epoch 6756/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4437 - val_loss: 0.2179\n",
      "Epoch 6757/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5629 - val_loss: 0.3139\n",
      "Epoch 6758/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9012 - val_loss: 0.8020\n",
      "Epoch 6759/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0541 - val_loss: 0.6048\n",
      "Epoch 6760/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9013 - val_loss: 0.3346\n",
      "Epoch 6761/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3494 - val_loss: 0.2255\n",
      "Epoch 6762/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4105 - val_loss: 0.5758\n",
      "Epoch 6763/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6508 - val_loss: 0.3126\n",
      "Epoch 6764/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4071 - val_loss: 0.2651\n",
      "Epoch 6765/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5014 - val_loss: 0.1650\n",
      "Epoch 6766/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2886 - val_loss: 0.2776\n",
      "Epoch 6767/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4680 - val_loss: 0.6289\n",
      "Epoch 6768/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4232 - val_loss: 0.2661\n",
      "Epoch 6769/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2161 - val_loss: 0.0995\n",
      "Epoch 6770/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1260 - val_loss: 0.2039\n",
      "Epoch 6771/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1177 - val_loss: 0.1319\n",
      "Epoch 6772/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0952 - val_loss: 0.2299\n",
      "Epoch 6773/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0925 - val_loss: 0.0966\n",
      "Epoch 6774/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1153 - val_loss: 0.0374\n",
      "Epoch 6775/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0668 - val_loss: 0.0545\n",
      "Epoch 6776/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0662 - val_loss: 0.0639\n",
      "Epoch 6777/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0524 - val_loss: 0.0286\n",
      "Epoch 6778/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0390 - val_loss: 0.0264\n",
      "Epoch 6779/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0401 - val_loss: 0.0535\n",
      "Epoch 6780/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0548 - val_loss: 0.0940\n",
      "Epoch 6781/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0701 - val_loss: 0.0624\n",
      "Epoch 6782/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0577 - val_loss: 0.0316\n",
      "Epoch 6783/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0465 - val_loss: 0.0655\n",
      "Epoch 6784/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0573 - val_loss: 0.0458\n",
      "Epoch 6785/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0813 - val_loss: 0.0780\n",
      "Epoch 6786/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0886 - val_loss: 0.0915\n",
      "Epoch 6787/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0663 - val_loss: 0.0866\n",
      "Epoch 6788/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0559 - val_loss: 0.0248\n",
      "Epoch 6789/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0584 - val_loss: 0.0645\n",
      "Epoch 6790/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1211 - val_loss: 0.1046\n",
      "Epoch 6791/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1206 - val_loss: 0.0761\n",
      "Epoch 6792/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.049 - 0s 176us/sample - loss: 0.0849 - val_loss: 0.1132\n",
      "Epoch 6793/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0541 - val_loss: 0.0631\n",
      "Epoch 6794/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0563 - val_loss: 0.0659\n",
      "Epoch 6795/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0781 - val_loss: 0.2242\n",
      "Epoch 6796/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1585 - val_loss: 0.0587\n",
      "Epoch 6797/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0681 - val_loss: 0.0303\n",
      "Epoch 6798/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0515 - val_loss: 0.0823\n",
      "Epoch 6799/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0367 - val_loss: 0.0597\n",
      "Epoch 6800/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0822 - val_loss: 0.0210\n",
      "Epoch 6801/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1046 - val_loss: 0.0800\n",
      "Epoch 6802/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1011 - val_loss: 0.2157\n",
      "Epoch 6803/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1590 - val_loss: 0.0189\n",
      "Epoch 6804/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1885 - val_loss: 0.0785\n",
      "Epoch 6805/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2493 - val_loss: 0.2139\n",
      "Epoch 6806/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2636 - val_loss: 0.0744\n",
      "Epoch 6807/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4058 - val_loss: 0.9743\n",
      "Epoch 6808/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0994 - val_loss: 0.6887\n",
      "Epoch 6809/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3049 - val_loss: 0.1484\n",
      "Epoch 6810/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1486 - val_loss: 0.0762\n",
      "Epoch 6811/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0919 - val_loss: 0.1288\n",
      "Epoch 6812/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1185 - val_loss: 0.2081\n",
      "Epoch 6813/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2264 - val_loss: 0.0700\n",
      "Epoch 6814/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1733 - val_loss: 0.2981\n",
      "Epoch 6815/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2856 - val_loss: 0.0939\n",
      "Epoch 6816/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1710 - val_loss: 0.1632\n",
      "Epoch 6817/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1650 - val_loss: 0.1356\n",
      "Epoch 6818/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2510 - val_loss: 0.3498\n",
      "Epoch 6819/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1472 - val_loss: 0.0196\n",
      "Epoch 6820/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0676 - val_loss: 0.0551\n",
      "Epoch 6821/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0392 - val_loss: 0.0619\n",
      "Epoch 6822/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0353 - val_loss: 0.0155\n",
      "Epoch 6823/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0208 - val_loss: 0.0165\n",
      "Epoch 6824/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0157 - val_loss: 0.0174\n",
      "Epoch 6825/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0192 - val_loss: 0.0167\n",
      "Epoch 6826/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0170 - val_loss: 0.0242\n",
      "Epoch 6827/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0224 - val_loss: 0.0187\n",
      "Epoch 6828/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0241 - val_loss: 0.0182\n",
      "Epoch 6829/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0461 - val_loss: 0.0288\n",
      "Epoch 6830/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0251 - val_loss: 0.0132\n",
      "Epoch 6831/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0265 - val_loss: 0.0175\n",
      "Epoch 6832/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0588 - val_loss: 0.0231\n",
      "Epoch 6833/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0449 - val_loss: 0.0392\n",
      "Epoch 6834/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0706 - val_loss: 0.0918\n",
      "Epoch 6835/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0668 - val_loss: 0.0492\n",
      "Epoch 6836/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0457 - val_loss: 0.0104\n",
      "Epoch 6837/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0410 - val_loss: 0.0239\n",
      "Epoch 6838/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0609 - val_loss: 0.1617\n",
      "Epoch 6839/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0715 - val_loss: 0.0134\n",
      "Epoch 6840/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0175 - val_loss: 0.0718\n",
      "Epoch 6841/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0241 - val_loss: 0.0199\n",
      "Epoch 6842/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0219 - val_loss: 0.0097\n",
      "Epoch 6843/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0147 - val_loss: 0.0151\n",
      "Epoch 6844/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0089 - val_loss: 0.0083\n",
      "Epoch 6845/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0091 - val_loss: 0.0093\n",
      "Epoch 6846/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0112 - val_loss: 0.0094\n",
      "Epoch 6847/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0117 - val_loss: 0.0472\n",
      "Epoch 6848/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0365 - val_loss: 0.0817\n",
      "Epoch 6849/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2578 - val_loss: 0.1177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6850/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0972 - val_loss: 0.0451\n",
      "Epoch 6851/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0849 - val_loss: 0.1444\n",
      "Epoch 6852/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0550 - val_loss: 0.0066\n",
      "Epoch 6853/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0164 - val_loss: 0.0224\n",
      "Epoch 6854/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0316 - val_loss: 0.0064\n",
      "Epoch 6855/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0165 - val_loss: 0.0155\n",
      "Epoch 6856/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0092 - val_loss: 0.0057\n",
      "Epoch 6857/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0061 - val_loss: 0.0071\n",
      "Epoch 6858/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0188 - val_loss: 0.0112\n",
      "Epoch 6859/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0302 - val_loss: 0.0208\n",
      "Epoch 6860/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0622 - val_loss: 0.2677\n",
      "Epoch 6861/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1407 - val_loss: 0.1465\n",
      "Epoch 6862/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0810 - val_loss: 0.0107\n",
      "Epoch 6863/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0524 - val_loss: 0.1135\n",
      "Epoch 6864/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0646 - val_loss: 0.0257\n",
      "Epoch 6865/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0343 - val_loss: 0.0051\n",
      "Epoch 6866/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 6867/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0139 - val_loss: 0.0244\n",
      "Epoch 6868/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0548 - val_loss: 0.0920\n",
      "Epoch 6869/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0459 - val_loss: 0.0108\n",
      "Epoch 6870/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0209 - val_loss: 0.1079\n",
      "Epoch 6871/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0932 - val_loss: 0.1357\n",
      "Epoch 6872/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0715 - val_loss: 0.0823\n",
      "Epoch 6873/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1650 - val_loss: 0.0454\n",
      "Epoch 6874/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3544 - val_loss: 0.4569\n",
      "Epoch 6875/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2591 - val_loss: 0.3163\n",
      "Epoch 6876/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1975 - val_loss: 0.0962\n",
      "Epoch 6877/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1113 - val_loss: 0.0592\n",
      "Epoch 6878/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0857 - val_loss: 0.2170\n",
      "Epoch 6879/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0869 - val_loss: 0.0204\n",
      "Epoch 6880/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0587 - val_loss: 0.0228\n",
      "Epoch 6881/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0184 - val_loss: 0.0029\n",
      "Epoch 6882/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 6883/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0033 - val_loss: 0.0072\n",
      "Epoch 6884/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 6885/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0035 - val_loss: 0.0070\n",
      "Epoch 6886/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0064 - val_loss: 0.0112\n",
      "Epoch 6887/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 6888/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0069 - val_loss: 0.0444\n",
      "Epoch 6889/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0514 - val_loss: 0.1805\n",
      "Epoch 6890/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.3092 - val_loss: 0.7455\n",
      "Epoch 6891/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.5883 - val_loss: 1.6683\n",
      "Epoch 6892/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7020 - val_loss: 5.6366\n",
      "Epoch 6893/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.5906 - val_loss: 4.8175\n",
      "Epoch 6894/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.9411 - val_loss: 4.2015\n",
      "Epoch 6895/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8214 - val_loss: 7.4870\n",
      "Epoch 6896/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.0884 - val_loss: 1.1109\n",
      "Epoch 6897/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6780 - val_loss: 0.3138\n",
      "Epoch 6898/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5409 - val_loss: 0.4526\n",
      "Epoch 6899/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4612 - val_loss: 0.2700\n",
      "Epoch 6900/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2057 - val_loss: 0.0076\n",
      "Epoch 6901/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0068 - val_loss: 0.0045\n",
      "Epoch 6902/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0102 - val_loss: 0.0101\n",
      "Epoch 6903/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0067 - val_loss: 0.0436\n",
      "Epoch 6904/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0343 - val_loss: 0.0835\n",
      "Epoch 6905/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0609 - val_loss: 0.0195\n",
      "Epoch 6906/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1277 - val_loss: 0.5513\n",
      "Epoch 6907/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5983 - val_loss: 0.4590\n",
      "Epoch 6908/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8589 - val_loss: 0.6288\n",
      "Epoch 6909/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.7193 - val_loss: 0.2603\n",
      "Epoch 6910/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0880 - val_loss: 1.5319\n",
      "Epoch 6911/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.1182 - val_loss: 0.2345\n",
      "Epoch 6912/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2618 - val_loss: 0.0186\n",
      "Epoch 6913/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1434 - val_loss: 0.2424\n",
      "Epoch 6914/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1966 - val_loss: 0.4213\n",
      "Epoch 6915/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3805 - val_loss: 0.0012\n",
      "Epoch 6916/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1969 - val_loss: 1.3008\n",
      "Epoch 6917/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6285 - val_loss: 4.2196\n",
      "Epoch 6918/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8542 - val_loss: 0.0976\n",
      "Epoch 6919/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.1145 - val_loss: 7.8889\n",
      "Epoch 6920/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.7963 - val_loss: 16.3770\n",
      "Epoch 6921/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 18.32 - 0s 147us/sample - loss: 22.1444 - val_loss: 10.4197\n",
      "Epoch 6922/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 7.6206 - val_loss: 13.6467\n",
      "Epoch 6923/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.3105 - val_loss: 135.8496\n",
      "Epoch 6924/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 175.8990 - val_loss: 11.3025\n",
      "Epoch 6925/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 337.4722 - val_loss: 277.6700\n",
      "Epoch 6926/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 290.5841 - val_loss: 423.6327\n",
      "Epoch 6927/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 396.2747 - val_loss: 204.2470\n",
      "Epoch 6928/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 934.9538 - val_loss: 1226.3915\n",
      "Epoch 6929/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1057.4304 - val_loss: 287.7793\n",
      "Epoch 6930/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 604.0618 - val_loss: 97.0397\n",
      "Epoch 6931/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 452.2838 - val_loss: 168.4995\n",
      "Epoch 6932/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 246.4225 - val_loss: 11.0406\n",
      "Epoch 6933/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 261.7398 - val_loss: 49.3994\n",
      "Epoch 6934/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 65.0055 - val_loss: 64.5152\n",
      "Epoch 6935/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 54.8940 - val_loss: 1.3067\n",
      "Epoch 6936/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16.8617 - val_loss: 7.4404\n",
      "Epoch 6937/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.2330 - val_loss: 1.0264\n",
      "Epoch 6938/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.4865 - val_loss: 6.6112\n",
      "Epoch 6939/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.4176 - val_loss: 0.7038\n",
      "Epoch 6940/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1209 - val_loss: 0.8473\n",
      "Epoch 6941/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1884 - val_loss: 3.6534\n",
      "Epoch 6942/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.2576 - val_loss: 6.9620\n",
      "Epoch 6943/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21.9715 - val_loss: 8.2850\n",
      "Epoch 6944/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17.8097 - val_loss: 8.3687\n",
      "Epoch 6945/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.8619 - val_loss: 11.9641\n",
      "Epoch 6946/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.3400 - val_loss: 1.6583\n",
      "Epoch 6947/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1018 - val_loss: 1.0832\n",
      "Epoch 6948/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5929 - val_loss: 1.5230\n",
      "Epoch 6949/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2721 - val_loss: 6.4639\n",
      "Epoch 6950/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9572 - val_loss: 1.6272\n",
      "Epoch 6951/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6152 - val_loss: 0.0023\n",
      "Epoch 6952/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.4359 - val_loss: 0.7031\n",
      "Epoch 6953/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.1531 - val_loss: 2.6704\n",
      "Epoch 6954/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.7508 - val_loss: 8.7176\n",
      "Epoch 6955/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.3577 - val_loss: 2.3296\n",
      "Epoch 6956/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.2687 - val_loss: 0.6832\n",
      "Epoch 6957/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1284 - val_loss: 1.2091\n",
      "Epoch 6958/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4852 - val_loss: 0.5731\n",
      "Epoch 6959/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5173 - val_loss: 0.5817\n",
      "Epoch 6960/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3172 - val_loss: 0.4202\n",
      "Epoch 6961/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1710 - val_loss: 0.0616\n",
      "Epoch 6962/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2080 - val_loss: 0.1712\n",
      "Epoch 6963/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5863 - val_loss: 0.4788\n",
      "Epoch 6964/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5526 - val_loss: 1.3057\n",
      "Epoch 6965/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9375 - val_loss: 2.5934\n",
      "Epoch 6966/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7361 - val_loss: 3.0473\n",
      "Epoch 6967/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4076 - val_loss: 0.2775\n",
      "Epoch 6968/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4744 - val_loss: 0.0371\n",
      "Epoch 6969/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4109 - val_loss: 0.4705\n",
      "Epoch 6970/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3422 - val_loss: 0.1820\n",
      "Epoch 6971/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0722 - val_loss: 0.0152\n",
      "Epoch 6972/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0591 - val_loss: 0.0548\n",
      "Epoch 6973/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0282 - val_loss: 0.0038\n",
      "Epoch 6974/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0761 - val_loss: 0.0525\n",
      "Epoch 6975/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1819 - val_loss: 0.6228\n",
      "Epoch 6976/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.0045 - val_loss: 1.8231\n",
      "Epoch 6977/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.4912 - val_loss: 37.7588\n",
      "Epoch 6978/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.6310 - val_loss: 4.3897\n",
      "Epoch 6979/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.4077 - val_loss: 0.8064\n",
      "Epoch 6980/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17.7136 - val_loss: 20.1807\n",
      "Epoch 6981/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 55.7740 - val_loss: 23.4581\n",
      "Epoch 6982/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 95.4061 - val_loss: 143.7818\n",
      "Epoch 6983/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 80.5833 - val_loss: 48.9868\n",
      "Epoch 6984/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 97.6647 - val_loss: 73.6084\n",
      "Epoch 6985/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 73.4605 - val_loss: 504.4713\n",
      "Epoch 6986/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1379.1964 - val_loss: 360.3103\n",
      "Epoch 6987/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 967.4524 - val_loss: 2009.5144\n",
      "Epoch 6988/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2851.8986 - val_loss: 2906.8234\n",
      "Epoch 6989/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1997.8092 - val_loss: 402.8517\n",
      "Epoch 6990/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2629.8852 - val_loss: 943.0284\n",
      "Epoch 6991/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 991.5398 - val_loss: 360.1765\n",
      "Epoch 6992/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 380.5318 - val_loss: 222.4868\n",
      "Epoch 6993/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 290.8359 - val_loss: 187.3492\n",
      "Epoch 6994/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 207.9468 - val_loss: 388.2251\n",
      "Epoch 6995/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 372.9535 - val_loss: 218.6606\n",
      "Epoch 6996/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 244.0571 - val_loss: 386.1928\n",
      "Epoch 6997/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1126.4600 - val_loss: 150.8738\n",
      "Epoch 6998/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 279.6973 - val_loss: 270.9841\n",
      "Epoch 6999/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 167.1588 - val_loss: 55.3261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7000/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 130.0195 - val_loss: 22.8797\n",
      "Epoch 7001/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 116.4963 - val_loss: 284.6091\n",
      "Epoch 7002/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 185.2727 - val_loss: 86.1309\n",
      "Epoch 7003/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 105.6793 - val_loss: 267.9368\n",
      "Epoch 7004/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 123.0964 - val_loss: 29.6134\n",
      "Epoch 7005/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 66.8031 - val_loss: 4.2506\n",
      "Epoch 7006/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.2808 - val_loss: 8.6986\n",
      "Epoch 7007/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 6.8421 - val_loss: 1.6110\n",
      "Epoch 7008/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.9277 - val_loss: 0.4717\n",
      "Epoch 7009/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.9347 - val_loss: 10.5420\n",
      "Epoch 7010/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.9479 - val_loss: 2.3016\n",
      "Epoch 7011/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.4102 - val_loss: 29.4227\n",
      "Epoch 7012/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 14.2486 - val_loss: 32.3461\n",
      "Epoch 7013/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.7886 - val_loss: 18.6408\n",
      "Epoch 7014/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 56.5224 - val_loss: 58.1214\n",
      "Epoch 7015/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 66.6549 - val_loss: 42.4928\n",
      "Epoch 7016/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 19.1045 - val_loss: 1.4608\n",
      "Epoch 7017/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.3796 - val_loss: 13.5732\n",
      "Epoch 7018/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 10.8008 - val_loss: 8.4331\n",
      "Epoch 7019/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.7411 - val_loss: 2.6767\n",
      "Epoch 7020/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.2889 - val_loss: 0.6114\n",
      "Epoch 7021/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.2083 - val_loss: 6.7728\n",
      "Epoch 7022/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.6675 - val_loss: 4.7661\n",
      "Epoch 7023/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.1248 - val_loss: 21.2164\n",
      "Epoch 7024/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 13.9101 - val_loss: 35.7693\n",
      "Epoch 7025/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 29.3491 - val_loss: 12.9713\n",
      "Epoch 7026/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.2197 - val_loss: 2.4033\n",
      "Epoch 7027/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.3084 - val_loss: 0.2319\n",
      "Epoch 7028/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8261 - val_loss: 2.2036\n",
      "Epoch 7029/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6606 - val_loss: 2.1525\n",
      "Epoch 7030/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1859 - val_loss: 0.9157\n",
      "Epoch 7031/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6317 - val_loss: 3.9477\n",
      "Epoch 7032/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.9534 - val_loss: 1.1491\n",
      "Epoch 7033/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.4578 - val_loss: 2.8466\n",
      "Epoch 7034/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.5509 - val_loss: 11.0964\n",
      "Epoch 7035/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.4848 - val_loss: 11.8118\n",
      "Epoch 7036/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 24.6983 - val_loss: 11.9843\n",
      "Epoch 7037/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 23.8352 - val_loss: 69.2438\n",
      "Epoch 7038/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 34.9190 - val_loss: 14.1151\n",
      "Epoch 7039/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.4593 - val_loss: 34.6023\n",
      "Epoch 7040/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 17.0453 - val_loss: 17.4581\n",
      "Epoch 7041/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.6897 - val_loss: 2.8304\n",
      "Epoch 7042/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.4795 - val_loss: 1.2673\n",
      "Epoch 7043/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.9891 - val_loss: 1.9836\n",
      "Epoch 7044/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.2960 - val_loss: 8.5202\n",
      "Epoch 7045/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.6557 - val_loss: 2.0144\n",
      "Epoch 7046/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1524 - val_loss: 1.0905\n",
      "Epoch 7047/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0526 - val_loss: 0.1626\n",
      "Epoch 7048/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3405 - val_loss: 1.0355\n",
      "Epoch 7049/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8863 - val_loss: 2.9519\n",
      "Epoch 7050/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9195 - val_loss: 0.6224\n",
      "Epoch 7051/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6032 - val_loss: 0.0797\n",
      "Epoch 7052/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0639 - val_loss: 0.0700\n",
      "Epoch 7053/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0499 - val_loss: 0.1317\n",
      "Epoch 7054/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0776 - val_loss: 0.1647\n",
      "Epoch 7055/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1349 - val_loss: 0.1329\n",
      "Epoch 7056/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0426 - val_loss: 0.0229\n",
      "Epoch 7057/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0156 - val_loss: 0.0169\n",
      "Epoch 7058/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0218 - val_loss: 0.0300\n",
      "Epoch 7059/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0832 - val_loss: 0.0385\n",
      "Epoch 7060/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0879 - val_loss: 0.0047\n",
      "Epoch 7061/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0546 - val_loss: 0.0114\n",
      "Epoch 7062/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0298 - val_loss: 0.0068\n",
      "Epoch 7063/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0994 - val_loss: 0.2539\n",
      "Epoch 7064/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2078 - val_loss: 0.0071\n",
      "Epoch 7065/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2139 - val_loss: 0.0127\n",
      "Epoch 7066/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1195 - val_loss: 0.0684\n",
      "Epoch 7067/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0463 - val_loss: 0.0611\n",
      "Epoch 7068/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0246 - val_loss: 0.0106\n",
      "Epoch 7069/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0071 - val_loss: 0.0199\n",
      "Epoch 7070/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0263 - val_loss: 0.0332\n",
      "Epoch 7071/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0892 - val_loss: 3.3111e-04\n",
      "Epoch 7072/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1641 - val_loss: 0.0848\n",
      "Epoch 7073/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1471 - val_loss: 0.0997\n",
      "Epoch 7074/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1305 - val_loss: 0.0765\n",
      "Epoch 7075/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1046 - val_loss: 0.0731\n",
      "Epoch 7076/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2210 - val_loss: 0.0446\n",
      "Epoch 7077/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0745 - val_loss: 5.6111e-04\n",
      "Epoch 7078/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0157 - val_loss: 0.0367\n",
      "Epoch 7079/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0175 - val_loss: 0.0234\n",
      "Epoch 7080/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0319 - val_loss: 0.0101\n",
      "Epoch 7081/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0264 - val_loss: 0.0185\n",
      "Epoch 7082/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0296 - val_loss: 0.0468\n",
      "Epoch 7083/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0655 - val_loss: 0.0198\n",
      "Epoch 7084/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0589 - val_loss: 0.0278\n",
      "Epoch 7085/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0284 - val_loss: 0.0359\n",
      "Epoch 7086/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0717 - val_loss: 0.0794\n",
      "Epoch 7087/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2322 - val_loss: 0.1527\n",
      "Epoch 7088/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0482 - val_loss: 0.0106\n",
      "Epoch 7089/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0293 - val_loss: 8.7793e-05\n",
      "Epoch 7090/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0448 - val_loss: 0.3319\n",
      "Epoch 7091/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1869 - val_loss: 0.0436\n",
      "Epoch 7092/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1272 - val_loss: 0.1693\n",
      "Epoch 7093/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1003 - val_loss: 0.0014\n",
      "Epoch 7094/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0775 - val_loss: 0.0146\n",
      "Epoch 7095/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2180 - val_loss: 1.0297\n",
      "Epoch 7096/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8310 - val_loss: 0.1377\n",
      "Epoch 7097/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7454 - val_loss: 0.4820\n",
      "Epoch 7098/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2657 - val_loss: 0.0519\n",
      "Epoch 7099/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0665 - val_loss: 0.1219\n",
      "Epoch 7100/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0827 - val_loss: 0.1117\n",
      "Epoch 7101/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1303 - val_loss: 0.0949\n",
      "Epoch 7102/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0597 - val_loss: 0.1652\n",
      "Epoch 7103/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2661 - val_loss: 0.0152\n",
      "Epoch 7104/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0959 - val_loss: 0.1730\n",
      "Epoch 7105/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0590 - val_loss: 8.8666e-04\n",
      "Epoch 7106/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 7107/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0038 - val_loss: 0.0028\n",
      "Epoch 7108/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0029 - val_loss: 0.0010\n",
      "Epoch 7109/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 7110/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0026 - val_loss: 0.0063\n",
      "Epoch 7111/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 7112/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0073 - val_loss: 0.0054\n",
      "Epoch 7113/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0089 - val_loss: 0.0040\n",
      "Epoch 7114/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0121 - val_loss: 0.0442\n",
      "Epoch 7115/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0206 - val_loss: 0.0077\n",
      "Epoch 7116/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0053 - val_loss: 0.0103\n",
      "Epoch 7117/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0555 - val_loss: 0.0552\n",
      "Epoch 7118/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0661 - val_loss: 0.0127\n",
      "Epoch 7119/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0194 - val_loss: 0.0019\n",
      "Epoch 7120/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0047 - val_loss: 0.0062\n",
      "Epoch 7121/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0053 - val_loss: 0.0084\n",
      "Epoch 7122/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0045 - val_loss: 0.0330\n",
      "Epoch 7123/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0183 - val_loss: 0.0249\n",
      "Epoch 7124/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0409 - val_loss: 0.0282\n",
      "Epoch 7125/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0229 - val_loss: 0.0122\n",
      "Epoch 7126/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0974 - val_loss: 0.1075\n",
      "Epoch 7127/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1580 - val_loss: 0.1603\n",
      "Epoch 7128/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1623 - val_loss: 0.0423\n",
      "Epoch 7129/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0597 - val_loss: 0.1846\n",
      "Epoch 7130/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1027 - val_loss: 0.0390\n",
      "Epoch 7131/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0407 - val_loss: 0.0418\n",
      "Epoch 7132/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0508 - val_loss: 0.0568\n",
      "Epoch 7133/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2495 - val_loss: 0.1389\n",
      "Epoch 7134/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1498 - val_loss: 0.4101\n",
      "Epoch 7135/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1800 - val_loss: 2.6371\n",
      "Epoch 7136/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.1595 - val_loss: 0.0183\n",
      "Epoch 7137/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.6696 - val_loss: 27.9234\n",
      "Epoch 7138/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 90.5474 - val_loss: 964.8803\n",
      "Epoch 7139/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 681.6654 - val_loss: 1752.8839\n",
      "Epoch 7140/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5712.0601 - val_loss: 2077.9078\n",
      "Epoch 7141/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6225.3872 - val_loss: 5155.8741\n",
      "Epoch 7142/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4873.7791 - val_loss: 14752.1014\n",
      "Epoch 7143/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16110.5731 - val_loss: 20915.5764\n",
      "Epoch 7144/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 21544.5790 - val_loss: 2984.2142\n",
      "Epoch 7145/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9495.2282 - val_loss: 9364.4831\n",
      "Epoch 7146/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6779.7457 - val_loss: 3079.2208\n",
      "Epoch 7147/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5976.7714 - val_loss: 8611.0999\n",
      "Epoch 7148/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 5606.0198 - val_loss: 2630.2480\n",
      "Epoch 7149/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4583.3460 - val_loss: 5140.1856\n",
      "Epoch 7150/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 3753.9410 - val_loss: 1791.2293\n",
      "Epoch 7151/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1496.4907 - val_loss: 53.8700\n",
      "Epoch 7152/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 711.4989 - val_loss: 45.5386\n",
      "Epoch 7153/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 252.3263 - val_loss: 172.7114\n",
      "Epoch 7154/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 223.6904 - val_loss: 155.2340\n",
      "Epoch 7155/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 70.0308 - val_loss: 99.2071\n",
      "Epoch 7156/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 43.9216 - val_loss: 37.2607\n",
      "Epoch 7157/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16.8923 - val_loss: 2.2475\n",
      "Epoch 7158/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.3890 - val_loss: 4.8590\n",
      "Epoch 7159/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.5974 - val_loss: 3.1052\n",
      "Epoch 7160/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.4884 - val_loss: 12.9642\n",
      "Epoch 7161/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.3190 - val_loss: 3.1553\n",
      "Epoch 7162/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.2460 - val_loss: 11.9507\n",
      "Epoch 7163/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.3710 - val_loss: 2.3983\n",
      "Epoch 7164/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4981 - val_loss: 1.8011\n",
      "Epoch 7165/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.3110 - val_loss: 5.8970\n",
      "Epoch 7166/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.5565 - val_loss: 0.3993\n",
      "Epoch 7167/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4672 - val_loss: 1.3118\n",
      "Epoch 7168/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9135 - val_loss: 0.2618\n",
      "Epoch 7169/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4515 - val_loss: 0.2360\n",
      "Epoch 7170/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3901 - val_loss: 0.3003\n",
      "Epoch 7171/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2943 - val_loss: 0.1842\n",
      "Epoch 7172/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2563 - val_loss: 0.2152\n",
      "Epoch 7173/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2923 - val_loss: 0.1809\n",
      "Epoch 7174/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3234 - val_loss: 0.1970\n",
      "Epoch 7175/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3062 - val_loss: 0.2653\n",
      "Epoch 7176/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2893 - val_loss: 0.2689\n",
      "Epoch 7177/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3277 - val_loss: 0.1608\n",
      "Epoch 7178/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2728 - val_loss: 0.2027\n",
      "Epoch 7179/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2346 - val_loss: 0.1694\n",
      "Epoch 7180/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2175 - val_loss: 0.3674\n",
      "Epoch 7181/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2930 - val_loss: 0.2685\n",
      "Epoch 7182/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2810 - val_loss: 0.1489\n",
      "Epoch 7183/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1950 - val_loss: 0.1433\n",
      "Epoch 7184/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1830 - val_loss: 0.1378\n",
      "Epoch 7185/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1745 - val_loss: 0.1668\n",
      "Epoch 7186/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1620 - val_loss: 0.1368\n",
      "Epoch 7187/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1570 - val_loss: 0.1499\n",
      "Epoch 7188/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1758 - val_loss: 0.1999\n",
      "Epoch 7189/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1943 - val_loss: 0.2596\n",
      "Epoch 7190/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3521 - val_loss: 0.2065\n",
      "Epoch 7191/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2895 - val_loss: 0.2197\n",
      "Epoch 7192/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2527 - val_loss: 0.1563\n",
      "Epoch 7193/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3307 - val_loss: 0.4753\n",
      "Epoch 7194/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4023 - val_loss: 0.2304\n",
      "Epoch 7195/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4089 - val_loss: 0.1750\n",
      "Epoch 7196/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2468 - val_loss: 0.3117\n",
      "Epoch 7197/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2390 - val_loss: 0.1538\n",
      "Epoch 7198/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1561 - val_loss: 0.1749\n",
      "Epoch 7199/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1577 - val_loss: 0.1384\n",
      "Epoch 7200/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1435 - val_loss: 0.1558\n",
      "Epoch 7201/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3026 - val_loss: 0.7925\n",
      "Epoch 7202/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4073 - val_loss: 0.2302\n",
      "Epoch 7203/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2418 - val_loss: 0.4892\n",
      "Epoch 7204/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3734 - val_loss: 0.4938\n",
      "Epoch 7205/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4115 - val_loss: 0.3118\n",
      "Epoch 7206/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1939 - val_loss: 0.1635\n",
      "Epoch 7207/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2782 - val_loss: 0.1089\n",
      "Epoch 7208/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1134 - val_loss: 0.1153\n",
      "Epoch 7209/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1315 - val_loss: 0.1515\n",
      "Epoch 7210/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2397 - val_loss: 0.1104\n",
      "Epoch 7211/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2819 - val_loss: 0.4324\n",
      "Epoch 7212/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3336 - val_loss: 0.3671\n",
      "Epoch 7213/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2784 - val_loss: 0.1069\n",
      "Epoch 7214/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1204 - val_loss: 0.1741\n",
      "Epoch 7215/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1229 - val_loss: 0.1087\n",
      "Epoch 7216/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1190 - val_loss: 0.2483\n",
      "Epoch 7217/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2488 - val_loss: 0.3611\n",
      "Epoch 7218/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2431 - val_loss: 0.1071\n",
      "Epoch 7219/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1575 - val_loss: 0.2818\n",
      "Epoch 7220/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2162 - val_loss: 0.3866\n",
      "Epoch 7221/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2787 - val_loss: 0.0993\n",
      "Epoch 7222/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2052 - val_loss: 0.3061\n",
      "Epoch 7223/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2331 - val_loss: 0.3484\n",
      "Epoch 7224/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2337 - val_loss: 0.0843\n",
      "Epoch 7225/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1373 - val_loss: 0.1650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7226/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1128 - val_loss: 0.1372\n",
      "Epoch 7227/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1081 - val_loss: 0.1054\n",
      "Epoch 7228/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1446 - val_loss: 0.1041\n",
      "Epoch 7229/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1324 - val_loss: 0.1016\n",
      "Epoch 7230/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0925 - val_loss: 0.0958\n",
      "Epoch 7231/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1158 - val_loss: 0.1250\n",
      "Epoch 7232/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0985 - val_loss: 0.0935\n",
      "Epoch 7233/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1037 - val_loss: 0.0804\n",
      "Epoch 7234/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0788 - val_loss: 0.1028\n",
      "Epoch 7235/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1809 - val_loss: 0.1730\n",
      "Epoch 7236/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1594 - val_loss: 0.1206\n",
      "Epoch 7237/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0947 - val_loss: 0.1092\n",
      "Epoch 7238/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0847 - val_loss: 0.0675\n",
      "Epoch 7239/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0998 - val_loss: 0.0907\n",
      "Epoch 7240/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1703 - val_loss: 0.1991\n",
      "Epoch 7241/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1030 - val_loss: 0.0989\n",
      "Epoch 7242/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1176 - val_loss: 0.0698\n",
      "Epoch 7243/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0835 - val_loss: 0.0670\n",
      "Epoch 7244/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0708 - val_loss: 0.0765\n",
      "Epoch 7245/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0710 - val_loss: 0.0751\n",
      "Epoch 7246/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1333 - val_loss: 0.2856\n",
      "Epoch 7247/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2170 - val_loss: 0.0807\n",
      "Epoch 7248/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1277 - val_loss: 0.0597\n",
      "Epoch 7249/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0657 - val_loss: 0.0579\n",
      "Epoch 7250/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0532 - val_loss: 0.0590\n",
      "Epoch 7251/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0558 - val_loss: 0.1010\n",
      "Epoch 7252/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0700 - val_loss: 0.0749\n",
      "Epoch 7253/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0509 - val_loss: 0.0571\n",
      "Epoch 7254/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0577 - val_loss: 0.0884\n",
      "Epoch 7255/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1075 - val_loss: 0.0613\n",
      "Epoch 7256/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1034 - val_loss: 0.1005\n",
      "Epoch 7257/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1167 - val_loss: 0.1848\n",
      "Epoch 7258/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1481 - val_loss: 0.2568\n",
      "Epoch 7259/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1764 - val_loss: 0.3066\n",
      "Epoch 7260/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1379 - val_loss: 0.2011\n",
      "Epoch 7261/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1249 - val_loss: 0.1443\n",
      "Epoch 7262/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1107 - val_loss: 0.1141\n",
      "Epoch 7263/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0660 - val_loss: 0.0694\n",
      "Epoch 7264/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0690 - val_loss: 0.0628\n",
      "Epoch 7265/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1559 - val_loss: 0.0922\n",
      "Epoch 7266/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2771 - val_loss: 0.0722\n",
      "Epoch 7267/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2212 - val_loss: 0.0904\n",
      "Epoch 7268/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2946 - val_loss: 0.2471\n",
      "Epoch 7269/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4964 - val_loss: 0.0557\n",
      "Epoch 7270/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2782 - val_loss: 0.1081\n",
      "Epoch 7271/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.1258 - val_loss: 0.1148\n",
      "Epoch 7272/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1057 - val_loss: 0.1047\n",
      "Epoch 7273/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0599 - val_loss: 0.0452\n",
      "Epoch 7274/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0458 - val_loss: 0.0529\n",
      "Epoch 7275/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0640 - val_loss: 0.0415\n",
      "Epoch 7276/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0545 - val_loss: 0.0406\n",
      "Epoch 7277/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1027 - val_loss: 0.0905\n",
      "Epoch 7278/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1299 - val_loss: 0.1637\n",
      "Epoch 7279/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2524 - val_loss: 0.0668\n",
      "Epoch 7280/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3613 - val_loss: 0.1414\n",
      "Epoch 7281/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5568 - val_loss: 0.0417\n",
      "Epoch 7282/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3351 - val_loss: 0.0642\n",
      "Epoch 7283/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2001 - val_loss: 0.2631\n",
      "Epoch 7284/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2400 - val_loss: 0.2829\n",
      "Epoch 7285/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2261 - val_loss: 0.2839\n",
      "Epoch 7286/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2302 - val_loss: 0.0903\n",
      "Epoch 7287/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0828 - val_loss: 0.0995\n",
      "Epoch 7288/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0866 - val_loss: 0.1242\n",
      "Epoch 7289/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1575 - val_loss: 0.1289\n",
      "Epoch 7290/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2508 - val_loss: 0.0408\n",
      "Epoch 7291/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2640 - val_loss: 0.0366\n",
      "Epoch 7292/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0527 - val_loss: 0.0330\n",
      "Epoch 7293/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0528 - val_loss: 0.0334\n",
      "Epoch 7294/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0309 - val_loss: 0.0307\n",
      "Epoch 7295/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0583 - val_loss: 0.0694\n",
      "Epoch 7296/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0604 - val_loss: 0.1232\n",
      "Epoch 7297/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1995 - val_loss: 0.1702\n",
      "Epoch 7298/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2813 - val_loss: 0.3955\n",
      "Epoch 7299/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1682 - val_loss: 0.4508\n",
      "Epoch 7300/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2783 - val_loss: 0.1601\n",
      "Epoch 7301/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1633 - val_loss: 0.0373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7302/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1239 - val_loss: 0.0361\n",
      "Epoch 7303/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0981 - val_loss: 0.0296\n",
      "Epoch 7304/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0722 - val_loss: 0.0417\n",
      "Epoch 7305/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0591 - val_loss: 0.0522\n",
      "Epoch 7306/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0444 - val_loss: 0.0361\n",
      "Epoch 7307/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0631 - val_loss: 0.2999\n",
      "Epoch 7308/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2645 - val_loss: 0.1657\n",
      "Epoch 7309/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3373 - val_loss: 0.7704\n",
      "Epoch 7310/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9360 - val_loss: 3.3765\n",
      "Epoch 7311/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.3193 - val_loss: 2.0936\n",
      "Epoch 7312/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9950 - val_loss: 1.9767\n",
      "Epoch 7313/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4346 - val_loss: 3.4059\n",
      "Epoch 7314/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.3267 - val_loss: 11.3070\n",
      "Epoch 7315/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.7523 - val_loss: 7.5745\n",
      "Epoch 7316/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.7172 - val_loss: 1.2831\n",
      "Epoch 7317/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2998 - val_loss: 1.5740\n",
      "Epoch 7318/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.0663 - val_loss: 1.6961\n",
      "Epoch 7319/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7754 - val_loss: 0.5499\n",
      "Epoch 7320/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2158 - val_loss: 0.2688\n",
      "Epoch 7321/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1475 - val_loss: 0.0449\n",
      "Epoch 7322/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1297 - val_loss: 0.2369\n",
      "Epoch 7323/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1335 - val_loss: 0.0514\n",
      "Epoch 7324/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0869 - val_loss: 0.2222\n",
      "Epoch 7325/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3597 - val_loss: 0.8623\n",
      "Epoch 7326/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4974 - val_loss: 0.2783\n",
      "Epoch 7327/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6326 - val_loss: 0.4910\n",
      "Epoch 7328/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5436 - val_loss: 0.0812\n",
      "Epoch 7329/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1930 - val_loss: 0.2269\n",
      "Epoch 7330/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.2525 - val_loss: 0.6425\n",
      "Epoch 7331/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6624 - val_loss: 0.1469\n",
      "Epoch 7332/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8475 - val_loss: 0.5675\n",
      "Epoch 7333/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3368 - val_loss: 0.0933\n",
      "Epoch 7334/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1446 - val_loss: 0.1366\n",
      "Epoch 7335/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1409 - val_loss: 0.0581\n",
      "Epoch 7336/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1160 - val_loss: 0.0869\n",
      "Epoch 7337/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0396 - val_loss: 0.1006\n",
      "Epoch 7338/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0770 - val_loss: 0.0586\n",
      "Epoch 7339/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0370 - val_loss: 0.0182\n",
      "Epoch 7340/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0347 - val_loss: 0.0262\n",
      "Epoch 7341/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0172 - val_loss: 0.0146\n",
      "Epoch 7342/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0162 - val_loss: 0.0131\n",
      "Epoch 7343/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0175 - val_loss: 0.0202\n",
      "Epoch 7344/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0180 - val_loss: 0.0376\n",
      "Epoch 7345/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0201 - val_loss: 0.0432\n",
      "Epoch 7346/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0136 - val_loss: 0.0141\n",
      "Epoch 7347/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0162 - val_loss: 0.0236\n",
      "Epoch 7348/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.015 - 0s 132us/sample - loss: 0.0146 - val_loss: 0.0271\n",
      "Epoch 7349/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0254 - val_loss: 0.0138\n",
      "Epoch 7350/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0160 - val_loss: 0.0103\n",
      "Epoch 7351/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.007 - 0s 132us/sample - loss: 0.0123 - val_loss: 0.0205\n",
      "Epoch 7352/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0809 - val_loss: 0.0256\n",
      "Epoch 7353/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0705 - val_loss: 0.0216\n",
      "Epoch 7354/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0286 - val_loss: 0.0746\n",
      "Epoch 7355/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0414 - val_loss: 0.0162\n",
      "Epoch 7356/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0238 - val_loss: 0.0157\n",
      "Epoch 7357/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0106 - val_loss: 0.0086\n",
      "Epoch 7358/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0090 - val_loss: 0.0093\n",
      "Epoch 7359/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0130 - val_loss: 0.0174\n",
      "Epoch 7360/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0162 - val_loss: 0.0178\n",
      "Epoch 7361/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0218 - val_loss: 0.0204\n",
      "Epoch 7362/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0271 - val_loss: 0.0153\n",
      "Epoch 7363/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0151 - val_loss: 0.0131\n",
      "Epoch 7364/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0078 - val_loss: 0.0110\n",
      "Epoch 7365/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0108 - val_loss: 0.0123\n",
      "Epoch 7366/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0101 - val_loss: 0.0092\n",
      "Epoch 7367/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0118 - val_loss: 0.0128\n",
      "Epoch 7368/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0121 - val_loss: 0.0126\n",
      "Epoch 7369/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0137 - val_loss: 0.0216\n",
      "Epoch 7370/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0148 - val_loss: 0.0271\n",
      "Epoch 7371/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0181 - val_loss: 0.0069\n",
      "Epoch 7372/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0102 - val_loss: 0.0162\n",
      "Epoch 7373/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0137 - val_loss: 0.0119\n",
      "Epoch 7374/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0092 - val_loss: 0.0077\n",
      "Epoch 7375/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0088 - val_loss: 0.0073\n",
      "Epoch 7376/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0150 - val_loss: 0.0123\n",
      "Epoch 7377/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0327 - val_loss: 0.0289\n",
      "Epoch 7378/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0333 - val_loss: 0.0103\n",
      "Epoch 7379/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0436 - val_loss: 0.6262\n",
      "Epoch 7380/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2980 - val_loss: 0.1653\n",
      "Epoch 7381/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3390 - val_loss: 0.2302\n",
      "Epoch 7382/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1585 - val_loss: 0.0850\n",
      "Epoch 7383/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1232 - val_loss: 0.1360\n",
      "Epoch 7384/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1123 - val_loss: 0.0721\n",
      "Epoch 7385/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0493 - val_loss: 0.0671\n",
      "Epoch 7386/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0576 - val_loss: 0.0285\n",
      "Epoch 7387/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0621 - val_loss: 0.2744\n",
      "Epoch 7388/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2123 - val_loss: 0.2259\n",
      "Epoch 7389/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4227 - val_loss: 0.2843\n",
      "Epoch 7390/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3392 - val_loss: 0.1736\n",
      "Epoch 7391/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2184 - val_loss: 0.4698\n",
      "Epoch 7392/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4187 - val_loss: 0.1099\n",
      "Epoch 7393/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.123 - 0s 147us/sample - loss: 1.0435 - val_loss: 0.2068\n",
      "Epoch 7394/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.1037 - val_loss: 0.0720\n",
      "Epoch 7395/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.9956 - val_loss: 0.0964\n",
      "Epoch 7396/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.7993 - val_loss: 0.5259\n",
      "Epoch 7397/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2394 - val_loss: 6.1433\n",
      "Epoch 7398/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.8553 - val_loss: 5.3273\n",
      "Epoch 7399/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.1258 - val_loss: 8.4215\n",
      "Epoch 7400/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.0253 - val_loss: 0.7013\n",
      "Epoch 7401/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.1003 - val_loss: 5.7024\n",
      "Epoch 7402/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.5462 - val_loss: 7.0024\n",
      "Epoch 7403/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8671 - val_loss: 3.1067\n",
      "Epoch 7404/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4902 - val_loss: 0.1731\n",
      "Epoch 7405/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4169 - val_loss: 0.2115\n",
      "Epoch 7406/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3777 - val_loss: 0.4347\n",
      "Epoch 7407/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3132 - val_loss: 1.6708\n",
      "Epoch 7408/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.9087 - val_loss: 7.0546\n",
      "Epoch 7409/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.5150 - val_loss: 25.7127\n",
      "Epoch 7410/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 45.8884 - val_loss: 14.0737\n",
      "Epoch 7411/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 20.8932 - val_loss: 13.7192\n",
      "Epoch 7412/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 54.5189 - val_loss: 19.8502\n",
      "Epoch 7413/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 30.8815 - val_loss: 1.9791\n",
      "Epoch 7414/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.2340 - val_loss: 35.7746\n",
      "Epoch 7415/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 21.4631 - val_loss: 51.4099\n",
      "Epoch 7416/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 44.2423 - val_loss: 175.8520\n",
      "Epoch 7417/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 65.4676 - val_loss: 29.6195\n",
      "Epoch 7418/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 19.7936 - val_loss: 14.8081\n",
      "Epoch 7419/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.4694 - val_loss: 14.2082\n",
      "Epoch 7420/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 16.6012 - val_loss: 23.3429\n",
      "Epoch 7421/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 10.0019 - val_loss: 6.6091\n",
      "Epoch 7422/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 10.3564 - val_loss: 23.3105\n",
      "Epoch 7423/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 13.5400 - val_loss: 8.1163\n",
      "Epoch 7424/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.5159 - val_loss: 13.0217\n",
      "Epoch 7425/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 14.9263 - val_loss: 14.7219\n",
      "Epoch 7426/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 12.4773 - val_loss: 13.0401\n",
      "Epoch 7427/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 28.9765 - val_loss: 4.5873\n",
      "Epoch 7428/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 46.9529 - val_loss: 105.5236\n",
      "Epoch 7429/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 46.8111 - val_loss: 19.6212\n",
      "Epoch 7430/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.4383 - val_loss: 9.9318\n",
      "Epoch 7431/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17.9821 - val_loss: 29.4036\n",
      "Epoch 7432/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 60.4284 - val_loss: 18.2472\n",
      "Epoch 7433/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 107.5280 - val_loss: 19.5181\n",
      "Epoch 7434/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 34.0203 - val_loss: 128.1292\n",
      "Epoch 7435/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 112.9240 - val_loss: 1.5158\n",
      "Epoch 7436/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 118.1121 - val_loss: 588.5873\n",
      "Epoch 7437/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 928.0972 - val_loss: 356.5494\n",
      "Epoch 7438/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5969.3855 - val_loss: 2544.5881\n",
      "Epoch 7439/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 23424.0802 - val_loss: 37043.9366\n",
      "Epoch 7440/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 39717.3159 - val_loss: 4144.7828\n",
      "Epoch 7441/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15864.4904 - val_loss: 42208.9074\n",
      "Epoch 7442/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 22971.2660 - val_loss: 24852.8402\n",
      "Epoch 7443/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10525.3961 - val_loss: 2927.5277\n",
      "Epoch 7444/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2920.4810 - val_loss: 184.1552\n",
      "Epoch 7445/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1439.3182 - val_loss: 99.6276\n",
      "Epoch 7446/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 856.8757 - val_loss: 586.3271\n",
      "Epoch 7447/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 919.9686 - val_loss: 948.3736\n",
      "Epoch 7448/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 498.3218 - val_loss: 488.4958\n",
      "Epoch 7449/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 190.3645 - val_loss: 325.6097\n",
      "Epoch 7450/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 200.3725 - val_loss: 219.6264\n",
      "Epoch 7451/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 120.9538 - val_loss: 23.1871\n",
      "Epoch 7452/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 25.69 - 0s 176us/sample - loss: 70.8116 - val_loss: 3.9082\n",
      "Epoch 7453/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 2.078 - 0s 191us/sample - loss: 57.6572 - val_loss: 24.3890\n",
      "Epoch 7454/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 26.5895 - val_loss: 25.3654\n",
      "Epoch 7455/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.2978 - val_loss: 32.0524\n",
      "Epoch 7456/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 20.2434 - val_loss: 13.2961\n",
      "Epoch 7457/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.6366 - val_loss: 1.6576\n",
      "Epoch 7458/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.1550 - val_loss: 1.6368\n",
      "Epoch 7459/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3.5146 - val_loss: 1.5811\n",
      "Epoch 7460/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0930 - val_loss: 1.4715\n",
      "Epoch 7461/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2742 - val_loss: 0.4650\n",
      "Epoch 7462/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8885 - val_loss: 0.1336\n",
      "Epoch 7463/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7973 - val_loss: 0.0614\n",
      "Epoch 7464/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7309 - val_loss: 0.0623\n",
      "Epoch 7465/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6165 - val_loss: 0.0631\n",
      "Epoch 7466/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5850 - val_loss: 0.0648\n",
      "Epoch 7467/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5540 - val_loss: 0.0304\n",
      "Epoch 7468/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5139 - val_loss: 0.0509\n",
      "Epoch 7469/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4493 - val_loss: 0.0418\n",
      "Epoch 7470/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4373 - val_loss: 0.0381\n",
      "Epoch 7471/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3893 - val_loss: 0.0439\n",
      "Epoch 7472/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3611 - val_loss: 0.0373\n",
      "Epoch 7473/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3327 - val_loss: 0.0495\n",
      "Epoch 7474/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3264 - val_loss: 0.0304\n",
      "Epoch 7475/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2940 - val_loss: 0.0442\n",
      "Epoch 7476/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2857 - val_loss: 0.0426\n",
      "Epoch 7477/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2827 - val_loss: 0.0267\n",
      "Epoch 7478/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2826 - val_loss: 0.0590\n",
      "Epoch 7479/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2410 - val_loss: 0.0504\n",
      "Epoch 7480/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2210 - val_loss: 0.0263\n",
      "Epoch 7481/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2037 - val_loss: 0.0248\n",
      "Epoch 7482/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1939 - val_loss: 0.0371\n",
      "Epoch 7483/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1702 - val_loss: 0.0336\n",
      "Epoch 7484/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1561 - val_loss: 0.0300\n",
      "Epoch 7485/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1524 - val_loss: 0.0281\n",
      "Epoch 7486/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1488 - val_loss: 0.0528\n",
      "Epoch 7487/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1659 - val_loss: 0.0824\n",
      "Epoch 7488/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1408 - val_loss: 0.0359\n",
      "Epoch 7489/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1237 - val_loss: 0.0376\n",
      "Epoch 7490/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1104 - val_loss: 0.0655\n",
      "Epoch 7491/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1130 - val_loss: 0.0320\n",
      "Epoch 7492/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0903 - val_loss: 0.0210\n",
      "Epoch 7493/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0868 - val_loss: 0.0250\n",
      "Epoch 7494/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0834 - val_loss: 0.0305\n",
      "Epoch 7495/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0787 - val_loss: 0.0361\n",
      "Epoch 7496/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0729 - val_loss: 0.0333\n",
      "Epoch 7497/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0769 - val_loss: 0.0314\n",
      "Epoch 7498/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0758 - val_loss: 0.0357\n",
      "Epoch 7499/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0720 - val_loss: 0.0204\n",
      "Epoch 7500/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0708 - val_loss: 0.0525\n",
      "Epoch 7501/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0816 - val_loss: 0.0212\n",
      "Epoch 7502/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0539 - val_loss: 0.0199\n",
      "Epoch 7503/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0511 - val_loss: 0.0198\n",
      "Epoch 7504/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0431 - val_loss: 0.0181\n",
      "Epoch 7505/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0436 - val_loss: 0.0280\n",
      "Epoch 7506/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0393 - val_loss: 0.0177\n",
      "Epoch 7507/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0394 - val_loss: 0.0175\n",
      "Epoch 7508/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0407 - val_loss: 0.0286\n",
      "Epoch 7509/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0524 - val_loss: 0.0166\n",
      "Epoch 7510/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0428 - val_loss: 0.0546\n",
      "Epoch 7511/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0527 - val_loss: 0.0247\n",
      "Epoch 7512/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0304 - val_loss: 0.0160\n",
      "Epoch 7513/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0262 - val_loss: 0.0178\n",
      "Epoch 7514/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0254 - val_loss: 0.0189\n",
      "Epoch 7515/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0266 - val_loss: 0.0179\n",
      "Epoch 7516/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0250 - val_loss: 0.0172\n",
      "Epoch 7517/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0337 - val_loss: 0.0179\n",
      "Epoch 7518/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0336 - val_loss: 0.0490\n",
      "Epoch 7519/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0487 - val_loss: 0.0281\n",
      "Epoch 7520/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0413 - val_loss: 0.0514\n",
      "Epoch 7521/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0619 - val_loss: 0.1151\n",
      "Epoch 7522/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0731 - val_loss: 0.0860\n",
      "Epoch 7523/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1135 - val_loss: 0.0838\n",
      "Epoch 7524/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0723 - val_loss: 0.0461\n",
      "Epoch 7525/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0727 - val_loss: 0.1272\n",
      "Epoch 7526/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0736 - val_loss: 0.0480\n",
      "Epoch 7527/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0472 - val_loss: 0.0812\n",
      "Epoch 7528/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0476 - val_loss: 0.0254\n",
      "Epoch 7529/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0206 - val_loss: 0.0128\n",
      "Epoch 7530/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0165 - val_loss: 0.0327\n",
      "Epoch 7531/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0252 - val_loss: 0.0171\n",
      "Epoch 7532/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0350 - val_loss: 0.0128\n",
      "Epoch 7533/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0150 - val_loss: 0.0135\n",
      "Epoch 7534/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0144 - val_loss: 0.0131\n",
      "Epoch 7535/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0245 - val_loss: 0.0282\n",
      "Epoch 7536/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0200 - val_loss: 0.0155\n",
      "Epoch 7537/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0174 - val_loss: 0.0179\n",
      "Epoch 7538/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0178 - val_loss: 0.0117\n",
      "Epoch 7539/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0175 - val_loss: 0.0134\n",
      "Epoch 7540/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0142 - val_loss: 0.0149\n",
      "Epoch 7541/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0151 - val_loss: 0.0128\n",
      "Epoch 7542/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0131 - val_loss: 0.0223\n",
      "Epoch 7543/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0176 - val_loss: 0.0202\n",
      "Epoch 7544/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0153 - val_loss: 0.0183\n",
      "Epoch 7545/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0194 - val_loss: 0.0229\n",
      "Epoch 7546/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0171 - val_loss: 0.0099\n",
      "Epoch 7547/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0126 - val_loss: 0.0251\n",
      "Epoch 7548/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0219 - val_loss: 0.0171\n",
      "Epoch 7549/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0254 - val_loss: 0.0333\n",
      "Epoch 7550/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0147 - val_loss: 0.0117\n",
      "Epoch 7551/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0161 - val_loss: 0.0216\n",
      "Epoch 7552/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0206 - val_loss: 0.0266\n",
      "Epoch 7553/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0204 - val_loss: 0.0138\n",
      "Epoch 7554/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0210 - val_loss: 0.0367\n",
      "Epoch 7555/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0236 - val_loss: 0.0176\n",
      "Epoch 7556/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0169 - val_loss: 0.0105\n",
      "Epoch 7557/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0109 - val_loss: 0.0159\n",
      "Epoch 7558/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0100 - val_loss: 0.0088\n",
      "Epoch 7559/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 7560/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0089 - val_loss: 0.0083\n",
      "Epoch 7561/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0121 - val_loss: 0.0145\n",
      "Epoch 7562/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0094 - val_loss: 0.0080\n",
      "Epoch 7563/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0091 - val_loss: 0.0103\n",
      "Epoch 7564/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0143 - val_loss: 0.0215\n",
      "Epoch 7565/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0250 - val_loss: 0.0225\n",
      "Epoch 7566/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0179 - val_loss: 0.0184\n",
      "Epoch 7567/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0197 - val_loss: 0.0093\n",
      "Epoch 7568/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0167 - val_loss: 0.0466\n",
      "Epoch 7569/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0321 - val_loss: 0.0272\n",
      "Epoch 7570/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0236 - val_loss: 0.0100\n",
      "Epoch 7571/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 7572/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0112 - val_loss: 0.0190\n",
      "Epoch 7573/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0107 - val_loss: 0.0135\n",
      "Epoch 7574/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0090 - val_loss: 0.0090\n",
      "Epoch 7575/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0090 - val_loss: 0.0077\n",
      "Epoch 7576/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0124 - val_loss: 0.0203\n",
      "Epoch 7577/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0139 - val_loss: 0.0152\n",
      "Epoch 7578/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0168 - val_loss: 0.0121\n",
      "Epoch 7579/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0153 - val_loss: 0.0118\n",
      "Epoch 7580/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0166 - val_loss: 0.0613\n",
      "Epoch 7581/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0372 - val_loss: 0.0309\n",
      "Epoch 7582/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.0269 - val_loss: 0.0060\n",
      "Epoch 7583/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0269 - val_loss: 0.0153\n",
      "Epoch 7584/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0205 - val_loss: 0.0401\n",
      "Epoch 7585/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0296 - val_loss: 0.0301\n",
      "Epoch 7586/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0171 - val_loss: 0.0139\n",
      "Epoch 7587/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0090 - val_loss: 0.0075\n",
      "Epoch 7588/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 7589/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0082 - val_loss: 0.0056\n",
      "Epoch 7590/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 7591/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0065 - val_loss: 0.0056\n",
      "Epoch 7592/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0052 - val_loss: 0.0061\n",
      "Epoch 7593/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0070 - val_loss: 0.0105\n",
      "Epoch 7594/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0084 - val_loss: 0.0080\n",
      "Epoch 7595/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0120 - val_loss: 0.0060\n",
      "Epoch 7596/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0062 - val_loss: 0.0049\n",
      "Epoch 7597/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0051 - val_loss: 0.0053\n",
      "Epoch 7598/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0070 - val_loss: 0.0052\n",
      "Epoch 7599/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0061 - val_loss: 0.0067\n",
      "Epoch 7600/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0060 - val_loss: 0.0053\n",
      "Epoch 7601/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0045 - val_loss: 0.0052\n",
      "Epoch 7602/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0089 - val_loss: 0.0160\n",
      "Epoch 7603/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0138 - val_loss: 0.0088\n",
      "Epoch 7604/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0118 - val_loss: 0.0146\n",
      "Epoch 7605/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0107 - val_loss: 0.0053\n",
      "Epoch 7606/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0115 - val_loss: 0.0162\n",
      "Epoch 7607/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0106 - val_loss: 0.0089\n",
      "Epoch 7608/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0081 - val_loss: 0.0047\n",
      "Epoch 7609/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 7610/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0059 - val_loss: 0.0084\n",
      "Epoch 7611/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0049 - val_loss: 0.0042\n",
      "Epoch 7612/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.008 - 0s 147us/sample - loss: 0.0067 - val_loss: 0.0036\n",
      "Epoch 7613/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0051 - val_loss: 0.0040\n",
      "Epoch 7614/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 7615/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0035 - val_loss: 0.0037\n",
      "Epoch 7616/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0037 - val_loss: 0.0033\n",
      "Epoch 7617/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0038 - val_loss: 0.0085\n",
      "Epoch 7618/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0064 - val_loss: 0.0037\n",
      "Epoch 7619/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0064 - val_loss: 0.0040\n",
      "Epoch 7620/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0085 - val_loss: 0.0040\n",
      "Epoch 7621/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0092 - val_loss: 0.0038\n",
      "Epoch 7622/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0060 - val_loss: 0.0079\n",
      "Epoch 7623/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0069 - val_loss: 0.0111\n",
      "Epoch 7624/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0125 - val_loss: 0.0142\n",
      "Epoch 7625/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0133 - val_loss: 0.0084\n",
      "Epoch 7626/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0077 - val_loss: 0.0102\n",
      "Epoch 7627/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0088 - val_loss: 0.0056\n",
      "Epoch 7628/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0054 - val_loss: 0.0035\n",
      "Epoch 7629/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0056 - val_loss: 0.0108\n",
      "Epoch 7630/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0078 - val_loss: 0.0306\n",
      "Epoch 7631/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0257 - val_loss: 0.0422\n",
      "Epoch 7632/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0205 - val_loss: 0.0448\n",
      "Epoch 7633/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0355 - val_loss: 0.0229\n",
      "Epoch 7634/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0195 - val_loss: 0.0445\n",
      "Epoch 7635/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0662 - val_loss: 0.0630\n",
      "Epoch 7636/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0460 - val_loss: 0.0119\n",
      "Epoch 7637/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0401 - val_loss: 0.0098\n",
      "Epoch 7638/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0429 - val_loss: 0.0255\n",
      "Epoch 7639/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0446 - val_loss: 0.0105\n",
      "Epoch 7640/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0503 - val_loss: 0.0688\n",
      "Epoch 7641/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0704 - val_loss: 0.0995\n",
      "Epoch 7642/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0359 - val_loss: 0.0119\n",
      "Epoch 7643/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0186 - val_loss: 0.0097\n",
      "Epoch 7644/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0185 - val_loss: 0.0187\n",
      "Epoch 7645/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0200 - val_loss: 0.0287\n",
      "Epoch 7646/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0199 - val_loss: 0.0609\n",
      "Epoch 7647/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0303 - val_loss: 0.0204\n",
      "Epoch 7648/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0354 - val_loss: 0.0037\n",
      "Epoch 7649/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0379 - val_loss: 0.0134\n",
      "Epoch 7650/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1163 - val_loss: 0.0341\n",
      "Epoch 7651/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0645 - val_loss: 0.0540\n",
      "Epoch 7652/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0431 - val_loss: 0.0155\n",
      "Epoch 7653/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0223 - val_loss: 0.0221\n",
      "Epoch 7654/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0273 - val_loss: 0.0084\n",
      "Epoch 7655/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0078 - val_loss: 0.0071\n",
      "Epoch 7656/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0115 - val_loss: 0.0044\n",
      "Epoch 7657/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0118 - val_loss: 0.0200\n",
      "Epoch 7658/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0157 - val_loss: 0.0104\n",
      "Epoch 7659/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0076 - val_loss: 0.0018\n",
      "Epoch 7660/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0051 - val_loss: 0.0051\n",
      "Epoch 7661/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0095 - val_loss: 0.0079\n",
      "Epoch 7662/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0187 - val_loss: 0.0077\n",
      "Epoch 7663/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0316 - val_loss: 0.0343\n",
      "Epoch 7664/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0350 - val_loss: 0.0512\n",
      "Epoch 7665/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0814 - val_loss: 0.1174\n",
      "Epoch 7666/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0788 - val_loss: 0.0547\n",
      "Epoch 7667/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0625 - val_loss: 0.0301\n",
      "Epoch 7668/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0490 - val_loss: 0.0391\n",
      "Epoch 7669/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0156 - val_loss: 0.0192\n",
      "Epoch 7670/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0098 - val_loss: 0.0060\n",
      "Epoch 7671/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0043 - val_loss: 0.0104\n",
      "Epoch 7672/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0049 - val_loss: 0.0012\n",
      "Epoch 7673/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0036 - val_loss: 0.0022\n",
      "Epoch 7674/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0036 - val_loss: 0.0134\n",
      "Epoch 7675/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0281 - val_loss: 0.0290\n",
      "Epoch 7676/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0207 - val_loss: 0.0102\n",
      "Epoch 7677/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0151 - val_loss: 0.0106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7678/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0053 - val_loss: 0.0086\n",
      "Epoch 7679/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0055 - val_loss: 0.0038\n",
      "Epoch 7680/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0034 - val_loss: 0.0037\n",
      "Epoch 7681/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0028 - val_loss: 0.0015\n",
      "Epoch 7682/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0044 - val_loss: 0.0075\n",
      "Epoch 7683/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0051 - val_loss: 0.0016\n",
      "Epoch 7684/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0034 - val_loss: 0.0054\n",
      "Epoch 7685/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0030 - val_loss: 0.0027\n",
      "Epoch 7686/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 7687/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 7688/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 7689/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0015 - val_loss: 9.4560e-04\n",
      "Epoch 7690/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 7691/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 7692/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 7693/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0024 - val_loss: 8.2250e-04\n",
      "Epoch 7694/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 7695/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0013 - val_loss: 7.8161e-04\n",
      "Epoch 7696/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.6691e-04 - val_loss: 7.8480e-04\n",
      "Epoch 7697/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.1179e-04 - val_loss: 6.9096e-04\n",
      "Epoch 7698/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.4231e-04 - val_loss: 0.0015\n",
      "Epoch 7699/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.3351e-04 - val_loss: 7.3356e-04\n",
      "Epoch 7700/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0012 - val_loss: 0.0043\n",
      "Epoch 7701/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0021 - val_loss: 6.9255e-04\n",
      "Epoch 7702/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 7703/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 7704/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0042 - val_loss: 0.0078\n",
      "Epoch 7705/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0130 - val_loss: 0.0196\n",
      "Epoch 7706/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0140 - val_loss: 0.0131\n",
      "Epoch 7707/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0150 - val_loss: 0.0196\n",
      "Epoch 7708/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0112 - val_loss: 0.0052\n",
      "Epoch 7709/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0065 - val_loss: 7.0892e-04\n",
      "Epoch 7710/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0013 - val_loss: 9.0417e-04\n",
      "Epoch 7711/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 7712/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 7713/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0027 - val_loss: 0.0124\n",
      "Epoch 7714/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0134 - val_loss: 0.0141\n",
      "Epoch 7715/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0275 - val_loss: 0.0252\n",
      "Epoch 7716/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0527 - val_loss: 0.1937\n",
      "Epoch 7717/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0764 - val_loss: 0.0641\n",
      "Epoch 7718/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0400 - val_loss: 0.1800\n",
      "Epoch 7719/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1224 - val_loss: 0.1435\n",
      "Epoch 7720/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1374 - val_loss: 0.3556\n",
      "Epoch 7721/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4272 - val_loss: 0.0158\n",
      "Epoch 7722/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.4331 - val_loss: 1.4993\n",
      "Epoch 7723/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8201 - val_loss: 0.7773\n",
      "Epoch 7724/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.5184 - val_loss: 0.1958\n",
      "Epoch 7725/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1215 - val_loss: 0.2033\n",
      "Epoch 7726/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2509 - val_loss: 0.7879\n",
      "Epoch 7727/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7516 - val_loss: 0.7784\n",
      "Epoch 7728/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.7028 - val_loss: 0.4432\n",
      "Epoch 7729/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4582 - val_loss: 0.6360\n",
      "Epoch 7730/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.9097 - val_loss: 0.0688\n",
      "Epoch 7731/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2684 - val_loss: 0.3877\n",
      "Epoch 7732/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.6780 - val_loss: 0.2354\n",
      "Epoch 7733/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5119 - val_loss: 0.3126\n",
      "Epoch 7734/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1561 - val_loss: 0.0456\n",
      "Epoch 7735/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0675 - val_loss: 0.0410\n",
      "Epoch 7736/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0833 - val_loss: 0.1579\n",
      "Epoch 7737/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1963 - val_loss: 0.1271\n",
      "Epoch 7738/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0858 - val_loss: 0.0627\n",
      "Epoch 7739/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1026 - val_loss: 0.1912\n",
      "Epoch 7740/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.5172 - val_loss: 0.3578\n",
      "Epoch 7741/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1287 - val_loss: 0.0709\n",
      "Epoch 7742/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0315 - val_loss: 0.0612\n",
      "Epoch 7743/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0425 - val_loss: 0.1312\n",
      "Epoch 7744/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0687 - val_loss: 0.0107\n",
      "Epoch 7745/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0314 - val_loss: 0.0130\n",
      "Epoch 7746/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0082 - val_loss: 0.0010\n",
      "Epoch 7747/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0011 - val_loss: 0.0042\n",
      "Epoch 7748/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0066 - val_loss: 0.0020\n",
      "Epoch 7749/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0065 - val_loss: 0.0010\n",
      "Epoch 7750/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0055 - val_loss: 6.0306e-04\n",
      "Epoch 7751/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0093 - val_loss: 0.0016\n",
      "Epoch 7752/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0302 - val_loss: 0.0050\n",
      "Epoch 7753/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0189 - val_loss: 0.0135\n",
      "Epoch 7754/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0486 - val_loss: 0.0608\n",
      "Epoch 7755/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0309 - val_loss: 0.0883\n",
      "Epoch 7756/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0933 - val_loss: 0.3512\n",
      "Epoch 7757/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4656 - val_loss: 0.0297\n",
      "Epoch 7758/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4019 - val_loss: 0.3581\n",
      "Epoch 7759/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.0095 - val_loss: 0.1848\n",
      "Epoch 7760/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.0106 - val_loss: 1.9772\n",
      "Epoch 7761/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.4717 - val_loss: 6.4192\n",
      "Epoch 7762/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2492 - val_loss: 1.2494\n",
      "Epoch 7763/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.9517 - val_loss: 0.2274\n",
      "Epoch 7764/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.225 - 0s 147us/sample - loss: 0.4785 - val_loss: 0.2470\n",
      "Epoch 7765/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1651 - val_loss: 0.3775\n",
      "Epoch 7766/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3380 - val_loss: 0.5711\n",
      "Epoch 7767/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6052 - val_loss: 0.4407\n",
      "Epoch 7768/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5715 - val_loss: 0.0193\n",
      "Epoch 7769/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1270 - val_loss: 0.0133\n",
      "Epoch 7770/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0165 - val_loss: 0.0035\n",
      "Epoch 7771/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0422 - val_loss: 0.0151\n",
      "Epoch 7772/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0273 - val_loss: 0.0309\n",
      "Epoch 7773/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0385 - val_loss: 0.0272\n",
      "Epoch 7774/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0217 - val_loss: 0.0373\n",
      "Epoch 7775/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0140 - val_loss: 0.0069\n",
      "Epoch 7776/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0148 - val_loss: 0.0120\n",
      "Epoch 7777/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0120 - val_loss: 0.0263\n",
      "Epoch 7778/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0181 - val_loss: 3.3581e-04\n",
      "Epoch 7779/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0212 - val_loss: 0.0121\n",
      "Epoch 7780/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0061 - val_loss: 0.0013\n",
      "Epoch 7781/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0073 - val_loss: 0.0048\n",
      "Epoch 7782/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0037 - val_loss: 0.0052\n",
      "Epoch 7783/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0031 - val_loss: 0.0016\n",
      "Epoch 7784/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.7225e-04 - val_loss: 4.7211e-04\n",
      "Epoch 7785/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 7786/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.9025e-04 - val_loss: 1.3531e-04\n",
      "Epoch 7787/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.3396e-05 - val_loss: 1.1180e-04\n",
      "Epoch 7788/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.9191e-04 - val_loss: 1.3744e-05\n",
      "Epoch 7789/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8251e-04 - val_loss: 2.1323e-04\n",
      "Epoch 7790/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6467e-04 - val_loss: 1.6628e-04\n",
      "Epoch 7791/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.1233e-04 - val_loss: 0.0034\n",
      "Epoch 7792/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0057 - val_loss: 2.5295e-04\n",
      "Epoch 7793/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0346 - val_loss: 0.0458\n",
      "Epoch 7794/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0388 - val_loss: 0.1317\n",
      "Epoch 7795/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0303 - val_loss: 0.0330\n",
      "Epoch 7796/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0291 - val_loss: 0.0132\n",
      "Epoch 7797/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2007 - val_loss: 0.3790\n",
      "Epoch 7798/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8912 - val_loss: 0.1731\n",
      "Epoch 7799/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3018 - val_loss: 0.2944\n",
      "Epoch 7800/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1609 - val_loss: 0.0268\n",
      "Epoch 7801/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0862 - val_loss: 0.0274\n",
      "Epoch 7802/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0351 - val_loss: 0.0522\n",
      "Epoch 7803/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0226 - val_loss: 0.0421\n",
      "Epoch 7804/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0983 - val_loss: 0.4946\n",
      "Epoch 7805/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2841 - val_loss: 0.1266\n",
      "Epoch 7806/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2870 - val_loss: 0.2337\n",
      "Epoch 7807/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3154 - val_loss: 0.4872\n",
      "Epoch 7808/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3253 - val_loss: 0.1966\n",
      "Epoch 7809/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3445 - val_loss: 0.5749\n",
      "Epoch 7810/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.7270 - val_loss: 1.1438\n",
      "Epoch 7811/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5385 - val_loss: 1.0216\n",
      "Epoch 7812/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.6145 - val_loss: 0.2357\n",
      "Epoch 7813/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4030 - val_loss: 0.5052\n",
      "Epoch 7814/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2811 - val_loss: 0.9497\n",
      "Epoch 7815/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.2321 - val_loss: 3.4105\n",
      "Epoch 7816/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 3.1675 - val_loss: 2.0024\n",
      "Epoch 7817/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 4.6282 - val_loss: 7.6754\n",
      "Epoch 7818/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 4.4737 - val_loss: 2.0857\n",
      "Epoch 7819/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.3105 - val_loss: 18.3186\n",
      "Epoch 7820/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 11.8555 - val_loss: 25.6969\n",
      "Epoch 7821/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 76.0949 - val_loss: 436.2978\n",
      "Epoch 7822/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 143.1326 - val_loss: 196.6669\n",
      "Epoch 7823/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 90.6730 - val_loss: 121.3079\n",
      "Epoch 7824/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 236.5039 - val_loss: 248.9721\n",
      "Epoch 7825/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 81.4395 - val_loss: 31.2819\n",
      "Epoch 7826/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 35.1803 - val_loss: 76.6914\n",
      "Epoch 7827/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 55.4197 - val_loss: 36.3895\n",
      "Epoch 7828/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 221us/sample - loss: 70.9484 - val_loss: 30.9964\n",
      "Epoch 7829/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 216.1702 - val_loss: 330.1608\n",
      "Epoch 7830/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 251.4044 - val_loss: 133.5366\n",
      "Epoch 7831/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 454.8686 - val_loss: 2917.5273\n",
      "Epoch 7832/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1328.7618 - val_loss: 194.2650\n",
      "Epoch 7833/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 815.7362 - val_loss: 818.2682\n",
      "Epoch 7834/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 624.9712 - val_loss: 119.0218\n",
      "Epoch 7835/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 242.2641 - val_loss: 337.7443\n",
      "Epoch 7836/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 278.512 - 0s 147us/sample - loss: 324.4026 - val_loss: 545.6950\n",
      "Epoch 7837/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 718.0929 - val_loss: 831.9455\n",
      "Epoch 7838/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 350.6806 - val_loss: 81.3803\n",
      "Epoch 7839/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 31.2535 - val_loss: 3.8445\n",
      "Epoch 7840/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21.1503 - val_loss: 37.9992\n",
      "Epoch 7841/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 23.8988 - val_loss: 18.5912\n",
      "Epoch 7842/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.7863 - val_loss: 0.6554\n",
      "Epoch 7843/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.3749 - val_loss: 14.2910\n",
      "Epoch 7844/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 38.3430 - val_loss: 4.3024\n",
      "Epoch 7845/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11.4465 - val_loss: 24.0775\n",
      "Epoch 7846/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 22.9743 - val_loss: 20.0610\n",
      "Epoch 7847/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.9559 - val_loss: 9.7786\n",
      "Epoch 7848/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 38.8225 - val_loss: 14.3637\n",
      "Epoch 7849/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 117.7790 - val_loss: 6.5851\n",
      "Epoch 7850/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 93.8368 - val_loss: 5.8878\n",
      "Epoch 7851/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 53.5630 - val_loss: 78.0850\n",
      "Epoch 7852/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 108.0616 - val_loss: 851.0423\n",
      "Epoch 7853/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2046.5165 - val_loss: 625.2130\n",
      "Epoch 7854/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 743.7139 - val_loss: 393.3143\n",
      "Epoch 7855/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 302.1480 - val_loss: 36.2316\n",
      "Epoch 7856/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 144.6852 - val_loss: 55.4342\n",
      "Epoch 7857/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 114.9835 - val_loss: 186.7528\n",
      "Epoch 7858/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 156.4441 - val_loss: 458.9634\n",
      "Epoch 7859/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 322.2905 - val_loss: 54.6503\n",
      "Epoch 7860/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 218.3257 - val_loss: 148.2032\n",
      "Epoch 7861/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 243.9258 - val_loss: 576.9205\n",
      "Epoch 7862/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 216.6920 - val_loss: 148.2866\n",
      "Epoch 7863/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 117.5010 - val_loss: 77.2573\n",
      "Epoch 7864/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 80.4056 - val_loss: 95.0396\n",
      "Epoch 7865/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 118.8599 - val_loss: 175.7665\n",
      "Epoch 7866/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 80.4208 - val_loss: 30.1799\n",
      "Epoch 7867/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 32.4291 - val_loss: 68.9959\n",
      "Epoch 7868/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 27.9183 - val_loss: 56.9350\n",
      "Epoch 7869/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 28.1244 - val_loss: 50.1079\n",
      "Epoch 7870/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 41.4966 - val_loss: 101.7636\n",
      "Epoch 7871/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 137.3371 - val_loss: 114.6554\n",
      "Epoch 7872/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 102.0220 - val_loss: 36.3487\n",
      "Epoch 7873/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 84.0614 - val_loss: 155.5225\n",
      "Epoch 7874/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 129.5807 - val_loss: 8.0083\n",
      "Epoch 7875/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 90.7036 - val_loss: 68.4964\n",
      "Epoch 7876/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 252.4545 - val_loss: 820.8756\n",
      "Epoch 7877/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 530.8267 - val_loss: 805.0979\n",
      "Epoch 7878/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 377.3796 - val_loss: 169.6625\n",
      "Epoch 7879/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 260.9147 - val_loss: 215.2261\n",
      "Epoch 7880/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 158.9134 - val_loss: 2.9779\n",
      "Epoch 7881/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 60.6078 - val_loss: 17.9545\n",
      "Epoch 7882/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 62.0021 - val_loss: 30.3841\n",
      "Epoch 7883/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 89.7671 - val_loss: 58.7607\n",
      "Epoch 7884/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 177.4410 - val_loss: 84.1727\n",
      "Epoch 7885/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 147.4877 - val_loss: 78.8760\n",
      "Epoch 7886/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 109.9497 - val_loss: 26.7451\n",
      "Epoch 7887/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 76.1690 - val_loss: 0.5609\n",
      "Epoch 7888/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 45.7551 - val_loss: 4.1177\n",
      "Epoch 7889/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11.5345 - val_loss: 5.4501\n",
      "Epoch 7890/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.5562 - val_loss: 0.2789\n",
      "Epoch 7891/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.1905 - val_loss: 7.9094\n",
      "Epoch 7892/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.2042 - val_loss: 7.2770\n",
      "Epoch 7893/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5.1904 - val_loss: 3.5650\n",
      "Epoch 7894/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.0774 - val_loss: 23.9044\n",
      "Epoch 7895/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.0843 - val_loss: 2.2194\n",
      "Epoch 7896/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.4202 - val_loss: 2.3953\n",
      "Epoch 7897/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.2682 - val_loss: 1.7288\n",
      "Epoch 7898/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.9711 - val_loss: 10.8517\n",
      "Epoch 7899/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.9499 - val_loss: 0.4347\n",
      "Epoch 7900/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4174 - val_loss: 0.0249\n",
      "Epoch 7901/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0810 - val_loss: 0.0396\n",
      "Epoch 7902/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0537 - val_loss: 0.0237\n",
      "Epoch 7903/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1130 - val_loss: 0.0341\n",
      "Epoch 7904/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0563 - val_loss: 0.0573\n",
      "Epoch 7905/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0419 - val_loss: 0.0793\n",
      "Epoch 7906/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0631 - val_loss: 0.0329\n",
      "Epoch 7907/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0180 - val_loss: 0.0988\n",
      "Epoch 7908/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0357 - val_loss: 0.0247\n",
      "Epoch 7909/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0761 - val_loss: 0.3923\n",
      "Epoch 7910/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4802 - val_loss: 8.9920\n",
      "Epoch 7911/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.2617 - val_loss: 0.6068\n",
      "Epoch 7912/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7654 - val_loss: 3.4299\n",
      "Epoch 7913/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.4504 - val_loss: 15.1381\n",
      "Epoch 7914/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.9605 - val_loss: 4.5162\n",
      "Epoch 7915/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 14.5504 - val_loss: 4.0128\n",
      "Epoch 7916/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.4170 - val_loss: 1.0715\n",
      "Epoch 7917/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.6149 - val_loss: 0.0951\n",
      "Epoch 7918/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.4875 - val_loss: 37.0808\n",
      "Epoch 7919/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 29.4848 - val_loss: 119.7434\n",
      "Epoch 7920/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 47.7206 - val_loss: 21.5116\n",
      "Epoch 7921/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 28.8352 - val_loss: 1.8533\n",
      "Epoch 7922/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14.5925 - val_loss: 23.4500\n",
      "Epoch 7923/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11.5107 - val_loss: 5.5674\n",
      "Epoch 7924/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.8862 - val_loss: 5.7444\n",
      "Epoch 7925/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.6887 - val_loss: 11.6846\n",
      "Epoch 7926/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 47.5886 - val_loss: 11.8954\n",
      "Epoch 7927/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.7861 - val_loss: 52.8215\n",
      "Epoch 7928/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 32.6381 - val_loss: 4.4590\n",
      "Epoch 7929/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.3872 - val_loss: 40.9122\n",
      "Epoch 7930/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 21.6251 - val_loss: 8.9017\n",
      "Epoch 7931/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 32.7111 - val_loss: 55.5826\n",
      "Epoch 7932/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 58.9738 - val_loss: 34.6216\n",
      "Epoch 7933/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 137.6582 - val_loss: 63.8142\n",
      "Epoch 7934/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 258.8420 - val_loss: 1762.1809\n",
      "Epoch 7935/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1694.0770 - val_loss: 1127.1445\n",
      "Epoch 7936/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 690.7956 - val_loss: 519.6849\n",
      "Epoch 7937/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 954.3313 - val_loss: 134.9114\n",
      "Epoch 7938/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 658.0372 - val_loss: 508.5391\n",
      "Epoch 7939/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 489.7068 - val_loss: 5.0596\n",
      "Epoch 7940/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 250.6734 - val_loss: 247.3920\n",
      "Epoch 7941/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 390.3332 - val_loss: 59.3783\n",
      "Epoch 7942/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 276.7127 - val_loss: 18.5017\n",
      "Epoch 7943/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 118.2249 - val_loss: 138.9068\n",
      "Epoch 7944/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 66.9308 - val_loss: 87.9788\n",
      "Epoch 7945/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 58.9445 - val_loss: 25.3292\n",
      "Epoch 7946/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 41.1852 - val_loss: 16.3289\n",
      "Epoch 7947/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 31.8978 - val_loss: 34.6382\n",
      "Epoch 7948/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.3916 - val_loss: 21.5671\n",
      "Epoch 7949/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 27.1907 - val_loss: 46.9361\n",
      "Epoch 7950/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 57.3794 - val_loss: 54.9132\n",
      "Epoch 7951/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 164.1835 - val_loss: 270.8917\n",
      "Epoch 7952/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 364.5824 - val_loss: 237.5659\n",
      "Epoch 7953/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 154.2243 - val_loss: 21.0082\n",
      "Epoch 7954/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 71.9411 - val_loss: 170.4268\n",
      "Epoch 7955/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 128.4704 - val_loss: 62.7456\n",
      "Epoch 7956/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 48.0645 - val_loss: 14.5424\n",
      "Epoch 7957/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 77.2940 - val_loss: 125.6768\n",
      "Epoch 7958/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 225.4595 - val_loss: 209.6905\n",
      "Epoch 7959/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 194.1857 - val_loss: 6.0267\n",
      "Epoch 7960/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 61.5653 - val_loss: 62.4844\n",
      "Epoch 7961/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 155.4154 - val_loss: 125.5319\n",
      "Epoch 7962/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 74.4774 - val_loss: 11.9717\n",
      "Epoch 7963/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21.9346 - val_loss: 13.4770\n",
      "Epoch 7964/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.7569 - val_loss: 4.2456\n",
      "Epoch 7965/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.5184 - val_loss: 0.7773\n",
      "Epoch 7966/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.6157 - val_loss: 13.0076\n",
      "Epoch 7967/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.8048 - val_loss: 5.3249\n",
      "Epoch 7968/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 22.4851 - val_loss: 59.0760\n",
      "Epoch 7969/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 46.6405 - val_loss: 29.5429\n",
      "Epoch 7970/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 78.9137 - val_loss: 108.8087\n",
      "Epoch 7971/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 114.1925 - val_loss: 365.2512\n",
      "Epoch 7972/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 111.7041 - val_loss: 484.4619\n",
      "Epoch 7973/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 283.9143 - val_loss: 395.7674\n",
      "Epoch 7974/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 357.9668 - val_loss: 349.2564\n",
      "Epoch 7975/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 571.2295 - val_loss: 35.7206\n",
      "Epoch 7976/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 182.4098 - val_loss: 96.6575\n",
      "Epoch 7977/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 205.7668 - val_loss: 708.2661\n",
      "Epoch 7978/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 549.6301 - val_loss: 1181.6862\n",
      "Epoch 7979/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2169.8491 - val_loss: 198.0202\n",
      "Epoch 7980/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 217.871 - 0s 132us/sample - loss: 858.4237 - val_loss: 2296.7055\n",
      "Epoch 7981/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2488.7468 - val_loss: 3696.5114\n",
      "Epoch 7982/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1782.7251 - val_loss: 30.6392\n",
      "Epoch 7983/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 349.7117 - val_loss: 1316.7634\n",
      "Epoch 7984/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 839.0262 - val_loss: 183.5525\n",
      "Epoch 7985/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1614.0671 - val_loss: 1147.5074\n",
      "Epoch 7986/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2017.4561 - val_loss: 4224.5118\n",
      "Epoch 7987/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1850.6624 - val_loss: 946.0159\n",
      "Epoch 7988/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 568.9241 - val_loss: 118.4722\n",
      "Epoch 7989/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 179.2698 - val_loss: 252.1188\n",
      "Epoch 7990/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 442.0904 - val_loss: 195.1287\n",
      "Epoch 7991/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 917.0244 - val_loss: 910.7055\n",
      "Epoch 7992/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1895.1107 - val_loss: 1598.1138\n",
      "Epoch 7993/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 4578.6008 - val_loss: 2199.6077\n",
      "Epoch 7994/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4012.3970 - val_loss: 1009.6684\n",
      "Epoch 7995/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6152.7075 - val_loss: 5990.6511\n",
      "Epoch 7996/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3830.0024 - val_loss: 36.3745\n",
      "Epoch 7997/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1924.3551 - val_loss: 1296.6927\n",
      "Epoch 7998/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1498.6372 - val_loss: 725.7201\n",
      "Epoch 7999/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 480.9822 - val_loss: 618.3201\n",
      "Epoch 8000/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 415.3346 - val_loss: 497.0031\n",
      "Epoch 8001/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 673.8342 - val_loss: 140.2978\n",
      "Epoch 8002/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 547.6114 - val_loss: 92.2845\n",
      "Epoch 8003/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 537.4545 - val_loss: 634.3278\n",
      "Epoch 8004/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 401.5494 - val_loss: 233.3363\n",
      "Epoch 8005/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 103.9954 - val_loss: 27.6704\n",
      "Epoch 8006/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 37.4610 - val_loss: 15.5558\n",
      "Epoch 8007/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15.5556 - val_loss: 9.4419\n",
      "Epoch 8008/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.2922 - val_loss: 6.2058\n",
      "Epoch 8009/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.2623 - val_loss: 0.7777\n",
      "Epoch 8010/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3445 - val_loss: 0.0999\n",
      "Epoch 8011/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4018 - val_loss: 0.0650\n",
      "Epoch 8012/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2685 - val_loss: 0.0188\n",
      "Epoch 8013/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1785 - val_loss: 0.0442\n",
      "Epoch 8014/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1547 - val_loss: 0.1220\n",
      "Epoch 8015/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1439 - val_loss: 0.0314\n",
      "Epoch 8016/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1080 - val_loss: 0.0207\n",
      "Epoch 8017/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1106 - val_loss: 0.0464\n",
      "Epoch 8018/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1223 - val_loss: 0.1578\n",
      "Epoch 8019/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1463 - val_loss: 0.0306\n",
      "Epoch 8020/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1389 - val_loss: 0.0354\n",
      "Epoch 8021/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1315 - val_loss: 0.0580\n",
      "Epoch 8022/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0927 - val_loss: 0.0227\n",
      "Epoch 8023/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0707 - val_loss: 0.0277\n",
      "Epoch 8024/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0686 - val_loss: 0.0240\n",
      "Epoch 8025/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0518 - val_loss: 0.0189\n",
      "Epoch 8026/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0477 - val_loss: 0.0281\n",
      "Epoch 8027/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0458 - val_loss: 0.0146\n",
      "Epoch 8028/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0414 - val_loss: 0.0231\n",
      "Epoch 8029/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0566 - val_loss: 0.0248\n",
      "Epoch 8030/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0387 - val_loss: 0.0258\n",
      "Epoch 8031/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0417 - val_loss: 0.0170\n",
      "Epoch 8032/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0360 - val_loss: 0.0364\n",
      "Epoch 8033/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0335 - val_loss: 0.0596\n",
      "Epoch 8034/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0342 - val_loss: 0.0160\n",
      "Epoch 8035/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0276 - val_loss: 0.0116\n",
      "Epoch 8036/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0309 - val_loss: 0.0150\n",
      "Epoch 8037/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0222 - val_loss: 0.0115\n",
      "Epoch 8038/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0208 - val_loss: 0.0227\n",
      "Epoch 8039/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0324 - val_loss: 0.0120\n",
      "Epoch 8040/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0196 - val_loss: 0.0155\n",
      "Epoch 8041/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0179 - val_loss: 0.0128\n",
      "Epoch 8042/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0183 - val_loss: 0.0101\n",
      "Epoch 8043/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0143 - val_loss: 0.0150\n",
      "Epoch 8044/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0124 - val_loss: 0.0105\n",
      "Epoch 8045/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0109 - val_loss: 0.0120\n",
      "Epoch 8046/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0114 - val_loss: 0.0113\n",
      "Epoch 8047/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0111 - val_loss: 0.0097\n",
      "Epoch 8048/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.0145 - val_loss: 0.0178\n",
      "Epoch 8049/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0172 - val_loss: 0.0209\n",
      "Epoch 8050/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0157 - val_loss: 0.0163\n",
      "Epoch 8051/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0147 - val_loss: 0.0115\n",
      "Epoch 8052/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0123 - val_loss: 0.0084\n",
      "Epoch 8053/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0107 - val_loss: 0.0229\n",
      "Epoch 8054/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0167 - val_loss: 0.0108\n",
      "Epoch 8055/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0185 - val_loss: 0.0175\n",
      "Epoch 8056/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0169 - val_loss: 0.0222\n",
      "Epoch 8057/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0225 - val_loss: 0.0155\n",
      "Epoch 8058/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0155 - val_loss: 0.0167\n",
      "Epoch 8059/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0125 - val_loss: 0.0105\n",
      "Epoch 8060/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0127 - val_loss: 0.0081\n",
      "Epoch 8061/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0162 - val_loss: 0.0252\n",
      "Epoch 8062/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0171 - val_loss: 0.0085\n",
      "Epoch 8063/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0074 - val_loss: 0.0160\n",
      "Epoch 8064/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0125 - val_loss: 0.0176\n",
      "Epoch 8065/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0109 - val_loss: 0.0240\n",
      "Epoch 8066/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0168 - val_loss: 0.0256\n",
      "Epoch 8067/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0160 - val_loss: 0.0343\n",
      "Epoch 8068/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0540 - val_loss: 0.0118\n",
      "Epoch 8069/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0471 - val_loss: 0.0213\n",
      "Epoch 8070/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0221 - val_loss: 0.0096\n",
      "Epoch 8071/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0244 - val_loss: 0.0262\n",
      "Epoch 8072/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0245 - val_loss: 0.0409\n",
      "Epoch 8073/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0199 - val_loss: 0.0104\n",
      "Epoch 8074/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0170 - val_loss: 0.0095\n",
      "Epoch 8075/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0133 - val_loss: 0.0061\n",
      "Epoch 8076/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0079 - val_loss: 0.0055\n",
      "Epoch 8077/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0063 - val_loss: 0.0176\n",
      "Epoch 8078/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0183 - val_loss: 0.0371\n",
      "Epoch 8079/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0184 - val_loss: 0.0053\n",
      "Epoch 8080/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0095 - val_loss: 0.0072\n",
      "Epoch 8081/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0110 - val_loss: 0.0224\n",
      "Epoch 8082/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0166 - val_loss: 0.0219\n",
      "Epoch 8083/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.032 - 0s 132us/sample - loss: 0.0417 - val_loss: 0.0142\n",
      "Epoch 8084/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0688 - val_loss: 0.0418\n",
      "Epoch 8085/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0222 - val_loss: 0.0174\n",
      "Epoch 8086/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0261 - val_loss: 0.0086\n",
      "Epoch 8087/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0131 - val_loss: 0.0042\n",
      "Epoch 8088/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 8089/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0056 - val_loss: 0.0102\n",
      "Epoch 8090/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0102 - val_loss: 0.0115\n",
      "Epoch 8091/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0380 - val_loss: 0.0219\n",
      "Epoch 8092/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0868 - val_loss: 0.1777\n",
      "Epoch 8093/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0866 - val_loss: 0.1032\n",
      "Epoch 8094/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1609 - val_loss: 0.0188\n",
      "Epoch 8095/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.2493 - val_loss: 0.1626\n",
      "Epoch 8096/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0772 - val_loss: 0.0251\n",
      "Epoch 8097/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0355 - val_loss: 0.0174\n",
      "Epoch 8098/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0293 - val_loss: 0.0795\n",
      "Epoch 8099/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0425 - val_loss: 0.0279\n",
      "Epoch 8100/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0386 - val_loss: 0.0946\n",
      "Epoch 8101/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0622 - val_loss: 0.1451\n",
      "Epoch 8102/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0606 - val_loss: 0.0187\n",
      "Epoch 8103/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0224 - val_loss: 0.0206\n",
      "Epoch 8104/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0174 - val_loss: 0.0149\n",
      "Epoch 8105/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0526 - val_loss: 0.0490\n",
      "Epoch 8106/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0437 - val_loss: 0.0422\n",
      "Epoch 8107/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0763 - val_loss: 0.0217\n",
      "Epoch 8108/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0686 - val_loss: 0.0961\n",
      "Epoch 8109/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0550 - val_loss: 0.0265\n",
      "Epoch 8110/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0294 - val_loss: 0.0272\n",
      "Epoch 8111/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.021 - 0s 147us/sample - loss: 0.0133 - val_loss: 0.0064\n",
      "Epoch 8112/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 8113/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0040 - val_loss: 0.0046\n",
      "Epoch 8114/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0040 - val_loss: 0.0034\n",
      "Epoch 8115/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0043 - val_loss: 0.0100\n",
      "Epoch 8116/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0223 - val_loss: 0.0652\n",
      "Epoch 8117/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0433 - val_loss: 0.0182\n",
      "Epoch 8118/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0116 - val_loss: 0.0069\n",
      "Epoch 8119/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0064 - val_loss: 0.0035\n",
      "Epoch 8120/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0034 - val_loss: 0.0029\n",
      "Epoch 8121/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 8122/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 8123/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0040 - val_loss: 0.0027\n",
      "Epoch 8124/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 8125/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 8126/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0020 - val_loss: 0.0045\n",
      "Epoch 8127/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0025 - val_loss: 0.0021\n",
      "Epoch 8128/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 8129/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0025 - val_loss: 0.0017\n",
      "Epoch 8130/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 8131/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 8132/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0042 - val_loss: 0.0080\n",
      "Epoch 8133/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0050 - val_loss: 0.0019\n",
      "Epoch 8134/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0073 - val_loss: 0.0096\n",
      "Epoch 8135/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0097 - val_loss: 0.0241\n",
      "Epoch 8136/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0168 - val_loss: 0.0065\n",
      "Epoch 8137/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0099 - val_loss: 0.0129\n",
      "Epoch 8138/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0088 - val_loss: 0.0126\n",
      "Epoch 8139/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0078 - val_loss: 0.0115\n",
      "Epoch 8140/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0044 - val_loss: 0.0027\n",
      "Epoch 8141/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0034 - val_loss: 0.0027\n",
      "Epoch 8142/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0036 - val_loss: 0.0015\n",
      "Epoch 8143/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0029 - val_loss: 0.0055\n",
      "Epoch 8144/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 8145/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0028 - val_loss: 0.0018\n",
      "Epoch 8146/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 8147/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 8148/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 0.0028\n",
      "Epoch 8149/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0023 - val_loss: 0.0032\n",
      "Epoch 8150/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0021 - val_loss: 0.0033\n",
      "Epoch 8151/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0033 - val_loss: 0.0021\n",
      "Epoch 8152/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 8153/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0028 - val_loss: 0.0080\n",
      "Epoch 8154/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0131 - val_loss: 0.0261\n",
      "Epoch 8155/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0440 - val_loss: 0.0295\n",
      "Epoch 8156/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0227 - val_loss: 0.0065\n",
      "Epoch 8157/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0397 - val_loss: 0.0420\n",
      "Epoch 8158/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1004 - val_loss: 0.0162\n",
      "Epoch 8159/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4384 - val_loss: 0.4906\n",
      "Epoch 8160/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6694 - val_loss: 0.7321\n",
      "Epoch 8161/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3804 - val_loss: 0.8753\n",
      "Epoch 8162/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9382 - val_loss: 0.6021\n",
      "Epoch 8163/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3953 - val_loss: 0.2352\n",
      "Epoch 8164/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7728 - val_loss: 0.3227\n",
      "Epoch 8165/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3492 - val_loss: 0.0797\n",
      "Epoch 8166/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1325 - val_loss: 0.1870\n",
      "Epoch 8167/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0811 - val_loss: 0.0921\n",
      "Epoch 8168/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0350 - val_loss: 7.0486e-04\n",
      "Epoch 8169/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0173 - val_loss: 0.0126\n",
      "Epoch 8170/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0362 - val_loss: 0.0223\n",
      "Epoch 8171/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0203 - val_loss: 0.0063\n",
      "Epoch 8172/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0059 - val_loss: 6.4165e-04\n",
      "Epoch 8173/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0046 - val_loss: 0.0029\n",
      "Epoch 8174/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0093 - val_loss: 0.0057\n",
      "Epoch 8175/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0071 - val_loss: 0.0048\n",
      "Epoch 8176/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 8177/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 8178/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0050 - val_loss: 0.0033\n",
      "Epoch 8179/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0060 - val_loss: 8.7829e-04\n",
      "Epoch 8180/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0089 - val_loss: 0.0019\n",
      "Epoch 8181/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 8182/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 6.4106e-04\n",
      "Epoch 8183/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0012 - val_loss: 8.8853e-04\n",
      "Epoch 8184/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0013 - val_loss: 5.7484e-04\n",
      "Epoch 8185/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.4184e-04 - val_loss: 9.4698e-04\n",
      "Epoch 8186/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.2970e-04 - val_loss: 0.0012\n",
      "Epoch 8187/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0013 - val_loss: 0.0064\n",
      "Epoch 8188/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0053 - val_loss: 0.0169\n",
      "Epoch 8189/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0179 - val_loss: 0.0030\n",
      "Epoch 8190/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0099 - val_loss: 0.0242\n",
      "Epoch 8191/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0091 - val_loss: 0.0195\n",
      "Epoch 8192/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0185 - val_loss: 0.0197\n",
      "Epoch 8193/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0564 - val_loss: 0.1025\n",
      "Epoch 8194/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0352 - val_loss: 0.0643\n",
      "Epoch 8195/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0735 - val_loss: 5.9992e-04\n",
      "Epoch 8196/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0565 - val_loss: 0.0052\n",
      "Epoch 8197/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0510 - val_loss: 0.0199\n",
      "Epoch 8198/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0148 - val_loss: 0.0199\n",
      "Epoch 8199/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0344 - val_loss: 0.0600\n",
      "Epoch 8200/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0281 - val_loss: 0.0349\n",
      "Epoch 8201/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0582 - val_loss: 0.0421\n",
      "Epoch 8202/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0427 - val_loss: 0.0692\n",
      "Epoch 8203/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1676 - val_loss: 0.2265\n",
      "Epoch 8204/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3217 - val_loss: 0.7527\n",
      "Epoch 8205/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4576 - val_loss: 1.4652\n",
      "Epoch 8206/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6572 - val_loss: 0.2147\n",
      "Epoch 8207/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0108 - val_loss: 3.9104\n",
      "Epoch 8208/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3.4223 - val_loss: 2.1607\n",
      "Epoch 8209/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6983 - val_loss: 0.1704\n",
      "Epoch 8210/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1784 - val_loss: 5.2043\n",
      "Epoch 8211/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.2349 - val_loss: 51.1751\n",
      "Epoch 8212/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 18.7063 - val_loss: 6.9165\n",
      "Epoch 8213/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.7219 - val_loss: 112.1673\n",
      "Epoch 8214/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 165.9503 - val_loss: 165.3237\n",
      "Epoch 8215/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 93.9367 - val_loss: 83.5759\n",
      "Epoch 8216/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 40.4843 - val_loss: 54.4089\n",
      "Epoch 8217/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 18.6681 - val_loss: 4.3907\n",
      "Epoch 8218/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14.1119 - val_loss: 65.7240\n",
      "Epoch 8219/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 68.1680 - val_loss: 0.2280\n",
      "Epoch 8220/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 18.8606 - val_loss: 8.2186\n",
      "Epoch 8221/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.5985 - val_loss: 20.4009\n",
      "Epoch 8222/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 39.6783 - val_loss: 2.7402\n",
      "Epoch 8223/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 35.8648 - val_loss: 59.4200\n",
      "Epoch 8224/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 60.0699 - val_loss: 35.1313\n",
      "Epoch 8225/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 173.0623 - val_loss: 307.6550\n",
      "Epoch 8226/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 468.4129 - val_loss: 335.3799\n",
      "Epoch 8227/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 416.5875 - val_loss: 643.1465\n",
      "Epoch 8228/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1328.4794 - val_loss: 49.5358\n",
      "Epoch 8229/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2775.0262 - val_loss: 552.8380\n",
      "Epoch 8230/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2171.0183 - val_loss: 5163.9666\n",
      "Epoch 8231/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9201.7742 - val_loss: 3926.0304\n",
      "Epoch 8232/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9983.7799 - val_loss: 10207.8559\n",
      "Epoch 8233/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6045.6528 - val_loss: 3739.0123\n",
      "Epoch 8234/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2822.7177 - val_loss: 1431.8678\n",
      "Epoch 8235/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 982.8472 - val_loss: 76.0431\n",
      "Epoch 8236/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1053.9744 - val_loss: 64.0679\n",
      "Epoch 8237/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1134.3871 - val_loss: 176.1035\n",
      "Epoch 8238/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 343.4136 - val_loss: 69.5430\n",
      "Epoch 8239/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 471.7840 - val_loss: 1076.2311\n",
      "Epoch 8240/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1293.4258 - val_loss: 3090.1134\n",
      "Epoch 8241/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1568.1619 - val_loss: 1104.5558\n",
      "Epoch 8242/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 709.0466 - val_loss: 345.6304\n",
      "Epoch 8243/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 251.2627 - val_loss: 64.0410\n",
      "Epoch 8244/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 136.1694 - val_loss: 6.7203\n",
      "Epoch 8245/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 51.9317 - val_loss: 55.6318\n",
      "Epoch 8246/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 30.4755 - val_loss: 46.4714\n",
      "Epoch 8247/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 33.4475 - val_loss: 32.0816\n",
      "Epoch 8248/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 22.0038 - val_loss: 25.0878\n",
      "Epoch 8249/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 21.6605 - val_loss: 26.6113\n",
      "Epoch 8250/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.5758 - val_loss: 9.7129\n",
      "Epoch 8251/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.9345 - val_loss: 0.4407\n",
      "Epoch 8252/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.9557 - val_loss: 2.3422\n",
      "Epoch 8253/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1673 - val_loss: 2.2644\n",
      "Epoch 8254/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8848 - val_loss: 0.4100\n",
      "Epoch 8255/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4959 - val_loss: 0.5170\n",
      "Epoch 8256/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3817 - val_loss: 0.0805\n",
      "Epoch 8257/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3362 - val_loss: 0.0589\n",
      "Epoch 8258/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1766 - val_loss: 0.0725\n",
      "Epoch 8259/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0892 - val_loss: 0.0348\n",
      "Epoch 8260/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0472 - val_loss: 0.0096\n",
      "Epoch 8261/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0335 - val_loss: 0.0043\n",
      "Epoch 8262/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0376 - val_loss: 0.0146\n",
      "Epoch 8263/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0309 - val_loss: 0.0053\n",
      "Epoch 8264/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0258 - val_loss: 0.0045\n",
      "Epoch 8265/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0213 - val_loss: 0.0011\n",
      "Epoch 8266/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0180 - val_loss: 0.0012\n",
      "Epoch 8267/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0170 - val_loss: 0.0014\n",
      "Epoch 8268/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0150 - val_loss: 0.0019\n",
      "Epoch 8269/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0143 - val_loss: 0.0011\n",
      "Epoch 8270/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0111 - val_loss: 0.0014\n",
      "Epoch 8271/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0096 - val_loss: 9.9932e-04\n",
      "Epoch 8272/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0088 - val_loss: 0.0010\n",
      "Epoch 8273/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0077 - val_loss: 0.0042\n",
      "Epoch 8274/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0094 - val_loss: 8.0384e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8275/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0074 - val_loss: 8.8620e-04\n",
      "Epoch 8276/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0058 - val_loss: 0.0050\n",
      "Epoch 8277/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0059 - val_loss: 0.0012\n",
      "Epoch 8278/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0045 - val_loss: 0.0011\n",
      "Epoch 8279/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0045 - val_loss: 0.0020\n",
      "Epoch 8280/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0036 - val_loss: 7.0225e-04\n",
      "Epoch 8281/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0034 - val_loss: 7.3716e-04\n",
      "Epoch 8282/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0030 - val_loss: 8.4675e-04\n",
      "Epoch 8283/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0025 - val_loss: 8.4080e-04\n",
      "Epoch 8284/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 8285/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0022 - val_loss: 6.6839e-04\n",
      "Epoch 8286/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 8287/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0043 - val_loss: 0.0077\n",
      "Epoch 8288/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0054 - val_loss: 0.0035\n",
      "Epoch 8289/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0042 - val_loss: 0.0073\n",
      "Epoch 8290/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0036 - val_loss: 0.0016\n",
      "Epoch 8291/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0020 - val_loss: 7.6716e-04\n",
      "Epoch 8292/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0017 - val_loss: 9.4482e-04\n",
      "Epoch 8293/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 5.4167e-04\n",
      "Epoch 8294/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0021 - val_loss: 8.7665e-04\n",
      "Epoch 8295/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 8296/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 8297/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 8298/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0024 - val_loss: 6.1393e-04\n",
      "Epoch 8299/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0011 - val_loss: 6.8072e-04\n",
      "Epoch 8300/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.3803e-04 - val_loss: 6.5993e-04\n",
      "Epoch 8301/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.1709e-04 - val_loss: 7.0039e-04\n",
      "Epoch 8302/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.1341e-04 - val_loss: 7.6726e-04\n",
      "Epoch 8303/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.8563e-04 - val_loss: 7.9738e-04\n",
      "Epoch 8304/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.7806e-04 - val_loss: 8.7789e-04\n",
      "Epoch 8305/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.2022e-04 - val_loss: 4.4883e-04\n",
      "Epoch 8306/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.8139e-04 - val_loss: 0.0011\n",
      "Epoch 8307/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 6.0211e-0 - 0s 147us/sample - loss: 6.3411e-04 - val_loss: 4.2152e-04\n",
      "Epoch 8308/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.5968e-04 - val_loss: 4.3511e-04\n",
      "Epoch 8309/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4491e-04 - val_loss: 7.1193e-04\n",
      "Epoch 8310/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.2697e-04 - val_loss: 5.2707e-04\n",
      "Epoch 8311/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.9708e-04 - val_loss: 0.0024\n",
      "Epoch 8312/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0020 - val_loss: 0.0052\n",
      "Epoch 8313/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0031 - val_loss: 5.8076e-04\n",
      "Epoch 8314/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 8315/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0033 - val_loss: 0.0028\n",
      "Epoch 8316/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 8317/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0019 - val_loss: 0.0035\n",
      "Epoch 8318/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 8319/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 8320/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 8321/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0011 - val_loss: 3.6216e-04\n",
      "Epoch 8322/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 8323/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.7271e-04 - val_loss: 0.0018\n",
      "Epoch 8324/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0010 - val_loss: 6.7401e-04\n",
      "Epoch 8325/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0013 - val_loss: 0.0024\n",
      "Epoch 8326/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0047 - val_loss: 0.0050\n",
      "Epoch 8327/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 8328/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0029 - val_loss: 0.0031\n",
      "Epoch 8329/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0039 - val_loss: 0.0062\n",
      "Epoch 8330/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0039 - val_loss: 0.0032\n",
      "Epoch 8331/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0033 - val_loss: 0.0037\n",
      "Epoch 8332/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0034 - val_loss: 5.8483e-04\n",
      "Epoch 8333/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 0.0031\n",
      "Epoch 8334/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 8335/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.7048e-04 - val_loss: 0.0022\n",
      "Epoch 8336/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0034 - val_loss: 0.0029\n",
      "Epoch 8337/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0023 - val_loss: 0.0019\n",
      "Epoch 8338/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0116 - val_loss: 0.0022\n",
      "Epoch 8339/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0137 - val_loss: 0.0097\n",
      "Epoch 8340/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0062 - val_loss: 0.0057\n",
      "Epoch 8341/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0033 - val_loss: 0.0024\n",
      "Epoch 8342/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0026 - val_loss: 0.0012\n",
      "Epoch 8343/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 8344/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0017 - val_loss: 1.6473e-04\n",
      "Epoch 8345/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 8.1809e-04 - val_loss: 0.0011\n",
      "Epoch 8346/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.7445e-04 - val_loss: 3.3187e-04\n",
      "Epoch 8347/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.4161e-04 - val_loss: 9.3036e-04\n",
      "Epoch 8348/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.9577e-04 - val_loss: 4.1885e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8349/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.3456e-04 - val_loss: 8.0665e-04\n",
      "Epoch 8350/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.6455e-04 - val_loss: 0.0010\n",
      "Epoch 8351/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 8352/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.5306e-04 - val_loss: 9.3673e-04\n",
      "Epoch 8353/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0013 - val_loss: 8.4536e-04\n",
      "Epoch 8354/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0021 - val_loss: 0.0098\n",
      "Epoch 8355/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0093 - val_loss: 0.0066\n",
      "Epoch 8356/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0074 - val_loss: 0.0109\n",
      "Epoch 8357/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0129 - val_loss: 0.0122\n",
      "Epoch 8358/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0109 - val_loss: 0.0025\n",
      "Epoch 8359/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0112 - val_loss: 0.0190\n",
      "Epoch 8360/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0373 - val_loss: 0.0353\n",
      "Epoch 8361/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0265 - val_loss: 0.0047\n",
      "Epoch 8362/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0413 - val_loss: 0.0261\n",
      "Epoch 8363/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0251 - val_loss: 0.0210\n",
      "Epoch 8364/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0078 - val_loss: 8.9448e-04\n",
      "Epoch 8365/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0027 - val_loss: 7.0477e-04\n",
      "Epoch 8366/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0064 - val_loss: 0.0049\n",
      "Epoch 8367/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0324 - val_loss: 0.0019\n",
      "Epoch 8368/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0284 - val_loss: 0.0239\n",
      "Epoch 8369/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0139 - val_loss: 0.0046\n",
      "Epoch 8370/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0025 - val_loss: 0.0047\n",
      "Epoch 8371/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0035 - val_loss: 0.0067\n",
      "Epoch 8372/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0074 - val_loss: 0.0143\n",
      "Epoch 8373/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0087 - val_loss: 0.0169\n",
      "Epoch 8374/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0126 - val_loss: 0.0257\n",
      "Epoch 8375/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0283 - val_loss: 0.0817\n",
      "Epoch 8376/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0582 - val_loss: 0.0327\n",
      "Epoch 8377/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0507 - val_loss: 0.0413\n",
      "Epoch 8378/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0384 - val_loss: 0.0338\n",
      "Epoch 8379/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0371 - val_loss: 0.0182\n",
      "Epoch 8380/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0142 - val_loss: 5.7473e-04\n",
      "Epoch 8381/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0057 - val_loss: 0.0025\n",
      "Epoch 8382/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0043 - val_loss: 0.0017\n",
      "Epoch 8383/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0065 - val_loss: 0.0248\n",
      "Epoch 8384/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0186 - val_loss: 0.0548\n",
      "Epoch 8385/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1343 - val_loss: 0.4562\n",
      "Epoch 8386/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2091 - val_loss: 0.2934\n",
      "Epoch 8387/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2319 - val_loss: 0.7256\n",
      "Epoch 8388/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3730 - val_loss: 0.8842\n",
      "Epoch 8389/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6636 - val_loss: 0.4431\n",
      "Epoch 8390/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3224 - val_loss: 0.7951\n",
      "Epoch 8391/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4159 - val_loss: 0.3395\n",
      "Epoch 8392/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2870 - val_loss: 0.7875\n",
      "Epoch 8393/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8005 - val_loss: 0.5634\n",
      "Epoch 8394/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5386 - val_loss: 0.2586\n",
      "Epoch 8395/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1037 - val_loss: 0.0073\n",
      "Epoch 8396/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0499 - val_loss: 0.1037\n",
      "Epoch 8397/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1475 - val_loss: 0.0633\n",
      "Epoch 8398/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1200 - val_loss: 0.0673\n",
      "Epoch 8399/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0786 - val_loss: 0.0200\n",
      "Epoch 8400/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1397 - val_loss: 0.2540\n",
      "Epoch 8401/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4692 - val_loss: 0.0048\n",
      "Epoch 8402/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4278 - val_loss: 0.0302\n",
      "Epoch 8403/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3646 - val_loss: 0.2372\n",
      "Epoch 8404/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3476 - val_loss: 0.2520\n",
      "Epoch 8405/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.0431 - val_loss: 1.0795\n",
      "Epoch 8406/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.0438 - val_loss: 4.5291\n",
      "Epoch 8407/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9126 - val_loss: 1.2672\n",
      "Epoch 8408/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9972 - val_loss: 5.8842\n",
      "Epoch 8409/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.8152 - val_loss: 48.3524\n",
      "Epoch 8410/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 24.8125 - val_loss: 3.4647\n",
      "Epoch 8411/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.3059 - val_loss: 37.4272\n",
      "Epoch 8412/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 36.3709 - val_loss: 105.1863\n",
      "Epoch 8413/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 94.6835 - val_loss: 118.0944\n",
      "Epoch 8414/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 125.2787 - val_loss: 36.0976\n",
      "Epoch 8415/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 93.8763 - val_loss: 1.3977\n",
      "Epoch 8416/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 72.1421 - val_loss: 4.4910\n",
      "Epoch 8417/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12.4164 - val_loss: 9.3440\n",
      "Epoch 8418/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.0008 - val_loss: 12.7146\n",
      "Epoch 8419/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.0456 - val_loss: 1.2785\n",
      "Epoch 8420/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.1554 - val_loss: 0.3252\n",
      "Epoch 8421/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.2873 - val_loss: 13.8927\n",
      "Epoch 8422/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 23.2321 - val_loss: 10.8851\n",
      "Epoch 8423/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 31.0446 - val_loss: 42.7210\n",
      "Epoch 8424/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 23.8858 - val_loss: 71.9854\n",
      "Epoch 8425/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 67.2274 - val_loss: 15.6729\n",
      "Epoch 8426/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 63.2016 - val_loss: 6.4455\n",
      "Epoch 8427/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 173.7837 - val_loss: 111.1364\n",
      "Epoch 8428/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 113.6373 - val_loss: 95.2746\n",
      "Epoch 8429/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 84.0251 - val_loss: 7.9580\n",
      "Epoch 8430/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 60.2207 - val_loss: 72.8419\n",
      "Epoch 8431/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 83.7328 - val_loss: 58.1491\n",
      "Epoch 8432/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 65.6506 - val_loss: 77.6630\n",
      "Epoch 8433/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 42.3810 - val_loss: 56.1657\n",
      "Epoch 8434/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 97.2996 - val_loss: 19.8553\n",
      "Epoch 8435/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 68.3793 - val_loss: 437.3917\n",
      "Epoch 8436/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 235.8752 - val_loss: 273.7771\n",
      "Epoch 8437/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 676.5587 - val_loss: 678.4299\n",
      "Epoch 8438/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1276.4890 - val_loss: 252.9822\n",
      "Epoch 8439/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 721.8395 - val_loss: 967.2752\n",
      "Epoch 8440/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1672.5227 - val_loss: 4887.2626\n",
      "Epoch 8441/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3780.4802 - val_loss: 3618.3656\n",
      "Epoch 8442/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1491.3415 - val_loss: 362.7320\n",
      "Epoch 8443/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 989.1110 - val_loss: 2700.6364\n",
      "Epoch 8444/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2958.0009 - val_loss: 3258.2566\n",
      "Epoch 8445/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4292.0567 - val_loss: 4185.9709\n",
      "Epoch 8446/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4638.6626 - val_loss: 1386.9709\n",
      "Epoch 8447/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1369.5948 - val_loss: 1140.9228\n",
      "Epoch 8448/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1021.8929 - val_loss: 930.2890\n",
      "Epoch 8449/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 487.8825 - val_loss: 395.4321\n",
      "Epoch 8450/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 238.1402 - val_loss: 536.8089\n",
      "Epoch 8451/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 354.7386 - val_loss: 1118.7199\n",
      "Epoch 8452/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 498.0444 - val_loss: 289.1733\n",
      "Epoch 8453/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 155.4515 - val_loss: 32.9275\n",
      "Epoch 8454/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 81.6293 - val_loss: 26.5470\n",
      "Epoch 8455/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 42.4068 - val_loss: 16.0079\n",
      "Epoch 8456/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 72.7394 - val_loss: 50.8501\n",
      "Epoch 8457/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 66.6888 - val_loss: 56.7592\n",
      "Epoch 8458/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 37.3629 - val_loss: 27.8656\n",
      "Epoch 8459/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 44.5377 - val_loss: 86.8232\n",
      "Epoch 8460/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 32.6582 - val_loss: 15.6441\n",
      "Epoch 8461/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 15.9954 - val_loss: 2.7932\n",
      "Epoch 8462/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.8749 - val_loss: 8.0864\n",
      "Epoch 8463/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.9827 - val_loss: 5.8600\n",
      "Epoch 8464/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.8650 - val_loss: 5.7821\n",
      "Epoch 8465/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.4763 - val_loss: 1.2953\n",
      "Epoch 8466/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.9089 - val_loss: 0.5416\n",
      "Epoch 8467/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4776 - val_loss: 0.3212\n",
      "Epoch 8468/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4467 - val_loss: 0.0514\n",
      "Epoch 8469/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6732 - val_loss: 1.7582\n",
      "Epoch 8470/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3426 - val_loss: 1.6998\n",
      "Epoch 8471/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3947 - val_loss: 0.9157\n",
      "Epoch 8472/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7321 - val_loss: 0.9044\n",
      "Epoch 8473/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.5844 - val_loss: 8.6226\n",
      "Epoch 8474/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.2729 - val_loss: 0.3490\n",
      "Epoch 8475/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.7282 - val_loss: 3.5047\n",
      "Epoch 8476/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8030 - val_loss: 1.6863\n",
      "Epoch 8477/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1933 - val_loss: 0.0420\n",
      "Epoch 8478/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6923 - val_loss: 1.0189\n",
      "Epoch 8479/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5725 - val_loss: 0.7266\n",
      "Epoch 8480/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.9247 - val_loss: 0.7276\n",
      "Epoch 8481/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8006 - val_loss: 2.3451\n",
      "Epoch 8482/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8329 - val_loss: 0.6842\n",
      "Epoch 8483/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4031 - val_loss: 2.0274\n",
      "Epoch 8484/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1437 - val_loss: 0.3516\n",
      "Epoch 8485/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2333 - val_loss: 1.8122\n",
      "Epoch 8486/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8987 - val_loss: 1.6938\n",
      "Epoch 8487/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9820 - val_loss: 0.9942\n",
      "Epoch 8488/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.9766 - val_loss: 7.5099\n",
      "Epoch 8489/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.6682 - val_loss: 2.6845\n",
      "Epoch 8490/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.5366 - val_loss: 0.3484\n",
      "Epoch 8491/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.5062 - val_loss: 5.7079\n",
      "Epoch 8492/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.4921 - val_loss: 1.9398\n",
      "Epoch 8493/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.4453 - val_loss: 1.4647\n",
      "Epoch 8494/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1129 - val_loss: 2.3850\n",
      "Epoch 8495/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4000 - val_loss: 0.3379\n",
      "Epoch 8496/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4650 - val_loss: 0.6660\n",
      "Epoch 8497/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5243 - val_loss: 0.5466\n",
      "Epoch 8498/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2717 - val_loss: 0.2698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8499/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3552 - val_loss: 0.5627\n",
      "Epoch 8500/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.4235 - val_loss: 0.8814\n",
      "Epoch 8501/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7760 - val_loss: 0.3120\n",
      "Epoch 8502/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6138 - val_loss: 1.5482\n",
      "Epoch 8503/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4813 - val_loss: 1.2957\n",
      "Epoch 8504/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.5871 - val_loss: 4.6421\n",
      "Epoch 8505/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.3343 - val_loss: 8.6447\n",
      "Epoch 8506/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 5.7547 - val_loss: 2.9056\n",
      "Epoch 8507/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6620 - val_loss: 0.4899\n",
      "Epoch 8508/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4122 - val_loss: 0.4833\n",
      "Epoch 8509/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.1832 - val_loss: 1.7142\n",
      "Epoch 8510/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2068 - val_loss: 0.5396\n",
      "Epoch 8511/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4593 - val_loss: 0.3089\n",
      "Epoch 8512/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2474 - val_loss: 0.1857\n",
      "Epoch 8513/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2138 - val_loss: 0.2030\n",
      "Epoch 8514/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3275 - val_loss: 0.6969\n",
      "Epoch 8515/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4880 - val_loss: 0.0391\n",
      "Epoch 8516/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3089 - val_loss: 0.6171\n",
      "Epoch 8517/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4875 - val_loss: 0.1139\n",
      "Epoch 8518/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4497 - val_loss: 0.3833\n",
      "Epoch 8519/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3207 - val_loss: 1.0691\n",
      "Epoch 8520/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1450 - val_loss: 3.8042\n",
      "Epoch 8521/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.4662 - val_loss: 0.5083\n",
      "Epoch 8522/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1765 - val_loss: 1.2992\n",
      "Epoch 8523/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.5862 - val_loss: 7.2099\n",
      "Epoch 8524/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.5152 - val_loss: 1.6424\n",
      "Epoch 8525/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9571 - val_loss: 0.2897\n",
      "Epoch 8526/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5197 - val_loss: 0.1236\n",
      "Epoch 8527/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3647 - val_loss: 0.6006\n",
      "Epoch 8528/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8639 - val_loss: 0.1927\n",
      "Epoch 8529/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2940 - val_loss: 0.5485\n",
      "Epoch 8530/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2603 - val_loss: 0.2069\n",
      "Epoch 8531/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3544 - val_loss: 0.8467\n",
      "Epoch 8532/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5576 - val_loss: 0.1728\n",
      "Epoch 8533/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1267 - val_loss: 0.0300\n",
      "Epoch 8534/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1007 - val_loss: 0.0990\n",
      "Epoch 8535/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0627 - val_loss: 0.0382\n",
      "Epoch 8536/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2190 - val_loss: 0.0073\n",
      "Epoch 8537/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1102 - val_loss: 0.0677\n",
      "Epoch 8538/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0491 - val_loss: 0.0600\n",
      "Epoch 8539/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0579 - val_loss: 0.0531\n",
      "Epoch 8540/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0667 - val_loss: 0.0076\n",
      "Epoch 8541/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0267 - val_loss: 0.0081\n",
      "Epoch 8542/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0067 - val_loss: 0.0107\n",
      "Epoch 8543/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0069 - val_loss: 0.0075\n",
      "Epoch 8544/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0165 - val_loss: 0.0258\n",
      "Epoch 8545/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0449 - val_loss: 0.0513\n",
      "Epoch 8546/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0435 - val_loss: 0.0714\n",
      "Epoch 8547/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0528 - val_loss: 0.0320\n",
      "Epoch 8548/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0220 - val_loss: 0.0435\n",
      "Epoch 8549/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0304 - val_loss: 0.0144\n",
      "Epoch 8550/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0396 - val_loss: 0.0547\n",
      "Epoch 8551/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0689 - val_loss: 0.0775\n",
      "Epoch 8552/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1738 - val_loss: 0.3919\n",
      "Epoch 8553/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2856 - val_loss: 0.4254\n",
      "Epoch 8554/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.1450 - val_loss: 4.3110\n",
      "Epoch 8555/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.0581 - val_loss: 2.9593\n",
      "Epoch 8556/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 31.6120 - val_loss: 38.7354\n",
      "Epoch 8557/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 253.0281 - val_loss: 416.6093\n",
      "Epoch 8558/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 837.6276 - val_loss: 36.8485\n",
      "Epoch 8559/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 652.2566 - val_loss: 166.9445\n",
      "Epoch 8560/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 361.1654 - val_loss: 1032.0071\n",
      "Epoch 8561/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 339.5125 - val_loss: 138.3770\n",
      "Epoch 8562/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 468.9079 - val_loss: 20.2630\n",
      "Epoch 8563/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 294.1786 - val_loss: 103.5250\n",
      "Epoch 8564/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 222.4313 - val_loss: 91.3789\n",
      "Epoch 8565/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 148.4411 - val_loss: 241.5265\n",
      "Epoch 8566/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 162.8252 - val_loss: 61.9336\n",
      "Epoch 8567/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 69.1283 - val_loss: 139.4594\n",
      "Epoch 8568/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 89.8209 - val_loss: 52.4132\n",
      "Epoch 8569/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 33.7581 - val_loss: 7.0867\n",
      "Epoch 8570/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.1676 - val_loss: 4.2593\n",
      "Epoch 8571/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15.7924 - val_loss: 18.7654\n",
      "Epoch 8572/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 20.0084 - val_loss: 25.2284\n",
      "Epoch 8573/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 44.4356 - val_loss: 55.4157\n",
      "Epoch 8574/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 22.0973 - val_loss: 10.5095\n",
      "Epoch 8575/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 13.7761 - val_loss: 1.9865\n",
      "Epoch 8576/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12.2195 - val_loss: 8.0412\n",
      "Epoch 8577/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 14.1173 - val_loss: 10.0886\n",
      "Epoch 8578/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.2229 - val_loss: 11.3525\n",
      "Epoch 8579/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.2404 - val_loss: 18.4813\n",
      "Epoch 8580/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.3664 - val_loss: 2.2826\n",
      "Epoch 8581/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 41.4593 - val_loss: 161.6956\n",
      "Epoch 8582/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 101.1947 - val_loss: 89.6278\n",
      "Epoch 8583/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 160.1676 - val_loss: 60.7687\n",
      "Epoch 8584/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 63.0659 - val_loss: 10.5688\n",
      "Epoch 8585/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 30.2442 - val_loss: 3.5596\n",
      "Epoch 8586/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.4899 - val_loss: 0.7725\n",
      "Epoch 8587/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.2855 - val_loss: 4.5786\n",
      "Epoch 8588/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4416 - val_loss: 1.2628\n",
      "Epoch 8589/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7389 - val_loss: 4.8305\n",
      "Epoch 8590/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.5093 - val_loss: 2.4670\n",
      "Epoch 8591/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1929 - val_loss: 1.1668\n",
      "Epoch 8592/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.3770 - val_loss: 0.5188\n",
      "Epoch 8593/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6730 - val_loss: 2.2766\n",
      "Epoch 8594/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6221 - val_loss: 0.7822\n",
      "Epoch 8595/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.5365 - val_loss: 0.1919\n",
      "Epoch 8596/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1262 - val_loss: 0.0504\n",
      "Epoch 8597/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2279 - val_loss: 0.0548\n",
      "Epoch 8598/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3797 - val_loss: 0.1439\n",
      "Epoch 8599/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6128 - val_loss: 0.2456\n",
      "Epoch 8600/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1475 - val_loss: 0.0530\n",
      "Epoch 8601/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0373 - val_loss: 0.0082\n",
      "Epoch 8602/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0414 - val_loss: 0.0308\n",
      "Epoch 8603/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1190 - val_loss: 0.1799\n",
      "Epoch 8604/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5841 - val_loss: 0.0160\n",
      "Epoch 8605/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7514 - val_loss: 0.6475\n",
      "Epoch 8606/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2563 - val_loss: 9.6166\n",
      "Epoch 8607/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.0042 - val_loss: 1.2756\n",
      "Epoch 8608/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.4680 - val_loss: 10.8130\n",
      "Epoch 8609/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.8142 - val_loss: 0.3926\n",
      "Epoch 8610/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9048 - val_loss: 0.2977\n",
      "Epoch 8611/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3781 - val_loss: 0.4551\n",
      "Epoch 8612/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3760 - val_loss: 0.3376\n",
      "Epoch 8613/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3555 - val_loss: 0.4480\n",
      "Epoch 8614/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5357 - val_loss: 1.6966\n",
      "Epoch 8615/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6205 - val_loss: 1.6447\n",
      "Epoch 8616/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.8044 - val_loss: 0.5114\n",
      "Epoch 8617/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.3290 - val_loss: 0.2021\n",
      "Epoch 8618/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3713 - val_loss: 0.1830\n",
      "Epoch 8619/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1492 - val_loss: 0.0211\n",
      "Epoch 8620/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0336 - val_loss: 0.0335\n",
      "Epoch 8621/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0195 - val_loss: 0.0107\n",
      "Epoch 8622/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0208 - val_loss: 0.0194\n",
      "Epoch 8623/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0302 - val_loss: 0.0157\n",
      "Epoch 8624/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0116 - val_loss: 0.0081\n",
      "Epoch 8625/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0586 - val_loss: 0.0147\n",
      "Epoch 8626/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0952 - val_loss: 0.0897\n",
      "Epoch 8627/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0683 - val_loss: 0.0943\n",
      "Epoch 8628/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1297 - val_loss: 0.2320\n",
      "Epoch 8629/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5979 - val_loss: 0.2001\n",
      "Epoch 8630/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2877 - val_loss: 0.1542\n",
      "Epoch 8631/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7371 - val_loss: 2.0834\n",
      "Epoch 8632/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5796 - val_loss: 0.4878\n",
      "Epoch 8633/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3082 - val_loss: 1.0219\n",
      "Epoch 8634/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.8916 - val_loss: 0.6758\n",
      "Epoch 8635/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15.4076 - val_loss: 18.5990\n",
      "Epoch 8636/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.3023 - val_loss: 10.8315\n",
      "Epoch 8637/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.7028 - val_loss: 1.1646\n",
      "Epoch 8638/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 24.0218 - val_loss: 86.9699\n",
      "Epoch 8639/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 22.3173 - val_loss: 1.2173\n",
      "Epoch 8640/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.5237 - val_loss: 13.3000\n",
      "Epoch 8641/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.9922 - val_loss: 2.8890\n",
      "Epoch 8642/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.1711 - val_loss: 3.6406\n",
      "Epoch 8643/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 22.9272 - val_loss: 104.7045\n",
      "Epoch 8644/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 40.7044 - val_loss: 20.6696\n",
      "Epoch 8645/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.8878 - val_loss: 90.6351\n",
      "Epoch 8646/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 69.7714 - val_loss: 22.5060\n",
      "Epoch 8647/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 56.0248 - val_loss: 59.4079\n",
      "Epoch 8648/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 83.0039 - val_loss: 69.2596\n",
      "Epoch 8649/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 156.6749 - val_loss: 132.1041\n",
      "Epoch 8650/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 261.0739 - val_loss: 60.9511\n",
      "Epoch 8651/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 163.9844 - val_loss: 457.9691\n",
      "Epoch 8652/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 164.0946 - val_loss: 3.9054\n",
      "Epoch 8653/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 22.1897 - val_loss: 65.2370\n",
      "Epoch 8654/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 59.8896 - val_loss: 342.1078\n",
      "Epoch 8655/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 109.8941 - val_loss: 38.7437\n",
      "Epoch 8656/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 30.3743 - val_loss: 86.7047\n",
      "Epoch 8657/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 62.8218 - val_loss: 134.8172\n",
      "Epoch 8658/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 297.4605 - val_loss: 10.0268\n",
      "Epoch 8659/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 141.6269 - val_loss: 2.8079\n",
      "Epoch 8660/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 293.5501 - val_loss: 116.2469\n",
      "Epoch 8661/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 162.3536 - val_loss: 45.7808\n",
      "Epoch 8662/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 196.2950 - val_loss: 161.8536\n",
      "Epoch 8663/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 623.6056 - val_loss: 390.1057\n",
      "Epoch 8664/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 704.7640 - val_loss: 1525.9348\n",
      "Epoch 8665/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1678.9797 - val_loss: 845.4946\n",
      "Epoch 8666/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 430.8633 - val_loss: 728.5920\n",
      "Epoch 8667/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 775.9175 - val_loss: 2201.5952\n",
      "Epoch 8668/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1717.6064 - val_loss: 1379.5885\n",
      "Epoch 8669/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 574.1602 - val_loss: 46.7339\n",
      "Epoch 8670/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 86.7960 - val_loss: 158.6462\n",
      "Epoch 8671/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 198.0894 - val_loss: 55.8821\n",
      "Epoch 8672/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 145.9610 - val_loss: 27.6286\n",
      "Epoch 8673/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 74.5449 - val_loss: 39.4556\n",
      "Epoch 8674/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.7294 - val_loss: 18.4941\n",
      "Epoch 8675/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 21.3885 - val_loss: 15.4374\n",
      "Epoch 8676/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 29.4016 - val_loss: 0.3930\n",
      "Epoch 8677/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 18.7546 - val_loss: 0.1464\n",
      "Epoch 8678/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.3571 - val_loss: 1.6395\n",
      "Epoch 8679/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1150 - val_loss: 6.6975\n",
      "Epoch 8680/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.4692 - val_loss: 5.0117\n",
      "Epoch 8681/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.4125 - val_loss: 5.8069\n",
      "Epoch 8682/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.1514 - val_loss: 1.2888\n",
      "Epoch 8683/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3366 - val_loss: 0.0686\n",
      "Epoch 8684/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8909 - val_loss: 0.4086\n",
      "Epoch 8685/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8459 - val_loss: 2.3633\n",
      "Epoch 8686/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5673 - val_loss: 0.8193\n",
      "Epoch 8687/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1677 - val_loss: 3.4928\n",
      "Epoch 8688/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4047 - val_loss: 1.2178\n",
      "Epoch 8689/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7596 - val_loss: 1.8505\n",
      "Epoch 8690/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3918 - val_loss: 1.6811\n",
      "Epoch 8691/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8138 - val_loss: 0.8101\n",
      "Epoch 8692/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4029 - val_loss: 1.6175\n",
      "Epoch 8693/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5922 - val_loss: 1.1481\n",
      "Epoch 8694/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3950 - val_loss: 0.2522\n",
      "Epoch 8695/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2908 - val_loss: 0.0597\n",
      "Epoch 8696/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0519 - val_loss: 0.0344\n",
      "Epoch 8697/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0182 - val_loss: 0.0147\n",
      "Epoch 8698/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0331 - val_loss: 0.0453\n",
      "Epoch 8699/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1098 - val_loss: 0.1043\n",
      "Epoch 8700/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0957 - val_loss: 0.0877\n",
      "Epoch 8701/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0415 - val_loss: 0.0522\n",
      "Epoch 8702/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0815 - val_loss: 0.1972\n",
      "Epoch 8703/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1601 - val_loss: 0.0922\n",
      "Epoch 8704/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1976 - val_loss: 0.0983\n",
      "Epoch 8705/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2525 - val_loss: 0.0123\n",
      "Epoch 8706/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1301 - val_loss: 0.2587\n",
      "Epoch 8707/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3078 - val_loss: 0.2144\n",
      "Epoch 8708/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5427 - val_loss: 0.0039\n",
      "Epoch 8709/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2130 - val_loss: 0.1154\n",
      "Epoch 8710/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1088 - val_loss: 0.4488\n",
      "Epoch 8711/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3914 - val_loss: 0.0910\n",
      "Epoch 8712/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5905 - val_loss: 0.4577\n",
      "Epoch 8713/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6984 - val_loss: 0.7778\n",
      "Epoch 8714/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1134 - val_loss: 0.2824\n",
      "Epoch 8715/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4772 - val_loss: 4.7042\n",
      "Epoch 8716/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.9612 - val_loss: 16.2295\n",
      "Epoch 8717/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 216.8217 - val_loss: 147.3984\n",
      "Epoch 8718/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 200.8669 - val_loss: 97.6268\n",
      "Epoch 8719/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 108.4691 - val_loss: 63.3923\n",
      "Epoch 8720/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 47.6739 - val_loss: 102.3779\n",
      "Epoch 8721/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 133.7346 - val_loss: 2.9975\n",
      "Epoch 8722/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 89.9186 - val_loss: 127.3147\n",
      "Epoch 8723/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 101.0417 - val_loss: 151.2156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8724/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 105.9315 - val_loss: 173.5428\n",
      "Epoch 8725/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 187.8204 - val_loss: 1.5753\n",
      "Epoch 8726/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 365.3019 - val_loss: 297.5735\n",
      "Epoch 8727/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 164.7400 - val_loss: 51.2155\n",
      "Epoch 8728/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 47.6799 - val_loss: 8.3761\n",
      "Epoch 8729/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.8780 - val_loss: 1.5907\n",
      "Epoch 8730/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1814 - val_loss: 0.3908\n",
      "Epoch 8731/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.7267 - val_loss: 5.4110\n",
      "Epoch 8732/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.9850 - val_loss: 10.6328\n",
      "Epoch 8733/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.2478 - val_loss: 3.8082\n",
      "Epoch 8734/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 17.1977 - val_loss: 20.1667\n",
      "Epoch 8735/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.5175 - val_loss: 10.3149\n",
      "Epoch 8736/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.8451 - val_loss: 3.7666\n",
      "Epoch 8737/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.3567 - val_loss: 0.3191\n",
      "Epoch 8738/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.0111 - val_loss: 8.2642\n",
      "Epoch 8739/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.6430 - val_loss: 13.6166\n",
      "Epoch 8740/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.5601 - val_loss: 2.4423\n",
      "Epoch 8741/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 15.0932 - val_loss: 58.7630\n",
      "Epoch 8742/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 49.3814 - val_loss: 32.7090\n",
      "Epoch 8743/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 144.6066 - val_loss: 265.7915\n",
      "Epoch 8744/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 114.7327 - val_loss: 455.5603\n",
      "Epoch 8745/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 266.7446 - val_loss: 211.8434\n",
      "Epoch 8746/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 318.1920 - val_loss: 802.0613\n",
      "Epoch 8747/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 900.9824 - val_loss: 2479.7893\n",
      "Epoch 8748/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1567.9436 - val_loss: 3.8762\n",
      "Epoch 8749/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 261.2449 - val_loss: 77.2842\n",
      "Epoch 8750/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 360.4469 - val_loss: 939.2853\n",
      "Epoch 8751/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 720.6743 - val_loss: 2487.2304\n",
      "Epoch 8752/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 816.5692 - val_loss: 251.5825\n",
      "Epoch 8753/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 355.8776 - val_loss: 1062.3207\n",
      "Epoch 8754/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 701.3297 - val_loss: 2052.5959\n",
      "Epoch 8755/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6041.4790 - val_loss: 19272.2385\n",
      "Epoch 8756/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14460.4598 - val_loss: 6772.0169\n",
      "Epoch 8757/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17087.0169 - val_loss: 192.1041\n",
      "Epoch 8758/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11792.2308 - val_loss: 17352.6468\n",
      "Epoch 8759/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13288.2782 - val_loss: 4754.4269\n",
      "Epoch 8760/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7538.7710 - val_loss: 23702.3207\n",
      "Epoch 8761/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 11792.6241 - val_loss: 4375.5460\n",
      "Epoch 8762/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7196.1574 - val_loss: 4267.4990\n",
      "Epoch 8763/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5609.9807 - val_loss: 3140.9328\n",
      "Epoch 8764/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2315.4318 - val_loss: 504.4075\n",
      "Epoch 8765/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1458.2196 - val_loss: 2221.9165\n",
      "Epoch 8766/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 881.9811 - val_loss: 224.8755\n",
      "Epoch 8767/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 376.2400 - val_loss: 409.0140\n",
      "Epoch 8768/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 171.3445 - val_loss: 173.8371\n",
      "Epoch 8769/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 101.7085 - val_loss: 14.2766\n",
      "Epoch 8770/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 35.0038 - val_loss: 1.5566\n",
      "Epoch 8771/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 14.3167 - val_loss: 4.6250\n",
      "Epoch 8772/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.9063 - val_loss: 9.0688\n",
      "Epoch 8773/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.9360 - val_loss: 7.5914\n",
      "Epoch 8774/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.5060 - val_loss: 5.7561\n",
      "Epoch 8775/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.3568 - val_loss: 0.8474\n",
      "Epoch 8776/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.9739 - val_loss: 1.7491\n",
      "Epoch 8777/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.7508 - val_loss: 0.4617\n",
      "Epoch 8778/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4242 - val_loss: 0.2388\n",
      "Epoch 8779/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8761 - val_loss: 0.1141\n",
      "Epoch 8780/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7296 - val_loss: 0.2210\n",
      "Epoch 8781/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6823 - val_loss: 0.0788\n",
      "Epoch 8782/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6563 - val_loss: 0.0524\n",
      "Epoch 8783/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5266 - val_loss: 0.0307\n",
      "Epoch 8784/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4933 - val_loss: 0.0336\n",
      "Epoch 8785/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.4805 - val_loss: 0.0605\n",
      "Epoch 8786/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4422 - val_loss: 0.1194\n",
      "Epoch 8787/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5023 - val_loss: 0.0368\n",
      "Epoch 8788/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3952 - val_loss: 0.1327\n",
      "Epoch 8789/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3810 - val_loss: 0.0287\n",
      "Epoch 8790/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3427 - val_loss: 0.1191\n",
      "Epoch 8791/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3224 - val_loss: 0.0654\n",
      "Epoch 8792/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3307 - val_loss: 0.0658\n",
      "Epoch 8793/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2873 - val_loss: 0.0373\n",
      "Epoch 8794/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2084 - val_loss: 0.0355\n",
      "Epoch 8795/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1979 - val_loss: 0.0250\n",
      "Epoch 8796/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1649 - val_loss: 0.0087\n",
      "Epoch 8797/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1476 - val_loss: 0.0287\n",
      "Epoch 8798/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1573 - val_loss: 0.0239\n",
      "Epoch 8799/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1469 - val_loss: 0.1091\n",
      "Epoch 8800/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1611 - val_loss: 0.0086\n",
      "Epoch 8801/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1437 - val_loss: 0.0223\n",
      "Epoch 8802/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1230 - val_loss: 0.0211\n",
      "Epoch 8803/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1323 - val_loss: 0.1784\n",
      "Epoch 8804/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1503 - val_loss: 0.0158\n",
      "Epoch 8805/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1131 - val_loss: 0.0307\n",
      "Epoch 8806/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0907 - val_loss: 0.0137\n",
      "Epoch 8807/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0639 - val_loss: 0.0083\n",
      "Epoch 8808/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0609 - val_loss: 0.0086\n",
      "Epoch 8809/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0542 - val_loss: 0.0081\n",
      "Epoch 8810/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0491 - val_loss: 0.0090\n",
      "Epoch 8811/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0475 - val_loss: 0.0067\n",
      "Epoch 8812/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0409 - val_loss: 0.0054\n",
      "Epoch 8813/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0376 - val_loss: 0.0064\n",
      "Epoch 8814/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0358 - val_loss: 0.0072\n",
      "Epoch 8815/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0376 - val_loss: 0.0161\n",
      "Epoch 8816/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0328 - val_loss: 0.0061\n",
      "Epoch 8817/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0284 - val_loss: 0.0090\n",
      "Epoch 8818/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0243 - val_loss: 0.0058\n",
      "Epoch 8819/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0251 - val_loss: 0.0063\n",
      "Epoch 8820/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0206 - val_loss: 0.0047\n",
      "Epoch 8821/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0182 - val_loss: 0.0087\n",
      "Epoch 8822/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0198 - val_loss: 0.0047\n",
      "Epoch 8823/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0172 - val_loss: 0.0200\n",
      "Epoch 8824/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0201 - val_loss: 0.0045\n",
      "Epoch 8825/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0146 - val_loss: 0.0060\n",
      "Epoch 8826/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0148 - val_loss: 0.0133\n",
      "Epoch 8827/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0147 - val_loss: 0.0040\n",
      "Epoch 8828/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0124 - val_loss: 0.0042\n",
      "Epoch 8829/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0103 - val_loss: 0.0040\n",
      "Epoch 8830/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0091 - val_loss: 0.0043\n",
      "Epoch 8831/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0098 - val_loss: 0.0042\n",
      "Epoch 8832/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0083 - val_loss: 0.0053\n",
      "Epoch 8833/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0081 - val_loss: 0.0049\n",
      "Epoch 8834/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0088 - val_loss: 0.0066\n",
      "Epoch 8835/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0098 - val_loss: 0.0090\n",
      "Epoch 8836/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0119 - val_loss: 0.0051\n",
      "Epoch 8837/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0081 - val_loss: 0.0043\n",
      "Epoch 8838/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0058 - val_loss: 0.0040\n",
      "Epoch 8839/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0056 - val_loss: 0.0044\n",
      "Epoch 8840/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0072 - val_loss: 0.0045\n",
      "Epoch 8841/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0067 - val_loss: 0.0040\n",
      "Epoch 8842/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0054 - val_loss: 0.0035\n",
      "Epoch 8843/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0055 - val_loss: 0.0121\n",
      "Epoch 8844/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0173 - val_loss: 0.0031\n",
      "Epoch 8845/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0156 - val_loss: 0.0120\n",
      "Epoch 8846/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0104 - val_loss: 0.0089\n",
      "Epoch 8847/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0106 - val_loss: 0.0052\n",
      "Epoch 8848/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0085 - val_loss: 0.0151\n",
      "Epoch 8849/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0093 - val_loss: 0.0046\n",
      "Epoch 8850/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0082 - val_loss: 0.0051\n",
      "Epoch 8851/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0063 - val_loss: 0.0045\n",
      "Epoch 8852/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0047 - val_loss: 0.0028\n",
      "Epoch 8853/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0031 - val_loss: 0.0027\n",
      "Epoch 8854/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0049 - val_loss: 0.0028\n",
      "Epoch 8855/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0049 - val_loss: 0.0048\n",
      "Epoch 8856/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0057 - val_loss: 0.0092\n",
      "Epoch 8857/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0093 - val_loss: 0.0115\n",
      "Epoch 8858/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0052 - val_loss: 0.0025\n",
      "Epoch 8859/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 8860/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0036 - val_loss: 0.0054\n",
      "Epoch 8861/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0035 - val_loss: 0.0075\n",
      "Epoch 8862/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0044 - val_loss: 0.0025\n",
      "Epoch 8863/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0026 - val_loss: 0.0038\n",
      "Epoch 8864/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0048 - val_loss: 0.0049\n",
      "Epoch 8865/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0051 - val_loss: 0.0043\n",
      "Epoch 8866/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0040 - val_loss: 0.0028\n",
      "Epoch 8867/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 8868/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0039 - val_loss: 0.0026\n",
      "Epoch 8869/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0027 - val_loss: 0.0021\n",
      "Epoch 8870/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 8871/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 8872/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0028 - val_loss: 0.0020\n",
      "Epoch 8873/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0020 - val_loss: 0.0028\n",
      "Epoch 8874/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0024 - val_loss: 0.0038\n",
      "Epoch 8875/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0023 - val_loss: 0.0019\n",
      "Epoch 8876/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0019 - val_loss: 0.0024\n",
      "Epoch 8877/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 8878/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.003 - 0s 176us/sample - loss: 0.0031 - val_loss: 0.0020\n",
      "Epoch 8879/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0035 - val_loss: 0.0021\n",
      "Epoch 8880/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0037 - val_loss: 0.0019\n",
      "Epoch 8881/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 8882/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0031 - val_loss: 0.0027\n",
      "Epoch 8883/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0030 - val_loss: 0.0026\n",
      "Epoch 8884/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0036 - val_loss: 0.0017\n",
      "Epoch 8885/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 8886/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0029 - val_loss: 0.0021\n",
      "Epoch 8887/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0043 - val_loss: 0.0047\n",
      "Epoch 8888/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0085 - val_loss: 0.0121\n",
      "Epoch 8889/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0063 - val_loss: 0.0057\n",
      "Epoch 8890/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 8891/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0050 - val_loss: 0.0074\n",
      "Epoch 8892/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0063 - val_loss: 0.0154\n",
      "Epoch 8893/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0064 - val_loss: 0.0046\n",
      "Epoch 8894/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0030 - val_loss: 0.0038\n",
      "Epoch 8895/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0026 - val_loss: 0.0056\n",
      "Epoch 8896/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0059 - val_loss: 0.0087\n",
      "Epoch 8897/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0043 - val_loss: 0.0061\n",
      "Epoch 8898/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0037 - val_loss: 0.0032\n",
      "Epoch 8899/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0024 - val_loss: 0.0014\n",
      "Epoch 8900/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 8901/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 8902/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0020 - val_loss: 0.0031\n",
      "Epoch 8903/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0028 - val_loss: 0.0046\n",
      "Epoch 8904/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0035 - val_loss: 0.0079\n",
      "Epoch 8905/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0060 - val_loss: 0.0069\n",
      "Epoch 8906/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0051 - val_loss: 0.0041\n",
      "Epoch 8907/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.004 - 0s 132us/sample - loss: 0.0030 - val_loss: 0.0047\n",
      "Epoch 8908/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0025 - val_loss: 0.0053\n",
      "Epoch 8909/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0044 - val_loss: 0.0064\n",
      "Epoch 8910/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0037 - val_loss: 0.0126\n",
      "Epoch 8911/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0080 - val_loss: 0.0068\n",
      "Epoch 8912/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0046 - val_loss: 0.0109\n",
      "Epoch 8913/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0074 - val_loss: 0.0108\n",
      "Epoch 8914/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0051 - val_loss: 0.0046\n",
      "Epoch 8915/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0038 - val_loss: 0.0018\n",
      "Epoch 8916/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0020 - val_loss: 9.6954e-04\n",
      "Epoch 8917/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0025 - val_loss: 0.0053\n",
      "Epoch 8918/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.005 - 0s 147us/sample - loss: 0.0027 - val_loss: 9.3713e-04\n",
      "Epoch 8919/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0014 - val_loss: 8.4064e-04\n",
      "Epoch 8920/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 0.0037\n",
      "Epoch 8921/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 8922/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0063 - val_loss: 0.0019\n",
      "Epoch 8923/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0040 - val_loss: 0.0028\n",
      "Epoch 8924/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 8925/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0021 - val_loss: 0.0036\n",
      "Epoch 8926/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 8927/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 8928/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 8929/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0022 - val_loss: 0.0019\n",
      "Epoch 8930/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0019 - val_loss: 6.9299e-04\n",
      "Epoch 8931/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 8932/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 8933/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 8934/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 8935/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 8936/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 8937/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0021 - val_loss: 0.0061\n",
      "Epoch 8938/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0110 - val_loss: 0.0050\n",
      "Epoch 8939/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0123 - val_loss: 5.8707e-04\n",
      "Epoch 8940/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0070 - val_loss: 0.0044\n",
      "Epoch 8941/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0029 - val_loss: 9.6914e-04\n",
      "Epoch 8942/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.3114e-04 - val_loss: 0.0012\n",
      "Epoch 8943/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.2848e-04 - val_loss: 0.0011\n",
      "Epoch 8944/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.5689e-04 - val_loss: 8.9177e-04\n",
      "Epoch 8945/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 7.1854e-04\n",
      "Epoch 8946/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.0222e-04 - val_loss: 5.2362e-04\n",
      "Epoch 8947/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.9902e-04 - val_loss: 8.2927e-04\n",
      "Epoch 8948/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.8540e-04 - val_loss: 4.5014e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8949/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.1168e-04 - val_loss: 5.7575e-04\n",
      "Epoch 8950/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.3434e-04 - val_loss: 6.2484e-04\n",
      "Epoch 8951/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 8952/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.7718e-04 - val_loss: 4.3759e-04\n",
      "Epoch 8953/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.6949e-04 - val_loss: 3.9828e-04\n",
      "Epoch 8954/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4321e-04 - val_loss: 4.1443e-04\n",
      "Epoch 8955/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.9572e-04 - val_loss: 0.0013\n",
      "Epoch 8956/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 8957/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 8958/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0019 - val_loss: 0.0028\n",
      "Epoch 8959/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 8960/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0015 - val_loss: 8.5685e-04\n",
      "Epoch 8961/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 8962/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0012 - val_loss: 3.1011e-04\n",
      "Epoch 8963/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.8042e-04 - val_loss: 4.9198e-04\n",
      "Epoch 8964/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3.4963e-04 - val_loss: 3.1658e-04\n",
      "Epoch 8965/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.1812e-04 - val_loss: 0.0012\n",
      "Epoch 8966/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.3902e-04 - val_loss: 0.0014\n",
      "Epoch 8967/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.3144e-04 - val_loss: 4.5515e-04\n",
      "Epoch 8968/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.3495e-04 - val_loss: 0.0022\n",
      "Epoch 8969/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0012 - val_loss: 0.0023\n",
      "Epoch 8970/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 8971/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 8972/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0022 - val_loss: 0.0045\n",
      "Epoch 8973/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0029 - val_loss: 7.1075e-04\n",
      "Epoch 8974/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0021 - val_loss: 0.0050\n",
      "Epoch 8975/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0028 - val_loss: 9.3030e-04\n",
      "Epoch 8976/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 8977/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0021 - val_loss: 4.8229e-04\n",
      "Epoch 8978/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.9000e-04 - val_loss: 4.3925e-04\n",
      "Epoch 8979/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.0130e-04 - val_loss: 9.7960e-04\n",
      "Epoch 8980/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 8981/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 3.3014e-04\n",
      "Epoch 8982/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.7635e-04 - val_loss: 4.7779e-04\n",
      "Epoch 8983/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.8351e-04 - val_loss: 2.1031e-04\n",
      "Epoch 8984/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.9557e-04 - val_loss: 0.0023\n",
      "Epoch 8985/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0013 - val_loss: 4.0675e-04\n",
      "Epoch 8986/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.5173e-04 - val_loss: 2.1861e-04\n",
      "Epoch 8987/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.1248e-04 - val_loss: 3.7380e-04\n",
      "Epoch 8988/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.0437e-04 - val_loss: 0.0010\n",
      "Epoch 8989/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.5500e-04 - val_loss: 0.0018\n",
      "Epoch 8990/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0011 - val_loss: 5.7899e-04\n",
      "Epoch 8991/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.5312e-04 - val_loss: 2.5861e-04\n",
      "Epoch 8992/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8820e-04 - val_loss: 7.3270e-04\n",
      "Epoch 8993/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0024 - val_loss: 0.0058\n",
      "Epoch 8994/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0026 - val_loss: 2.4557e-04\n",
      "Epoch 8995/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0027 - val_loss: 0.0068\n",
      "Epoch 8996/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0035 - val_loss: 0.0057\n",
      "Epoch 8997/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0122 - val_loss: 0.0051\n",
      "Epoch 8998/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0324 - val_loss: 0.0089\n",
      "Epoch 8999/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0159 - val_loss: 0.0012\n",
      "Epoch 9000/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0072 - val_loss: 0.0024\n",
      "Epoch 9001/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0041 - val_loss: 0.0053\n",
      "Epoch 9002/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0044 - val_loss: 4.0183e-04\n",
      "Epoch 9003/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0017 - val_loss: 0.0056\n",
      "Epoch 9004/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0057 - val_loss: 0.0075\n",
      "Epoch 9005/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0056 - val_loss: 0.0134\n",
      "Epoch 9006/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0220 - val_loss: 0.0131\n",
      "Epoch 9007/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0634 - val_loss: 0.1416\n",
      "Epoch 9008/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0775 - val_loss: 0.0939\n",
      "Epoch 9009/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0819 - val_loss: 0.1075\n",
      "Epoch 9010/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0612 - val_loss: 0.0981\n",
      "Epoch 9011/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0626 - val_loss: 0.0021\n",
      "Epoch 9012/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1128 - val_loss: 0.0614\n",
      "Epoch 9013/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0962 - val_loss: 0.0345\n",
      "Epoch 9014/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1195 - val_loss: 0.2001\n",
      "Epoch 9015/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2544 - val_loss: 0.0260\n",
      "Epoch 9016/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2882 - val_loss: 0.2542\n",
      "Epoch 9017/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1717 - val_loss: 0.1323\n",
      "Epoch 9018/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1257 - val_loss: 0.0859\n",
      "Epoch 9019/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0728 - val_loss: 0.0562\n",
      "Epoch 9020/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0258 - val_loss: 0.0064\n",
      "Epoch 9021/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0123 - val_loss: 0.0172\n",
      "Epoch 9022/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0195 - val_loss: 0.0359\n",
      "Epoch 9023/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0272 - val_loss: 0.0332\n",
      "Epoch 9024/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0209 - val_loss: 0.0386\n",
      "Epoch 9025/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0356 - val_loss: 0.0561\n",
      "Epoch 9026/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0189 - val_loss: 0.0093\n",
      "Epoch 9027/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0193 - val_loss: 0.0345\n",
      "Epoch 9028/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0799 - val_loss: 0.1740\n",
      "Epoch 9029/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3663 - val_loss: 0.0698\n",
      "Epoch 9030/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3326 - val_loss: 0.4525\n",
      "Epoch 9031/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5653 - val_loss: 0.5305\n",
      "Epoch 9032/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2283 - val_loss: 0.6322\n",
      "Epoch 9033/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6528 - val_loss: 0.7156\n",
      "Epoch 9034/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3065 - val_loss: 0.2017\n",
      "Epoch 9035/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1422 - val_loss: 0.5971\n",
      "Epoch 9036/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0535 - val_loss: 0.2862\n",
      "Epoch 9037/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3049 - val_loss: 0.4516\n",
      "Epoch 9038/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4591 - val_loss: 0.2128\n",
      "Epoch 9039/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.1343 - val_loss: 0.3501\n",
      "Epoch 9040/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4947 - val_loss: 0.8762\n",
      "Epoch 9041/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6846 - val_loss: 0.7655\n",
      "Epoch 9042/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5320 - val_loss: 0.0856\n",
      "Epoch 9043/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1179 - val_loss: 0.1889\n",
      "Epoch 9044/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1775 - val_loss: 0.4697\n",
      "Epoch 9045/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4959 - val_loss: 0.4102\n",
      "Epoch 9046/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4140 - val_loss: 2.7975\n",
      "Epoch 9047/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8460 - val_loss: 0.2398\n",
      "Epoch 9048/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3728 - val_loss: 0.0860\n",
      "Epoch 9049/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0455 - val_loss: 0.0901\n",
      "Epoch 9050/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7133 - val_loss: 0.1205\n",
      "Epoch 9051/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7497 - val_loss: 1.1989\n",
      "Epoch 9052/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4457 - val_loss: 0.2494\n",
      "Epoch 9053/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2773 - val_loss: 0.0315\n",
      "Epoch 9054/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1575 - val_loss: 0.1932\n",
      "Epoch 9055/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1262 - val_loss: 0.0608\n",
      "Epoch 9056/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1574 - val_loss: 0.2176\n",
      "Epoch 9057/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3337 - val_loss: 0.1125\n",
      "Epoch 9058/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1441 - val_loss: 0.0249\n",
      "Epoch 9059/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1916 - val_loss: 0.0260\n",
      "Epoch 9060/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1160 - val_loss: 1.0078\n",
      "Epoch 9061/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6601 - val_loss: 11.3295\n",
      "Epoch 9062/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14.9373 - val_loss: 19.6922\n",
      "Epoch 9063/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 28.9762 - val_loss: 17.0476\n",
      "Epoch 9064/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26.3930 - val_loss: 36.0112\n",
      "Epoch 9065/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 26.2348 - val_loss: 11.4821\n",
      "Epoch 9066/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 167.1536 - val_loss: 11.8671\n",
      "Epoch 9067/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 72.6864 - val_loss: 100.1206\n",
      "Epoch 9068/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1019.6674 - val_loss: 1165.1399\n",
      "Epoch 9069/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 850.2667 - val_loss: 2400.4565\n",
      "Epoch 9070/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1948.0195 - val_loss: 2741.1926\n",
      "Epoch 9071/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2120.2623 - val_loss: 867.9495\n",
      "Epoch 9072/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 308.5336 - val_loss: 322.3770\n",
      "Epoch 9073/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 271.6572 - val_loss: 122.3683\n",
      "Epoch 9074/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 169.3660 - val_loss: 316.1510\n",
      "Epoch 9075/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 386.5658 - val_loss: 185.8737\n",
      "Epoch 9076/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 151.2154 - val_loss: 53.3546\n",
      "Epoch 9077/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 79.8336 - val_loss: 20.3938\n",
      "Epoch 9078/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 89.3646 - val_loss: 349.6684\n",
      "Epoch 9079/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 268.4877 - val_loss: 153.1561\n",
      "Epoch 9080/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 107.4140 - val_loss: 167.4314\n",
      "Epoch 9081/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 299.3415 - val_loss: 557.0853\n",
      "Epoch 9082/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 579.1633 - val_loss: 292.9452\n",
      "Epoch 9083/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 308.0026 - val_loss: 359.3028\n",
      "Epoch 9084/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 194.4578 - val_loss: 191.5709\n",
      "Epoch 9085/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 81.5392 - val_loss: 72.1108\n",
      "Epoch 9086/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 41.7464 - val_loss: 24.1728\n",
      "Epoch 9087/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.9557 - val_loss: 7.7417\n",
      "Epoch 9088/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.6957 - val_loss: 0.6660\n",
      "Epoch 9089/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.5109 - val_loss: 0.7928\n",
      "Epoch 9090/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.9288 - val_loss: 11.1880\n",
      "Epoch 9091/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.6042 - val_loss: 8.6217\n",
      "Epoch 9092/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.7502 - val_loss: 4.0274\n",
      "Epoch 9093/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.5882 - val_loss: 2.5800\n",
      "Epoch 9094/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 4.9565 - val_loss: 21.4532\n",
      "Epoch 9095/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 12.7104 - val_loss: 14.8664\n",
      "Epoch 9096/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 28.3047 - val_loss: 61.3046\n",
      "Epoch 9097/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 39.3082 - val_loss: 98.0689\n",
      "Epoch 9098/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 221us/sample - loss: 67.9456 - val_loss: 52.9317\n",
      "Epoch 9099/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 46.4805 - val_loss: 10.3846\n",
      "Epoch 9100/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.8756 - val_loss: 0.1157\n",
      "Epoch 9101/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.0389 - val_loss: 1.0189\n",
      "Epoch 9102/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.0690 - val_loss: 12.8833\n",
      "Epoch 9103/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5.2937 - val_loss: 11.6577\n",
      "Epoch 9104/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.9891 - val_loss: 3.1736\n",
      "Epoch 9105/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.6731 - val_loss: 0.8151\n",
      "Epoch 9106/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7720 - val_loss: 0.2927\n",
      "Epoch 9107/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0460 - val_loss: 0.0464\n",
      "Epoch 9108/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7819 - val_loss: 0.0274\n",
      "Epoch 9109/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5230 - val_loss: 1.1200\n",
      "Epoch 9110/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.5429 - val_loss: 0.2239\n",
      "Epoch 9111/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1932 - val_loss: 0.1839\n",
      "Epoch 9112/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.5608 - val_loss: 0.0879\n",
      "Epoch 9113/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.1913 - val_loss: 0.2073\n",
      "Epoch 9114/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 6.4212 - val_loss: 3.4642\n",
      "Epoch 9115/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.8826 - val_loss: 13.0606\n",
      "Epoch 9116/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 23.1728 - val_loss: 6.2271\n",
      "Epoch 9117/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 37.2748 - val_loss: 1.0557\n",
      "Epoch 9118/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 32.0074 - val_loss: 63.0760\n",
      "Epoch 9119/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 71.2388 - val_loss: 370.5679\n",
      "Epoch 9120/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 336.2175 - val_loss: 511.5068\n",
      "Epoch 9121/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 184.5738 - val_loss: 228.0058\n",
      "Epoch 9122/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 158.0281 - val_loss: 265.3951\n",
      "Epoch 9123/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 185.9642 - val_loss: 69.0897\n",
      "Epoch 9124/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 163.8388 - val_loss: 64.1322\n",
      "Epoch 9125/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 301.9213 - val_loss: 14.7102\n",
      "Epoch 9126/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 236.4208 - val_loss: 97.0700\n",
      "Epoch 9127/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 488.8449 - val_loss: 225.0945\n",
      "Epoch 9128/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1212.3886 - val_loss: 527.8474\n",
      "Epoch 9129/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1220.1399 - val_loss: 4480.2482\n",
      "Epoch 9130/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 2722.8386 - val_loss: 1142.6981\n",
      "Epoch 9131/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 874.6429 - val_loss: 363.8670\n",
      "Epoch 9132/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 531.3363 - val_loss: 22.7343\n",
      "Epoch 9133/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 265.8476 - val_loss: 68.1740\n",
      "Epoch 9134/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 294.8078 - val_loss: 309.4739\n",
      "Epoch 9135/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 589.8106 - val_loss: 462.7975\n",
      "Epoch 9136/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 417.5909 - val_loss: 243.6615\n",
      "Epoch 9137/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 271.3907 - val_loss: 385.9146\n",
      "Epoch 9138/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 254.8336 - val_loss: 32.4482\n",
      "Epoch 9139/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 91.2136 - val_loss: 53.7921\n",
      "Epoch 9140/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 112.1050 - val_loss: 498.5438\n",
      "Epoch 9141/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 234.1990 - val_loss: 93.0672\n",
      "Epoch 9142/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 45.7890 - val_loss: 29.7877\n",
      "Epoch 9143/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 61.3151 - val_loss: 158.6166\n",
      "Epoch 9144/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 63.1909 - val_loss: 5.7379\n",
      "Epoch 9145/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 63.8020 - val_loss: 282.1524\n",
      "Epoch 9146/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 167.6888 - val_loss: 96.8887\n",
      "Epoch 9147/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 47.9031 - val_loss: 14.7058\n",
      "Epoch 9148/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 20.2703 - val_loss: 12.3807\n",
      "Epoch 9149/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 19.0738 - val_loss: 49.6527\n",
      "Epoch 9150/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 25.2643 - val_loss: 22.7973\n",
      "Epoch 9151/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 22.0721 - val_loss: 1.5570\n",
      "Epoch 9152/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.7243 - val_loss: 33.1315\n",
      "Epoch 9153/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 50.2896 - val_loss: 23.1637\n",
      "Epoch 9154/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.5641 - val_loss: 20.1081\n",
      "Epoch 9155/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 63.7341 - val_loss: 78.1746\n",
      "Epoch 9156/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 226.1193 - val_loss: 101.0230\n",
      "Epoch 9157/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 79.4424 - val_loss: 0.3823\n",
      "Epoch 9158/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.7215 - val_loss: 9.7668\n",
      "Epoch 9159/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.7098 - val_loss: 0.0684\n",
      "Epoch 9160/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.6729 - val_loss: 0.9076\n",
      "Epoch 9161/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9708 - val_loss: 0.6299\n",
      "Epoch 9162/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.4795 - val_loss: 0.0203\n",
      "Epoch 9163/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.1560 - val_loss: 0.1819\n",
      "Epoch 9164/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2263 - val_loss: 0.1891\n",
      "Epoch 9165/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2074 - val_loss: 0.0376\n",
      "Epoch 9166/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0470 - val_loss: 0.0429\n",
      "Epoch 9167/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0764 - val_loss: 0.0441\n",
      "Epoch 9168/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0458 - val_loss: 0.0169\n",
      "Epoch 9169/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0275 - val_loss: 0.0081\n",
      "Epoch 9170/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0227 - val_loss: 0.0238\n",
      "Epoch 9171/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0391 - val_loss: 0.0173\n",
      "Epoch 9172/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0281 - val_loss: 0.0674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9173/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0342 - val_loss: 0.0164\n",
      "Epoch 9174/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0192 - val_loss: 0.0183\n",
      "Epoch 9175/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0170 - val_loss: 0.0045\n",
      "Epoch 9176/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0214 - val_loss: 0.0040\n",
      "Epoch 9177/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0203 - val_loss: 0.0051\n",
      "Epoch 9178/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0135 - val_loss: 0.0044\n",
      "Epoch 9179/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0055 - val_loss: 0.0068\n",
      "Epoch 9180/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0115 - val_loss: 0.0103\n",
      "Epoch 9181/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0203 - val_loss: 0.0330\n",
      "Epoch 9182/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0376 - val_loss: 0.2207\n",
      "Epoch 9183/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1498 - val_loss: 0.0694\n",
      "Epoch 9184/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1100 - val_loss: 0.0147\n",
      "Epoch 9185/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1193 - val_loss: 0.1603\n",
      "Epoch 9186/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1878 - val_loss: 0.3117\n",
      "Epoch 9187/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3277 - val_loss: 0.1357\n",
      "Epoch 9188/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1594 - val_loss: 0.0122\n",
      "Epoch 9189/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0945 - val_loss: 0.0135\n",
      "Epoch 9190/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0608 - val_loss: 0.0633\n",
      "Epoch 9191/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1752 - val_loss: 0.1812\n",
      "Epoch 9192/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2716 - val_loss: 0.1588\n",
      "Epoch 9193/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3992 - val_loss: 1.8367\n",
      "Epoch 9194/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.6178 - val_loss: 5.1200\n",
      "Epoch 9195/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.8893 - val_loss: 3.6125\n",
      "Epoch 9196/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.2583 - val_loss: 3.8826\n",
      "Epoch 9197/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.0291 - val_loss: 1.5505\n",
      "Epoch 9198/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.9523 - val_loss: 5.7321\n",
      "Epoch 9199/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.8778 - val_loss: 17.8991\n",
      "Epoch 9200/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 34.7117 - val_loss: 33.3238\n",
      "Epoch 9201/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 32.2179 - val_loss: 17.7660\n",
      "Epoch 9202/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 55.0914 - val_loss: 23.8170\n",
      "Epoch 9203/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 47.1201 - val_loss: 120.2462\n",
      "Epoch 9204/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 103.4358 - val_loss: 154.5085\n",
      "Epoch 9205/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 72.2219 - val_loss: 38.5359\n",
      "Epoch 9206/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21.8909 - val_loss: 39.1999\n",
      "Epoch 9207/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 75.3405 - val_loss: 26.1269\n",
      "Epoch 9208/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 99.4078 - val_loss: 109.6899\n",
      "Epoch 9209/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 207.0822 - val_loss: 400.7194\n",
      "Epoch 9210/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 188.3533 - val_loss: 588.0402\n",
      "Epoch 9211/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 651.9968 - val_loss: 293.5759\n",
      "Epoch 9212/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 484.7043 - val_loss: 903.8054\n",
      "Epoch 9213/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 227.7564 - val_loss: 40.3918\n",
      "Epoch 9214/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 37.8429 - val_loss: 15.3868\n",
      "Epoch 9215/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 26.1639 - val_loss: 35.8371\n",
      "Epoch 9216/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 20.7257 - val_loss: 15.0831\n",
      "Epoch 9217/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 10.3574 - val_loss: 1.4192\n",
      "Epoch 9218/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.1590 - val_loss: 1.4291\n",
      "Epoch 9219/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.7647 - val_loss: 0.5099\n",
      "Epoch 9220/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.4807 - val_loss: 0.5543\n",
      "Epoch 9221/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.8865 - val_loss: 16.6861\n",
      "Epoch 9222/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.8056 - val_loss: 6.2254\n",
      "Epoch 9223/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.8658 - val_loss: 30.4494\n",
      "Epoch 9224/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 82.9511 - val_loss: 79.1118\n",
      "Epoch 9225/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 361.5627 - val_loss: 517.7644\n",
      "Epoch 9226/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 403.4360 - val_loss: 1103.6043\n",
      "Epoch 9227/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 994.7877 - val_loss: 941.2558\n",
      "Epoch 9228/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1714.8137 - val_loss: 322.7674\n",
      "Epoch 9229/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2037.9175 - val_loss: 1282.3381\n",
      "Epoch 9230/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2078.9291 - val_loss: 1950.8697\n",
      "Epoch 9231/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 865.2336 - val_loss: 791.7682\n",
      "Epoch 9232/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 453.2867 - val_loss: 96.4873\n",
      "Epoch 9233/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 196.0363 - val_loss: 327.3069\n",
      "Epoch 9234/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 213.9615 - val_loss: 258.6251\n",
      "Epoch 9235/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 826.4257 - val_loss: 1006.0090\n",
      "Epoch 9236/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 924.6674 - val_loss: 754.0720\n",
      "Epoch 9237/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 688.5344 - val_loss: 1216.5811\n",
      "Epoch 9238/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1250.7234 - val_loss: 453.6230\n",
      "Epoch 9239/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 808.3426 - val_loss: 538.9471\n",
      "Epoch 9240/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 916.6950 - val_loss: 1727.0566\n",
      "Epoch 9241/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1208.4854 - val_loss: 293.8924\n",
      "Epoch 9242/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 332.5709 - val_loss: 290.4051\n",
      "Epoch 9243/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 247.1634 - val_loss: 79.3144\n",
      "Epoch 9244/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 105.3468 - val_loss: 57.6922\n",
      "Epoch 9245/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 69.1865 - val_loss: 10.1923\n",
      "Epoch 9246/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 38.7935 - val_loss: 42.9099\n",
      "Epoch 9247/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 52.0130 - val_loss: 93.8733\n",
      "Epoch 9248/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 90.2750 - val_loss: 65.1199\n",
      "Epoch 9249/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 110.7253 - val_loss: 135.4130\n",
      "Epoch 9250/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 70.5445 - val_loss: 90.3046\n",
      "Epoch 9251/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 30.7287 - val_loss: 29.8703\n",
      "Epoch 9252/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 11.6279 - val_loss: 2.9631\n",
      "Epoch 9253/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.2531 - val_loss: 13.0327\n",
      "Epoch 9254/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.1331 - val_loss: 3.1072\n",
      "Epoch 9255/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.1786 - val_loss: 5.7850\n",
      "Epoch 9256/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.9931 - val_loss: 4.0120\n",
      "Epoch 9257/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.9941 - val_loss: 5.2171\n",
      "Epoch 9258/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.2936 - val_loss: 0.0519\n",
      "Epoch 9259/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.7900 - val_loss: 1.5328\n",
      "Epoch 9260/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4430 - val_loss: 2.6211\n",
      "Epoch 9261/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1506 - val_loss: 1.1816\n",
      "Epoch 9262/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8136 - val_loss: 1.6478\n",
      "Epoch 9263/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2594 - val_loss: 1.7570\n",
      "Epoch 9264/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2134 - val_loss: 0.0718\n",
      "Epoch 9265/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6443 - val_loss: 0.5291\n",
      "Epoch 9266/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3952 - val_loss: 1.3095\n",
      "Epoch 9267/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.7418 - val_loss: 2.2155\n",
      "Epoch 9268/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3345 - val_loss: 0.3472\n",
      "Epoch 9269/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3147 - val_loss: 0.3619\n",
      "Epoch 9270/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2771 - val_loss: 0.1724\n",
      "Epoch 9271/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3353 - val_loss: 0.2198\n",
      "Epoch 9272/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3959 - val_loss: 0.5975\n",
      "Epoch 9273/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2989 - val_loss: 0.0480\n",
      "Epoch 9274/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2684 - val_loss: 0.3890\n",
      "Epoch 9275/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.4927 - val_loss: 0.2773\n",
      "Epoch 9276/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1627 - val_loss: 0.0548\n",
      "Epoch 9277/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1009 - val_loss: 0.1421\n",
      "Epoch 9278/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1336 - val_loss: 0.0354\n",
      "Epoch 9279/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1289 - val_loss: 0.1642\n",
      "Epoch 9280/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0918 - val_loss: 0.0281\n",
      "Epoch 9281/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0991 - val_loss: 0.2048\n",
      "Epoch 9282/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1365 - val_loss: 0.0401\n",
      "Epoch 9283/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0347 - val_loss: 0.0371\n",
      "Epoch 9284/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0838 - val_loss: 0.1459\n",
      "Epoch 9285/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0856 - val_loss: 0.0569\n",
      "Epoch 9286/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1573 - val_loss: 0.1789\n",
      "Epoch 9287/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1316 - val_loss: 0.1548\n",
      "Epoch 9288/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0917 - val_loss: 0.0261\n",
      "Epoch 9289/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0573 - val_loss: 0.0458\n",
      "Epoch 9290/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0603 - val_loss: 0.0684\n",
      "Epoch 9291/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0734 - val_loss: 0.0587\n",
      "Epoch 9292/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0419 - val_loss: 0.1313\n",
      "Epoch 9293/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0787 - val_loss: 0.0278\n",
      "Epoch 9294/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1064 - val_loss: 0.1778\n",
      "Epoch 9295/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1722 - val_loss: 0.8838\n",
      "Epoch 9296/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6757 - val_loss: 0.2015\n",
      "Epoch 9297/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4904 - val_loss: 0.0935\n",
      "Epoch 9298/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5590 - val_loss: 0.4402\n",
      "Epoch 9299/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3093 - val_loss: 0.0688\n",
      "Epoch 9300/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0683 - val_loss: 0.1574\n",
      "Epoch 9301/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1458 - val_loss: 0.1722\n",
      "Epoch 9302/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1250 - val_loss: 0.1957\n",
      "Epoch 9303/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2169 - val_loss: 0.1742\n",
      "Epoch 9304/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1852 - val_loss: 0.1702\n",
      "Epoch 9305/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2933 - val_loss: 0.2337\n",
      "Epoch 9306/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2490 - val_loss: 0.3087\n",
      "Epoch 9307/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3160 - val_loss: 0.3917\n",
      "Epoch 9308/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2499 - val_loss: 0.2632\n",
      "Epoch 9309/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1324 - val_loss: 0.0914\n",
      "Epoch 9310/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0873 - val_loss: 0.0391\n",
      "Epoch 9311/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0954 - val_loss: 0.0237\n",
      "Epoch 9312/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1111 - val_loss: 0.2190\n",
      "Epoch 9313/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3165 - val_loss: 0.6367\n",
      "Epoch 9314/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1014 - val_loss: 0.9530\n",
      "Epoch 9315/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4141 - val_loss: 0.3414\n",
      "Epoch 9316/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1919 - val_loss: 0.0265\n",
      "Epoch 9317/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0298 - val_loss: 0.0305\n",
      "Epoch 9318/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0668 - val_loss: 0.2447\n",
      "Epoch 9319/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1451 - val_loss: 0.0224\n",
      "Epoch 9320/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1092 - val_loss: 0.2549\n",
      "Epoch 9321/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1938 - val_loss: 0.2524\n",
      "Epoch 9322/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3402 - val_loss: 0.6876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9323/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6447 - val_loss: 1.1929\n",
      "Epoch 9324/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8920 - val_loss: 0.4774\n",
      "Epoch 9325/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5841 - val_loss: 0.1731\n",
      "Epoch 9326/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4148 - val_loss: 0.5925\n",
      "Epoch 9327/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3220 - val_loss: 0.4960\n",
      "Epoch 9328/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2066 - val_loss: 0.4428\n",
      "Epoch 9329/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4029 - val_loss: 0.5192\n",
      "Epoch 9330/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.4710 - val_loss: 1.9970\n",
      "Epoch 9331/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.5194 - val_loss: 5.9320\n",
      "Epoch 9332/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14.0148 - val_loss: 24.2546\n",
      "Epoch 9333/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 25.5449 - val_loss: 34.8777\n",
      "Epoch 9334/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 24.9888 - val_loss: 108.0535\n",
      "Epoch 9335/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 65.5730 - val_loss: 226.1711\n",
      "Epoch 9336/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 278.0237 - val_loss: 349.1941\n",
      "Epoch 9337/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 147.1678 - val_loss: 32.4018\n",
      "Epoch 9338/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 29.3564 - val_loss: 6.8565\n",
      "Epoch 9339/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 26.1492 - val_loss: 143.8515\n",
      "Epoch 9340/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 97.5893 - val_loss: 89.0028\n",
      "Epoch 9341/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 51.1781 - val_loss: 67.0398\n",
      "Epoch 9342/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 49.6430 - val_loss: 1.9367\n",
      "Epoch 9343/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 76.9962 - val_loss: 108.3729\n",
      "Epoch 9344/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 112.966 - 0s 132us/sample - loss: 79.6766 - val_loss: 107.9906\n",
      "Epoch 9345/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 163.4659 - val_loss: 278.2700\n",
      "Epoch 9346/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 199.3140 - val_loss: 908.3624\n",
      "Epoch 9347/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2245.2085 - val_loss: 5088.9689\n",
      "Epoch 9348/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1804.5995 - val_loss: 2168.6210\n",
      "Epoch 9349/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2475.5275 - val_loss: 4860.2770\n",
      "Epoch 9350/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2595.8643 - val_loss: 4054.0489\n",
      "Epoch 9351/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1937.6299 - val_loss: 162.8222\n",
      "Epoch 9352/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1638.3325 - val_loss: 815.7018\n",
      "Epoch 9353/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2858.4787 - val_loss: 2899.2148\n",
      "Epoch 9354/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1288.0485 - val_loss: 165.2291\n",
      "Epoch 9355/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 301.7539 - val_loss: 418.0206\n",
      "Epoch 9356/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 306.4804 - val_loss: 338.5804\n",
      "Epoch 9357/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 150.2071 - val_loss: 245.3468\n",
      "Epoch 9358/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 205.2881 - val_loss: 11.5778\n",
      "Epoch 9359/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 108.5750 - val_loss: 183.9614\n",
      "Epoch 9360/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 85.9395 - val_loss: 76.0252\n",
      "Epoch 9361/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 43.4528 - val_loss: 23.6546\n",
      "Epoch 9362/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 40.6515 - val_loss: 60.1613\n",
      "Epoch 9363/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 105.4182 - val_loss: 338.7578\n",
      "Epoch 9364/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 247.4148 - val_loss: 396.7513\n",
      "Epoch 9365/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 468.5805 - val_loss: 18.5026\n",
      "Epoch 9366/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 468.1876 - val_loss: 815.7717\n",
      "Epoch 9367/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 400.3252 - val_loss: 398.7100\n",
      "Epoch 9368/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 609.7229 - val_loss: 248.3229\n",
      "Epoch 9369/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1575.9171 - val_loss: 668.2518\n",
      "Epoch 9370/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3299.9411 - val_loss: 9243.7413\n",
      "Epoch 9371/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6384.4104 - val_loss: 4397.5968\n",
      "Epoch 9372/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2048.3454 - val_loss: 2886.5170\n",
      "Epoch 9373/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1489.6972 - val_loss: 1042.4016\n",
      "Epoch 9374/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1460.8034 - val_loss: 1059.0254\n",
      "Epoch 9375/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1196.3882 - val_loss: 566.9020\n",
      "Epoch 9376/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 726.4860 - val_loss: 8.3994\n",
      "Epoch 9377/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 546.9017 - val_loss: 1216.3202\n",
      "Epoch 9378/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 806.9456 - val_loss: 759.9374\n",
      "Epoch 9379/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 542.1803 - val_loss: 845.6115\n",
      "Epoch 9380/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 441.0150 - val_loss: 280.6418\n",
      "Epoch 9381/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 404.9454 - val_loss: 21.6634\n",
      "Epoch 9382/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 136.7479 - val_loss: 3.6141\n",
      "Epoch 9383/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 48.6451 - val_loss: 2.8951\n",
      "Epoch 9384/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 31.7833 - val_loss: 3.2275\n",
      "Epoch 9385/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.0739 - val_loss: 9.2769\n",
      "Epoch 9386/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.8965 - val_loss: 22.8564\n",
      "Epoch 9387/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.4555 - val_loss: 14.5472\n",
      "Epoch 9388/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.3751 - val_loss: 11.8761\n",
      "Epoch 9389/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.4715 - val_loss: 9.7507\n",
      "Epoch 9390/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.0025 - val_loss: 3.4805\n",
      "Epoch 9391/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.4640 - val_loss: 1.2500\n",
      "Epoch 9392/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6074 - val_loss: 0.2920\n",
      "Epoch 9393/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3704 - val_loss: 0.4973\n",
      "Epoch 9394/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4073 - val_loss: 0.6126\n",
      "Epoch 9395/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6964 - val_loss: 1.2993\n",
      "Epoch 9396/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9321 - val_loss: 0.2863\n",
      "Epoch 9397/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2245 - val_loss: 0.1214\n",
      "Epoch 9398/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2008 - val_loss: 0.2957\n",
      "Epoch 9399/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2116 - val_loss: 0.2216\n",
      "Epoch 9400/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1196 - val_loss: 0.0649\n",
      "Epoch 9401/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0853 - val_loss: 0.1481\n",
      "Epoch 9402/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1266 - val_loss: 0.0745\n",
      "Epoch 9403/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1406 - val_loss: 0.0703\n",
      "Epoch 9404/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2050 - val_loss: 0.4270\n",
      "Epoch 9405/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2940 - val_loss: 0.1825\n",
      "Epoch 9406/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2527 - val_loss: 0.1130\n",
      "Epoch 9407/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1308 - val_loss: 0.0994\n",
      "Epoch 9408/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2259 - val_loss: 0.2206\n",
      "Epoch 9409/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1333 - val_loss: 0.1472\n",
      "Epoch 9410/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1813 - val_loss: 0.4137\n",
      "Epoch 9411/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1913 - val_loss: 0.1056\n",
      "Epoch 9412/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1101 - val_loss: 0.0599\n",
      "Epoch 9413/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0742 - val_loss: 0.0561\n",
      "Epoch 9414/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0902 - val_loss: 0.0708\n",
      "Epoch 9415/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.077 - 0s 162us/sample - loss: 0.1218 - val_loss: 0.0516\n",
      "Epoch 9416/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1419 - val_loss: 0.0866\n",
      "Epoch 9417/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1463 - val_loss: 0.1165\n",
      "Epoch 9418/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0860 - val_loss: 0.1043\n",
      "Epoch 9419/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0630 - val_loss: 0.1514\n",
      "Epoch 9420/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1987 - val_loss: 0.2963\n",
      "Epoch 9421/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2110 - val_loss: 0.1280\n",
      "Epoch 9422/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1078 - val_loss: 0.0505\n",
      "Epoch 9423/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1097 - val_loss: 0.1156\n",
      "Epoch 9424/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1530 - val_loss: 0.3048\n",
      "Epoch 9425/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2132 - val_loss: 0.1902\n",
      "Epoch 9426/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1610 - val_loss: 0.0509\n",
      "Epoch 9427/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1275 - val_loss: 0.2293\n",
      "Epoch 9428/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2779 - val_loss: 0.3883\n",
      "Epoch 9429/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1766 - val_loss: 0.3382\n",
      "Epoch 9430/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3036 - val_loss: 0.4144\n",
      "Epoch 9431/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2916 - val_loss: 0.0384\n",
      "Epoch 9432/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1940 - val_loss: 0.4664\n",
      "Epoch 9433/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3819 - val_loss: 0.9938\n",
      "Epoch 9434/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4907 - val_loss: 0.3420\n",
      "Epoch 9435/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1798 - val_loss: 0.1612\n",
      "Epoch 9436/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1453 - val_loss: 0.0861\n",
      "Epoch 9437/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0716 - val_loss: 0.0593\n",
      "Epoch 9438/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0490 - val_loss: 0.0383\n",
      "Epoch 9439/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0481 - val_loss: 0.0753\n",
      "Epoch 9440/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0656 - val_loss: 0.0363\n",
      "Epoch 9441/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0397 - val_loss: 0.0409\n",
      "Epoch 9442/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0445 - val_loss: 0.0390\n",
      "Epoch 9443/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0360 - val_loss: 0.0981\n",
      "Epoch 9444/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0819 - val_loss: 0.0917\n",
      "Epoch 9445/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0752 - val_loss: 0.1206\n",
      "Epoch 9446/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0933 - val_loss: 0.1943\n",
      "Epoch 9447/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1539 - val_loss: 0.1283\n",
      "Epoch 9448/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1429 - val_loss: 0.0291\n",
      "Epoch 9449/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0834 - val_loss: 0.1754\n",
      "Epoch 9450/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0784 - val_loss: 0.0393\n",
      "Epoch 9451/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0370 - val_loss: 0.0367\n",
      "Epoch 9452/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0556 - val_loss: 0.0624\n",
      "Epoch 9453/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1952 - val_loss: 0.3851\n",
      "Epoch 9454/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2770 - val_loss: 1.3594\n",
      "Epoch 9455/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8571 - val_loss: 0.2268\n",
      "Epoch 9456/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3730 - val_loss: 0.3562\n",
      "Epoch 9457/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2396 - val_loss: 0.1663\n",
      "Epoch 9458/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1380 - val_loss: 0.1440\n",
      "Epoch 9459/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6883 - val_loss: 0.4791\n",
      "Epoch 9460/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3977 - val_loss: 0.5374\n",
      "Epoch 9461/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2365 - val_loss: 0.4554\n",
      "Epoch 9462/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2474 - val_loss: 0.0633\n",
      "Epoch 9463/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0699 - val_loss: 0.0565\n",
      "Epoch 9464/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0439 - val_loss: 0.0837\n",
      "Epoch 9465/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0615 - val_loss: 0.0766\n",
      "Epoch 9466/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0730 - val_loss: 0.0879\n",
      "Epoch 9467/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1346 - val_loss: 0.0575\n",
      "Epoch 9468/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0755 - val_loss: 0.2027\n",
      "Epoch 9469/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1849 - val_loss: 0.0678\n",
      "Epoch 9470/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1962 - val_loss: 0.2435\n",
      "Epoch 9471/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3298 - val_loss: 1.6220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9472/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.9982 - val_loss: 0.0906\n",
      "Epoch 9473/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8083 - val_loss: 1.7704\n",
      "Epoch 9474/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7840 - val_loss: 0.2182\n",
      "Epoch 9475/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.6196 - val_loss: 2.3462\n",
      "Epoch 9476/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.5467 - val_loss: 0.9684\n",
      "Epoch 9477/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.9123 - val_loss: 0.5595\n",
      "Epoch 9478/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.3911 - val_loss: 0.3497\n",
      "Epoch 9479/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1713 - val_loss: 0.0823\n",
      "Epoch 9480/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0381 - val_loss: 0.0998\n",
      "Epoch 9481/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0795 - val_loss: 0.1741\n",
      "Epoch 9482/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1970 - val_loss: 0.2794\n",
      "Epoch 9483/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1736 - val_loss: 0.0456\n",
      "Epoch 9484/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0319 - val_loss: 0.0186\n",
      "Epoch 9485/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0247 - val_loss: 0.0258\n",
      "Epoch 9486/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0171 - val_loss: 0.0137\n",
      "Epoch 9487/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0130 - val_loss: 0.0176\n",
      "Epoch 9488/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0275 - val_loss: 0.0177\n",
      "Epoch 9489/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0659 - val_loss: 0.0196\n",
      "Epoch 9490/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0423 - val_loss: 0.0371\n",
      "Epoch 9491/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0403 - val_loss: 0.0547\n",
      "Epoch 9492/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0306 - val_loss: 0.0117\n",
      "Epoch 9493/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0160 - val_loss: 0.0152\n",
      "Epoch 9494/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0170 - val_loss: 0.0199\n",
      "Epoch 9495/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.013 - 0s 162us/sample - loss: 0.0211 - val_loss: 0.0173\n",
      "Epoch 9496/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0186 - val_loss: 0.0219\n",
      "Epoch 9497/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0184 - val_loss: 0.0303\n",
      "Epoch 9498/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0209 - val_loss: 0.0116\n",
      "Epoch 9499/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0719 - val_loss: 0.0309\n",
      "Epoch 9500/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1709 - val_loss: 0.1976\n",
      "Epoch 9501/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1282 - val_loss: 0.0632\n",
      "Epoch 9502/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1053 - val_loss: 0.0376\n",
      "Epoch 9503/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0785 - val_loss: 0.1377\n",
      "Epoch 9504/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3600 - val_loss: 0.3816\n",
      "Epoch 9505/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5810 - val_loss: 0.0082\n",
      "Epoch 9506/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7276 - val_loss: 1.5858\n",
      "Epoch 9507/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1553 - val_loss: 11.1077\n",
      "Epoch 9508/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.3851 - val_loss: 5.8144\n",
      "Epoch 9509/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.8781 - val_loss: 2.4206\n",
      "Epoch 9510/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.6276 - val_loss: 11.4903\n",
      "Epoch 9511/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.0894 - val_loss: 12.6046\n",
      "Epoch 9512/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.1611 - val_loss: 3.4826\n",
      "Epoch 9513/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.0573 - val_loss: 1.5626\n",
      "Epoch 9514/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1153 - val_loss: 1.1506\n",
      "Epoch 9515/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1358 - val_loss: 0.2854\n",
      "Epoch 9516/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2433 - val_loss: 0.3273\n",
      "Epoch 9517/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6455 - val_loss: 0.4081\n",
      "Epoch 9518/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3388 - val_loss: 0.0669\n",
      "Epoch 9519/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4059 - val_loss: 0.0871\n",
      "Epoch 9520/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3214 - val_loss: 0.0652\n",
      "Epoch 9521/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1788 - val_loss: 0.0488\n",
      "Epoch 9522/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7274 - val_loss: 0.4404\n",
      "Epoch 9523/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.0914 - val_loss: 1.0856\n",
      "Epoch 9524/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.0027 - val_loss: 0.7792\n",
      "Epoch 9525/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9886 - val_loss: 0.2536\n",
      "Epoch 9526/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.1100 - val_loss: 0.2737\n",
      "Epoch 9527/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 4.7533 - val_loss: 1.3601\n",
      "Epoch 9528/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.6695 - val_loss: 2.4854\n",
      "Epoch 9529/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3768 - val_loss: 1.7181\n",
      "Epoch 9530/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 4.5717 - val_loss: 14.3896\n",
      "Epoch 9531/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19.2175 - val_loss: 25.6888\n",
      "Epoch 9532/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 12.0378 - val_loss: 19.9239\n",
      "Epoch 9533/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9.2618 - val_loss: 2.9781\n",
      "Epoch 9534/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.2012 - val_loss: 1.5745\n",
      "Epoch 9535/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.9446 - val_loss: 0.4284\n",
      "Epoch 9536/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 4.7915 - val_loss: 1.4962\n",
      "Epoch 9537/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1417 - val_loss: 0.0164\n",
      "Epoch 9538/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6025 - val_loss: 0.0045\n",
      "Epoch 9539/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3992 - val_loss: 0.5587\n",
      "Epoch 9540/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2123 - val_loss: 0.1937\n",
      "Epoch 9541/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1690 - val_loss: 0.0487\n",
      "Epoch 9542/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0958 - val_loss: 0.0231\n",
      "Epoch 9543/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0501 - val_loss: 0.0094\n",
      "Epoch 9544/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0121 - val_loss: 0.0054\n",
      "Epoch 9545/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0056 - val_loss: 0.0045\n",
      "Epoch 9546/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0042 - val_loss: 0.0046\n",
      "Epoch 9547/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0046 - val_loss: 0.0192\n",
      "Epoch 9548/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0175 - val_loss: 0.0209\n",
      "Epoch 9549/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0702 - val_loss: 0.1212\n",
      "Epoch 9550/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0453 - val_loss: 0.0093\n",
      "Epoch 9551/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0084 - val_loss: 0.0067\n",
      "Epoch 9552/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0052 - val_loss: 0.0051\n",
      "Epoch 9553/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0084 - val_loss: 0.0200\n",
      "Epoch 9554/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0304 - val_loss: 0.0360\n",
      "Epoch 9555/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0335 - val_loss: 0.0725\n",
      "Epoch 9556/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1482 - val_loss: 0.2671\n",
      "Epoch 9557/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.7145 - val_loss: 3.6899\n",
      "Epoch 9558/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.0974 - val_loss: 4.6535\n",
      "Epoch 9559/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.5739 - val_loss: 2.5132\n",
      "Epoch 9560/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0908 - val_loss: 0.6888\n",
      "Epoch 9561/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7087 - val_loss: 0.8295\n",
      "Epoch 9562/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7685 - val_loss: 0.1144\n",
      "Epoch 9563/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8839 - val_loss: 0.1550\n",
      "Epoch 9564/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2175 - val_loss: 2.6121\n",
      "Epoch 9565/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.7601 - val_loss: 1.9626\n",
      "Epoch 9566/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.6236 - val_loss: 0.0248\n",
      "Epoch 9567/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.0447 - val_loss: 0.6652\n",
      "Epoch 9568/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.7283 - val_loss: 0.3405\n",
      "Epoch 9569/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.4586 - val_loss: 6.2231\n",
      "Epoch 9570/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.5390 - val_loss: 26.6405\n",
      "Epoch 9571/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 8.4747 - val_loss: 10.7034\n",
      "Epoch 9572/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12.1550 - val_loss: 47.3800\n",
      "Epoch 9573/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 86.7790 - val_loss: 180.0894\n",
      "Epoch 9574/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 253.7859 - val_loss: 207.2750\n",
      "Epoch 9575/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 426.1914 - val_loss: 412.9554\n",
      "Epoch 9576/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 728.9720 - val_loss: 90.5929\n",
      "Epoch 9577/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 348.7297 - val_loss: 1447.2979\n",
      "Epoch 9578/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1032.4945 - val_loss: 1022.5988\n",
      "Epoch 9579/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1326.2441 - val_loss: 1804.5986\n",
      "Epoch 9580/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2398.6026 - val_loss: 3650.4100\n",
      "Epoch 9581/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4143.1014 - val_loss: 3379.1950\n",
      "Epoch 9582/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4451.5359 - val_loss: 538.9381\n",
      "Epoch 9583/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3313.8671 - val_loss: 2704.6970\n",
      "Epoch 9584/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1363.4160 - val_loss: 592.2482\n",
      "Epoch 9585/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 756.8445 - val_loss: 330.9029\n",
      "Epoch 9586/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 583.0968 - val_loss: 310.5499\n",
      "Epoch 9587/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 220.1016 - val_loss: 200.5787\n",
      "Epoch 9588/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 183.0274 - val_loss: 2.2197\n",
      "Epoch 9589/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 74.7296 - val_loss: 99.2561\n",
      "Epoch 9590/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 76.3869 - val_loss: 38.2213\n",
      "Epoch 9591/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 32.4546 - val_loss: 11.2791\n",
      "Epoch 9592/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 26.8742 - val_loss: 13.1581\n",
      "Epoch 9593/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.5783 - val_loss: 6.6646\n",
      "Epoch 9594/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.6242 - val_loss: 10.5962\n",
      "Epoch 9595/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.1390 - val_loss: 6.0563\n",
      "Epoch 9596/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.2855 - val_loss: 14.8398\n",
      "Epoch 9597/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 7.7414 - val_loss: 15.1617\n",
      "Epoch 9598/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.8829 - val_loss: 2.8706\n",
      "Epoch 9599/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6835 - val_loss: 0.8338\n",
      "Epoch 9600/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.5669 - val_loss: 1.0533\n",
      "Epoch 9601/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8428 - val_loss: 0.2107\n",
      "Epoch 9602/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6708 - val_loss: 0.7767\n",
      "Epoch 9603/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5893 - val_loss: 1.0880\n",
      "Epoch 9604/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2575 - val_loss: 0.6889\n",
      "Epoch 9605/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8563 - val_loss: 1.4279\n",
      "Epoch 9606/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9104 - val_loss: 0.1170\n",
      "Epoch 9607/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3548 - val_loss: 0.1238\n",
      "Epoch 9608/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2813 - val_loss: 0.1557\n",
      "Epoch 9609/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2321 - val_loss: 0.1780\n",
      "Epoch 9610/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1954 - val_loss: 0.1876\n",
      "Epoch 9611/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1762 - val_loss: 0.1370\n",
      "Epoch 9612/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1792 - val_loss: 0.0526\n",
      "Epoch 9613/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1717 - val_loss: 0.1031\n",
      "Epoch 9614/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1038 - val_loss: 0.0426\n",
      "Epoch 9615/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0598 - val_loss: 0.0578\n",
      "Epoch 9616/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1230 - val_loss: 0.0411\n",
      "Epoch 9617/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0884 - val_loss: 0.1467\n",
      "Epoch 9618/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3463 - val_loss: 0.3829\n",
      "Epoch 9619/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5319 - val_loss: 0.4260\n",
      "Epoch 9620/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3987 - val_loss: 0.1858\n",
      "Epoch 9621/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1229 - val_loss: 0.1267\n",
      "Epoch 9622/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1875 - val_loss: 0.1363\n",
      "Epoch 9623/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1398 - val_loss: 0.0522\n",
      "Epoch 9624/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0405 - val_loss: 0.0308\n",
      "Epoch 9625/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0751 - val_loss: 0.2372\n",
      "Epoch 9626/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1853 - val_loss: 0.1088\n",
      "Epoch 9627/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0899 - val_loss: 0.1701\n",
      "Epoch 9628/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2688 - val_loss: 0.2307\n",
      "Epoch 9629/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2522 - val_loss: 0.1378\n",
      "Epoch 9630/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1225 - val_loss: 0.0605\n",
      "Epoch 9631/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0900 - val_loss: 0.0388\n",
      "Epoch 9632/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0832 - val_loss: 0.0845\n",
      "Epoch 9633/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0495 - val_loss: 0.0643\n",
      "Epoch 9634/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0850 - val_loss: 0.1126\n",
      "Epoch 9635/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3026 - val_loss: 0.2862\n",
      "Epoch 9636/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7919 - val_loss: 1.9292\n",
      "Epoch 9637/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0752 - val_loss: 3.3548\n",
      "Epoch 9638/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9791 - val_loss: 0.6218\n",
      "Epoch 9639/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.2879 - val_loss: 3.5313\n",
      "Epoch 9640/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.0781 - val_loss: 6.2814\n",
      "Epoch 9641/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.1008 - val_loss: 0.0690\n",
      "Epoch 9642/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.4016 - val_loss: 6.4314\n",
      "Epoch 9643/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.9220 - val_loss: 0.7918\n",
      "Epoch 9644/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7624 - val_loss: 0.3871\n",
      "Epoch 9645/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5638 - val_loss: 0.5240\n",
      "Epoch 9646/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4422 - val_loss: 0.3584\n",
      "Epoch 9647/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6221 - val_loss: 0.8873\n",
      "Epoch 9648/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3853 - val_loss: 1.8153\n",
      "Epoch 9649/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.0842 - val_loss: 7.6764\n",
      "Epoch 9650/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.6735 - val_loss: 6.1886\n",
      "Epoch 9651/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.1896 - val_loss: 2.5307\n",
      "Epoch 9652/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2289 - val_loss: 1.1976\n",
      "Epoch 9653/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.4136 - val_loss: 8.4316\n",
      "Epoch 9654/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.8617 - val_loss: 4.2960\n",
      "Epoch 9655/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.2742 - val_loss: 0.4598\n",
      "Epoch 9656/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.3742 - val_loss: 10.3151\n",
      "Epoch 9657/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.3874 - val_loss: 7.3474\n",
      "Epoch 9658/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 5.201 - 0s 176us/sample - loss: 1.9278 - val_loss: 0.3456\n",
      "Epoch 9659/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.9376 - val_loss: 0.1306\n",
      "Epoch 9660/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1457 - val_loss: 0.9983\n",
      "Epoch 9661/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.6025 - val_loss: 0.1767\n",
      "Epoch 9662/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0990 - val_loss: 0.0658\n",
      "Epoch 9663/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0396 - val_loss: 0.0521\n",
      "Epoch 9664/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0611 - val_loss: 0.0659\n",
      "Epoch 9665/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0488 - val_loss: 0.0455\n",
      "Epoch 9666/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0579 - val_loss: 0.1176\n",
      "Epoch 9667/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0942 - val_loss: 0.0279\n",
      "Epoch 9668/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0962 - val_loss: 0.2861\n",
      "Epoch 9669/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1230 - val_loss: 0.0239\n",
      "Epoch 9670/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0825 - val_loss: 0.0344\n",
      "Epoch 9671/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0284 - val_loss: 0.0138\n",
      "Epoch 9672/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0239 - val_loss: 0.0768\n",
      "Epoch 9673/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0633 - val_loss: 0.1425\n",
      "Epoch 9674/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0649 - val_loss: 0.0252\n",
      "Epoch 9675/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0366 - val_loss: 0.0330\n",
      "Epoch 9676/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0653 - val_loss: 0.0827\n",
      "Epoch 9677/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1095 - val_loss: 0.0327\n",
      "Epoch 9678/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0788 - val_loss: 0.0213\n",
      "Epoch 9679/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0853 - val_loss: 0.0146\n",
      "Epoch 9680/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1821 - val_loss: 0.0447\n",
      "Epoch 9681/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3089 - val_loss: 0.2605\n",
      "Epoch 9682/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1134 - val_loss: 0.0819\n",
      "Epoch 9683/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1351 - val_loss: 0.3039\n",
      "Epoch 9684/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3290 - val_loss: 0.8822\n",
      "Epoch 9685/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4191 - val_loss: 3.8770\n",
      "Epoch 9686/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.9494 - val_loss: 1.6001\n",
      "Epoch 9687/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7338 - val_loss: 0.7812\n",
      "Epoch 9688/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5194 - val_loss: 0.3508\n",
      "Epoch 9689/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1792 - val_loss: 0.3232\n",
      "Epoch 9690/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.8201 - val_loss: 6.9243\n",
      "Epoch 9691/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.5047 - val_loss: 2.8311\n",
      "Epoch 9692/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3.4361 - val_loss: 5.7400\n",
      "Epoch 9693/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.4952 - val_loss: 51.4762\n",
      "Epoch 9694/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 36.2974 - val_loss: 105.1152\n",
      "Epoch 9695/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 29.7160 - val_loss: 30.4596\n",
      "Epoch 9696/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.4121 - val_loss: 4.8400\n",
      "Epoch 9697/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.4668 - val_loss: 34.8464\n",
      "Epoch 9698/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 26.0495 - val_loss: 2.4932\n",
      "Epoch 9699/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 27.0512 - val_loss: 79.1529\n",
      "Epoch 9700/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 67.7950 - val_loss: 69.7086\n",
      "Epoch 9701/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 196.8862 - val_loss: 62.5133\n",
      "Epoch 9702/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 318.5702 - val_loss: 35.2761\n",
      "Epoch 9703/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 282.5773 - val_loss: 658.0149\n",
      "Epoch 9704/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 649.5892 - val_loss: 1948.3318\n",
      "Epoch 9705/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1289.3566 - val_loss: 3331.1611\n",
      "Epoch 9706/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2558.3620 - val_loss: 3013.0246\n",
      "Epoch 9707/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1529.4314 - val_loss: 475.5937\n",
      "Epoch 9708/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 916.4270 - val_loss: 667.2329\n",
      "Epoch 9709/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 426.8384 - val_loss: 826.0644\n",
      "Epoch 9710/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 842.1496 - val_loss: 1052.8416\n",
      "Epoch 9711/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 551.3502 - val_loss: 177.5728\n",
      "Epoch 9712/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 307.5131 - val_loss: 341.6237\n",
      "Epoch 9713/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 185.7524 - val_loss: 79.2494\n",
      "Epoch 9714/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 58.6042 - val_loss: 6.9373\n",
      "Epoch 9715/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.5648 - val_loss: 10.6329\n",
      "Epoch 9716/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.3193 - val_loss: 21.3253\n",
      "Epoch 9717/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.9781 - val_loss: 11.8979\n",
      "Epoch 9718/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16.6245 - val_loss: 0.5787\n",
      "Epoch 9719/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.1335 - val_loss: 8.3813\n",
      "Epoch 9720/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.0971 - val_loss: 15.8717\n",
      "Epoch 9721/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 10.5711 - val_loss: 11.0146\n",
      "Epoch 9722/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.0331 - val_loss: 5.5821\n",
      "Epoch 9723/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.2221 - val_loss: 1.7301\n",
      "Epoch 9724/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8861 - val_loss: 0.1653\n",
      "Epoch 9725/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3943 - val_loss: 0.1830\n",
      "Epoch 9726/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3627 - val_loss: 0.3543\n",
      "Epoch 9727/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4938 - val_loss: 0.6119\n",
      "Epoch 9728/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3821 - val_loss: 0.2655\n",
      "Epoch 9729/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5878 - val_loss: 0.6272\n",
      "Epoch 9730/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6872 - val_loss: 0.8258\n",
      "Epoch 9731/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2715 - val_loss: 0.1585\n",
      "Epoch 9732/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1116 - val_loss: 0.0694\n",
      "Epoch 9733/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0610 - val_loss: 0.0130\n",
      "Epoch 9734/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0514 - val_loss: 0.1252\n",
      "Epoch 9735/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0575 - val_loss: 0.0017\n",
      "Epoch 9736/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0142 - val_loss: 0.0039\n",
      "Epoch 9737/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0085 - val_loss: 0.0058\n",
      "Epoch 9738/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0263 - val_loss: 0.0056\n",
      "Epoch 9739/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0206 - val_loss: 0.0035\n",
      "Epoch 9740/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0101 - val_loss: 0.0169\n",
      "Epoch 9741/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0088 - val_loss: 0.0046\n",
      "Epoch 9742/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0030 - val_loss: 0.0021\n",
      "Epoch 9743/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0038 - val_loss: 0.0066\n",
      "Epoch 9744/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0096 - val_loss: 0.0335\n",
      "Epoch 9745/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0152 - val_loss: 0.0115\n",
      "Epoch 9746/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0052 - val_loss: 0.0012\n",
      "Epoch 9747/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0036 - val_loss: 0.0056\n",
      "Epoch 9748/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0076 - val_loss: 0.0345\n",
      "Epoch 9749/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0235 - val_loss: 0.0231\n",
      "Epoch 9750/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0183 - val_loss: 0.0091\n",
      "Epoch 9751/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0488 - val_loss: 0.0034\n",
      "Epoch 9752/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0686 - val_loss: 0.0202\n",
      "Epoch 9753/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0773 - val_loss: 0.1316\n",
      "Epoch 9754/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0722 - val_loss: 0.0141\n",
      "Epoch 9755/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0164 - val_loss: 0.0091\n",
      "Epoch 9756/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0113 - val_loss: 0.0139\n",
      "Epoch 9757/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0291 - val_loss: 0.0499\n",
      "Epoch 9758/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0310 - val_loss: 0.0522\n",
      "Epoch 9759/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0585 - val_loss: 0.2040\n",
      "Epoch 9760/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1590 - val_loss: 0.3647\n",
      "Epoch 9761/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1628 - val_loss: 0.1454\n",
      "Epoch 9762/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0737 - val_loss: 0.0680\n",
      "Epoch 9763/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0872 - val_loss: 0.0418\n",
      "Epoch 9764/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0304 - val_loss: 0.0119\n",
      "Epoch 9765/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0082 - val_loss: 0.0031\n",
      "Epoch 9766/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0071 - val_loss: 0.0140\n",
      "Epoch 9767/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0201 - val_loss: 0.0412\n",
      "Epoch 9768/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0288 - val_loss: 0.0474\n",
      "Epoch 9769/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0399 - val_loss: 0.0089\n",
      "Epoch 9770/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0083 - val_loss: 0.0459\n",
      "Epoch 9771/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0529 - val_loss: 0.1413\n",
      "Epoch 9772/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1166 - val_loss: 0.0497\n",
      "Epoch 9773/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2319 - val_loss: 0.0383\n",
      "Epoch 9774/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1208 - val_loss: 0.0639\n",
      "Epoch 9775/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1442 - val_loss: 0.1948\n",
      "Epoch 9776/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1022 - val_loss: 0.2367\n",
      "Epoch 9777/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2711 - val_loss: 0.2204\n",
      "Epoch 9778/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5798 - val_loss: 0.0493\n",
      "Epoch 9779/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7560 - val_loss: 1.2081\n",
      "Epoch 9780/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4964 - val_loss: 0.5287\n",
      "Epoch 9781/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7011 - val_loss: 0.9139\n",
      "Epoch 9782/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.9913 - val_loss: 2.0590\n",
      "Epoch 9783/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7496 - val_loss: 1.5680\n",
      "Epoch 9784/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4554 - val_loss: 0.0833\n",
      "Epoch 9785/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4531 - val_loss: 0.2606\n",
      "Epoch 9786/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9629 - val_loss: 0.4575\n",
      "Epoch 9787/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0234 - val_loss: 1.3438\n",
      "Epoch 9788/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5141 - val_loss: 0.2233\n",
      "Epoch 9789/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1812 - val_loss: 0.0721\n",
      "Epoch 9790/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1098 - val_loss: 0.1430\n",
      "Epoch 9791/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0636 - val_loss: 0.0485\n",
      "Epoch 9792/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0361 - val_loss: 0.0109\n",
      "Epoch 9793/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0752 - val_loss: 0.3165\n",
      "Epoch 9794/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2261 - val_loss: 0.0440\n",
      "Epoch 9795/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0702 - val_loss: 0.0580\n",
      "Epoch 9796/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0238 - val_loss: 0.0646\n",
      "Epoch 9797/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0209 - val_loss: 0.0011\n",
      "Epoch 9798/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0050 - val_loss: 0.0011\n",
      "Epoch 9799/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.7844e-04 - val_loss: 0.0014\n",
      "Epoch 9800/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0039 - val_loss: 0.0031\n",
      "Epoch 9801/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0072 - val_loss: 0.0020\n",
      "Epoch 9802/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 9803/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0014 - val_loss: 1.2317e-04\n",
      "Epoch 9804/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.3600e-04 - val_loss: 0.0028\n",
      "Epoch 9805/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0031 - val_loss: 0.0116\n",
      "Epoch 9806/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 9807/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0035 - val_loss: 8.7857e-04\n",
      "Epoch 9808/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0054 - val_loss: 0.0189\n",
      "Epoch 9809/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0074 - val_loss: 0.0016\n",
      "Epoch 9810/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0110 - val_loss: 4.5217e-04\n",
      "Epoch 9811/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0017 - val_loss: 1.3375e-04\n",
      "Epoch 9812/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0045 - val_loss: 0.0112\n",
      "Epoch 9813/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0233 - val_loss: 0.0943\n",
      "Epoch 9814/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1678 - val_loss: 0.7605\n",
      "Epoch 9815/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4535 - val_loss: 0.4658\n",
      "Epoch 9816/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2860 - val_loss: 0.0210\n",
      "Epoch 9817/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2674 - val_loss: 1.0214\n",
      "Epoch 9818/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1179 - val_loss: 0.5332\n",
      "Epoch 9819/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9019 - val_loss: 0.2203\n",
      "Epoch 9820/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4407 - val_loss: 0.4809\n",
      "Epoch 9821/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.6799 - val_loss: 0.2565\n",
      "Epoch 9822/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2417 - val_loss: 0.2641\n",
      "Epoch 9823/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.2137 - val_loss: 0.0638\n",
      "Epoch 9824/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0652 - val_loss: 0.0656\n",
      "Epoch 9825/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0958 - val_loss: 0.0988\n",
      "Epoch 9826/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3036 - val_loss: 0.8099\n",
      "Epoch 9827/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5995 - val_loss: 0.8182\n",
      "Epoch 9828/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.9674 - val_loss: 0.1981\n",
      "Epoch 9829/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.0209 - val_loss: 83.9345\n",
      "Epoch 9830/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 74.2066 - val_loss: 549.0258\n",
      "Epoch 9831/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2158.4767 - val_loss: 1976.1120\n",
      "Epoch 9832/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8067.9898 - val_loss: 29817.9082\n",
      "Epoch 9833/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18615.4436 - val_loss: 2733.7944\n",
      "Epoch 9834/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13290.3965 - val_loss: 9147.1267\n",
      "Epoch 9835/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13765.1343 - val_loss: 21013.5172\n",
      "Epoch 9836/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 7527.6101 - val_loss: 514.8406\n",
      "Epoch 9837/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2820.8971 - val_loss: 4086.0995\n",
      "Epoch 9838/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2297.2350 - val_loss: 174.8484\n",
      "Epoch 9839/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1826.3919 - val_loss: 2884.4774\n",
      "Epoch 9840/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1395.8587 - val_loss: 681.2553\n",
      "Epoch 9841/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 965.8468 - val_loss: 978.5982\n",
      "Epoch 9842/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 677.4481 - val_loss: 75.2864\n",
      "Epoch 9843/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 269.7173 - val_loss: 271.6711\n",
      "Epoch 9844/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 127.1957 - val_loss: 29.5855\n",
      "Epoch 9845/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 47.8254 - val_loss: 12.2692\n",
      "Epoch 9846/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 10.0245 - val_loss: 8.2676\n",
      "Epoch 9847/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 191us/sample - loss: 6.9934 - val_loss: 6.0064\n",
      "Epoch 9848/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.8587 - val_loss: 2.8059\n",
      "Epoch 9849/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.6664 - val_loss: 0.9529\n",
      "Epoch 9850/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3148 - val_loss: 1.2060\n",
      "Epoch 9851/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0452 - val_loss: 0.2133\n",
      "Epoch 9852/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5751 - val_loss: 0.0016\n",
      "Epoch 9853/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5461 - val_loss: 0.1904\n",
      "Epoch 9854/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4240 - val_loss: 0.0441\n",
      "Epoch 9855/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3303 - val_loss: 0.0742\n",
      "Epoch 9856/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2796 - val_loss: 0.0027\n",
      "Epoch 9857/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2143 - val_loss: 0.0092\n",
      "Epoch 9858/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2015 - val_loss: 0.0047\n",
      "Epoch 9859/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1717 - val_loss: 0.0096\n",
      "Epoch 9860/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1640 - val_loss: 0.0036\n",
      "Epoch 9861/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1550 - val_loss: 0.0058\n",
      "Epoch 9862/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1322 - val_loss: 0.0026\n",
      "Epoch 9863/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1185 - val_loss: 0.0039\n",
      "Epoch 9864/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1157 - val_loss: 0.0051\n",
      "Epoch 9865/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1014 - val_loss: 0.0040\n",
      "Epoch 9866/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0903 - val_loss: 0.0111\n",
      "Epoch 9867/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0857 - val_loss: 0.0029\n",
      "Epoch 9868/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0770 - val_loss: 0.0053\n",
      "Epoch 9869/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0699 - val_loss: 0.0051\n",
      "Epoch 9870/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0692 - val_loss: 0.0105\n",
      "Epoch 9871/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0631 - val_loss: 0.0015\n",
      "Epoch 9872/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0542 - val_loss: 0.0052\n",
      "Epoch 9873/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0477 - val_loss: 0.0046\n",
      "Epoch 9874/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0470 - val_loss: 0.0041\n",
      "Epoch 9875/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0407 - val_loss: 0.0067\n",
      "Epoch 9876/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0368 - val_loss: 0.0027\n",
      "Epoch 9877/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0324 - val_loss: 0.0041\n",
      "Epoch 9878/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0295 - val_loss: 0.0020\n",
      "Epoch 9879/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0280 - val_loss: 0.0023\n",
      "Epoch 9880/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0243 - val_loss: 0.0014\n",
      "Epoch 9881/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0213 - val_loss: 0.0024\n",
      "Epoch 9882/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0205 - val_loss: 0.0015\n",
      "Epoch 9883/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0175 - val_loss: 0.0020\n",
      "Epoch 9884/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0166 - val_loss: 0.0013\n",
      "Epoch 9885/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0155 - val_loss: 0.0012\n",
      "Epoch 9886/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0139 - val_loss: 9.8624e-04\n",
      "Epoch 9887/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0131 - val_loss: 0.0010\n",
      "Epoch 9888/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0116 - val_loss: 0.0024\n",
      "Epoch 9889/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0119 - val_loss: 0.0049\n",
      "Epoch 9890/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0122 - val_loss: 0.0032\n",
      "Epoch 9891/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0097 - val_loss: 0.0010\n",
      "Epoch 9892/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0078 - val_loss: 0.0011\n",
      "Epoch 9893/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0071 - val_loss: 0.0011\n",
      "Epoch 9894/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0066 - val_loss: 0.0011\n",
      "Epoch 9895/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0056 - val_loss: 9.1544e-04\n",
      "Epoch 9896/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0053 - val_loss: 0.0018\n",
      "Epoch 9897/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0050 - val_loss: 0.0012\n",
      "Epoch 9898/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0044 - val_loss: 0.0012\n",
      "Epoch 9899/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0048 - val_loss: 0.0015\n",
      "Epoch 9900/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0043 - val_loss: 0.0011\n",
      "Epoch 9901/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0038 - val_loss: 0.0019\n",
      "Epoch 9902/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0053 - val_loss: 0.0089\n",
      "Epoch 9903/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0069 - val_loss: 0.0046\n",
      "Epoch 9904/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0043 - val_loss: 0.0021\n",
      "Epoch 9905/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0029 - val_loss: 0.0018\n",
      "Epoch 9906/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0024 - val_loss: 9.1872e-04\n",
      "Epoch 9907/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0023 - val_loss: 0.0011\n",
      "Epoch 9908/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 9909/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0030 - val_loss: 0.0016\n",
      "Epoch 9910/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0025 - val_loss: 8.7833e-04\n",
      "Epoch 9911/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 9912/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 9913/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0014 - val_loss: 7.0583e-04\n",
      "Epoch 9914/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 9915/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0013 - val_loss: 7.4429e-04\n",
      "Epoch 9916/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 7.5788e-04\n",
      "Epoch 9917/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 7.3025e-04\n",
      "Epoch 9918/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 9919/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 9920/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0023 - val_loss: 0.0035\n",
      "Epoch 9921/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0021 - val_loss: 7.7649e-04\n",
      "Epoch 9922/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.9559e-04 - val_loss: 0.0011\n",
      "Epoch 9923/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0011 - val_loss: 6.3852e-04\n",
      "Epoch 9924/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.2793e-04 - val_loss: 9.7293e-04\n",
      "Epoch 9925/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 9926/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 9927/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 9928/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 9929/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0014 - val_loss: 9.8607e-04\n",
      "Epoch 9930/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 9931/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0020 - val_loss: 8.0132e-04\n",
      "Epoch 9932/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.0010 - val_loss: 6.9573e-04\n",
      "Epoch 9933/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 9934/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0014 - val_loss: 6.6669e-04\n",
      "Epoch 9935/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.1957e-04 - val_loss: 7.2035e-04\n",
      "Epoch 9936/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.5410e-04 - val_loss: 7.9115e-04\n",
      "Epoch 9937/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.2476e-04 - val_loss: 6.0062e-04\n",
      "Epoch 9938/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.3985e-04 - val_loss: 6.8870e-04\n",
      "Epoch 9939/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 7.4121e-04 - val_loss: 6.3434e-04\n",
      "Epoch 9940/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0022 - val_loss: 0.0018\n",
      "Epoch 9941/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0012 - val_loss: 0.0020\n",
      "Epoch 9942/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 9943/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 9944/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0018 - val_loss: 0.0032\n",
      "Epoch 9945/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 9946/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 9947/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0031 - val_loss: 0.0037\n",
      "Epoch 9948/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0032 - val_loss: 0.0058\n",
      "Epoch 9949/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0040 - val_loss: 0.0085\n",
      "Epoch 9950/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0054 - val_loss: 0.0141\n",
      "Epoch 9951/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0086 - val_loss: 0.0262\n",
      "Epoch 9952/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0132 - val_loss: 0.0084\n",
      "Epoch 9953/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0054 - val_loss: 0.0059\n",
      "Epoch 9954/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 9955/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0031 - val_loss: 0.0036\n",
      "Epoch 9956/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 9957/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 9958/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 9959/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.9660e-04 - val_loss: 0.0020\n",
      "Epoch 9960/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 4.8124e-04\n",
      "Epoch 9961/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.1736e-04 - val_loss: 4.8250e-04\n",
      "Epoch 9962/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.2697e-04 - val_loss: 9.3743e-04\n",
      "Epoch 9963/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.5965e-04 - val_loss: 3.4679e-04\n",
      "Epoch 9964/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.4337e-04 - val_loss: 3.9748e-04\n",
      "Epoch 9965/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.8527e-04 - val_loss: 6.9018e-04\n",
      "Epoch 9966/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.9829e-04 - val_loss: 3.2736e-04\n",
      "Epoch 9967/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.7663e-04 - val_loss: 5.3158e-04\n",
      "Epoch 9968/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4713e-04 - val_loss: 6.1856e-04\n",
      "Epoch 9969/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.6039e-04 - val_loss: 8.3255e-04\n",
      "Epoch 9970/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.1678e-04 - val_loss: 0.0020\n",
      "Epoch 9971/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0023 - val_loss: 0.0036\n",
      "Epoch 9972/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0020 - val_loss: 3.8122e-04\n",
      "Epoch 9973/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.5501e-04 - val_loss: 3.7362e-04\n",
      "Epoch 9974/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.4914e-04 - val_loss: 4.4957e-04\n",
      "Epoch 9975/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.0348e-04 - val_loss: 4.4862e-04\n",
      "Epoch 9976/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.8966e-04 - val_loss: 7.0134e-04\n",
      "Epoch 9977/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.7526e-04 - val_loss: 2.7308e-04\n",
      "Epoch 9978/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.9467e-04 - val_loss: 3.8799e-04\n",
      "Epoch 9979/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.7254e-04 - val_loss: 5.1685e-04\n",
      "Epoch 9980/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.6601e-04 - val_loss: 3.7247e-04\n",
      "Epoch 9981/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1103e-04 - val_loss: 8.5130e-04\n",
      "Epoch 9982/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0010 - val_loss: 6.3892e-04\n",
      "Epoch 9983/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 9984/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.2878e-04 - val_loss: 0.0012\n",
      "Epoch 9985/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.5461e-04 - val_loss: 6.6202e-04\n",
      "Epoch 9986/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.3320e-04 - val_loss: 5.8261e-04\n",
      "Epoch 9987/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.9477e-04 - val_loss: 4.8243e-04\n",
      "Epoch 9988/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.3517e-04 - val_loss: 4.4756e-04\n",
      "Epoch 9989/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.3408e-04 - val_loss: 2.5401e-04\n",
      "Epoch 9990/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3.3397e-04 - val_loss: 3.6541e-04\n",
      "Epoch 9991/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.1015e-04 - val_loss: 0.0011\n",
      "Epoch 9992/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5.6589e-04 - val_loss: 9.6822e-04\n",
      "Epoch 9993/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.6376e-04 - val_loss: 3.1562e-04\n",
      "Epoch 9994/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.1480e-04 - val_loss: 2.9818e-04\n",
      "Epoch 9995/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 4.5270e-04 - val_loss: 2.3843e-04\n",
      "Epoch 9996/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.2363e-04 - val_loss: 8.2251e-04\n",
      "Epoch 9997/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.8317e-04 - val_loss: 0.0015\n",
      "Epoch 9998/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0012 - val_loss: 7.1901e-04\n",
      "Epoch 9999/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.5587e-04 - val_loss: 2.3084e-04\n",
      "Epoch 10000/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.7538e-04 - val_loss: 4.1941e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xf7601d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(5, activation='relu', input_shape=(2,)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer=Adam(lr=0.01), loss='mse')\n",
    "model.fit(X_train, Y_train, validation_split = 0.2, batch_size=10, epochs=10000)\n",
    "# Note: sometimes this works with 5 nodes in the first layer, sometimes it doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "15/15 [==============================] - 0s 67us/sample - loss: 2.9126e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00029125611763447523"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict = model.predict(X_test)\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_test[:,0], Y_test, linestyle='none', marker='.', color='green', label='Test data')\n",
    "plt.plot(X_test[:,0], Y_predict, linestyle='none', marker='x', color='red', label='Prediction')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_test[:,1], Y_test, linestyle='none', marker='.', color='green', label='Test data')\n",
    "plt.plot(X_test[:,1], Y_predict, linestyle='none', marker='x', color='red', label='Prediction')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 8.628742  , -0.3436197 , -0.26067543, -0.57977366, -0.36334723],\n",
      "       [ 2.2679458 ,  5.9586544 , -0.272936  , -0.0180831 , -0.74057484]],\n",
      "      dtype=float32), array([6.8550544, 9.344908 , 0.       , 0.       , 0.       ],\n",
      "      dtype=float32)]\n",
      "[array([[ 6.7663417 ],\n",
      "       [-7.6100593 ],\n",
      "       [-0.8818157 ],\n",
      "       [ 0.09153748],\n",
      "       [ 0.90168595]], dtype=float32), array([3.7669234], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.get_weights())\n",
    "\n",
    "#blob1 = np.dot(X_test[0,:], model.layers[0].get_weights()[0][0]) + model.layers[0].get_weights()[1][0]\n",
    "#blob1 = np.dot(X_test[0,:], model.layers[0].get_weights()[0][1]) + model.layers[0].get_weights()[1][1]\n",
    "#blob3 = blob1*model.layers[1].get_weights()[0][0] + blob2*model.layers[1].get_weights()[0][1] + model.layers[1].get_weights()[1]\n",
    "#print(blob3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, '$y$')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=p.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.scatter(xs=X_test[:,0], ys=X_test[:,1], zs=Y_test, zdir='z', s=20, c=None, depthshade=True, color='blue')\n",
    "ax.scatter(xs=X_test[:,0], ys=X_test[:,1], zs=Y_predict, zdir='z', s=20, c=None, depthshade=True, color='red')\n",
    "#ax.legend()\n",
    "\n",
    "ax.set_xlabel('$x1$', fontsize=20)\n",
    "ax.set_ylabel('$x2$', fontsize=20)\n",
    "#ax.yaxis._axinfo['label']['space_factor'] = 3.0\n",
    "# set z ticks and labels\n",
    "#ax.set_zticks([-2, 0, 2])\n",
    "# change fontsize\n",
    "#for t in ax.zaxis.get_major_ticks(): t.label.set_fontsize(10)\n",
    "# disable auto rotation\n",
    "ax.xaxis.set_rotate_label(False)\n",
    "ax.yaxis.set_rotate_label(False)\n",
    "ax.zaxis.set_rotate_label(False) \n",
    "ax.set_zlabel('$y$', fontsize=30, rotation = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []\n",
    "\n",
    "for h in range(40):\n",
    "    print(h)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, activation='relu', input_shape=(2,)))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=Adam(lr=0.01), metrics=['accuracy'], loss='mse')\n",
    "    history = model.fit(X_train, Y_train, validation_split = 0.2, batch_size=10, epochs=2000)\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "#g = sns.FacetGrid(att, col=\"subject\", col_wrap=5, height=1.5)\n",
    "for j in range(40):\n",
    "    history = histories[j]\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'History' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-4e5cd743a3d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ticks\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor_codes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFacetGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistories\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_wrap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda32019\\lib\\site-packages\\seaborn\\axisgrid.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, row, col, hue, col_wrap, sharex, sharey, height, aspect, palette, row_order, col_order, hue_order, hue_kws, dropna, legend_out, despine, margin_titles, xlim, ylim, subplot_kws, gridspec_kws, size)\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[0mcol_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mcol_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;31m# Additional dict of kwarg -> list of values for mapping the hue var\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'History' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "g = sns.FacetGrid(histories[0], col=\"loss\", col_wrap=5, height=1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.183190864675185, 14.574544345631319, 18.806698406443875, 22.753503673216876, 35.60586995237014, 38.56389643164242, 43.10318498050465, 46.17045155693503, 46.19259183547076, 57.276226941277, 58.81032410789938, 64.37696501787971, 68.04124029944924, 69.97638421900132, 79.39875670040355, 84.1803258447086, 84.63603165570427, 87.06439887776094, 87.16400976742015, 89.68673481660731, 91.23650225471047, 92.74423015818877, 98.9371300865622, 39986230.18014706, 39987903.48529412, 39988730.96438419, 39989510.63235294, 39990506.676470585, 39994626.40441176, 39995180.86580882, 39996378.78308824, 39998162.2757353, 40000838.615033485, 40003700.73529412, 40007881.698529415, 40010691.268437326, 40010995.36029412, 40012022.551470585, 610097498.3529412, 610098916.7058823]\n"
     ]
    }
   ],
   "source": [
    "at1000 = []\n",
    "for j in range(40):\n",
    "    at1000.append(histories[j].history['loss'][1000])\n",
    "at1000.sort()\n",
    "print(at1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
