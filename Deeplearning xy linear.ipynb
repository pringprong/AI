{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow.keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "import random\n",
    "import math\n",
    "from datetime import datetime\n",
    "from numpy import *\n",
    "import pylab as p\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "array_len = 100\n",
    "range_width = 1000\n",
    "\n",
    "first_input = np.empty(array_len)\n",
    "second_input = np.empty(array_len)\n",
    "for i in range(array_len):\n",
    "  first_input[i] = random.randint(0, range_width)\n",
    "  second_input[i] = random.randint(0, range_width)\n",
    "\n",
    "x = np.vstack((first_input, second_input)).T\n",
    "x1 = first_input*2 + second_input*3 + 4\n",
    "x2 = first_input*5 - second_input*6 - 7\n",
    "y = x1*8+ x2*9 + 10\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#training_frac = 0.85\n",
    "#train_max_index = math.floor(array_len * training_frac)\n",
    "#X_train = x[:train_max_index,:]\n",
    "#Y_train = y[:train_max_index]\n",
    "#X_test = x[train_max_index:,:]\n",
    "#Y_test = y[train_max_index:]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.15)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68 samples, validate on 17 samples\n",
      "Epoch 1/10000\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 611420895.0588 - val_loss: 719597180.2353\n",
      "Epoch 2/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 610129468.2353 - val_loss: 718468999.5294\n",
      "Epoch 3/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 609344768.9412 - val_loss: 717611256.4706\n",
      "Epoch 4/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 608594864.9412 - val_loss: 716668839.5294\n",
      "Epoch 5/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 607382948.2353 - val_loss: 715261475.7647\n",
      "Epoch 6/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 605858421.6471 - val_loss: 713303160.4706\n",
      "Epoch 7/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 603469476.7059 - val_loss: 710716826.3529\n",
      "Epoch 8/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 600508111.0588 - val_loss: 707418066.8235\n",
      "Epoch 9/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 596745520.9412 - val_loss: 703471239.5294\n",
      "Epoch 10/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 592105072.9412 - val_loss: 698820562.8235\n",
      "Epoch 11/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 586754444.2353 - val_loss: 693358972.2353\n",
      "Epoch 12/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 580508563.7647 - val_loss: 686999698.8235\n",
      "Epoch 13/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 573894505.4118 - val_loss: 679421822.1176\n",
      "Epoch 14/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 564924571.7647 - val_loss: 670864282.3529\n",
      "Epoch 15/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 555162186.3529 - val_loss: 659967465.4118\n",
      "Epoch 16/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 544211833.4118 - val_loss: 646843038.1176\n",
      "Epoch 17/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 529752871.5294 - val_loss: 632782708.7059\n",
      "Epoch 18/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 514892991.0588 - val_loss: 617419584.0000\n",
      "Epoch 19/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 497972238.1176 - val_loss: 601495186.8235\n",
      "Epoch 20/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 482153636.2353 - val_loss: 584366012.2353\n",
      "Epoch 21/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 463518047.0588 - val_loss: 567801596.2353\n",
      "Epoch 22/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 446546277.6471 - val_loss: 550920256.9412\n",
      "Epoch 23/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 428551388.7059 - val_loss: 534182384.9412\n",
      "Epoch 24/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 410414308.7059 - val_loss: 517797010.8235\n",
      "Epoch 25/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 393665398.1176 - val_loss: 501491260.2353\n",
      "Epoch 26/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 378615521.4118 - val_loss: 485300092.2353\n",
      "Epoch 27/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 360148778.3529 - val_loss: 470689156.7059\n",
      "Epoch 28/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 344905376.0000 - val_loss: 456001565.1765\n",
      "Epoch 29/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 331577583.0588 - val_loss: 440735045.6471\n",
      "Epoch 30/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 316426214.1176 - val_loss: 426332963.7647\n",
      "Epoch 31/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 302329182.1176 - val_loss: 411913746.8235\n",
      "Epoch 32/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 290172566.1176 - val_loss: 396991433.4118\n",
      "Epoch 33/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 276217391.5294 - val_loss: 382300272.9412\n",
      "Epoch 34/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 263850294.3529 - val_loss: 367439408.9412\n",
      "Epoch 35/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 251344805.1765 - val_loss: 352924777.4118\n",
      "Epoch 36/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 239596620.2353 - val_loss: 338433731.7647\n",
      "Epoch 37/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 227103892.7059 - val_loss: 324472152.4706\n",
      "Epoch 38/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 216341117.8824 - val_loss: 309892722.3529\n",
      "Epoch 39/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 205884144.0000 - val_loss: 295118023.5294\n",
      "Epoch 40/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 194543271.7647 - val_loss: 280781594.3529\n",
      "Epoch 41/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 183789354.8235 - val_loss: 266817507.7647\n",
      "Epoch 42/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 172768846.1176 - val_loss: 252913406.1176\n",
      "Epoch 43/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 163282682.7059 - val_loss: 239298388.7059\n",
      "Epoch 44/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 153100628.2353 - val_loss: 226404310.5882\n",
      "Epoch 45/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 143363351.2941 - val_loss: 213652400.9412\n",
      "Epoch 46/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 134797700.3529 - val_loss: 200713992.4706\n",
      "Epoch 47/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 125191153.0588 - val_loss: 188932528.9412\n",
      "Epoch 48/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 116811183.7647 - val_loss: 177338315.5294\n",
      "Epoch 49/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 108587229.4118 - val_loss: 165773804.2353\n",
      "Epoch 50/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 100525270.1176 - val_loss: 155321799.5294\n",
      "Epoch 51/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 92901895.6471 - val_loss: 145093517.1765\n",
      "Epoch 52/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 85967151.4118 - val_loss: 134959525.6471\n",
      "Epoch 53/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 79369117.0588 - val_loss: 125442581.6471\n",
      "Epoch 54/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 72852896.0000 - val_loss: 116614656.0000\n",
      "Epoch 55/10000\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 66690479.2353 - val_loss: 108223488.9412\n",
      "Epoch 56/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 61176615.1765 - val_loss: 100228341.6471\n",
      "Epoch 57/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 56042039.7647 - val_loss: 92824014.5882\n",
      "Epoch 58/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 51072957.4706 - val_loss: 85685209.1765\n",
      "Epoch 59/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 46097343.1176 - val_loss: 79303162.3529\n",
      "Epoch 60/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 41943732.9412 - val_loss: 73250627.2353\n",
      "Epoch 61/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 38306940.3529 - val_loss: 67456565.8824\n",
      "Epoch 62/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 34771843.4412 - val_loss: 61913227.2941\n",
      "Epoch 63/10000\n",
      "68/68 [==============================] - 0s 941us/sample - loss: 31111360.4706 - val_loss: 57092143.2941\n",
      "Epoch 64/10000\n",
      "68/68 [==============================] - 0s 706us/sample - loss: 28301624.5294 - val_loss: 52508021.6471\n",
      "Epoch 65/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 25441706.9706 - val_loss: 48332869.4118\n",
      "Epoch 66/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 23174637.9412 - val_loss: 44279637.6471\n",
      "Epoch 67/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 20763920.8235 - val_loss: 40781972.9412\n",
      "Epoch 68/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 191us/sample - loss: 18811878.1471 - val_loss: 37513559.5294\n",
      "Epoch 69/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 16965121.4706 - val_loss: 34570844.0000\n",
      "Epoch 70/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 6557802.500 - 0s 353us/sample - loss: 15332101.6029 - val_loss: 31878844.9412\n",
      "Epoch 71/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 13821012.6544 - val_loss: 29512335.7059\n",
      "Epoch 72/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 12685916.0221 - val_loss: 27184978.1176\n",
      "Epoch 73/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 11349816.1029 - val_loss: 25248824.7059\n",
      "Epoch 74/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 5873067.500 - 0s 368us/sample - loss: 10441510.6654 - val_loss: 23294755.4118\n",
      "Epoch 75/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 9457406.2353 - val_loss: 21603321.6176\n",
      "Epoch 76/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8580544.5368 - val_loss: 20102114.4118\n",
      "Epoch 77/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 7823625.9246 - val_loss: 18760150.5487\n",
      "Epoch 78/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7238850.2610 - val_loss: 17447881.4118\n",
      "Epoch 79/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6688074.3906 - val_loss: 16233595.9926\n",
      "Epoch 80/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6096976.4779 - val_loss: 15229094.7500\n",
      "Epoch 81/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5678032.9435 - val_loss: 14264608.1765\n",
      "Epoch 82/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5238028.0797 - val_loss: 13432396.3235\n",
      "Epoch 83/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4884330.2358 - val_loss: 12633580.8235\n",
      "Epoch 84/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4531083.7269 - val_loss: 11909680.7941\n",
      "Epoch 85/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4305257.2315 - val_loss: 11140133.5551\n",
      "Epoch 86/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3965896.0048 - val_loss: 10555784.5882\n",
      "Epoch 87/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3756519.9449 - val_loss: 9961270.8824\n",
      "Epoch 88/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3537976.3022 - val_loss: 9423419.8529\n",
      "Epoch 89/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3338956.2803 - val_loss: 8916469.8118\n",
      "Epoch 90/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3123574.2151 - val_loss: 8524591.3051\n",
      "Epoch 91/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 2973470.1893 - val_loss: 8112749.0846\n",
      "Epoch 92/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2806956.0129 - val_loss: 7721909.8824\n",
      "Epoch 93/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2645547.3346 - val_loss: 7392323.9412\n",
      "Epoch 94/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2516684.1264 - val_loss: 7052153.2500\n",
      "Epoch 95/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2387466.9095 - val_loss: 6741325.7059\n",
      "Epoch 96/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2272310.7013 - val_loss: 6451597.2059\n",
      "Epoch 97/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2174055.6719 - val_loss: 6144428.4701\n",
      "Epoch 98/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2059615.5322 - val_loss: 5876786.4710\n",
      "Epoch 99/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1950482.6199 - val_loss: 5677356.7353\n",
      "Epoch 100/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1864836.6521 - val_loss: 5451267.7941\n",
      "Epoch 101/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1801199.7142 - val_loss: 5165765.2941\n",
      "Epoch 102/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1696073.5386 - val_loss: 4957279.5800\n",
      "Epoch 103/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1627818.9405 - val_loss: 4754759.6765\n",
      "Epoch 104/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1563303.4720 - val_loss: 4552002.2495\n",
      "Epoch 105/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1483483.5512 - val_loss: 4418505.7702\n",
      "Epoch 106/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1434738.8479 - val_loss: 4243619.4706\n",
      "Epoch 107/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1369000.9099 - val_loss: 4113925.9412\n",
      "Epoch 108/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 1321295.3849 - val_loss: 3954446.4412\n",
      "Epoch 109/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1265797.6926 - val_loss: 3825931.7652\n",
      "Epoch 110/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1223048.5577 - val_loss: 3680535.0009\n",
      "Epoch 111/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 1176609.1744 - val_loss: 3536318.0000\n",
      "Epoch 112/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1127289.4822 - val_loss: 3416732.3382\n",
      "Epoch 113/10000\n",
      "68/68 [==============================] - 0s 544us/sample - loss: 1087845.0859 - val_loss: 3302119.4779\n",
      "Epoch 114/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 1047098.5201 - val_loss: 3212897.6765\n",
      "Epoch 115/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1013626.2893 - val_loss: 3092413.9283\n",
      "Epoch 116/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 970123.8378 - val_loss: 3008624.3235\n",
      "Epoch 117/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 936953.2029 - val_loss: 2909708.3382\n",
      "Epoch 118/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 905402.5701 - val_loss: 2803874.9265\n",
      "Epoch 119/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 877719.3396 - val_loss: 2692363.6434\n",
      "Epoch 120/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 843912.8308 - val_loss: 2604428.7158\n",
      "Epoch 121/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 814404.0365 - val_loss: 2530906.6471\n",
      "Epoch 122/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 792046.8585 - val_loss: 2449158.8952\n",
      "Epoch 123/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 765637.2930 - val_loss: 2376090.9559\n",
      "Epoch 124/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 739135.7826 - val_loss: 2321424.1749\n",
      "Epoch 125/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 717794.0924 - val_loss: 2252843.4412\n",
      "Epoch 126/10000\n",
      "68/68 [==============================] - 0s 588us/sample - loss: 695716.2525 - val_loss: 2185177.0147\n",
      "Epoch 127/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 674396.8609 - val_loss: 2119901.7932\n",
      "Epoch 128/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 652863.7269 - val_loss: 2050045.2516\n",
      "Epoch 129/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 631444.1814 - val_loss: 1988288.4136\n",
      "Epoch 130/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 615822.0142 - val_loss: 1920308.6098\n",
      "Epoch 131/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 594527.7547 - val_loss: 1865807.5147\n",
      "Epoch 132/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 576424.0859 - val_loss: 1812611.4444\n",
      "Epoch 133/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 558884.1242 - val_loss: 1765711.3971\n",
      "Epoch 134/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 545738.0372 - val_loss: 1704100.1313\n",
      "Epoch 135/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 527938.5418 - val_loss: 1650110.9412\n",
      "Epoch 136/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 509552.2425 - val_loss: 1610899.2132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 497549.6675 - val_loss: 1556399.3032\n",
      "Epoch 138/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 483589.9460 - val_loss: 1501288.3603\n",
      "Epoch 139/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 464523.3168 - val_loss: 1463537.8631\n",
      "Epoch 140/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 451733.8403 - val_loss: 1424085.1137\n",
      "Epoch 141/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 440952.5384 - val_loss: 1374828.9975\n",
      "Epoch 142/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 426143.6246 - val_loss: 1333983.3210\n",
      "Epoch 143/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 414470.6072 - val_loss: 1292301.8377\n",
      "Epoch 144/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 400362.4033 - val_loss: 1258968.7971\n",
      "Epoch 145/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 388385.0483 - val_loss: 1229389.3545\n",
      "Epoch 146/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 380400.5321 - val_loss: 1185130.1048\n",
      "Epoch 147/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 366021.6571 - val_loss: 1155791.6977\n",
      "Epoch 148/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 357481.5123 - val_loss: 1117744.0866\n",
      "Epoch 149/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 346897.6667 - val_loss: 1082135.2956\n",
      "Epoch 150/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 335603.2852 - val_loss: 1053467.4782\n",
      "Epoch 151/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 329950.6257 - val_loss: 1014032.5379\n",
      "Epoch 152/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 316492.9994 - val_loss: 991446.2169\n",
      "Epoch 153/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 309769.8580 - val_loss: 957542.2757\n",
      "Epoch 154/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 299034.4362 - val_loss: 932306.0079\n",
      "Epoch 155/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 294549.6006 - val_loss: 892779.0172\n",
      "Epoch 156/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 7785.67 - 0s 265us/sample - loss: 283397.9119 - val_loss: 866946.4293\n",
      "Epoch 157/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 277498.4110 - val_loss: 837794.3456\n",
      "Epoch 158/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 267227.5435 - val_loss: 819172.0773\n",
      "Epoch 159/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 261126.8300 - val_loss: 795092.0401\n",
      "Epoch 160/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 255747.8308 - val_loss: 767657.4228\n",
      "Epoch 161/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 247790.6075 - val_loss: 744686.9963\n",
      "Epoch 162/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 241896.9231 - val_loss: 719024.7710\n",
      "Epoch 163/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 233642.3030 - val_loss: 702211.9338\n",
      "Epoch 164/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 228824.3412 - val_loss: 678785.4180\n",
      "Epoch 165/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 222386.5280 - val_loss: 658156.0882\n",
      "Epoch 166/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 215836.3377 - val_loss: 639816.3164\n",
      "Epoch 167/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 211090.6045 - val_loss: 618012.3226\n",
      "Epoch 168/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 205332.3771 - val_loss: 599406.7426\n",
      "Epoch 169/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 199394.1427 - val_loss: 581180.2598\n",
      "Epoch 170/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 195668.1921 - val_loss: 559799.3721\n",
      "Epoch 171/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 188401.2425 - val_loss: 548002.5662\n",
      "Epoch 172/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 185749.2669 - val_loss: 524624.9301\n",
      "Epoch 173/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 178568.9256 - val_loss: 510825.6912\n",
      "Epoch 174/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 177316.5652 - val_loss: 487984.1250\n",
      "Epoch 175/10000\n",
      "68/68 [==============================] - 0s 838us/sample - loss: 169631.3882 - val_loss: 476589.2448\n",
      "Epoch 176/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 165759.8917 - val_loss: 462032.9733\n",
      "Epoch 177/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 162286.7133 - val_loss: 444474.9136\n",
      "Epoch 178/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 157779.8476 - val_loss: 430152.9996\n",
      "Epoch 179/10000\n",
      "68/68 [==============================] - 0s 735us/sample - loss: 153483.4871 - val_loss: 417407.3327\n",
      "Epoch 180/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 149615.9437 - val_loss: 405212.7482\n",
      "Epoch 181/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 146155.8612 - val_loss: 392592.2897\n",
      "Epoch 182/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 142409.8218 - val_loss: 380462.4191\n",
      "Epoch 183/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 139653.8897 - val_loss: 366939.3199\n",
      "Epoch 184/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 136410.3262 - val_loss: 353784.3191\n",
      "Epoch 185/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 133028.2276 - val_loss: 340824.9522\n",
      "Epoch 186/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 129222.3088 - val_loss: 330860.3989\n",
      "Epoch 187/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 125746.8290 - val_loss: 319727.2023\n",
      "Epoch 188/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 123321.1065 - val_loss: 307669.5355\n",
      "Epoch 189/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 120036.8438 - val_loss: 296570.9540\n",
      "Epoch 190/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 116335.0869 - val_loss: 288996.1291\n",
      "Epoch 191/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 113843.0079 - val_loss: 278011.5827\n",
      "Epoch 192/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 110827.1366 - val_loss: 267973.4412\n",
      "Epoch 193/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 108305.2192 - val_loss: 257921.3603\n",
      "Epoch 194/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 104918.4504 - val_loss: 250702.5179\n",
      "Epoch 195/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 102510.4897 - val_loss: 241745.0415\n",
      "Epoch 196/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 99986.5743 - val_loss: 233414.2500\n",
      "Epoch 197/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 97716.3380 - val_loss: 224699.0607\n",
      "Epoch 198/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 96205.5349 - val_loss: 214455.7588\n",
      "Epoch 199/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 93289.4340 - val_loss: 205402.5395\n",
      "Epoch 200/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 90248.3416 - val_loss: 200505.6159\n",
      "Epoch 201/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 88856.0323 - val_loss: 190876.8732\n",
      "Epoch 202/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 86137.7922 - val_loss: 182608.1006\n",
      "Epoch 203/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 83815.2300 - val_loss: 175877.0956\n",
      "Epoch 204/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 81711.8896 - val_loss: 171714.2858\n",
      "Epoch 205/10000\n",
      "68/68 [==============================] - 0s 941us/sample - loss: 79830.1798 - val_loss: 164230.6985\n",
      "Epoch 206/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 77756.0330 - val_loss: 156897.3061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 75720.2744 - val_loss: 151401.4955\n",
      "Epoch 208/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 74387.5567 - val_loss: 144491.0540\n",
      "Epoch 209/10000\n",
      "68/68 [==============================] - 0s 897us/sample - loss: 71788.1185 - val_loss: 140382.4017\n",
      "Epoch 210/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 71097.9062 - val_loss: 133058.1564\n",
      "Epoch 211/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 68409.2764 - val_loss: 128456.1207\n",
      "Epoch 212/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 66810.7716 - val_loss: 124263.7702\n",
      "Epoch 213/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 65748.2166 - val_loss: 117719.6106\n",
      "Epoch 214/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 63833.7568 - val_loss: 112799.8792\n",
      "Epoch 215/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 61865.8896 - val_loss: 109279.5740\n",
      "Epoch 216/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 60560.6048 - val_loss: 104935.3722\n",
      "Epoch 217/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 59175.2706 - val_loss: 99292.6898\n",
      "Epoch 218/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 57534.8942 - val_loss: 95041.5197\n",
      "Epoch 219/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 56326.6937 - val_loss: 90619.5842\n",
      "Epoch 220/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 55089.0359 - val_loss: 85167.0657\n",
      "Epoch 221/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 53176.5014 - val_loss: 81853.0283\n",
      "Epoch 222/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 52176.3275 - val_loss: 77332.9099\n",
      "Epoch 223/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 50354.4348 - val_loss: 74694.0901\n",
      "Epoch 224/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 49012.2833 - val_loss: 71340.9974\n",
      "Epoch 225/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 47797.2157 - val_loss: 67946.7408\n",
      "Epoch 226/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 46557.1983 - val_loss: 64882.3869\n",
      "Epoch 227/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 45524.8040 - val_loss: 60859.8773\n",
      "Epoch 228/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 44515.3226 - val_loss: 56820.3527\n",
      "Epoch 229/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 42852.0038 - val_loss: 54525.6305\n",
      "Epoch 230/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 1389.67 - 0s 221us/sample - loss: 41812.5961 - val_loss: 51193.6981\n",
      "Epoch 231/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 40987.1258 - val_loss: 48003.3594\n",
      "Epoch 232/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 39416.1628 - val_loss: 46141.0671\n",
      "Epoch 233/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 38317.3822 - val_loss: 43913.1587\n",
      "Epoch 234/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 37803.7293 - val_loss: 40063.9712\n",
      "Epoch 235/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 36133.3669 - val_loss: 38192.5511\n",
      "Epoch 236/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 35288.8594 - val_loss: 35956.6142\n",
      "Epoch 237/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 34484.5975 - val_loss: 33590.3221\n",
      "Epoch 238/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 33255.3601 - val_loss: 32052.8748\n",
      "Epoch 239/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 32862.6036 - val_loss: 28879.3283\n",
      "Epoch 240/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 31437.3463 - val_loss: 27278.4797\n",
      "Epoch 241/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 30664.4579 - val_loss: 25511.5723\n",
      "Epoch 242/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 29633.6927 - val_loss: 24209.0133\n",
      "Epoch 243/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 28946.5783 - val_loss: 22475.7144\n",
      "Epoch 244/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 28176.5521 - val_loss: 20903.5074\n",
      "Epoch 245/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 27730.6073 - val_loss: 19086.7980\n",
      "Epoch 246/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 26484.1667 - val_loss: 18004.7167\n",
      "Epoch 247/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 26172.4535 - val_loss: 16559.4060\n",
      "Epoch 248/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 25165.0074 - val_loss: 15415.0113\n",
      "Epoch 249/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 24489.8858 - val_loss: 14259.2890\n",
      "Epoch 250/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 23744.2501 - val_loss: 13059.5483\n",
      "Epoch 251/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 23143.9630 - val_loss: 12115.3034\n",
      "Epoch 252/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 22375.5912 - val_loss: 11327.2867\n",
      "Epoch 253/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 22016.3511 - val_loss: 10231.5296\n",
      "Epoch 254/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 21307.2591 - val_loss: 9436.8577\n",
      "Epoch 255/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 20620.7830 - val_loss: 8826.3286\n",
      "Epoch 256/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 20196.6184 - val_loss: 7978.3472\n",
      "Epoch 257/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 19510.6694 - val_loss: 7602.3529\n",
      "Epoch 258/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 19095.9152 - val_loss: 6653.4740\n",
      "Epoch 259/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18475.9552 - val_loss: 6018.6654\n",
      "Epoch 260/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 17911.0701 - val_loss: 5455.8476\n",
      "Epoch 261/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 17466.3938 - val_loss: 5038.0884\n",
      "Epoch 262/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 16842.0679 - val_loss: 4666.0725\n",
      "Epoch 263/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16466.7100 - val_loss: 4219.8608\n",
      "Epoch 264/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16112.7209 - val_loss: 3887.4786\n",
      "Epoch 265/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 15527.1472 - val_loss: 3715.8357\n",
      "Epoch 266/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 15212.4846 - val_loss: 3303.8834\n",
      "Epoch 267/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14648.9727 - val_loss: 3058.8140\n",
      "Epoch 268/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 14358.8043 - val_loss: 2886.2618\n",
      "Epoch 269/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 13847.6986 - val_loss: 2645.4709\n",
      "Epoch 270/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 13593.6413 - val_loss: 2299.8617\n",
      "Epoch 271/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 13141.5381 - val_loss: 2083.9676\n",
      "Epoch 272/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 12776.2005 - val_loss: 2067.6541\n",
      "Epoch 273/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12362.9699 - val_loss: 1738.6145\n",
      "Epoch 274/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 12054.1273 - val_loss: 1486.8414\n",
      "Epoch 275/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11855.3601 - val_loss: 1491.6844\n",
      "Epoch 276/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 11357.3187 - val_loss: 1304.4373\n",
      "Epoch 277/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11206.0528 - val_loss: 1293.8326\n",
      "Epoch 278/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 10703.8989 - val_loss: 1100.8296\n",
      "Epoch 279/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 10497.5619 - val_loss: 1086.2191\n",
      "Epoch 280/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 10134.8320 - val_loss: 1033.7285\n",
      "Epoch 281/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9901.8427 - val_loss: 899.3610\n",
      "Epoch 282/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9634.1633 - val_loss: 848.8736\n",
      "Epoch 283/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9332.2325 - val_loss: 761.3465\n",
      "Epoch 284/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 9082.9334 - val_loss: 654.7049\n",
      "Epoch 285/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 8824.9465 - val_loss: 673.9580\n",
      "Epoch 286/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 8646.6394 - val_loss: 837.8696\n",
      "Epoch 287/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 8380.9650 - val_loss: 796.7747\n",
      "Epoch 288/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 8146.1588 - val_loss: 697.4897\n",
      "Epoch 289/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 7895.9056 - val_loss: 755.8082\n",
      "Epoch 290/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 7662.7339 - val_loss: 778.2534\n",
      "Epoch 291/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 7611.3289 - val_loss: 704.2172\n",
      "Epoch 292/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 7307.7393 - val_loss: 708.6371\n",
      "Epoch 293/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 7107.7932 - val_loss: 772.9930\n",
      "Epoch 294/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 6905.8367 - val_loss: 661.4787\n",
      "Epoch 295/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 6767.5720 - val_loss: 694.4032\n",
      "Epoch 296/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 6634.6800 - val_loss: 660.7080\n",
      "Epoch 297/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 6387.3072 - val_loss: 612.8967\n",
      "Epoch 298/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6257.9201 - val_loss: 625.1465\n",
      "Epoch 299/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 6037.5383 - val_loss: 568.8207\n",
      "Epoch 300/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 5958.3873 - val_loss: 581.3381\n",
      "Epoch 301/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 5761.1288 - val_loss: 616.5655\n",
      "Epoch 302/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 5600.9092 - val_loss: 563.0910\n",
      "Epoch 303/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 5458.7419 - val_loss: 570.7371\n",
      "Epoch 304/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 5371.7545 - val_loss: 546.6339\n",
      "Epoch 305/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 5178.6744 - val_loss: 566.0169\n",
      "Epoch 306/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5000.6078 - val_loss: 522.2271\n",
      "Epoch 307/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4963.2530 - val_loss: 549.8265\n",
      "Epoch 308/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4732.4499 - val_loss: 540.5095\n",
      "Epoch 309/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4648.8011 - val_loss: 544.9136\n",
      "Epoch 310/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4450.7503 - val_loss: 452.3642\n",
      "Epoch 311/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 4389.8580 - val_loss: 531.2439\n",
      "Epoch 312/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 4228.2688 - val_loss: 523.1107\n",
      "Epoch 313/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4084.3228 - val_loss: 518.2154\n",
      "Epoch 314/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 3964.0846 - val_loss: 480.8990\n",
      "Epoch 315/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 3859.8029 - val_loss: 478.9260\n",
      "Epoch 316/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3779.2514 - val_loss: 467.9168\n",
      "Epoch 317/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3630.9944 - val_loss: 426.1980\n",
      "Epoch 318/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3540.4072 - val_loss: 419.1866\n",
      "Epoch 319/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 452.592 - 0s 206us/sample - loss: 3461.1087 - val_loss: 423.1738\n",
      "Epoch 320/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3344.9373 - val_loss: 420.5861\n",
      "Epoch 321/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 3289.3094 - val_loss: 411.3651\n",
      "Epoch 322/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3192.6763 - val_loss: 456.6739\n",
      "Epoch 323/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3073.6372 - val_loss: 395.3958\n",
      "Epoch 324/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 2997.5366 - val_loss: 399.2208\n",
      "Epoch 325/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 2909.3748 - val_loss: 399.2306\n",
      "Epoch 326/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 2853.1463 - val_loss: 410.9584\n",
      "Epoch 327/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 2750.3649 - val_loss: 375.2939\n",
      "Epoch 328/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2666.4568 - val_loss: 348.4964\n",
      "Epoch 329/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 2612.3325 - val_loss: 357.8390\n",
      "Epoch 330/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2531.5143 - val_loss: 356.1311\n",
      "Epoch 331/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2462.8997 - val_loss: 390.1626\n",
      "Epoch 332/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2387.7367 - val_loss: 364.3293\n",
      "Epoch 333/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 2364.9774 - val_loss: 393.4895\n",
      "Epoch 334/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 2261.9308 - val_loss: 369.0382\n",
      "Epoch 335/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2236.1557 - val_loss: 372.7815\n",
      "Epoch 336/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2130.7913 - val_loss: 336.0681\n",
      "Epoch 337/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2078.8278 - val_loss: 357.3439\n",
      "Epoch 338/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2022.1832 - val_loss: 316.1406\n",
      "Epoch 339/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1959.7719 - val_loss: 336.5251\n",
      "Epoch 340/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1906.2573 - val_loss: 322.3075\n",
      "Epoch 341/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1861.1072 - val_loss: 336.5257\n",
      "Epoch 342/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 1818.2111 - val_loss: 331.2122\n",
      "Epoch 343/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1753.5217 - val_loss: 327.7098\n",
      "Epoch 344/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1697.8646 - val_loss: 303.2032\n",
      "Epoch 345/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1659.5234 - val_loss: 285.2382\n",
      "Epoch 346/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 1619.9308 - val_loss: 287.6912\n",
      "Epoch 347/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 1607.5263 - val_loss: 340.5435\n",
      "Epoch 348/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 1519.3739 - val_loss: 307.4841\n",
      "Epoch 349/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1523.2622 - val_loss: 303.2703\n",
      "Epoch 350/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 1436.6918 - val_loss: 300.7210\n",
      "Epoch 351/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 1417.5178 - val_loss: 286.1333\n",
      "Epoch 352/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1370.6908 - val_loss: 293.4193\n",
      "Epoch 353/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1343.3802 - val_loss: 276.8257\n",
      "Epoch 354/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1304.6639 - val_loss: 314.6897\n",
      "Epoch 355/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 1252.7867 - val_loss: 287.6567\n",
      "Epoch 356/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1242.4032 - val_loss: 267.4027\n",
      "Epoch 357/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 1194.0147 - val_loss: 291.9875\n",
      "Epoch 358/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1168.2566 - val_loss: 284.7489\n",
      "Epoch 359/10000\n",
      "68/68 [==============================] - 0s 956us/sample - loss: 1134.1788 - val_loss: 260.8993\n",
      "Epoch 360/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1101.2699 - val_loss: 273.3861\n",
      "Epoch 361/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1071.7477 - val_loss: 271.9938\n",
      "Epoch 362/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1052.4159 - val_loss: 240.0785\n",
      "Epoch 363/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1039.8616 - val_loss: 280.1948\n",
      "Epoch 364/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 991.0206 - val_loss: 253.6611\n",
      "Epoch 365/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 964.4297 - val_loss: 251.8745\n",
      "Epoch 366/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 937.6167 - val_loss: 270.4004\n",
      "Epoch 367/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 914.7673 - val_loss: 244.9633\n",
      "Epoch 368/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 891.4620 - val_loss: 244.4976\n",
      "Epoch 369/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 870.5773 - val_loss: 243.7524\n",
      "Epoch 370/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 853.1997 - val_loss: 249.0257\n",
      "Epoch 371/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 830.8414 - val_loss: 248.4341\n",
      "Epoch 372/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 807.3233 - val_loss: 240.7174\n",
      "Epoch 373/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 787.6954 - val_loss: 222.4863\n",
      "Epoch 374/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 764.5328 - val_loss: 223.9544\n",
      "Epoch 375/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 766.3963 - val_loss: 240.0621\n",
      "Epoch 376/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 737.6099 - val_loss: 217.2394\n",
      "Epoch 377/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 730.5632 - val_loss: 236.7944\n",
      "Epoch 378/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 703.6492 - val_loss: 220.8089\n",
      "Epoch 379/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 684.3741 - val_loss: 229.7034\n",
      "Epoch 380/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 670.2064 - val_loss: 216.5038\n",
      "Epoch 381/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 655.4341 - val_loss: 236.4629\n",
      "Epoch 382/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 642.1149 - val_loss: 235.6102\n",
      "Epoch 383/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 622.2771 - val_loss: 226.4045\n",
      "Epoch 384/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 606.6891 - val_loss: 220.8296\n",
      "Epoch 385/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 600.8886 - val_loss: 227.7411\n",
      "Epoch 386/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 592.8847 - val_loss: 208.3780\n",
      "Epoch 387/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 578.3216 - val_loss: 228.7003\n",
      "Epoch 388/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 556.8836 - val_loss: 216.1987\n",
      "Epoch 389/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 554.4645 - val_loss: 201.5403\n",
      "Epoch 390/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 533.0808 - val_loss: 213.7592\n",
      "Epoch 391/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 533.4940 - val_loss: 236.6326\n",
      "Epoch 392/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 517.7908 - val_loss: 202.5457\n",
      "Epoch 393/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 507.3942 - val_loss: 211.1934\n",
      "Epoch 394/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 490.1440 - val_loss: 212.9091\n",
      "Epoch 395/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 488.2522 - val_loss: 205.9123\n",
      "Epoch 396/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 476.7394 - val_loss: 196.8433\n",
      "Epoch 397/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 466.8532 - val_loss: 204.8597\n",
      "Epoch 398/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 460.9020 - val_loss: 197.7713\n",
      "Epoch 399/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 447.0822 - val_loss: 198.4249\n",
      "Epoch 400/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 436.8090 - val_loss: 211.1073\n",
      "Epoch 401/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 433.1592 - val_loss: 211.3444\n",
      "Epoch 402/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 424.1976 - val_loss: 198.9762\n",
      "Epoch 403/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 413.9134 - val_loss: 196.5836\n",
      "Epoch 404/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 413.9919 - val_loss: 210.3418\n",
      "Epoch 405/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 400.8991 - val_loss: 199.2772\n",
      "Epoch 406/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 395.8848 - val_loss: 191.5943\n",
      "Epoch 407/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 387.9124 - val_loss: 204.9192\n",
      "Epoch 408/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 389.4534 - val_loss: 196.3117\n",
      "Epoch 409/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 379.8373 - val_loss: 213.2731\n",
      "Epoch 410/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 380.4162 - val_loss: 181.2762\n",
      "Epoch 411/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 366.4489 - val_loss: 197.6354\n",
      "Epoch 412/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 367.3600 - val_loss: 202.9223\n",
      "Epoch 413/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 361.3976 - val_loss: 184.8325\n",
      "Epoch 414/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 351.9470 - val_loss: 207.3099\n",
      "Epoch 415/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 345.5794 - val_loss: 196.7789\n",
      "Epoch 416/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 336.6459 - val_loss: 188.1161\n",
      "Epoch 417/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 333.1926 - val_loss: 187.2140\n",
      "Epoch 418/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 327.0508 - val_loss: 182.1961\n",
      "Epoch 419/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 321.0716 - val_loss: 187.3114\n",
      "Epoch 420/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 328.6836 - val_loss: 191.6182\n",
      "Epoch 421/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 312.4786 - val_loss: 183.1503\n",
      "Epoch 422/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 315.1855 - val_loss: 178.9073\n",
      "Epoch 423/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 313.7559 - val_loss: 206.4853\n",
      "Epoch 424/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 305.3104 - val_loss: 183.8510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 425/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 300.5883 - val_loss: 178.4191\n",
      "Epoch 426/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 303.4438 - val_loss: 190.2745\n",
      "Epoch 427/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 295.1773 - val_loss: 177.4012\n",
      "Epoch 428/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 289.0031 - val_loss: 177.6052\n",
      "Epoch 429/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 286.7790 - val_loss: 177.3905\n",
      "Epoch 430/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 282.8585 - val_loss: 176.9325\n",
      "Epoch 431/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 280.2024 - val_loss: 181.5613\n",
      "Epoch 432/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 277.3536 - val_loss: 174.8413\n",
      "Epoch 433/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 275.0508 - val_loss: 174.9495\n",
      "Epoch 434/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 272.9129 - val_loss: 172.0198\n",
      "Epoch 435/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 268.1756 - val_loss: 183.5076\n",
      "Epoch 436/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 268.5446 - val_loss: 178.8038\n",
      "Epoch 437/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 279.0867 - val_loss: 166.9396\n",
      "Epoch 438/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 275.2391 - val_loss: 196.1373\n",
      "Epoch 439/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 259.4301 - val_loss: 170.2323\n",
      "Epoch 440/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 264.2713 - val_loss: 168.6253\n",
      "Epoch 441/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 256.2312 - val_loss: 170.6560\n",
      "Epoch 442/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 254.3511 - val_loss: 179.7464\n",
      "Epoch 443/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 260.0594 - val_loss: 172.9057\n",
      "Epoch 444/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 248.4866 - val_loss: 176.1883\n",
      "Epoch 445/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 248.1344 - val_loss: 173.6528\n",
      "Epoch 446/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 250.8595 - val_loss: 166.7076\n",
      "Epoch 447/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 247.5774 - val_loss: 177.6803\n",
      "Epoch 448/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 248.5227 - val_loss: 167.2415\n",
      "Epoch 449/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 241.3868 - val_loss: 176.6397\n",
      "Epoch 450/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 239.8621 - val_loss: 170.6493\n",
      "Epoch 451/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 243.3635 - val_loss: 165.7626\n",
      "Epoch 452/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 239.3395 - val_loss: 172.2250\n",
      "Epoch 453/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 235.6810 - val_loss: 168.9044\n",
      "Epoch 454/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 239.3132 - val_loss: 165.8053\n",
      "Epoch 455/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 232.3286 - val_loss: 169.1396\n",
      "Epoch 456/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 248.3353 - val_loss: 181.5173\n",
      "Epoch 457/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 241.0199 - val_loss: 172.6569\n",
      "Epoch 458/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 235.4620 - val_loss: 179.5549\n",
      "Epoch 459/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 230.4077 - val_loss: 176.1678\n",
      "Epoch 460/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 225.4685 - val_loss: 164.2496\n",
      "Epoch 461/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 227.7366 - val_loss: 164.1118\n",
      "Epoch 462/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 225.0205 - val_loss: 165.7818\n",
      "Epoch 463/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 225.1881 - val_loss: 163.7918\n",
      "Epoch 464/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 222.5765 - val_loss: 163.5684\n",
      "Epoch 465/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 220.8950 - val_loss: 169.1830\n",
      "Epoch 466/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 221.8673 - val_loss: 157.8076\n",
      "Epoch 467/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 222.5953 - val_loss: 158.0850\n",
      "Epoch 468/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 217.1235 - val_loss: 166.2584\n",
      "Epoch 469/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 232.2295 - val_loss: 178.9549\n",
      "Epoch 470/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 225.0095 - val_loss: 166.3937\n",
      "Epoch 471/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 217.8644 - val_loss: 174.1736\n",
      "Epoch 472/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 219.9454 - val_loss: 169.7629\n",
      "Epoch 473/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 215.5885 - val_loss: 161.0865\n",
      "Epoch 474/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 222.3487 - val_loss: 157.1183\n",
      "Epoch 475/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 208.5991 - val_loss: 170.4197\n",
      "Epoch 476/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 217.3210 - val_loss: 172.2197\n",
      "Epoch 477/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 208.5479 - val_loss: 159.0210\n",
      "Epoch 478/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 219.3278 - val_loss: 157.3417\n",
      "Epoch 479/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 212.1386 - val_loss: 176.1293\n",
      "Epoch 480/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 212.6939 - val_loss: 157.5256\n",
      "Epoch 481/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 212.3476 - val_loss: 158.6357\n",
      "Epoch 482/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 220.6778 - val_loss: 169.5996\n",
      "Epoch 483/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 208.3321 - val_loss: 155.8820\n",
      "Epoch 484/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 210.5384 - val_loss: 156.8526\n",
      "Epoch 485/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 205.8082 - val_loss: 161.5885\n",
      "Epoch 486/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 206.2515 - val_loss: 160.2008\n",
      "Epoch 487/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 207.5876 - val_loss: 158.6452\n",
      "Epoch 488/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 205.1732 - val_loss: 164.9219\n",
      "Epoch 489/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 216.5534 - val_loss: 154.3727\n",
      "Epoch 490/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 206.1299 - val_loss: 167.4993\n",
      "Epoch 491/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 206.0449 - val_loss: 151.7199\n",
      "Epoch 492/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 204.6942 - val_loss: 154.6479\n",
      "Epoch 493/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 202.0758 - val_loss: 156.9393\n",
      "Epoch 494/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 212.3039 - val_loss: 159.2412\n",
      "Epoch 495/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 218.2655 - val_loss: 155.8537\n",
      "Epoch 496/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 196.2922 - val_loss: 172.3975\n",
      "Epoch 497/10000\n",
      "68/68 [==============================] - 0s 559us/sample - loss: 207.1682 - val_loss: 160.9273\n",
      "Epoch 498/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 200.0446 - val_loss: 152.2758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 499/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 209.8751 - val_loss: 152.6984\n",
      "Epoch 500/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 197.2512 - val_loss: 171.5202\n",
      "Epoch 501/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 205.0267 - val_loss: 154.5527\n",
      "Epoch 502/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 203.1569 - val_loss: 156.6971\n",
      "Epoch 503/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 219.4773 - val_loss: 150.9687\n",
      "Epoch 504/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 194.6274 - val_loss: 175.7292\n",
      "Epoch 505/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 212.3013 - val_loss: 152.3661\n",
      "Epoch 506/10000\n",
      "68/68 [==============================] - 0s 706us/sample - loss: 198.2888 - val_loss: 154.0449\n",
      "Epoch 507/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 204.2511 - val_loss: 150.8989\n",
      "Epoch 508/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 198.6456 - val_loss: 157.0627\n",
      "Epoch 509/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 198.9191 - val_loss: 155.7682\n",
      "Epoch 510/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 196.5951 - val_loss: 151.4421\n",
      "Epoch 511/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 211.033 - 0s 500us/sample - loss: 196.5497 - val_loss: 154.0535\n",
      "Epoch 512/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 199.5439 - val_loss: 149.5579\n",
      "Epoch 513/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 202.9085 - val_loss: 157.9932\n",
      "Epoch 514/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 201.5886 - val_loss: 149.9054\n",
      "Epoch 515/10000\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 196.0453 - val_loss: 155.1521\n",
      "Epoch 516/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 198.2626 - val_loss: 154.3295\n",
      "Epoch 517/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 194.1912 - val_loss: 153.7252\n",
      "Epoch 518/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 195.0362 - val_loss: 152.0307\n",
      "Epoch 519/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 200.3981 - val_loss: 158.0269\n",
      "Epoch 520/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 204.2217 - val_loss: 151.1995\n",
      "Epoch 521/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 205.5232 - val_loss: 163.5477\n",
      "Epoch 522/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 191.9570 - val_loss: 150.7092\n",
      "Epoch 523/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 200.9609 - val_loss: 150.9363\n",
      "Epoch 524/10000\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 193.1846 - val_loss: 146.8669\n",
      "Epoch 525/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 195.6932 - val_loss: 152.3107\n",
      "Epoch 526/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 193.2957 - val_loss: 154.5152\n",
      "Epoch 527/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 203.1583 - val_loss: 147.5889\n",
      "Epoch 528/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 197.5109 - val_loss: 158.1792\n",
      "Epoch 529/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 193.8878 - val_loss: 147.7625\n",
      "Epoch 530/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 193.7334 - val_loss: 150.3409\n",
      "Epoch 531/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 193.8977 - val_loss: 151.6592\n",
      "Epoch 532/10000\n",
      "68/68 [==============================] - 0s 853us/sample - loss: 197.0495 - val_loss: 145.9579\n",
      "Epoch 533/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 196.1742 - val_loss: 147.8370\n",
      "Epoch 534/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 189.6541 - val_loss: 146.9057\n",
      "Epoch 535/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 191.1840 - val_loss: 146.5877\n",
      "Epoch 536/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 195.1661 - val_loss: 153.8531\n",
      "Epoch 537/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 193.8136 - val_loss: 148.6692\n",
      "Epoch 538/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 195.0749 - val_loss: 145.2332\n",
      "Epoch 539/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 195.6222 - val_loss: 168.3106\n",
      "Epoch 540/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 195.3621 - val_loss: 146.7486\n",
      "Epoch 541/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 192.9514 - val_loss: 156.0753\n",
      "Epoch 542/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 193.7315 - val_loss: 144.7293\n",
      "Epoch 543/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 203.3613 - val_loss: 147.4561\n",
      "Epoch 544/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 192.7697 - val_loss: 167.8970\n",
      "Epoch 545/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 196.6428 - val_loss: 144.1677\n",
      "Epoch 546/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 192.7177 - val_loss: 143.5608\n",
      "Epoch 547/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 190.0616 - val_loss: 144.0111\n",
      "Epoch 548/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 190.6147 - val_loss: 156.0343\n",
      "Epoch 549/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 187.2075 - val_loss: 144.1424\n",
      "Epoch 550/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 195.1565 - val_loss: 145.2966\n",
      "Epoch 551/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 188.4767 - val_loss: 156.8093\n",
      "Epoch 552/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 212.9775 - val_loss: 140.8533\n",
      "Epoch 553/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 189.2742 - val_loss: 156.8055\n",
      "Epoch 554/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 187.1690 - val_loss: 142.8369\n",
      "Epoch 555/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 186.5520 - val_loss: 144.2205\n",
      "Epoch 556/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 196.5908 - val_loss: 148.5638\n",
      "Epoch 557/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 187.8661 - val_loss: 144.8525\n",
      "Epoch 558/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 201.5219 - val_loss: 151.5078\n",
      "Epoch 559/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 180.5202 - val_loss: 142.2561\n",
      "Epoch 560/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 191.6094 - val_loss: 147.2314\n",
      "Epoch 561/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 183.8896 - val_loss: 143.0324\n",
      "Epoch 562/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 186.6262 - val_loss: 143.8036\n",
      "Epoch 563/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 189.8658 - val_loss: 140.5066\n",
      "Epoch 564/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 187.6340 - val_loss: 138.5317\n",
      "Epoch 565/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 180.1307 - val_loss: 155.8786\n",
      "Epoch 566/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 191.3134 - val_loss: 141.2101\n",
      "Epoch 567/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 191.3789 - val_loss: 144.3400\n",
      "Epoch 568/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 182.0460 - val_loss: 139.2366\n",
      "Epoch 569/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 203.9521 - val_loss: 144.8167\n",
      "Epoch 570/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 178.4863 - val_loss: 141.4920\n",
      "Epoch 571/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 186.2872 - val_loss: 149.3030\n",
      "Epoch 572/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 191us/sample - loss: 181.7332 - val_loss: 144.6850\n",
      "Epoch 573/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 186.8771 - val_loss: 138.7813\n",
      "Epoch 574/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 180.4860 - val_loss: 145.3893\n",
      "Epoch 575/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 183.4984 - val_loss: 139.5958\n",
      "Epoch 576/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 183.4027 - val_loss: 137.8048\n",
      "Epoch 577/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 179.1789 - val_loss: 148.1318\n",
      "Epoch 578/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 182.3432 - val_loss: 137.3959\n",
      "Epoch 579/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 180.6686 - val_loss: 141.3894\n",
      "Epoch 580/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 182.1925 - val_loss: 136.4661\n",
      "Epoch 581/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 182.8133 - val_loss: 144.0402\n",
      "Epoch 582/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 179.9463 - val_loss: 138.7721\n",
      "Epoch 583/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 177.3762 - val_loss: 138.3337\n",
      "Epoch 584/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 180.8332 - val_loss: 138.4190\n",
      "Epoch 585/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 176.6943 - val_loss: 135.4668\n",
      "Epoch 586/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 179.5112 - val_loss: 139.2529\n",
      "Epoch 587/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 176.6001 - val_loss: 134.2721\n",
      "Epoch 588/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 176.5927 - val_loss: 137.2534\n",
      "Epoch 589/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 179.2134 - val_loss: 136.0091\n",
      "Epoch 590/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 175.7897 - val_loss: 138.1057\n",
      "Epoch 591/10000\n",
      "68/68 [==============================] - 0s 765us/sample - loss: 178.6707 - val_loss: 138.1420\n",
      "Epoch 592/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 181.3246 - val_loss: 138.9179\n",
      "Epoch 593/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 173.8600 - val_loss: 132.9013\n",
      "Epoch 594/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 184.9521 - val_loss: 139.3589\n",
      "Epoch 595/10000\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 199.1391 - val_loss: 132.2707\n",
      "Epoch 596/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 178.5032 - val_loss: 153.9315\n",
      "Epoch 597/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 175.7514 - val_loss: 134.2624\n",
      "Epoch 598/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 174.6628 - val_loss: 137.7946\n",
      "Epoch 599/10000\n",
      "68/68 [==============================] - 0s 779us/sample - loss: 182.9854 - val_loss: 137.0199\n",
      "Epoch 600/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 174.7929 - val_loss: 131.3205\n",
      "Epoch 601/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 179.8039 - val_loss: 140.0085\n",
      "Epoch 602/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 194.9444 - val_loss: 130.0706\n",
      "Epoch 603/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 182.7105 - val_loss: 131.1592\n",
      "Epoch 604/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 177.8410 - val_loss: 137.3153\n",
      "Epoch 605/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 171.2868 - val_loss: 135.0260\n",
      "Epoch 606/10000\n",
      "68/68 [==============================] - 0s 647us/sample - loss: 171.9170 - val_loss: 137.7646\n",
      "Epoch 607/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 172.5258 - val_loss: 131.6184\n",
      "Epoch 608/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 173.9328 - val_loss: 133.5473\n",
      "Epoch 609/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 175.1959 - val_loss: 130.6611\n",
      "Epoch 610/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 169.9105 - val_loss: 139.0097\n",
      "Epoch 611/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 172.9801 - val_loss: 129.9247\n",
      "Epoch 612/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 183.8016 - val_loss: 129.4611\n",
      "Epoch 613/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 181.1477 - val_loss: 144.0004\n",
      "Epoch 614/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 187.3495 - val_loss: 132.5496\n",
      "Epoch 615/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 167.9917 - val_loss: 139.7516\n",
      "Epoch 616/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 176.7794 - val_loss: 128.4054\n",
      "Epoch 617/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 166.3658 - val_loss: 136.3359\n",
      "Epoch 618/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 169.8487 - val_loss: 134.1715\n",
      "Epoch 619/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 179.9516 - val_loss: 132.7290\n",
      "Epoch 620/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 178.0267 - val_loss: 130.9127\n",
      "Epoch 621/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 174.0052 - val_loss: 127.6194\n",
      "Epoch 622/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 188.9352 - val_loss: 139.9112\n",
      "Epoch 623/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 195.7991 - val_loss: 132.3658\n",
      "Epoch 624/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 176.7255 - val_loss: 152.7680\n",
      "Epoch 625/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 165.2392 - val_loss: 130.8405\n",
      "Epoch 626/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 169.1404 - val_loss: 136.4719\n",
      "Epoch 627/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 173.6279 - val_loss: 127.2706\n",
      "Epoch 628/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 174.2520 - val_loss: 132.7905\n",
      "Epoch 629/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 167.4119 - val_loss: 125.3891\n",
      "Epoch 630/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 172.3774 - val_loss: 129.9320\n",
      "Epoch 631/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 167.8105 - val_loss: 125.9825\n",
      "Epoch 632/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 172.3891 - val_loss: 140.3679\n",
      "Epoch 633/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 168.9676 - val_loss: 126.9168\n",
      "Epoch 634/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 167.7016 - val_loss: 134.5752\n",
      "Epoch 635/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 165.2285 - val_loss: 122.7753\n",
      "Epoch 636/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 163.0268 - val_loss: 128.7043\n",
      "Epoch 637/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 161.7410 - val_loss: 128.5736\n",
      "Epoch 638/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 161.4994 - val_loss: 127.3866\n",
      "Epoch 639/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 163.9401 - val_loss: 132.0431\n",
      "Epoch 640/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 166.5435 - val_loss: 126.2990\n",
      "Epoch 641/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 161.3821 - val_loss: 123.7061\n",
      "Epoch 642/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 160.9813 - val_loss: 120.4305\n",
      "Epoch 643/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 159.6100 - val_loss: 127.7335\n",
      "Epoch 644/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 171.3673 - val_loss: 129.0357\n",
      "Epoch 645/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 171.3405 - val_loss: 129.8930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 646/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 173.2184 - val_loss: 122.6745\n",
      "Epoch 647/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 170.9053 - val_loss: 120.3609\n",
      "Epoch 648/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 158.3261 - val_loss: 139.1846\n",
      "Epoch 649/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 160.3496 - val_loss: 121.7514\n",
      "Epoch 650/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 159.3102 - val_loss: 128.6755\n",
      "Epoch 651/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 158.9823 - val_loss: 124.0621\n",
      "Epoch 652/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 160.6047 - val_loss: 121.6751\n",
      "Epoch 653/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 157.7158 - val_loss: 131.0061\n",
      "Epoch 654/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 167.3863 - val_loss: 121.6856\n",
      "Epoch 655/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 165.3712 - val_loss: 127.7799\n",
      "Epoch 656/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 159.7628 - val_loss: 122.1167\n",
      "Epoch 657/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 173.5911 - val_loss: 119.0893\n",
      "Epoch 658/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 158.7282 - val_loss: 117.4601\n",
      "Epoch 659/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 153.3582 - val_loss: 127.1855\n",
      "Epoch 660/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 158.5478 - val_loss: 117.5947\n",
      "Epoch 661/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 155.4947 - val_loss: 131.0087\n",
      "Epoch 662/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 157.3143 - val_loss: 123.6970\n",
      "Epoch 663/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 154.9831 - val_loss: 120.0033\n",
      "Epoch 664/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 154.0613 - val_loss: 117.9366\n",
      "Epoch 665/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 168.3261 - val_loss: 118.2446\n",
      "Epoch 666/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 158.2418 - val_loss: 115.4698\n",
      "Epoch 667/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 156.1635 - val_loss: 119.3780\n",
      "Epoch 668/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 56.25 - 0s 176us/sample - loss: 164.5103 - val_loss: 118.3094\n",
      "Epoch 669/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 157.5790 - val_loss: 119.4847\n",
      "Epoch 670/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 155.0006 - val_loss: 122.0960\n",
      "Epoch 671/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 175.3791 - val_loss: 114.9650\n",
      "Epoch 672/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 179.4150 - val_loss: 120.2892\n",
      "Epoch 673/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 173.7577 - val_loss: 120.6028\n",
      "Epoch 674/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 155.8035 - val_loss: 127.3556\n",
      "Epoch 675/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 164.4380 - val_loss: 112.7089\n",
      "Epoch 676/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 154.9803 - val_loss: 122.0985\n",
      "Epoch 677/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 160.2129 - val_loss: 115.0624\n",
      "Epoch 678/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 151.4218 - val_loss: 138.3286\n",
      "Epoch 679/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 152.8934 - val_loss: 123.3570\n",
      "Epoch 680/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 157.5944 - val_loss: 144.3048\n",
      "Epoch 681/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 153.8148 - val_loss: 116.6567\n",
      "Epoch 682/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 153.8562 - val_loss: 115.6190\n",
      "Epoch 683/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 150.6821 - val_loss: 119.4264\n",
      "Epoch 684/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 147.5370 - val_loss: 114.7646\n",
      "Epoch 685/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 145.9690 - val_loss: 114.9172\n",
      "Epoch 686/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 149.9507 - val_loss: 111.7995\n",
      "Epoch 687/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 148.2593 - val_loss: 110.7316\n",
      "Epoch 688/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 149.5290 - val_loss: 112.3422\n",
      "Epoch 689/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 153.6877 - val_loss: 116.8182\n",
      "Epoch 690/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 145.5204 - val_loss: 113.6378\n",
      "Epoch 691/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 149.3606 - val_loss: 115.6021\n",
      "Epoch 692/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 149.9664 - val_loss: 122.5244\n",
      "Epoch 693/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 146.5902 - val_loss: 110.5212\n",
      "Epoch 694/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 148.0798 - val_loss: 112.2030\n",
      "Epoch 695/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 144.7034 - val_loss: 110.5623\n",
      "Epoch 696/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 143.1414 - val_loss: 113.0300\n",
      "Epoch 697/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 147.2418 - val_loss: 107.2132\n",
      "Epoch 698/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 147.0502 - val_loss: 111.3165\n",
      "Epoch 699/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 144.4962 - val_loss: 112.6172\n",
      "Epoch 700/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 148.0183 - val_loss: 108.7850\n",
      "Epoch 701/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 149.5782 - val_loss: 107.8947\n",
      "Epoch 702/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 146.4573 - val_loss: 107.5418\n",
      "Epoch 703/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 152.4103 - val_loss: 119.1606\n",
      "Epoch 704/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 141.2951 - val_loss: 111.7719\n",
      "Epoch 705/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 143.6363 - val_loss: 112.5649\n",
      "Epoch 706/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 139.9933 - val_loss: 105.1026\n",
      "Epoch 707/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 138.8136 - val_loss: 106.1019\n",
      "Epoch 708/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 143.3331 - val_loss: 109.6589\n",
      "Epoch 709/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 138.1906 - val_loss: 113.6228\n",
      "Epoch 710/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 140.5899 - val_loss: 106.5195\n",
      "Epoch 711/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 139.2712 - val_loss: 103.3927\n",
      "Epoch 712/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 152.6421 - val_loss: 118.8831\n",
      "Epoch 713/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 158.4947 - val_loss: 103.9140\n",
      "Epoch 714/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 142.1355 - val_loss: 110.8058\n",
      "Epoch 715/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 152.9858 - val_loss: 111.4075\n",
      "Epoch 716/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 162.5528 - val_loss: 144.5736\n",
      "Epoch 717/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 162.0263 - val_loss: 108.1540\n",
      "Epoch 718/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 143.1558 - val_loss: 126.2905\n",
      "Epoch 719/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 136.2146 - val_loss: 103.3322\n",
      "Epoch 720/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 154.5390 - val_loss: 106.0008\n",
      "Epoch 721/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 144.1490 - val_loss: 102.0964\n",
      "Epoch 722/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 134.9624 - val_loss: 111.8142\n",
      "Epoch 723/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 131.9777 - val_loss: 102.1174\n",
      "Epoch 724/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 132.9360 - val_loss: 111.3488\n",
      "Epoch 725/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 139.0513 - val_loss: 105.9258\n",
      "Epoch 726/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 133.1861 - val_loss: 113.1905\n",
      "Epoch 727/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 132.9536 - val_loss: 106.0762\n",
      "Epoch 728/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 95.21 - 0s 162us/sample - loss: 149.3020 - val_loss: 105.7626\n",
      "Epoch 729/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 132.6767 - val_loss: 99.7819\n",
      "Epoch 730/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 135.6682 - val_loss: 103.8992\n",
      "Epoch 731/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 145.0263 - val_loss: 105.1384\n",
      "Epoch 732/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 152.6467 - val_loss: 130.7363\n",
      "Epoch 733/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 147.2031 - val_loss: 106.2057\n",
      "Epoch 734/10000\n",
      "68/68 [==============================] - 0s 735us/sample - loss: 134.7954 - val_loss: 115.4324\n",
      "Epoch 735/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 133.9724 - val_loss: 102.0409\n",
      "Epoch 736/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 130.3924 - val_loss: 101.1405\n",
      "Epoch 737/10000\n",
      "68/68 [==============================] - 0s 868us/sample - loss: 131.6380 - val_loss: 104.6791\n",
      "Epoch 738/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 149.2290 - val_loss: 97.1031\n",
      "Epoch 739/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 139.7156 - val_loss: 105.1766\n",
      "Epoch 740/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 144.2966 - val_loss: 107.4150\n",
      "Epoch 741/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 145.8349 - val_loss: 105.3804\n",
      "Epoch 742/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 129.3165 - val_loss: 97.0036\n",
      "Epoch 743/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 126.9974 - val_loss: 104.4633\n",
      "Epoch 744/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 153.7052 - val_loss: 116.7480\n",
      "Epoch 745/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 143.2432 - val_loss: 101.9963\n",
      "Epoch 746/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 134.6834 - val_loss: 102.0379\n",
      "Epoch 747/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 137.5505 - val_loss: 97.4806\n",
      "Epoch 748/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 129.5388 - val_loss: 109.3042\n",
      "Epoch 749/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 124.2603 - val_loss: 96.0641\n",
      "Epoch 750/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 124.5049 - val_loss: 123.0831\n",
      "Epoch 751/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 141.1829 - val_loss: 122.1740\n",
      "Epoch 752/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 163.3209 - val_loss: 113.9930\n",
      "Epoch 753/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 134.4413 - val_loss: 94.0732\n",
      "Epoch 754/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 154.8394 - val_loss: 100.5637\n",
      "Epoch 755/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 152.0599 - val_loss: 123.7616\n",
      "Epoch 756/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 138.8520 - val_loss: 109.0352\n",
      "Epoch 757/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 144.9684 - val_loss: 157.7899\n",
      "Epoch 758/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 144.5723 - val_loss: 120.0129\n",
      "Epoch 759/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 125.1610 - val_loss: 169.1719\n",
      "Epoch 760/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 159.3501 - val_loss: 114.2477\n",
      "Epoch 761/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 133.3321 - val_loss: 125.7166\n",
      "Epoch 762/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 121.1993 - val_loss: 107.4522\n",
      "Epoch 763/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 132.4303 - val_loss: 104.3392\n",
      "Epoch 764/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 117.9766 - val_loss: 92.7555\n",
      "Epoch 765/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 120.9380 - val_loss: 95.8444\n",
      "Epoch 766/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 122.6490 - val_loss: 91.9589\n",
      "Epoch 767/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 123.0637 - val_loss: 97.6812\n",
      "Epoch 768/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 122.7031 - val_loss: 88.7920\n",
      "Epoch 769/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 117.4413 - val_loss: 91.5195\n",
      "Epoch 770/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 118.6342 - val_loss: 89.5383\n",
      "Epoch 771/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 122.2589 - val_loss: 92.2563\n",
      "Epoch 772/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 131.5987 - val_loss: 101.1915\n",
      "Epoch 773/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 156.449 - 0s 206us/sample - loss: 123.5113 - val_loss: 88.1871\n",
      "Epoch 774/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 130.0914 - val_loss: 97.7998\n",
      "Epoch 775/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 118.4451 - val_loss: 91.6688\n",
      "Epoch 776/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 115.1280 - val_loss: 102.7787\n",
      "Epoch 777/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 124.6486 - val_loss: 89.2738\n",
      "Epoch 778/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 114.6952 - val_loss: 90.0971\n",
      "Epoch 779/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 123.1252 - val_loss: 85.9346\n",
      "Epoch 780/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 126.8124 - val_loss: 88.2616\n",
      "Epoch 781/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 120.9468 - val_loss: 90.5439\n",
      "Epoch 782/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 120.2311 - val_loss: 101.0744\n",
      "Epoch 783/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 120.1049 - val_loss: 84.9385\n",
      "Epoch 784/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 112.2948 - val_loss: 98.7091\n",
      "Epoch 785/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 114.2636 - val_loss: 93.8162\n",
      "Epoch 786/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 137.3599 - val_loss: 106.7056\n",
      "Epoch 787/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 132.1064 - val_loss: 90.5457\n",
      "Epoch 788/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 120.5077 - val_loss: 89.1407\n",
      "Epoch 789/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 114.3411 - val_loss: 109.9753\n",
      "Epoch 790/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 122.4496 - val_loss: 99.3611\n",
      "Epoch 791/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 116.1258 - val_loss: 103.1524\n",
      "Epoch 792/10000\n",
      "68/68 [==============================] - 0s 838us/sample - loss: 108.3382 - val_loss: 86.1768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 793/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 119.5749 - val_loss: 86.4660\n",
      "Epoch 794/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 122.7180 - val_loss: 96.3088\n",
      "Epoch 795/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 119.5229 - val_loss: 89.4214\n",
      "Epoch 796/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 123.2019 - val_loss: 92.7006\n",
      "Epoch 797/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 109.9524 - val_loss: 82.4625\n",
      "Epoch 798/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 119.4681 - val_loss: 86.4744\n",
      "Epoch 799/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 112.2736 - val_loss: 84.2481\n",
      "Epoch 800/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 107.6877 - val_loss: 82.2676\n",
      "Epoch 801/10000\n",
      "68/68 [==============================] - 0s 765us/sample - loss: 112.4765 - val_loss: 84.9500\n",
      "Epoch 802/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 114.8864 - val_loss: 79.3414\n",
      "Epoch 803/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 110.0827 - val_loss: 96.6766\n",
      "Epoch 804/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 114.8821 - val_loss: 86.3527\n",
      "Epoch 805/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 111.8129 - val_loss: 78.9900\n",
      "Epoch 806/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 111.1705 - val_loss: 82.6278\n",
      "Epoch 807/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 103.0022 - val_loss: 80.7056\n",
      "Epoch 808/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 104.1562 - val_loss: 77.5415\n",
      "Epoch 809/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 106.7239 - val_loss: 80.9705\n",
      "Epoch 810/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 120.0300 - val_loss: 91.9295\n",
      "Epoch 811/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 114.2300 - val_loss: 90.1379\n",
      "Epoch 812/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 105.5625 - val_loss: 78.4070\n",
      "Epoch 813/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 108.0004 - val_loss: 74.8339\n",
      "Epoch 814/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 100.8092 - val_loss: 96.2015\n",
      "Epoch 815/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 119.3356 - val_loss: 100.4651\n",
      "Epoch 816/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 102.2509 - val_loss: 110.6989\n",
      "Epoch 817/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 111.5324 - val_loss: 79.8512\n",
      "Epoch 818/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 97.6044 - val_loss: 104.7095\n",
      "Epoch 819/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 118.4704 - val_loss: 92.3129\n",
      "Epoch 820/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 121.3116 - val_loss: 76.3952\n",
      "Epoch 821/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 101.9498 - val_loss: 82.9408\n",
      "Epoch 822/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 102.1650 - val_loss: 95.0972\n",
      "Epoch 823/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 110.5560 - val_loss: 170.5397\n",
      "Epoch 824/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 131.7917 - val_loss: 114.1088\n",
      "Epoch 825/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 155.9521 - val_loss: 105.2170\n",
      "Epoch 826/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 141.2408 - val_loss: 158.7434\n",
      "Epoch 827/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 106.4797 - val_loss: 85.3945\n",
      "Epoch 828/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 98.9903 - val_loss: 86.8698\n",
      "Epoch 829/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 100.9330 - val_loss: 82.9748\n",
      "Epoch 830/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 100.4014 - val_loss: 71.6204\n",
      "Epoch 831/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 98.8493 - val_loss: 78.3656\n",
      "Epoch 832/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 101.8435 - val_loss: 86.6018\n",
      "Epoch 833/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 115.5450 - val_loss: 87.3175\n",
      "Epoch 834/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 97.0041 - val_loss: 92.1631\n",
      "Epoch 835/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 106.5747 - val_loss: 80.6442\n",
      "Epoch 836/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 105.5375 - val_loss: 97.7766\n",
      "Epoch 837/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 100.3293 - val_loss: 89.0887\n",
      "Epoch 838/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 98.3399 - val_loss: 68.9390\n",
      "Epoch 839/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 96.6137 - val_loss: 97.4814\n",
      "Epoch 840/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 110.7915 - val_loss: 79.4448\n",
      "Epoch 841/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 108.4804 - val_loss: 70.0660\n",
      "Epoch 842/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 102.5641 - val_loss: 82.5180\n",
      "Epoch 843/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 103.8419 - val_loss: 76.4287\n",
      "Epoch 844/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 117.4218 - val_loss: 122.1532\n",
      "Epoch 845/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 113.1930 - val_loss: 99.0014\n",
      "Epoch 846/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 108.0692 - val_loss: 73.4166\n",
      "Epoch 847/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 91.5742 - val_loss: 66.6921\n",
      "Epoch 848/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 97.2267 - val_loss: 65.9820\n",
      "Epoch 849/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 91.8801 - val_loss: 69.3671\n",
      "Epoch 850/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 95.0092 - val_loss: 91.0477\n",
      "Epoch 851/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 97.6957 - val_loss: 72.0995\n",
      "Epoch 852/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 103.0085 - val_loss: 68.4004\n",
      "Epoch 853/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 93.3174 - val_loss: 73.6160\n",
      "Epoch 854/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 109.4319 - val_loss: 109.5620\n",
      "Epoch 855/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 96.6953 - val_loss: 66.1352\n",
      "Epoch 856/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 99.8569 - val_loss: 88.1843\n",
      "Epoch 857/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 98.5548 - val_loss: 116.7541\n",
      "Epoch 858/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 108.1874 - val_loss: 67.1020\n",
      "Epoch 859/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 103.2274 - val_loss: 67.0346\n",
      "Epoch 860/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 89.3385 - val_loss: 66.4179\n",
      "Epoch 861/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 100.9160 - val_loss: 111.1460\n",
      "Epoch 862/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 109.6459 - val_loss: 76.1766\n",
      "Epoch 863/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 91.2078 - val_loss: 69.0108\n",
      "Epoch 864/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 84.1528 - val_loss: 67.5033\n",
      "Epoch 865/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 89.2005 - val_loss: 66.1775\n",
      "Epoch 866/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 83.5490 - val_loss: 63.4548\n",
      "Epoch 867/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 91.3438 - val_loss: 66.2158\n",
      "Epoch 868/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 86.8930 - val_loss: 65.2887\n",
      "Epoch 869/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 81.0974 - val_loss: 67.9059\n",
      "Epoch 870/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 89.7998 - val_loss: 63.2718\n",
      "Epoch 871/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 97.6328 - val_loss: 132.6477\n",
      "Epoch 872/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 135.7533 - val_loss: 130.9616\n",
      "Epoch 873/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 129.8441 - val_loss: 83.4305\n",
      "Epoch 874/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 129.7334 - val_loss: 155.1283\n",
      "Epoch 875/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 150.8011 - val_loss: 80.6343\n",
      "Epoch 876/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 155.7090 - val_loss: 119.8694\n",
      "Epoch 877/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 122.1390 - val_loss: 72.3387\n",
      "Epoch 878/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 114.4433 - val_loss: 88.3969\n",
      "Epoch 879/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 91.9424 - val_loss: 63.5381\n",
      "Epoch 880/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 82.0355 - val_loss: 64.1524\n",
      "Epoch 881/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 80.3775 - val_loss: 63.3071\n",
      "Epoch 882/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 81.5955 - val_loss: 57.4170\n",
      "Epoch 883/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 90.5416 - val_loss: 71.9446\n",
      "Epoch 884/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 95.1603 - val_loss: 61.9556\n",
      "Epoch 885/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 90.7888 - val_loss: 65.4709\n",
      "Epoch 886/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 96.3630 - val_loss: 119.3693\n",
      "Epoch 887/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 95.8874 - val_loss: 127.7125\n",
      "Epoch 888/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 120.3969 - val_loss: 60.5832\n",
      "Epoch 889/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 93.8090 - val_loss: 77.5004\n",
      "Epoch 890/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 81.1865 - val_loss: 75.4070\n",
      "Epoch 891/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 91.9685 - val_loss: 62.6317\n",
      "Epoch 892/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 93.5396 - val_loss: 114.8761\n",
      "Epoch 893/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 97.4240 - val_loss: 57.6510\n",
      "Epoch 894/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 103.0420 - val_loss: 107.8260\n",
      "Epoch 895/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 109.2754 - val_loss: 94.6568\n",
      "Epoch 896/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 88.5824 - val_loss: 63.0542\n",
      "Epoch 897/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 84.2002 - val_loss: 60.2131\n",
      "Epoch 898/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 78.6861 - val_loss: 62.2688\n",
      "Epoch 899/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 74.1804 - val_loss: 72.8941\n",
      "Epoch 900/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 78.3514 - val_loss: 56.8738\n",
      "Epoch 901/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 94.2697 - val_loss: 61.6974\n",
      "Epoch 902/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 81.6760 - val_loss: 64.8259\n",
      "Epoch 903/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 90.6125 - val_loss: 65.9170\n",
      "Epoch 904/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 74.9110 - val_loss: 55.4748\n",
      "Epoch 905/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 70.2533 - val_loss: 55.5007\n",
      "Epoch 906/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 69.0162 - val_loss: 57.8600\n",
      "Epoch 907/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 79.7245 - val_loss: 75.7955\n",
      "Epoch 908/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 73.3334 - val_loss: 52.3595\n",
      "Epoch 909/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 80.28 - 0s 206us/sample - loss: 69.7050 - val_loss: 62.4842\n",
      "Epoch 910/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 71.1063 - val_loss: 55.9700\n",
      "Epoch 911/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 70.0280 - val_loss: 53.9181\n",
      "Epoch 912/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 69.0525 - val_loss: 65.5239\n",
      "Epoch 913/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 88.6906 - val_loss: 52.4242\n",
      "Epoch 914/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 109.8277 - val_loss: 69.3498\n",
      "Epoch 915/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 77.4861 - val_loss: 58.4584\n",
      "Epoch 916/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 85.6501 - val_loss: 80.4985\n",
      "Epoch 917/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 88.7736 - val_loss: 65.2835\n",
      "Epoch 918/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 87.6040 - val_loss: 69.8759\n",
      "Epoch 919/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 66.5966 - val_loss: 50.2866\n",
      "Epoch 920/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 69.5386 - val_loss: 49.7541\n",
      "Epoch 921/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 66.0580 - val_loss: 60.5184\n",
      "Epoch 922/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 70.9778 - val_loss: 47.8073\n",
      "Epoch 923/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 69.4943 - val_loss: 71.5132\n",
      "Epoch 924/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 70.1586 - val_loss: 52.1582\n",
      "Epoch 925/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 76.8235 - val_loss: 68.2801\n",
      "Epoch 926/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 84.7482 - val_loss: 46.9135\n",
      "Epoch 927/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 84.9381 - val_loss: 46.4759\n",
      "Epoch 928/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 65.3940 - val_loss: 46.6283\n",
      "Epoch 929/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 61.5190 - val_loss: 47.2200\n",
      "Epoch 930/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 72.0002 - val_loss: 57.3841\n",
      "Epoch 931/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 76.82 - 0s 162us/sample - loss: 67.4610 - val_loss: 53.9542\n",
      "Epoch 932/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 63.7596 - val_loss: 50.5063\n",
      "Epoch 933/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 62.4236 - val_loss: 43.6027\n",
      "Epoch 934/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 71.4584 - val_loss: 86.8351\n",
      "Epoch 935/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 79.4980 - val_loss: 50.7230\n",
      "Epoch 936/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 70.0713 - val_loss: 84.3556\n",
      "Epoch 937/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 69.9976 - val_loss: 49.5138\n",
      "Epoch 938/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 64.1810 - val_loss: 59.4122\n",
      "Epoch 939/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 60.0894 - val_loss: 111.6847\n",
      "Epoch 940/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 75.3440 - val_loss: 48.6876\n",
      "Epoch 941/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 80.9829 - val_loss: 57.0377\n",
      "Epoch 942/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 78.9577 - val_loss: 88.9936\n",
      "Epoch 943/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 96.8399 - val_loss: 42.7977\n",
      "Epoch 944/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 64.9714 - val_loss: 56.7844\n",
      "Epoch 945/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 65.6127 - val_loss: 48.3135\n",
      "Epoch 946/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 77.2680 - val_loss: 54.8209\n",
      "Epoch 947/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 67.7035 - val_loss: 54.3638\n",
      "Epoch 948/10000\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 68.9824 - val_loss: 47.4196\n",
      "Epoch 949/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 71.4317 - val_loss: 51.4288\n",
      "Epoch 950/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 76.6992 - val_loss: 56.2774\n",
      "Epoch 951/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 73.8609 - val_loss: 70.8014\n",
      "Epoch 952/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 58.9824 - val_loss: 44.9033\n",
      "Epoch 953/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 56.1861 - val_loss: 41.4877\n",
      "Epoch 954/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 62.4188 - val_loss: 43.0412\n",
      "Epoch 955/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 53.3401 - val_loss: 42.1165\n",
      "Epoch 956/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 54.5095 - val_loss: 45.9662\n",
      "Epoch 957/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 52.9263 - val_loss: 40.6621\n",
      "Epoch 958/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 58.5706 - val_loss: 56.3925\n",
      "Epoch 959/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 63.1861 - val_loss: 41.0622\n",
      "Epoch 960/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 64.0755 - val_loss: 85.3834\n",
      "Epoch 961/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 61.9758 - val_loss: 42.0256\n",
      "Epoch 962/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 58.8399 - val_loss: 38.6610\n",
      "Epoch 963/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 50.7399 - val_loss: 40.4852\n",
      "Epoch 964/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 55.7847 - val_loss: 42.1756\n",
      "Epoch 965/10000\n",
      "68/68 [==============================] - 0s 706us/sample - loss: 52.3898 - val_loss: 73.7872\n",
      "Epoch 966/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 67.5689 - val_loss: 57.2622\n",
      "Epoch 967/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 62.5817 - val_loss: 48.1598\n",
      "Epoch 968/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 59.4172 - val_loss: 59.7410\n",
      "Epoch 969/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 68.9838 - val_loss: 43.6751\n",
      "Epoch 970/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 90.2803 - val_loss: 91.3260\n",
      "Epoch 971/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 58.2585 - val_loss: 42.7240\n",
      "Epoch 972/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 53.0561 - val_loss: 61.7356\n",
      "Epoch 973/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 67.7715 - val_loss: 34.9894\n",
      "Epoch 974/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 53.9459 - val_loss: 42.5204\n",
      "Epoch 975/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 66.4365 - val_loss: 49.1759\n",
      "Epoch 976/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 61.0690 - val_loss: 47.3551\n",
      "Epoch 977/10000\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 51.8187 - val_loss: 56.8677\n",
      "Epoch 978/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 56.4515 - val_loss: 38.3600\n",
      "Epoch 979/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 49.6555 - val_loss: 34.6655\n",
      "Epoch 980/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 47.8056 - val_loss: 46.7179\n",
      "Epoch 981/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 52.8124 - val_loss: 40.6763\n",
      "Epoch 982/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 44.9725 - val_loss: 36.1037\n",
      "Epoch 983/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 48.7886 - val_loss: 43.4111\n",
      "Epoch 984/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 53.3236 - val_loss: 36.2601\n",
      "Epoch 985/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 44.5092 - val_loss: 36.0217\n",
      "Epoch 986/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 47.5726 - val_loss: 33.1133\n",
      "Epoch 987/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 45.0430 - val_loss: 39.9280\n",
      "Epoch 988/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 55.2393 - val_loss: 72.8349\n",
      "Epoch 989/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 70.1863 - val_loss: 48.2331\n",
      "Epoch 990/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 75.6189 - val_loss: 39.7891\n",
      "Epoch 991/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 50.3655 - val_loss: 41.7198\n",
      "Epoch 992/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 47.1531 - val_loss: 49.1729\n",
      "Epoch 993/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 51.8585 - val_loss: 52.6203\n",
      "Epoch 994/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 58.9180 - val_loss: 47.7225\n",
      "Epoch 995/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 56.9145 - val_loss: 33.2324\n",
      "Epoch 996/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 43.7781 - val_loss: 31.9259\n",
      "Epoch 997/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 41.8896 - val_loss: 36.9670\n",
      "Epoch 998/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 54.6606 - val_loss: 57.9404\n",
      "Epoch 999/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 64.9067 - val_loss: 86.6833\n",
      "Epoch 1000/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 80.8745 - val_loss: 47.9585\n",
      "Epoch 1001/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 67.1701 - val_loss: 33.0496\n",
      "Epoch 1002/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 61.1683 - val_loss: 29.8480\n",
      "Epoch 1003/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 55.2063 - val_loss: 59.9314\n",
      "Epoch 1004/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 51.7086 - val_loss: 48.6803\n",
      "Epoch 1005/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 53.1885 - val_loss: 28.4442\n",
      "Epoch 1006/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 52.7720 - val_loss: 35.9066\n",
      "Epoch 1007/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 69.3843 - val_loss: 70.5498\n",
      "Epoch 1008/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 60.0940 - val_loss: 91.4077\n",
      "Epoch 1009/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 83.3416 - val_loss: 60.2951\n",
      "Epoch 1010/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 66.2650 - val_loss: 28.2653\n",
      "Epoch 1011/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 43.2161 - val_loss: 34.8449\n",
      "Epoch 1012/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 43.8479 - val_loss: 61.2829\n",
      "Epoch 1013/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 52.7937 - val_loss: 61.1169\n",
      "Epoch 1014/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 61.6546 - val_loss: 76.7394\n",
      "Epoch 1015/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 49.4813 - val_loss: 27.2485\n",
      "Epoch 1016/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 206us/sample - loss: 48.7708 - val_loss: 30.6398\n",
      "Epoch 1017/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 40.1422 - val_loss: 37.2737\n",
      "Epoch 1018/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 42.1618 - val_loss: 28.7348\n",
      "Epoch 1019/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 34.3435 - val_loss: 32.2839\n",
      "Epoch 1020/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 37.7170 - val_loss: 27.5431\n",
      "Epoch 1021/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 40.2651 - val_loss: 27.4692\n",
      "Epoch 1022/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 42.0139 - val_loss: 115.9328\n",
      "Epoch 1023/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 43.0335 - val_loss: 29.1218\n",
      "Epoch 1024/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 33.3630 - val_loss: 28.0167\n",
      "Epoch 1025/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 39.7423 - val_loss: 27.8706\n",
      "Epoch 1026/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 34.3557 - val_loss: 29.1031\n",
      "Epoch 1027/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 35.2471 - val_loss: 31.2892\n",
      "Epoch 1028/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 38.7983 - val_loss: 41.8075\n",
      "Epoch 1029/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 46.8799 - val_loss: 30.1257\n",
      "Epoch 1030/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 43.2708 - val_loss: 42.1692\n",
      "Epoch 1031/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 65.5488 - val_loss: 47.9323\n",
      "Epoch 1032/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 85.8836 - val_loss: 29.7838\n",
      "Epoch 1033/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 69.5365 - val_loss: 46.6292\n",
      "Epoch 1034/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 52.4544 - val_loss: 50.8816\n",
      "Epoch 1035/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 51.1573 - val_loss: 39.3089\n",
      "Epoch 1036/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 36.5712 - val_loss: 29.7043\n",
      "Epoch 1037/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 33.5911 - val_loss: 26.2634\n",
      "Epoch 1038/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 31.1720 - val_loss: 29.8267\n",
      "Epoch 1039/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 30.3026 - val_loss: 26.6738\n",
      "Epoch 1040/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 32.3778 - val_loss: 30.7897\n",
      "Epoch 1041/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 37.9459 - val_loss: 22.0619\n",
      "Epoch 1042/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 31.9051 - val_loss: 28.4296\n",
      "Epoch 1043/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 29.9432 - val_loss: 50.4071\n",
      "Epoch 1044/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 48.6077 - val_loss: 57.4424\n",
      "Epoch 1045/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 40.7946 - val_loss: 38.6768\n",
      "Epoch 1046/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 34.8665 - val_loss: 21.9546\n",
      "Epoch 1047/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 27.8309 - val_loss: 24.9310\n",
      "Epoch 1048/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 32.3722 - val_loss: 20.5641\n",
      "Epoch 1049/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 31.5860 - val_loss: 23.2877\n",
      "Epoch 1050/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 29.4801 - val_loss: 20.2649\n",
      "Epoch 1051/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 28.0296 - val_loss: 20.2389\n",
      "Epoch 1052/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 27.0348 - val_loss: 22.6605\n",
      "Epoch 1053/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 29.0497 - val_loss: 19.8795\n",
      "Epoch 1054/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26.2985 - val_loss: 19.4777\n",
      "Epoch 1055/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 31.3950 - val_loss: 19.3477\n",
      "Epoch 1056/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 37.2346 - val_loss: 22.6638\n",
      "Epoch 1057/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 39.0231 - val_loss: 35.2792\n",
      "Epoch 1058/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 37.7551 - val_loss: 29.0333\n",
      "Epoch 1059/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 33.3565 - val_loss: 40.1022\n",
      "Epoch 1060/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 31.3478 - val_loss: 85.5271\n",
      "Epoch 1061/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 50.9404 - val_loss: 57.0043\n",
      "Epoch 1062/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 44.3477 - val_loss: 30.5574\n",
      "Epoch 1063/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 30.9972 - val_loss: 18.6835\n",
      "Epoch 1064/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 28.1397 - val_loss: 21.3681\n",
      "Epoch 1065/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 27.1849 - val_loss: 30.1555\n",
      "Epoch 1066/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 36.3366 - val_loss: 40.2499\n",
      "Epoch 1067/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 29.4418 - val_loss: 19.5275\n",
      "Epoch 1068/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 24.1073 - val_loss: 35.1232\n",
      "Epoch 1069/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 37.4964 - val_loss: 20.9405\n",
      "Epoch 1070/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 34.0636 - val_loss: 19.9660\n",
      "Epoch 1071/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 27.2036 - val_loss: 17.7553\n",
      "Epoch 1072/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 30.4575 - val_loss: 16.8189\n",
      "Epoch 1073/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 24.2129 - val_loss: 23.9613\n",
      "Epoch 1074/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 23.9247 - val_loss: 16.6670\n",
      "Epoch 1075/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 25.5993 - val_loss: 18.5300\n",
      "Epoch 1076/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26.9582 - val_loss: 16.7559\n",
      "Epoch 1077/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21.7865 - val_loss: 19.2560\n",
      "Epoch 1078/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 22.7604 - val_loss: 19.7160\n",
      "Epoch 1079/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 31.4321 - val_loss: 17.3374\n",
      "Epoch 1080/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 26.7861 - val_loss: 31.2123\n",
      "Epoch 1081/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 30.9251 - val_loss: 33.0610\n",
      "Epoch 1082/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 37.4167 - val_loss: 33.6699\n",
      "Epoch 1083/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 26.4304 - val_loss: 29.5916\n",
      "Epoch 1084/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 41.3344 - val_loss: 62.7087\n",
      "Epoch 1085/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 42.4939 - val_loss: 68.6631\n",
      "Epoch 1086/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 61.7155 - val_loss: 129.1340\n",
      "Epoch 1087/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 56.9050 - val_loss: 50.8478\n",
      "Epoch 1088/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 33.0613 - val_loss: 33.5588\n",
      "Epoch 1089/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 36.1327 - val_loss: 20.5622\n",
      "Epoch 1090/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 191us/sample - loss: 40.9246 - val_loss: 15.5502\n",
      "Epoch 1091/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 28.1459 - val_loss: 16.0713\n",
      "Epoch 1092/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 20.4887 - val_loss: 19.4376\n",
      "Epoch 1093/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 20.2502 - val_loss: 13.5278\n",
      "Epoch 1094/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 20.9529 - val_loss: 20.7364\n",
      "Epoch 1095/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 23.4544 - val_loss: 15.1150\n",
      "Epoch 1096/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 20.7514 - val_loss: 19.3724\n",
      "Epoch 1097/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 19.7796 - val_loss: 13.5876\n",
      "Epoch 1098/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 17.8027 - val_loss: 13.9730\n",
      "Epoch 1099/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 19.5952 - val_loss: 13.1959\n",
      "Epoch 1100/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 19.6423 - val_loss: 22.8281\n",
      "Epoch 1101/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 33.4788 - val_loss: 14.9199\n",
      "Epoch 1102/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 23.3310 - val_loss: 16.9228\n",
      "Epoch 1103/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 31.3123 - val_loss: 18.6382\n",
      "Epoch 1104/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 30.8541 - val_loss: 44.3686\n",
      "Epoch 1105/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 51.1472 - val_loss: 102.1172\n",
      "Epoch 1106/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 73.8647 - val_loss: 34.6200\n",
      "Epoch 1107/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 53.8238 - val_loss: 18.9133\n",
      "Epoch 1108/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 43.1717 - val_loss: 31.6740\n",
      "Epoch 1109/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 40.3904 - val_loss: 24.4748\n",
      "Epoch 1110/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 22.8919 - val_loss: 16.0741\n",
      "Epoch 1111/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 20.6176 - val_loss: 25.0604\n",
      "Epoch 1112/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 25.2929 - val_loss: 15.5412\n",
      "Epoch 1113/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 22.8638 - val_loss: 29.9223\n",
      "Epoch 1114/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 28.2275 - val_loss: 18.8131\n",
      "Epoch 1115/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 29.0947 - val_loss: 39.0672\n",
      "Epoch 1116/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 21.4225 - val_loss: 20.4987\n",
      "Epoch 1117/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 19.9788 - val_loss: 24.1697\n",
      "Epoch 1118/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 17.1530 - val_loss: 16.2512\n",
      "Epoch 1119/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19.5874 - val_loss: 14.2700\n",
      "Epoch 1120/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 18.8553 - val_loss: 15.8547\n",
      "Epoch 1121/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 16.1980 - val_loss: 11.6695\n",
      "Epoch 1122/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.1623 - val_loss: 10.0595\n",
      "Epoch 1123/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16.4867 - val_loss: 12.3506\n",
      "Epoch 1124/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 15.4406 - val_loss: 11.4643\n",
      "Epoch 1125/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 14.3309 - val_loss: 13.5965\n",
      "Epoch 1126/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15.8782 - val_loss: 11.4438\n",
      "Epoch 1127/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 19.2449 - val_loss: 16.5380\n",
      "Epoch 1128/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 25.3052 - val_loss: 28.8134\n",
      "Epoch 1129/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 23.7509 - val_loss: 10.3936\n",
      "Epoch 1130/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.5054 - val_loss: 11.7911\n",
      "Epoch 1131/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.8935 - val_loss: 9.6244\n",
      "Epoch 1132/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15.4642 - val_loss: 12.6146\n",
      "Epoch 1133/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.4533 - val_loss: 10.9454\n",
      "Epoch 1134/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 13.3733 - val_loss: 10.1973\n",
      "Epoch 1135/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 12.7815 - val_loss: 25.0823\n",
      "Epoch 1136/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.8306 - val_loss: 41.6138\n",
      "Epoch 1137/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 19.3367 - val_loss: 13.1208\n",
      "Epoch 1138/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 13.3655 - val_loss: 15.1584\n",
      "Epoch 1139/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 20.7952 - val_loss: 43.9121\n",
      "Epoch 1140/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 31.8074 - val_loss: 10.9978\n",
      "Epoch 1141/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 15.9205 - val_loss: 8.4200\n",
      "Epoch 1142/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.5964 - val_loss: 19.9752\n",
      "Epoch 1143/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 16.0380 - val_loss: 10.0429\n",
      "Epoch 1144/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 13.3461 - val_loss: 11.6760\n",
      "Epoch 1145/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 16.8418 - val_loss: 8.4106\n",
      "Epoch 1146/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 13.6763 - val_loss: 8.1352\n",
      "Epoch 1147/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 12.6687 - val_loss: 8.6404\n",
      "Epoch 1148/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.6834 - val_loss: 8.7754\n",
      "Epoch 1149/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 12.0083 - val_loss: 11.5894\n",
      "Epoch 1150/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 25.3560 - val_loss: 15.5361\n",
      "Epoch 1151/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 26.4917 - val_loss: 31.2108\n",
      "Epoch 1152/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 21.7485 - val_loss: 35.5351\n",
      "Epoch 1153/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 21.3553 - val_loss: 15.0971\n",
      "Epoch 1154/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 17.7242 - val_loss: 8.5244\n",
      "Epoch 1155/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 10.7838 - val_loss: 14.6490\n",
      "Epoch 1156/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.4437 - val_loss: 9.3777\n",
      "Epoch 1157/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 10.6602 - val_loss: 8.2849\n",
      "Epoch 1158/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.6022 - val_loss: 7.6588\n",
      "Epoch 1159/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.5877 - val_loss: 6.5138\n",
      "Epoch 1160/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.3335 - val_loss: 8.1239\n",
      "Epoch 1161/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.9031 - val_loss: 6.6059\n",
      "Epoch 1162/10000\n",
      "68/68 [==============================] - 0s 765us/sample - loss: 9.5495 - val_loss: 7.9425\n",
      "Epoch 1163/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.5345 - val_loss: 7.3239\n",
      "Epoch 1164/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 9.4173 - val_loss: 13.5834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1165/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.0200 - val_loss: 7.1571\n",
      "Epoch 1166/10000\n",
      "68/68 [==============================] - 0s 838us/sample - loss: 8.7415 - val_loss: 7.9184\n",
      "Epoch 1167/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 11.9517 - val_loss: 6.1473\n",
      "Epoch 1168/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 10.3157 - val_loss: 7.7987\n",
      "Epoch 1169/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.1782 - val_loss: 6.2864\n",
      "Epoch 1170/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 10.0881 - val_loss: 6.5740\n",
      "Epoch 1171/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 13.0223 - val_loss: 27.5586\n",
      "Epoch 1172/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 30.4909 - val_loss: 24.1848\n",
      "Epoch 1173/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 25.4708 - val_loss: 17.2167\n",
      "Epoch 1174/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 13.6826 - val_loss: 6.2650\n",
      "Epoch 1175/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 8.9964 - val_loss: 17.7029\n",
      "Epoch 1176/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11.2654 - val_loss: 5.7347\n",
      "Epoch 1177/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 16.5661 - val_loss: 16.6669\n",
      "Epoch 1178/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 19.4886 - val_loss: 9.4130\n",
      "Epoch 1179/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 15.1180 - val_loss: 9.3635\n",
      "Epoch 1180/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 12.5997 - val_loss: 5.0629\n",
      "Epoch 1181/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.4734 - val_loss: 16.6202\n",
      "Epoch 1182/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 10.3775 - val_loss: 5.1230\n",
      "Epoch 1183/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.8527 - val_loss: 6.9947\n",
      "Epoch 1184/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 10.0943 - val_loss: 6.5678\n",
      "Epoch 1185/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 12.3980 - val_loss: 11.2225\n",
      "Epoch 1186/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 10.3897 - val_loss: 13.9008\n",
      "Epoch 1187/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 14.9135 - val_loss: 11.0990\n",
      "Epoch 1188/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.9616 - val_loss: 6.3462\n",
      "Epoch 1189/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.2047 - val_loss: 6.7387\n",
      "Epoch 1190/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.0083 - val_loss: 4.5217\n",
      "Epoch 1191/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5.7029 - val_loss: 7.6406\n",
      "Epoch 1192/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 12.2967 - val_loss: 8.8301\n",
      "Epoch 1193/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.9477 - val_loss: 8.8730\n",
      "Epoch 1194/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11.2386 - val_loss: 5.5389\n",
      "Epoch 1195/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 17.6954 - val_loss: 59.6729\n",
      "Epoch 1196/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 33.0618 - val_loss: 13.3672\n",
      "Epoch 1197/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 38.3971 - val_loss: 48.9873\n",
      "Epoch 1198/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 33.4891 - val_loss: 30.6084\n",
      "Epoch 1199/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 35.3364 - val_loss: 5.5627\n",
      "Epoch 1200/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 23.2370 - val_loss: 49.0877\n",
      "Epoch 1201/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 28.9393 - val_loss: 76.7075\n",
      "Epoch 1202/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 24.1883 - val_loss: 14.5893\n",
      "Epoch 1203/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 14.8832 - val_loss: 24.1144\n",
      "Epoch 1204/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 19.5355 - val_loss: 7.2129\n",
      "Epoch 1205/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 12.9773 - val_loss: 6.3433\n",
      "Epoch 1206/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 11.9785 - val_loss: 4.9468\n",
      "Epoch 1207/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 11.9196 - val_loss: 20.5198\n",
      "Epoch 1208/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 23.5668 - val_loss: 34.8169\n",
      "Epoch 1209/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 25.2670 - val_loss: 53.7149\n",
      "Epoch 1210/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 49.7697 - val_loss: 15.0699\n",
      "Epoch 1211/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 35.3931 - val_loss: 27.1987\n",
      "Epoch 1212/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 29.3795 - val_loss: 34.0724\n",
      "Epoch 1213/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 26.6420 - val_loss: 8.8060\n",
      "Epoch 1214/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 15.4822 - val_loss: 13.4540\n",
      "Epoch 1215/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 10.9733 - val_loss: 4.7052\n",
      "Epoch 1216/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.7564 - val_loss: 11.6844\n",
      "Epoch 1217/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.4535 - val_loss: 5.9167\n",
      "Epoch 1218/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 5.8680 - val_loss: 11.7356\n",
      "Epoch 1219/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 7.6581 - val_loss: 3.5818\n",
      "Epoch 1220/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 5.4054 - val_loss: 9.3289\n",
      "Epoch 1221/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.8759 - val_loss: 3.6726\n",
      "Epoch 1222/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.4931 - val_loss: 3.5810\n",
      "Epoch 1223/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.1432 - val_loss: 6.6898\n",
      "Epoch 1224/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 6.4583 - val_loss: 4.2507\n",
      "Epoch 1225/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.4748 - val_loss: 4.5618\n",
      "Epoch 1226/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.3490 - val_loss: 5.6882\n",
      "Epoch 1227/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 4.2153 - val_loss: 2.8116\n",
      "Epoch 1228/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.6367 - val_loss: 2.8485\n",
      "Epoch 1229/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.0485 - val_loss: 12.2213\n",
      "Epoch 1230/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.0736 - val_loss: 3.7940\n",
      "Epoch 1231/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 4.1485 - val_loss: 3.5150\n",
      "Epoch 1232/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.6619 - val_loss: 9.2599\n",
      "Epoch 1233/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.2273 - val_loss: 4.6124\n",
      "Epoch 1234/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 4.7459 - val_loss: 6.0971\n",
      "Epoch 1235/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.4062 - val_loss: 4.5118\n",
      "Epoch 1236/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 6.6708 - val_loss: 2.6020\n",
      "Epoch 1237/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.4499 - val_loss: 3.4675\n",
      "Epoch 1238/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4980 - val_loss: 5.0451\n",
      "Epoch 1239/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4702 - val_loss: 2.6890\n",
      "Epoch 1240/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 235us/sample - loss: 4.9482 - val_loss: 2.8021\n",
      "Epoch 1241/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 3.6856 - val_loss: 3.3804\n",
      "Epoch 1242/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.7809 - val_loss: 4.2245\n",
      "Epoch 1243/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 4.9307 - val_loss: 4.7160\n",
      "Epoch 1244/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 6.6160 - val_loss: 8.1589\n",
      "Epoch 1245/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 7.0531 - val_loss: 2.4905\n",
      "Epoch 1246/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.5421 - val_loss: 2.8766\n",
      "Epoch 1247/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.9931 - val_loss: 7.2601\n",
      "Epoch 1248/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 6.1976 - val_loss: 11.9991\n",
      "Epoch 1249/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.8685 - val_loss: 24.2160\n",
      "Epoch 1250/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 14.4709 - val_loss: 6.5283\n",
      "Epoch 1251/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 20.9030 - val_loss: 16.1514\n",
      "Epoch 1252/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 38.3575 - val_loss: 8.3140\n",
      "Epoch 1253/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 57.7844 - val_loss: 305.5770\n",
      "Epoch 1254/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 95.7418 - val_loss: 172.9149\n",
      "Epoch 1255/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 46.3620 - val_loss: 30.3596\n",
      "Epoch 1256/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 22.6424 - val_loss: 14.0810\n",
      "Epoch 1257/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 11.1635 - val_loss: 8.6183\n",
      "Epoch 1258/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 8.5204 - val_loss: 5.0250\n",
      "Epoch 1259/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 5.5832 - val_loss: 2.3990\n",
      "Epoch 1260/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 5.4141 - val_loss: 4.1487\n",
      "Epoch 1261/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 4.7618 - val_loss: 7.5844\n",
      "Epoch 1262/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.1446 - val_loss: 1.6585\n",
      "Epoch 1263/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.9661 - val_loss: 4.4966\n",
      "Epoch 1264/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 2.7623 - val_loss: 1.8963\n",
      "Epoch 1265/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 2.6121 - val_loss: 1.5708\n",
      "Epoch 1266/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 2.5683 - val_loss: 3.0097\n",
      "Epoch 1267/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 2.2508 - val_loss: 1.3996\n",
      "Epoch 1268/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 2.2351 - val_loss: 1.3720\n",
      "Epoch 1269/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.2650 - val_loss: 1.4137\n",
      "Epoch 1270/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 5.0681 - val_loss: 1.8381\n",
      "Epoch 1271/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.4152 - val_loss: 1.6683\n",
      "Epoch 1272/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 6.2481 - val_loss: 4.3503\n",
      "Epoch 1273/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 5.7092 - val_loss: 3.6581\n",
      "Epoch 1274/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.1679 - val_loss: 1.8426\n",
      "Epoch 1275/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.9061 - val_loss: 1.4932\n",
      "Epoch 1276/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6629 - val_loss: 1.8589\n",
      "Epoch 1277/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.0426 - val_loss: 2.0716\n",
      "Epoch 1278/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 1.9014 - val_loss: 1.4085\n",
      "Epoch 1279/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.9341 - val_loss: 1.5419\n",
      "Epoch 1280/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.0397 - val_loss: 6.6217\n",
      "Epoch 1281/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 7.0227 - val_loss: 1.9828\n",
      "Epoch 1282/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.6175 - val_loss: 1.8175\n",
      "Epoch 1283/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.8491 - val_loss: 3.1222\n",
      "Epoch 1284/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.7885 - val_loss: 5.8421\n",
      "Epoch 1285/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.6648 - val_loss: 5.4724\n",
      "Epoch 1286/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.6384 - val_loss: 7.9475\n",
      "Epoch 1287/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.4036 - val_loss: 3.0458\n",
      "Epoch 1288/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.3188 - val_loss: 2.1220\n",
      "Epoch 1289/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 2.0010 - val_loss: 1.4705\n",
      "Epoch 1290/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.3364 - val_loss: 1.9700\n",
      "Epoch 1291/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1.6082 - val_loss: 2.5231\n",
      "Epoch 1292/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.0438 - val_loss: 1.3684\n",
      "Epoch 1293/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 1.7982 - val_loss: 1.3817\n",
      "Epoch 1294/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 1.5087 - val_loss: 1.8795\n",
      "Epoch 1295/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.8824 - val_loss: 7.1759\n",
      "Epoch 1296/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.1595 - val_loss: 20.1025\n",
      "Epoch 1297/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 10.9710 - val_loss: 8.6530\n",
      "Epoch 1298/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 15.9920 - val_loss: 24.2539\n",
      "Epoch 1299/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 21.2088 - val_loss: 29.5596\n",
      "Epoch 1300/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.6569 - val_loss: 35.8238\n",
      "Epoch 1301/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 12.0262 - val_loss: 13.7094\n",
      "Epoch 1302/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.1761 - val_loss: 3.6099\n",
      "Epoch 1303/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5.1718 - val_loss: 7.3471\n",
      "Epoch 1304/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1629 - val_loss: 6.4585\n",
      "Epoch 1305/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.5432 - val_loss: 1.7131\n",
      "Epoch 1306/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.0730 - val_loss: 0.7485\n",
      "Epoch 1307/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.3138 - val_loss: 0.9268\n",
      "Epoch 1308/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.1075 - val_loss: 1.3481\n",
      "Epoch 1309/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.9228 - val_loss: 1.0831\n",
      "Epoch 1310/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4571 - val_loss: 9.5970\n",
      "Epoch 1311/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 10.1964 - val_loss: 1.5299\n",
      "Epoch 1312/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.9184 - val_loss: 0.7724\n",
      "Epoch 1313/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.7171 - val_loss: 1.5277\n",
      "Epoch 1314/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.4401 - val_loss: 0.8032\n",
      "Epoch 1315/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2498 - val_loss: 1.0994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1316/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.3306 - val_loss: 3.3002\n",
      "Epoch 1317/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 2.7756 - val_loss: 0.7638\n",
      "Epoch 1318/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 5.4481 - val_loss: 4.9252\n",
      "Epoch 1319/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 4.5000 - val_loss: 15.8125\n",
      "Epoch 1320/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 7.6819 - val_loss: 3.8727\n",
      "Epoch 1321/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.9715 - val_loss: 1.2187\n",
      "Epoch 1322/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.4719 - val_loss: 3.5646\n",
      "Epoch 1323/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 2.0045 - val_loss: 1.1579\n",
      "Epoch 1324/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.4867 - val_loss: 0.7757\n",
      "Epoch 1325/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1.0491 - val_loss: 1.1212\n",
      "Epoch 1326/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 1.1241 - val_loss: 2.1680\n",
      "Epoch 1327/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 1.4043 - val_loss: 0.9765\n",
      "Epoch 1328/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4094 - val_loss: 1.5193\n",
      "Epoch 1329/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.7644 - val_loss: 0.4232\n",
      "Epoch 1330/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9988 - val_loss: 1.8048\n",
      "Epoch 1331/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 1.4852 - val_loss: 0.7697\n",
      "Epoch 1332/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8458 - val_loss: 0.4429\n",
      "Epoch 1333/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.7101 - val_loss: 2.7933\n",
      "Epoch 1334/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.4260 - val_loss: 0.8038\n",
      "Epoch 1335/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2606 - val_loss: 3.4456\n",
      "Epoch 1336/10000\n",
      "68/68 [==============================] - 0s 559us/sample - loss: 2.8573 - val_loss: 11.7868\n",
      "Epoch 1337/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5.0608 - val_loss: 3.0035\n",
      "Epoch 1338/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 2.7000 - val_loss: 8.5947\n",
      "Epoch 1339/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.8421 - val_loss: 2.1332\n",
      "Epoch 1340/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 1.2350 - val_loss: 0.6187\n",
      "Epoch 1341/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 1.0042 - val_loss: 0.6545\n",
      "Epoch 1342/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.9781 - val_loss: 0.3841\n",
      "Epoch 1343/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.8050 - val_loss: 0.3690\n",
      "Epoch 1344/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.4589 - val_loss: 2.1149\n",
      "Epoch 1345/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 1.6783 - val_loss: 0.4081\n",
      "Epoch 1346/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7051 - val_loss: 0.3492\n",
      "Epoch 1347/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.8408 - val_loss: 0.8899\n",
      "Epoch 1348/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.6347 - val_loss: 0.2664\n",
      "Epoch 1349/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9526 - val_loss: 0.9308\n",
      "Epoch 1350/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.6443 - val_loss: 1.3573\n",
      "Epoch 1351/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.9669 - val_loss: 2.8381\n",
      "Epoch 1352/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 1.1079 - val_loss: 0.4863\n",
      "Epoch 1353/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.1633 - val_loss: 8.7081\n",
      "Epoch 1354/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 6.1427 - val_loss: 2.2859\n",
      "Epoch 1355/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.2019 - val_loss: 15.0202\n",
      "Epoch 1356/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 7.1606 - val_loss: 1.1326\n",
      "Epoch 1357/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.6148 - val_loss: 9.1036\n",
      "Epoch 1358/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.3166 - val_loss: 0.2308\n",
      "Epoch 1359/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.2937 - val_loss: 1.0061\n",
      "Epoch 1360/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.8533 - val_loss: 0.7302\n",
      "Epoch 1361/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.0484 - val_loss: 1.1133\n",
      "Epoch 1362/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.1997 - val_loss: 0.6697\n",
      "Epoch 1363/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 1.0706 - val_loss: 6.3376\n",
      "Epoch 1364/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.5165 - val_loss: 6.7318\n",
      "Epoch 1365/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 3.2583 - val_loss: 5.4549\n",
      "Epoch 1366/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.4369 - val_loss: 1.8306\n",
      "Epoch 1367/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.0363 - val_loss: 3.5834\n",
      "Epoch 1368/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.5807 - val_loss: 9.0624\n",
      "Epoch 1369/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.0047 - val_loss: 4.0885\n",
      "Epoch 1370/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 3.6508 - val_loss: 3.4222\n",
      "Epoch 1371/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.5468 - val_loss: 3.1653\n",
      "Epoch 1372/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.3381 - val_loss: 3.7550\n",
      "Epoch 1373/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.7522 - val_loss: 0.6134\n",
      "Epoch 1374/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5068 - val_loss: 0.2962\n",
      "Epoch 1375/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8850 - val_loss: 3.4722\n",
      "Epoch 1376/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5481 - val_loss: 0.9381\n",
      "Epoch 1377/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.0132 - val_loss: 1.3776\n",
      "Epoch 1378/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.5965 - val_loss: 0.2610\n",
      "Epoch 1379/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1613 - val_loss: 1.3325\n",
      "Epoch 1380/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1996 - val_loss: 1.8863\n",
      "Epoch 1381/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.8777 - val_loss: 1.1839\n",
      "Epoch 1382/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 0.4151 - val_loss: 0.5813\n",
      "Epoch 1383/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6916 - val_loss: 0.7038\n",
      "Epoch 1384/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.7798 - val_loss: 1.3837\n",
      "Epoch 1385/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 1.074 - 0s 147us/sample - loss: 1.1707 - val_loss: 4.9194\n",
      "Epoch 1386/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 3.2189 - val_loss: 1.1087\n",
      "Epoch 1387/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.2999 - val_loss: 8.2573\n",
      "Epoch 1388/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.0935 - val_loss: 7.4951\n",
      "Epoch 1389/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.2205 - val_loss: 0.6505\n",
      "Epoch 1390/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 3.4966 - val_loss: 0.4586\n",
      "Epoch 1391/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 176us/sample - loss: 1.2439 - val_loss: 1.2611\n",
      "Epoch 1392/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.0660 - val_loss: 9.8016\n",
      "Epoch 1393/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 17.7759 - val_loss: 98.2281\n",
      "Epoch 1394/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 33.0570 - val_loss: 11.1738\n",
      "Epoch 1395/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.7854 - val_loss: 0.3891\n",
      "Epoch 1396/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 7.4430 - val_loss: 0.2799\n",
      "Epoch 1397/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1.4181 - val_loss: 0.2061\n",
      "Epoch 1398/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5332 - val_loss: 0.1552\n",
      "Epoch 1399/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.3842 - val_loss: 0.7284\n",
      "Epoch 1400/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.7196 - val_loss: 1.4506\n",
      "Epoch 1401/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.8752 - val_loss: 1.9330\n",
      "Epoch 1402/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.2519 - val_loss: 3.1490\n",
      "Epoch 1403/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.0751 - val_loss: 0.6912\n",
      "Epoch 1404/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5667 - val_loss: 0.1281\n",
      "Epoch 1405/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3001 - val_loss: 1.2365\n",
      "Epoch 1406/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7399 - val_loss: 1.2268\n",
      "Epoch 1407/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.763 - 0s 147us/sample - loss: 0.3798 - val_loss: 0.1260\n",
      "Epoch 1408/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.8528 - val_loss: 0.1848\n",
      "Epoch 1409/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2117 - val_loss: 0.1351\n",
      "Epoch 1410/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2731 - val_loss: 1.3666\n",
      "Epoch 1411/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9276 - val_loss: 2.1355\n",
      "Epoch 1412/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.6401 - val_loss: 7.0413\n",
      "Epoch 1413/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.9798 - val_loss: 6.6664\n",
      "Epoch 1414/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.9793 - val_loss: 0.3194\n",
      "Epoch 1415/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.5651 - val_loss: 12.1436\n",
      "Epoch 1416/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 4.1382 - val_loss: 4.2033\n",
      "Epoch 1417/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1172 - val_loss: 5.5237\n",
      "Epoch 1418/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.6846 - val_loss: 0.7883\n",
      "Epoch 1419/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3483 - val_loss: 2.3782\n",
      "Epoch 1420/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.0380 - val_loss: 0.1665\n",
      "Epoch 1421/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1634 - val_loss: 2.2663\n",
      "Epoch 1422/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.8396 - val_loss: 0.9864\n",
      "Epoch 1423/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.4813 - val_loss: 15.6981\n",
      "Epoch 1424/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.6723 - val_loss: 48.8035\n",
      "Epoch 1425/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 77.5941 - val_loss: 608.9974\n",
      "Epoch 1426/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 494.6506 - val_loss: 98.4080\n",
      "Epoch 1427/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 744.4701 - val_loss: 707.0183\n",
      "Epoch 1428/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 514.4813 - val_loss: 116.8388\n",
      "Epoch 1429/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 652.2778 - val_loss: 2119.2731\n",
      "Epoch 1430/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2362.7301 - val_loss: 4139.6298\n",
      "Epoch 1431/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1853.2900 - val_loss: 5536.1713\n",
      "Epoch 1432/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3778.9598 - val_loss: 5364.1600\n",
      "Epoch 1433/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5974.1746 - val_loss: 6605.0879\n",
      "Epoch 1434/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5122.4565 - val_loss: 10733.3525\n",
      "Epoch 1435/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3943.9340 - val_loss: 4420.0480\n",
      "Epoch 1436/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 1705.4149 - val_loss: 2798.9779\n",
      "Epoch 1437/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 2248.1612 - val_loss: 1789.6218\n",
      "Epoch 1438/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4910.6916 - val_loss: 3932.5629\n",
      "Epoch 1439/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2087.6993 - val_loss: 4512.0509\n",
      "Epoch 1440/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2713.6186 - val_loss: 1380.6893\n",
      "Epoch 1441/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1378.9010 - val_loss: 68.7185\n",
      "Epoch 1442/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 531.8976 - val_loss: 1085.8719\n",
      "Epoch 1443/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 571.8039 - val_loss: 475.7852\n",
      "Epoch 1444/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 296.2309 - val_loss: 109.7949\n",
      "Epoch 1445/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 320.4239 - val_loss: 338.3962\n",
      "Epoch 1446/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 103.8490 - val_loss: 48.6707\n",
      "Epoch 1447/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 56.3151 - val_loss: 8.4417\n",
      "Epoch 1448/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 22.6057 - val_loss: 63.3104\n",
      "Epoch 1449/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 19.4361 - val_loss: 10.4263\n",
      "Epoch 1450/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 10.6463 - val_loss: 4.8169\n",
      "Epoch 1451/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 2.9994 - val_loss: 10.0225\n",
      "Epoch 1452/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.5811 - val_loss: 5.6227\n",
      "Epoch 1453/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.3009 - val_loss: 3.0516\n",
      "Epoch 1454/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 1.2302 - val_loss: 1.3287\n",
      "Epoch 1455/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6319 - val_loss: 0.5079\n",
      "Epoch 1456/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4223 - val_loss: 0.8127\n",
      "Epoch 1457/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2806 - val_loss: 0.3842\n",
      "Epoch 1458/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 0.0928 - val_loss: 0.1213\n",
      "Epoch 1459/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0474 - val_loss: 0.0208\n",
      "Epoch 1460/10000\n",
      "68/68 [==============================] - 0s 559us/sample - loss: 0.0174 - val_loss: 0.0215\n",
      "Epoch 1461/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0154 - val_loss: 0.0155\n",
      "Epoch 1462/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0156 - val_loss: 0.0228\n",
      "Epoch 1463/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0127 - val_loss: 0.0114\n",
      "Epoch 1464/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0147 - val_loss: 0.0093\n",
      "Epoch 1465/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0090 - val_loss: 0.0037\n",
      "Epoch 1466/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0095 - val_loss: 0.0347\n",
      "Epoch 1467/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0129 - val_loss: 0.0015\n",
      "Epoch 1468/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0062 - val_loss: 0.0022\n",
      "Epoch 1469/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0035 - val_loss: 0.0025\n",
      "Epoch 1470/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 1471/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 1472/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0014 - val_loss: 6.8764e-04\n",
      "Epoch 1473/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0014 - val_loss: 5.5576e-04\n",
      "Epoch 1474/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 9.9493e-04 - val_loss: 8.3450e-04\n",
      "Epoch 1475/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 9.5200e-04 - val_loss: 7.4983e-04\n",
      "Epoch 1476/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0018 - val_loss: 9.5237e-04\n",
      "Epoch 1477/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 1478/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 1479/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 8.7436e-04 - val_loss: 8.0713e-04\n",
      "Epoch 1480/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.0681e-04 - val_loss: 9.3575e-04\n",
      "Epoch 1481/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 1482/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0011 - val_loss: 0.0022\n",
      "Epoch 1483/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0021 - val_loss: 0.0019\n",
      "Epoch 1484/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 0.0016 - val_loss: 7.1758e-04\n",
      "Epoch 1485/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0012 - val_loss: 5.8477e-04\n",
      "Epoch 1486/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0012 - val_loss: 5.9883e-04\n",
      "Epoch 1487/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 1488/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 1489/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0015 - val_loss: 0.0027\n",
      "Epoch 1490/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0016 - val_loss: 9.4053e-04\n",
      "Epoch 1491/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0014 - val_loss: 5.5038e-04\n",
      "Epoch 1492/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 8.4073e-04 - val_loss: 4.6224e-04\n",
      "Epoch 1493/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.3201e-04 - val_loss: 0.0016\n",
      "Epoch 1494/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 1495/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0031 - val_loss: 0.0021\n",
      "Epoch 1496/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 1497/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0019 - val_loss: 9.0141e-04\n",
      "Epoch 1498/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 0.0010\n",
      "Epoch 1499/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.0012 - val_loss: 6.4439e-04\n",
      "Epoch 1500/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 5.5881e-04\n",
      "Epoch 1501/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 6.7591e-04 - val_loss: 8.1944e-04\n",
      "Epoch 1502/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.3448e-04 - val_loss: 6.7366e-04\n",
      "Epoch 1503/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 7.8297e-04 - val_loss: 0.0015\n",
      "Epoch 1504/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 1505/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 0.0032 - val_loss: 0.0086\n",
      "Epoch 1506/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0074 - val_loss: 0.0141\n",
      "Epoch 1507/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0040 - val_loss: 0.0019\n",
      "Epoch 1508/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0024 - val_loss: 4.3204e-04\n",
      "Epoch 1509/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0013 - val_loss: 2.9683e-04\n",
      "Epoch 1510/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 5.7677e-04 - val_loss: 2.8819e-04\n",
      "Epoch 1511/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.8263e-04 - val_loss: 2.7360e-04\n",
      "Epoch 1512/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.9003e-04 - val_loss: 4.7381e-04\n",
      "Epoch 1513/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 4.5724e-04 - val_loss: 2.9960e-04\n",
      "Epoch 1514/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 5.3317e-04 - val_loss: 2.9927e-04\n",
      "Epoch 1515/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5.4422e-04 - val_loss: 5.6765e-04\n",
      "Epoch 1516/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.3415e-04 - val_loss: 0.0011\n",
      "Epoch 1517/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.9351e-04 - val_loss: 0.0010\n",
      "Epoch 1518/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.1759e-04 - val_loss: 3.1102e-04\n",
      "Epoch 1519/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 5.5962e-04 - val_loss: 8.1946e-04\n",
      "Epoch 1520/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.6044e-04 - val_loss: 2.9541e-04\n",
      "Epoch 1521/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 4.9098e-04 - val_loss: 0.0011\n",
      "Epoch 1522/10000\n",
      "68/68 [==============================] - 0s 588us/sample - loss: 9.7830e-04 - val_loss: 0.0014\n",
      "Epoch 1523/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0012 - val_loss: 7.4030e-04\n",
      "Epoch 1524/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0011 - val_loss: 5.3843e-04\n",
      "Epoch 1525/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 8.2421e-04 - val_loss: 8.3975e-04\n",
      "Epoch 1526/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0011 - val_loss: 5.5057e-04\n",
      "Epoch 1527/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 9.4964e-04 - val_loss: 6.7199e-04\n",
      "Epoch 1528/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.1940e-04 - val_loss: 6.0191e-04\n",
      "Epoch 1529/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.6931e-04 - val_loss: 0.0050\n",
      "Epoch 1530/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0035 - val_loss: 0.0109\n",
      "Epoch 1531/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0063 - val_loss: 7.5675e-04\n",
      "Epoch 1532/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0012 - val_loss: 4.0084e-04\n",
      "Epoch 1533/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 1534/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 1535/10000\n",
      "68/68 [==============================] - 0s 853us/sample - loss: 7.1649e-04 - val_loss: 0.0020\n",
      "Epoch 1536/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.8005e-04 - val_loss: 3.8775e-04\n",
      "Epoch 1537/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.4444e-04 - val_loss: 1.4025e-04\n",
      "Epoch 1538/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 750us/sample - loss: 2.3905e-04 - val_loss: 1.7407e-04\n",
      "Epoch 1539/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.6533e-04 - val_loss: 9.1103e-04\n",
      "Epoch 1540/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.9079e-04 - val_loss: 4.0059e-04\n",
      "Epoch 1541/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.8676e-04 - val_loss: 4.1660e-04\n",
      "Epoch 1542/10000\n",
      "68/68 [==============================] - 0s 824us/sample - loss: 5.5847e-04 - val_loss: 5.8752e-04\n",
      "Epoch 1543/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.5504e-04 - val_loss: 3.0573e-04\n",
      "Epoch 1544/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 2.2807e-04 - val_loss: 2.8918e-04\n",
      "Epoch 1545/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 4.8358e-04 - val_loss: 3.7943e-04\n",
      "Epoch 1546/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 5.4563e-04 - val_loss: 4.0199e-04\n",
      "Epoch 1547/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 3.0475e-04 - val_loss: 5.8267e-04\n",
      "Epoch 1548/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 6.4731e-04 - val_loss: 4.5390e-04\n",
      "Epoch 1549/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.0012 - val_loss: 4.5313e-04\n",
      "Epoch 1550/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0017 - val_loss: 0.0027\n",
      "Epoch 1551/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 6.5508e-04 - val_loss: 4.6055e-04\n",
      "Epoch 1552/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 3.2587e-04 - val_loss: 0.0010\n",
      "Epoch 1553/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 5.9132e-04 - val_loss: 6.1032e-04\n",
      "Epoch 1554/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.0652e-04 - val_loss: 4.7526e-04\n",
      "Epoch 1555/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.8887e-04 - val_loss: 8.6483e-04\n",
      "Epoch 1556/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.2673e-04 - val_loss: 0.0010\n",
      "Epoch 1557/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.8347e-04 - val_loss: 0.0015\n",
      "Epoch 1558/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0010 - val_loss: 0.0041\n",
      "Epoch 1559/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0079 - val_loss: 0.0039\n",
      "Epoch 1560/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0139 - val_loss: 0.0243\n",
      "Epoch 1561/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0136 - val_loss: 0.0085\n",
      "Epoch 1562/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0078 - val_loss: 0.0174\n",
      "Epoch 1563/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0130 - val_loss: 0.0055\n",
      "Epoch 1564/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0086 - val_loss: 0.0138\n",
      "Epoch 1565/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0185 - val_loss: 0.0273\n",
      "Epoch 1566/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0112 - val_loss: 0.0064\n",
      "Epoch 1567/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0148 - val_loss: 0.0208\n",
      "Epoch 1568/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0094 - val_loss: 0.0065\n",
      "Epoch 1569/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0034 - val_loss: 0.0012\n",
      "Epoch 1570/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0019 - val_loss: 0.0042\n",
      "Epoch 1571/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 1572/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.2273e-04 - val_loss: 2.9386e-04\n",
      "Epoch 1573/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.3346e-04 - val_loss: 1.6548e-04\n",
      "Epoch 1574/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.0551e-04 - val_loss: 1.3743e-04\n",
      "Epoch 1575/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.3675e-04 - val_loss: 1.2366e-04\n",
      "Epoch 1576/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 9.0482e-05 - val_loss: 2.3052e-04\n",
      "Epoch 1577/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1465e-04 - val_loss: 2.0777e-04\n",
      "Epoch 1578/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.7276e-04 - val_loss: 1.2007e-04\n",
      "Epoch 1579/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.3464e-04 - val_loss: 2.8348e-04\n",
      "Epoch 1580/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.5143e-04 - val_loss: 2.9061e-04\n",
      "Epoch 1581/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.0047e-04 - val_loss: 0.0023\n",
      "Epoch 1582/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0013 - val_loss: 7.8651e-05\n",
      "Epoch 1583/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.3791e-04 - val_loss: 5.1730e-04\n",
      "Epoch 1584/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 3.0433e-04 - val_loss: 1.3677e-04\n",
      "Epoch 1585/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 5.3552e-04 - val_loss: 0.0010\n",
      "Epoch 1586/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 8.7321e-04 - val_loss: 8.0348e-04\n",
      "Epoch 1587/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 6.0120e-04 - val_loss: 1.5811e-04\n",
      "Epoch 1588/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0010 - val_loss: 0.0046\n",
      "Epoch 1589/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0030 - val_loss: 0.0032\n",
      "Epoch 1590/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0027 - val_loss: 0.0013\n",
      "Epoch 1591/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0021 - val_loss: 0.0138\n",
      "Epoch 1592/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0065 - val_loss: 0.0021\n",
      "Epoch 1593/10000\n",
      "68/68 [==============================] - 0s 588us/sample - loss: 0.0027 - val_loss: 0.0045\n",
      "Epoch 1594/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0027 - val_loss: 0.0018\n",
      "Epoch 1595/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 8.7698e-04 - val_loss: 0.0017\n",
      "Epoch 1596/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.4031e-04 - val_loss: 2.6000e-04\n",
      "Epoch 1597/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 5.4869e-04 - val_loss: 1.1742e-04\n",
      "Epoch 1598/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1.9616e-04 - val_loss: 3.1033e-04\n",
      "Epoch 1599/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4876e-04 - val_loss: 8.0747e-05\n",
      "Epoch 1600/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 6.4301e-0 - 0s 147us/sample - loss: 1.7799e-04 - val_loss: 3.1434e-04\n",
      "Epoch 1601/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 2.4120e-04 - val_loss: 3.6052e-04\n",
      "Epoch 1602/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1.7127e-04 - val_loss: 4.5880e-04\n",
      "Epoch 1603/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.5611e-04 - val_loss: 1.6139e-04\n",
      "Epoch 1604/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.0617e-04 - val_loss: 4.8404e-04\n",
      "Epoch 1605/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.3530e-04 - val_loss: 1.0683e-04\n",
      "Epoch 1606/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.8070e-04 - val_loss: 4.6988e-04\n",
      "Epoch 1607/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.8108e-04 - val_loss: 3.1656e-04\n",
      "Epoch 1608/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.9967e-04 - val_loss: 1.0990e-04\n",
      "Epoch 1609/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.5911e-04 - val_loss: 1.1057e-04\n",
      "Epoch 1610/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3945e-04 - val_loss: 3.9785e-04\n",
      "Epoch 1611/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2482e-04 - val_loss: 1.7682e-04\n",
      "Epoch 1612/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7870e-04 - val_loss: 5.3175e-04\n",
      "Epoch 1613/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.2008e-04 - val_loss: 4.7281e-05\n",
      "Epoch 1614/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 8.3717e-04 - val_loss: 0.0014\n",
      "Epoch 1615/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0044 - val_loss: 0.0198\n",
      "Epoch 1616/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0067 - val_loss: 0.0062\n",
      "Epoch 1617/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0252 - val_loss: 0.0132\n",
      "Epoch 1618/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0394 - val_loss: 0.0105\n",
      "Epoch 1619/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0275 - val_loss: 0.0823\n",
      "Epoch 1620/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0282 - val_loss: 0.0285\n",
      "Epoch 1621/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0117 - val_loss: 0.0062\n",
      "Epoch 1622/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0046 - val_loss: 0.0154\n",
      "Epoch 1623/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0060 - val_loss: 0.0048\n",
      "Epoch 1624/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0034 - val_loss: 0.0019\n",
      "Epoch 1625/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0073 - val_loss: 0.0174\n",
      "Epoch 1626/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0088 - val_loss: 0.0028\n",
      "Epoch 1627/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0024 - val_loss: 2.9836e-04\n",
      "Epoch 1628/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.7702e-04 - val_loss: 2.3660e-05\n",
      "Epoch 1629/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.3493e-05 - val_loss: 1.8888e-04\n",
      "Epoch 1630/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.1815e-04 - val_loss: 0.0010\n",
      "Epoch 1631/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.2317e-04 - val_loss: 1.4764e-04\n",
      "Epoch 1632/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.5872e-04 - val_loss: 4.7850e-05\n",
      "Epoch 1633/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.4009e-04 - val_loss: 1.6184e-04\n",
      "Epoch 1634/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0015 - val_loss: 9.8987e-04\n",
      "Epoch 1635/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 4.8656e-04 - val_loss: 9.4072e-05\n",
      "Epoch 1636/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 1.4929e-04 - val_loss: 1.2816e-04\n",
      "Epoch 1637/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.0011 - val_loss: 0.0114\n",
      "Epoch 1638/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 0.0046 - val_loss: 0.0017\n",
      "Epoch 1639/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 1640/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0041 - val_loss: 0.0089\n",
      "Epoch 1641/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0035 - val_loss: 0.0067\n",
      "Epoch 1642/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0033 - val_loss: 0.0029\n",
      "Epoch 1643/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0034 - val_loss: 0.0060\n",
      "Epoch 1644/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 1645/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0011 - val_loss: 0.0044\n",
      "Epoch 1646/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0029 - val_loss: 0.0056\n",
      "Epoch 1647/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0049 - val_loss: 0.0075\n",
      "Epoch 1648/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0134 - val_loss: 0.1187\n",
      "Epoch 1649/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.3053 - val_loss: 0.3329\n",
      "Epoch 1650/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4438 - val_loss: 0.9209\n",
      "Epoch 1651/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.4988 - val_loss: 2.9732\n",
      "Epoch 1652/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0413 - val_loss: 2.4550\n",
      "Epoch 1653/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 1.6451 - val_loss: 0.7056\n",
      "Epoch 1654/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.8095 - val_loss: 2.1312\n",
      "Epoch 1655/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.3679 - val_loss: 0.7489\n",
      "Epoch 1656/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3354 - val_loss: 0.9031\n",
      "Epoch 1657/10000\n",
      "68/68 [==============================] - 0s 794us/sample - loss: 0.9338 - val_loss: 0.9258\n",
      "Epoch 1658/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.5462 - val_loss: 0.1515\n",
      "Epoch 1659/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4373 - val_loss: 0.3924\n",
      "Epoch 1660/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2686 - val_loss: 0.7007\n",
      "Epoch 1661/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2301 - val_loss: 0.9024\n",
      "Epoch 1662/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 1.1226 - val_loss: 2.2576\n",
      "Epoch 1663/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.1786 - val_loss: 12.8021\n",
      "Epoch 1664/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.4352 - val_loss: 21.9462\n",
      "Epoch 1665/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.3945 - val_loss: 1.0602\n",
      "Epoch 1666/10000\n",
      "68/68 [==============================] - 0s 706us/sample - loss: 2.5270 - val_loss: 4.7759\n",
      "Epoch 1667/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.4488 - val_loss: 6.5972\n",
      "Epoch 1668/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 13.6628 - val_loss: 13.6810\n",
      "Epoch 1669/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.1488 - val_loss: 28.6141\n",
      "Epoch 1670/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 41.9434 - val_loss: 6.0250\n",
      "Epoch 1671/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 103.4462 - val_loss: 881.9995\n",
      "Epoch 1672/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 986.4187 - val_loss: 1403.6899\n",
      "Epoch 1673/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2799.7064 - val_loss: 6165.2404\n",
      "Epoch 1674/10000\n",
      "68/68 [==============================] - 0s 838us/sample - loss: 2271.7580 - val_loss: 4858.9055\n",
      "Epoch 1675/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2560.4942 - val_loss: 2593.9246\n",
      "Epoch 1676/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2331.7900 - val_loss: 3491.0974\n",
      "Epoch 1677/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 1284.4289 - val_loss: 876.2756\n",
      "Epoch 1678/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1218.3573 - val_loss: 5140.7428\n",
      "Epoch 1679/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 1621.9513 - val_loss: 3258.9153\n",
      "Epoch 1680/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1053.6787 - val_loss: 708.4446\n",
      "Epoch 1681/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 499.4023 - val_loss: 234.2696\n",
      "Epoch 1682/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 257.4052 - val_loss: 832.9558\n",
      "Epoch 1683/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 648.4007 - val_loss: 219.0739\n",
      "Epoch 1684/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 235us/sample - loss: 431.2348 - val_loss: 1211.0015\n",
      "Epoch 1685/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 399.9167 - val_loss: 171.2773\n",
      "Epoch 1686/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 234.4881 - val_loss: 155.2667\n",
      "Epoch 1687/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 510.3431 - val_loss: 787.6034\n",
      "Epoch 1688/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 287.1396 - val_loss: 3.2743\n",
      "Epoch 1689/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 100.2457 - val_loss: 135.0371\n",
      "Epoch 1690/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 45.8132 - val_loss: 20.0063\n",
      "Epoch 1691/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 11.6295 - val_loss: 6.3388\n",
      "Epoch 1692/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.3501 - val_loss: 2.1045\n",
      "Epoch 1693/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.0437 - val_loss: 2.1029\n",
      "Epoch 1694/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5.7248 - val_loss: 7.6079\n",
      "Epoch 1695/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 23.2182 - val_loss: 18.4280\n",
      "Epoch 1696/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 19.0290 - val_loss: 30.2479\n",
      "Epoch 1697/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 35.2038 - val_loss: 60.6828\n",
      "Epoch 1698/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 24.5564 - val_loss: 8.3883\n",
      "Epoch 1699/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 18.6786 - val_loss: 28.1347\n",
      "Epoch 1700/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 13.6112 - val_loss: 7.4544\n",
      "Epoch 1701/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.5851 - val_loss: 10.3702\n",
      "Epoch 1702/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4130 - val_loss: 1.9939\n",
      "Epoch 1703/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9558 - val_loss: 3.9910\n",
      "Epoch 1704/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 3.2814 - val_loss: 1.6903\n",
      "Epoch 1705/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.6754 - val_loss: 4.6507\n",
      "Epoch 1706/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.5307 - val_loss: 0.9581\n",
      "Epoch 1707/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5174 - val_loss: 0.5118\n",
      "Epoch 1708/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0904 - val_loss: 0.0376\n",
      "Epoch 1709/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0242 - val_loss: 0.0186\n",
      "Epoch 1710/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0191 - val_loss: 0.0195\n",
      "Epoch 1711/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0314 - val_loss: 0.0571\n",
      "Epoch 1712/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0351 - val_loss: 0.0156\n",
      "Epoch 1713/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0128 - val_loss: 0.0135\n",
      "Epoch 1714/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 1715/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0069 - val_loss: 0.0121\n",
      "Epoch 1716/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0091 - val_loss: 0.0113\n",
      "Epoch 1717/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0118 - val_loss: 0.0151\n",
      "Epoch 1718/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0239 - val_loss: 0.0530\n",
      "Epoch 1719/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0279 - val_loss: 0.0158\n",
      "Epoch 1720/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0321 - val_loss: 0.1545\n",
      "Epoch 1721/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0841 - val_loss: 0.0141\n",
      "Epoch 1722/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0069 - val_loss: 0.0025\n",
      "Epoch 1723/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0135 - val_loss: 0.0308\n",
      "Epoch 1724/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0188 - val_loss: 0.0284\n",
      "Epoch 1725/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0170 - val_loss: 0.0255\n",
      "Epoch 1726/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0074 - val_loss: 0.0017\n",
      "Epoch 1727/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0060 - val_loss: 0.0036\n",
      "Epoch 1728/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0052 - val_loss: 0.0045\n",
      "Epoch 1729/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0035 - val_loss: 0.0011\n",
      "Epoch 1730/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 1731/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0022 - val_loss: 0.0060\n",
      "Epoch 1732/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0035 - val_loss: 0.0015\n",
      "Epoch 1733/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0120 - val_loss: 0.0015\n",
      "Epoch 1734/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0097 - val_loss: 0.0078\n",
      "Epoch 1735/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0090 - val_loss: 0.0076\n",
      "Epoch 1736/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0139 - val_loss: 0.0179\n",
      "Epoch 1737/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.0244 - val_loss: 0.0429\n",
      "Epoch 1738/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0168 - val_loss: 0.0083\n",
      "Epoch 1739/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.0498 - val_loss: 0.1235\n",
      "Epoch 1740/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.0903 - val_loss: 0.1346\n",
      "Epoch 1741/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0359 - val_loss: 0.0262\n",
      "Epoch 1742/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0233 - val_loss: 0.0042\n",
      "Epoch 1743/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.0148 - val_loss: 0.0011\n",
      "Epoch 1744/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0061 - val_loss: 0.0013\n",
      "Epoch 1745/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 1746/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0024 - val_loss: 0.0065\n",
      "Epoch 1747/10000\n",
      "68/68 [==============================] - 0s 956us/sample - loss: 0.0056 - val_loss: 0.0129\n",
      "Epoch 1748/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0096 - val_loss: 0.0023\n",
      "Epoch 1749/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 1750/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0039 - val_loss: 0.0084\n",
      "Epoch 1751/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.0036 - val_loss: 0.0112\n",
      "Epoch 1752/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0055 - val_loss: 0.0037\n",
      "Epoch 1753/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0060 - val_loss: 6.4963e-04\n",
      "Epoch 1754/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0074 - val_loss: 0.0079\n",
      "Epoch 1755/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 1756/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0177 - val_loss: 0.0229\n",
      "Epoch 1757/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0139 - val_loss: 0.0248\n",
      "Epoch 1758/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0072 - val_loss: 0.0093\n",
      "Epoch 1759/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0188 - val_loss: 0.0139\n",
      "Epoch 1760/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0785 - val_loss: 0.2344\n",
      "Epoch 1761/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 0.2198 - val_loss: 0.0073\n",
      "Epoch 1762/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1150 - val_loss: 0.0495\n",
      "Epoch 1763/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0457 - val_loss: 0.0311\n",
      "Epoch 1764/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1212 - val_loss: 0.0737\n",
      "Epoch 1765/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1007 - val_loss: 0.0394\n",
      "Epoch 1766/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0920 - val_loss: 0.0139\n",
      "Epoch 1767/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1271 - val_loss: 0.2188\n",
      "Epoch 1768/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1041 - val_loss: 0.0962\n",
      "Epoch 1769/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0803 - val_loss: 0.0361\n",
      "Epoch 1770/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0109 - val_loss: 0.2244\n",
      "Epoch 1771/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1077 - val_loss: 0.2032\n",
      "Epoch 1772/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1060 - val_loss: 0.0027\n",
      "Epoch 1773/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0162 - val_loss: 0.0051\n",
      "Epoch 1774/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0084 - val_loss: 0.0073\n",
      "Epoch 1775/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0047 - val_loss: 0.0069\n",
      "Epoch 1776/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0042 - val_loss: 0.0130\n",
      "Epoch 1777/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0400 - val_loss: 0.2164\n",
      "Epoch 1778/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1654 - val_loss: 0.8352\n",
      "Epoch 1779/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.1177 - val_loss: 1.2351\n",
      "Epoch 1780/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 16.6104 - val_loss: 45.6581\n",
      "Epoch 1781/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 135.1133 - val_loss: 50.1159\n",
      "Epoch 1782/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 39.5233 - val_loss: 67.8573\n",
      "Epoch 1783/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 36.5008 - val_loss: 13.7385\n",
      "Epoch 1784/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 18.0660 - val_loss: 16.3638\n",
      "Epoch 1785/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.9700 - val_loss: 2.2123\n",
      "Epoch 1786/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.8750 - val_loss: 3.8930\n",
      "Epoch 1787/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.6809 - val_loss: 3.5627\n",
      "Epoch 1788/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.4546 - val_loss: 18.1667\n",
      "Epoch 1789/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 15.5067 - val_loss: 10.0344\n",
      "Epoch 1790/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 17.0664 - val_loss: 30.2040\n",
      "Epoch 1791/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 30.5985 - val_loss: 16.7466\n",
      "Epoch 1792/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 18.3769 - val_loss: 28.3809\n",
      "Epoch 1793/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 9.2340 - val_loss: 14.7612\n",
      "Epoch 1794/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 9.5597 - val_loss: 13.1093\n",
      "Epoch 1795/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11.5867 - val_loss: 26.9403\n",
      "Epoch 1796/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 20.5748 - val_loss: 154.3631\n",
      "Epoch 1797/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 100.2783 - val_loss: 59.4041\n",
      "Epoch 1798/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 465.1775 - val_loss: 1859.0024\n",
      "Epoch 1799/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1854.0469 - val_loss: 2037.5560\n",
      "Epoch 1800/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 878.9923 - val_loss: 209.6678\n",
      "Epoch 1801/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 607.8590 - val_loss: 925.4091\n",
      "Epoch 1802/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2365.3488 - val_loss: 3771.5758\n",
      "Epoch 1803/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4195.4686 - val_loss: 6316.8014\n",
      "Epoch 1804/10000\n",
      "68/68 [==============================] - 0s 588us/sample - loss: 2208.4156 - val_loss: 73.6163\n",
      "Epoch 1805/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 867.8873 - val_loss: 1425.0451\n",
      "Epoch 1806/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 571.3523 - val_loss: 1125.0003\n",
      "Epoch 1807/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 655.2641 - val_loss: 9.8320\n",
      "Epoch 1808/10000\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 307.6828 - val_loss: 780.8418\n",
      "Epoch 1809/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 463.6642 - val_loss: 711.4801\n",
      "Epoch 1810/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 903.2491 - val_loss: 3587.4284\n",
      "Epoch 1811/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1100.4508 - val_loss: 274.3844\n",
      "Epoch 1812/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 225.8286 - val_loss: 389.0077\n",
      "Epoch 1813/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 151.3307 - val_loss: 109.8305\n",
      "Epoch 1814/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 58.3414 - val_loss: 9.2976\n",
      "Epoch 1815/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 25.6073 - val_loss: 13.8903\n",
      "Epoch 1816/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 24.3134 - val_loss: 35.4396\n",
      "Epoch 1817/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 35.2265 - val_loss: 65.9812\n",
      "Epoch 1818/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.8021 - val_loss: 16.4197\n",
      "Epoch 1819/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 8.4170 - val_loss: 2.2119\n",
      "Epoch 1820/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 4.7486 - val_loss: 5.5564\n",
      "Epoch 1821/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.5917 - val_loss: 6.3938\n",
      "Epoch 1822/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 12.3570 - val_loss: 4.5491\n",
      "Epoch 1823/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 14.4625 - val_loss: 53.1152\n",
      "Epoch 1824/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 56.4939 - val_loss: 35.4678\n",
      "Epoch 1825/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 118.1101 - val_loss: 451.6192\n",
      "Epoch 1826/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 335.6528 - val_loss: 94.2953\n",
      "Epoch 1827/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 456.9920 - val_loss: 52.6784\n",
      "Epoch 1828/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 161.5868 - val_loss: 158.9110\n",
      "Epoch 1829/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 188.8200 - val_loss: 390.2823\n",
      "Epoch 1830/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 85.2494 - val_loss: 287.9874\n",
      "Epoch 1831/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 127.1129 - val_loss: 252.8267\n",
      "Epoch 1832/10000\n",
      "68/68 [==============================] - 0s 779us/sample - loss: 123.5071 - val_loss: 80.9546\n",
      "Epoch 1833/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 279us/sample - loss: 154.0782 - val_loss: 545.0646\n",
      "Epoch 1834/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 285.1156 - val_loss: 40.0776\n",
      "Epoch 1835/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 274.9096 - val_loss: 196.6538\n",
      "Epoch 1836/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 192.1200 - val_loss: 300.8865\n",
      "Epoch 1837/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 121.4771 - val_loss: 89.5604\n",
      "Epoch 1838/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 49.3802 - val_loss: 45.9934\n",
      "Epoch 1839/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 62.5833 - val_loss: 43.0483\n",
      "Epoch 1840/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 46.5773 - val_loss: 83.2199\n",
      "Epoch 1841/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 37.2940 - val_loss: 30.6085\n",
      "Epoch 1842/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 61.5574 - val_loss: 299.2181\n",
      "Epoch 1843/10000\n",
      "68/68 [==============================] - 0s 647us/sample - loss: 133.1725 - val_loss: 37.7129\n",
      "Epoch 1844/10000\n",
      "68/68 [==============================] - 0s 809us/sample - loss: 52.4544 - val_loss: 28.2402\n",
      "Epoch 1845/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 36.5354 - val_loss: 46.1862\n",
      "Epoch 1846/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 15.9422 - val_loss: 10.0437\n",
      "Epoch 1847/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.6764 - val_loss: 1.4792\n",
      "Epoch 1848/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 1.4327 - val_loss: 2.2495\n",
      "Epoch 1849/10000\n",
      "68/68 [==============================] - 0s 794us/sample - loss: 0.7970 - val_loss: 1.2354\n",
      "Epoch 1850/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.4403 - val_loss: 3.4514\n",
      "Epoch 1851/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.6367 - val_loss: 3.1996\n",
      "Epoch 1852/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.0713 - val_loss: 0.0665\n",
      "Epoch 1853/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 0.2726 - val_loss: 0.0908\n",
      "Epoch 1854/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.3047 - val_loss: 0.2441\n",
      "Epoch 1855/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.1532 - val_loss: 0.0279\n",
      "Epoch 1856/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0755 - val_loss: 0.0723\n",
      "Epoch 1857/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0344 - val_loss: 0.0665\n",
      "Epoch 1858/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0323 - val_loss: 0.0324\n",
      "Epoch 1859/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0624 - val_loss: 0.0183\n",
      "Epoch 1860/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 0.0338 - val_loss: 0.0263\n",
      "Epoch 1861/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.0801 - val_loss: 0.1029\n",
      "Epoch 1862/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1995 - val_loss: 0.8973\n",
      "Epoch 1863/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.3658 - val_loss: 0.0866\n",
      "Epoch 1864/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.1197 - val_loss: 0.0812\n",
      "Epoch 1865/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.1091 - val_loss: 0.0550\n",
      "Epoch 1866/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.1931 - val_loss: 0.1475\n",
      "Epoch 1867/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.1528 - val_loss: 0.1528\n",
      "Epoch 1868/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1504 - val_loss: 0.2158\n",
      "Epoch 1869/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1385 - val_loss: 0.1562\n",
      "Epoch 1870/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3663 - val_loss: 0.1000\n",
      "Epoch 1871/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2377 - val_loss: 0.4087\n",
      "Epoch 1872/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1367 - val_loss: 0.0663\n",
      "Epoch 1873/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0830 - val_loss: 0.0167\n",
      "Epoch 1874/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1420 - val_loss: 0.1585\n",
      "Epoch 1875/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2288 - val_loss: 0.3787\n",
      "Epoch 1876/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.1315 - val_loss: 0.0408\n",
      "Epoch 1877/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0423 - val_loss: 0.0566\n",
      "Epoch 1878/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0247 - val_loss: 0.0192\n",
      "Epoch 1879/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.0617 - val_loss: 0.0112\n",
      "Epoch 1880/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 0.1413 - val_loss: 0.0191\n",
      "Epoch 1881/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5036 - val_loss: 0.2395\n",
      "Epoch 1882/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.2480 - val_loss: 0.4327\n",
      "Epoch 1883/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.7647 - val_loss: 0.4608\n",
      "Epoch 1884/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.1759 - val_loss: 0.2989\n",
      "Epoch 1885/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5013 - val_loss: 2.0262\n",
      "Epoch 1886/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.0186 - val_loss: 0.7962\n",
      "Epoch 1887/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.2567 - val_loss: 0.4630\n",
      "Epoch 1888/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 1.5817 - val_loss: 0.1125\n",
      "Epoch 1889/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 3.0456 - val_loss: 4.0014\n",
      "Epoch 1890/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.8000 - val_loss: 24.2426\n",
      "Epoch 1891/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 10.9432 - val_loss: 10.5183\n",
      "Epoch 1892/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 8.979 - 0s 927us/sample - loss: 6.4407 - val_loss: 9.6772\n",
      "Epoch 1893/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 5.8838 - val_loss: 5.6045\n",
      "Epoch 1894/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.0396 - val_loss: 6.7698\n",
      "Epoch 1895/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 3.7815 - val_loss: 12.1963\n",
      "Epoch 1896/10000\n",
      "68/68 [==============================] - 0s 735us/sample - loss: 5.9753 - val_loss: 8.0414\n",
      "Epoch 1897/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 1.9284 - val_loss: 1.0035\n",
      "Epoch 1898/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.4647 - val_loss: 2.0082\n",
      "Epoch 1899/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 2.9657 - val_loss: 11.4490\n",
      "Epoch 1900/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 6.9370 - val_loss: 0.3760\n",
      "Epoch 1901/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 2.9091 - val_loss: 7.3689\n",
      "Epoch 1902/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.4525 - val_loss: 1.6880\n",
      "Epoch 1903/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 2.1840 - val_loss: 0.1463\n",
      "Epoch 1904/10000\n",
      "68/68 [==============================] - 0s 750us/sample - loss: 0.6098 - val_loss: 0.0814\n",
      "Epoch 1905/10000\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 0.2749 - val_loss: 0.4277\n",
      "Epoch 1906/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3853 - val_loss: 0.4101\n",
      "Epoch 1907/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.7063 - val_loss: 0.0582\n",
      "Epoch 1908/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.4202 - val_loss: 0.2982\n",
      "Epoch 1909/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6448 - val_loss: 0.6632\n",
      "Epoch 1910/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4376 - val_loss: 2.0007\n",
      "Epoch 1911/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.8246 - val_loss: 3.8635\n",
      "Epoch 1912/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 5.3712 - val_loss: 5.7939\n",
      "Epoch 1913/10000\n",
      "68/68 [==============================] - 0s 794us/sample - loss: 9.2752 - val_loss: 11.4264\n",
      "Epoch 1914/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 11.4204 - val_loss: 11.8350\n",
      "Epoch 1915/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.7083 - val_loss: 10.0971\n",
      "Epoch 1916/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.9457 - val_loss: 7.5405\n",
      "Epoch 1917/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.9978 - val_loss: 5.0355\n",
      "Epoch 1918/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.8375 - val_loss: 9.1513\n",
      "Epoch 1919/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.6005 - val_loss: 6.6738\n",
      "Epoch 1920/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.8381 - val_loss: 13.5475\n",
      "Epoch 1921/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 5.8598 - val_loss: 0.2081\n",
      "Epoch 1922/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3.8242 - val_loss: 18.4591\n",
      "Epoch 1923/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 11.5708 - val_loss: 19.6078\n",
      "Epoch 1924/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 76.1820 - val_loss: 362.8361\n",
      "Epoch 1925/10000\n",
      "68/68 [==============================] - 0s 647us/sample - loss: 606.3359 - val_loss: 82.8271\n",
      "Epoch 1926/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 164.5582 - val_loss: 1353.6514\n",
      "Epoch 1927/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 495.0933 - val_loss: 161.7097\n",
      "Epoch 1928/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 294.3611 - val_loss: 54.8721\n",
      "Epoch 1929/10000\n",
      "68/68 [==============================] - 0s 868us/sample - loss: 108.1969 - val_loss: 26.3761\n",
      "Epoch 1930/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 152.9107 - val_loss: 161.0690\n",
      "Epoch 1931/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 149.8584 - val_loss: 342.4760\n",
      "Epoch 1932/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 634.2217 - val_loss: 5963.5561\n",
      "Epoch 1933/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1607.6073 - val_loss: 296.3059\n",
      "Epoch 1934/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 466.0117 - val_loss: 93.4766\n",
      "Epoch 1935/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 625.9167 - val_loss: 362.3757\n",
      "Epoch 1936/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 246.3184 - val_loss: 1326.9187\n",
      "Epoch 1937/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 761.7726 - val_loss: 1374.1429\n",
      "Epoch 1938/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 485.5717 - val_loss: 590.0517\n",
      "Epoch 1939/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 565.7744 - val_loss: 360.5221\n",
      "Epoch 1940/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 848.6330 - val_loss: 785.0312\n",
      "Epoch 1941/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 491.8618 - val_loss: 1658.4853\n",
      "Epoch 1942/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 755.6906 - val_loss: 965.2897\n",
      "Epoch 1943/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 360.1889 - val_loss: 122.3259\n",
      "Epoch 1944/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 200.2023 - val_loss: 75.0482\n",
      "Epoch 1945/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 199.3932 - val_loss: 381.2144\n",
      "Epoch 1946/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 222.7958 - val_loss: 283.2297\n",
      "Epoch 1947/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 106.9503 - val_loss: 61.2311\n",
      "Epoch 1948/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 74.9233 - val_loss: 131.8295\n",
      "Epoch 1949/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 42.6079 - val_loss: 79.5624\n",
      "Epoch 1950/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 66.1442 - val_loss: 4.5370\n",
      "Epoch 1951/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 26.6428 - val_loss: 39.7750\n",
      "Epoch 1952/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.5708 - val_loss: 6.3846\n",
      "Epoch 1953/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.5724 - val_loss: 5.3501\n",
      "Epoch 1954/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 15.0765 - val_loss: 13.9674\n",
      "Epoch 1955/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 4.2980 - val_loss: 10.0407\n",
      "Epoch 1956/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.9128 - val_loss: 19.9253\n",
      "Epoch 1957/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 31.2745 - val_loss: 18.6935\n",
      "Epoch 1958/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 158.7577 - val_loss: 284.9294\n",
      "Epoch 1959/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 259.7187 - val_loss: 514.1504\n",
      "Epoch 1960/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1071.3791 - val_loss: 1561.8897\n",
      "Epoch 1961/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1539.2708 - val_loss: 2622.2223\n",
      "Epoch 1962/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1455.3969 - val_loss: 776.1675\n",
      "Epoch 1963/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1529.9585 - val_loss: 5026.1667\n",
      "Epoch 1964/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2341.8214 - val_loss: 537.7840\n",
      "Epoch 1965/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 713.2131 - val_loss: 747.3273\n",
      "Epoch 1966/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 337.7479 - val_loss: 271.5376\n",
      "Epoch 1967/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 219.7876 - val_loss: 33.7539\n",
      "Epoch 1968/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 102.4165 - val_loss: 78.9336\n",
      "Epoch 1969/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 88.1323 - val_loss: 88.5604\n",
      "Epoch 1970/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 60.8407 - val_loss: 221.6969\n",
      "Epoch 1971/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 108.3848 - val_loss: 109.5016\n",
      "Epoch 1972/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 283.6427 - val_loss: 460.5638\n",
      "Epoch 1973/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 818.9472 - val_loss: 618.4459\n",
      "Epoch 1974/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 398.1889 - val_loss: 129.5344\n",
      "Epoch 1975/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 116.1771 - val_loss: 22.9811\n",
      "Epoch 1976/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 202.7146 - val_loss: 80.1937\n",
      "Epoch 1977/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 122.8937 - val_loss: 116.5297\n",
      "Epoch 1978/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 50.8951 - val_loss: 13.3031\n",
      "Epoch 1979/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.9070 - val_loss: 14.8536\n",
      "Epoch 1980/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.8837 - val_loss: 7.9898\n",
      "Epoch 1981/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.5937 - val_loss: 26.5795\n",
      "Epoch 1982/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 176us/sample - loss: 11.1159 - val_loss: 7.0479\n",
      "Epoch 1983/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.1537 - val_loss: 1.7431\n",
      "Epoch 1984/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.7390 - val_loss: 2.7054\n",
      "Epoch 1985/10000\n",
      "68/68 [==============================] - 0s 868us/sample - loss: 3.8538 - val_loss: 3.3974\n",
      "Epoch 1986/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 1.393 - 0s 206us/sample - loss: 7.6654 - val_loss: 28.2287\n",
      "Epoch 1987/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 27.4331 - val_loss: 14.5273\n",
      "Epoch 1988/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 48.7091 - val_loss: 134.4682\n",
      "Epoch 1989/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 151.2765 - val_loss: 14.0176\n",
      "Epoch 1990/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 73.1272 - val_loss: 128.5259\n",
      "Epoch 1991/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 56.3463 - val_loss: 40.0551\n",
      "Epoch 1992/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 56.6562 - val_loss: 68.2092\n",
      "Epoch 1993/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 87.8981 - val_loss: 124.7665\n",
      "Epoch 1994/10000\n",
      "68/68 [==============================] - 0s 750us/sample - loss: 53.1410 - val_loss: 9.4566\n",
      "Epoch 1995/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.0337 - val_loss: 0.1274\n",
      "Epoch 1996/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 10.9295 - val_loss: 33.3617\n",
      "Epoch 1997/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 29.5254 - val_loss: 1.2753\n",
      "Epoch 1998/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 33.5896 - val_loss: 24.6615\n",
      "Epoch 1999/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.5049 - val_loss: 13.0644\n",
      "Epoch 2000/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 9.4996 - val_loss: 15.9456\n",
      "Epoch 2001/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 6.4644 - val_loss: 8.6137\n",
      "Epoch 2002/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.3510 - val_loss: 0.3149\n",
      "Epoch 2003/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7053 - val_loss: 0.6073\n",
      "Epoch 2004/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3516 - val_loss: 1.3967\n",
      "Epoch 2005/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7169 - val_loss: 3.6013\n",
      "Epoch 2006/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.2590 - val_loss: 1.4411\n",
      "Epoch 2007/10000\n",
      "68/68 [==============================] - 0s 647us/sample - loss: 0.9370 - val_loss: 0.2388\n",
      "Epoch 2008/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.3749 - val_loss: 0.5653\n",
      "Epoch 2009/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7795 - val_loss: 1.1886\n",
      "Epoch 2010/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8136 - val_loss: 0.6386\n",
      "Epoch 2011/10000\n",
      "68/68 [==============================] - 0s 882us/sample - loss: 1.4097 - val_loss: 1.2438\n",
      "Epoch 2012/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6536 - val_loss: 1.5695\n",
      "Epoch 2013/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.7135 - val_loss: 1.4507\n",
      "Epoch 2014/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.1282 - val_loss: 0.2463\n",
      "Epoch 2015/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 1.1091 - val_loss: 2.0994\n",
      "Epoch 2016/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 2.1586 - val_loss: 1.7529\n",
      "Epoch 2017/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.8977 - val_loss: 1.6438\n",
      "Epoch 2018/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.8800 - val_loss: 0.5916\n",
      "Epoch 2019/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.4269 - val_loss: 0.4103\n",
      "Epoch 2020/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3553 - val_loss: 1.4579\n",
      "Epoch 2021/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1633 - val_loss: 1.0734\n",
      "Epoch 2022/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6524 - val_loss: 1.2545\n",
      "Epoch 2023/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.4454 - val_loss: 0.5436\n",
      "Epoch 2024/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2076 - val_loss: 0.2033\n",
      "Epoch 2025/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1696 - val_loss: 0.1858\n",
      "Epoch 2026/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1180 - val_loss: 0.7460\n",
      "Epoch 2027/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5285 - val_loss: 2.2622\n",
      "Epoch 2028/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 1.6081 - val_loss: 2.4484\n",
      "Epoch 2029/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.8206 - val_loss: 0.9476\n",
      "Epoch 2030/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 2.9056 - val_loss: 10.3386\n",
      "Epoch 2031/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.8600 - val_loss: 19.8708\n",
      "Epoch 2032/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 12.6163 - val_loss: 14.3016\n",
      "Epoch 2033/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 17.1590 - val_loss: 8.9774\n",
      "Epoch 2034/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.6966 - val_loss: 23.5260\n",
      "Epoch 2035/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 11.4321 - val_loss: 19.4163\n",
      "Epoch 2036/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.6430 - val_loss: 5.6965\n",
      "Epoch 2037/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.5065 - val_loss: 0.9157\n",
      "Epoch 2038/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9796 - val_loss: 15.0241\n",
      "Epoch 2039/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.7449 - val_loss: 6.6641\n",
      "Epoch 2040/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.0028 - val_loss: 5.8550\n",
      "Epoch 2041/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 4.0488 - val_loss: 38.2801\n",
      "Epoch 2042/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 23.1087 - val_loss: 25.0116\n",
      "Epoch 2043/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 27.7433 - val_loss: 3.9788\n",
      "Epoch 2044/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.4531 - val_loss: 1.5239\n",
      "Epoch 2045/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 2.0107 - val_loss: 5.3596\n",
      "Epoch 2046/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.2618 - val_loss: 6.6229\n",
      "Epoch 2047/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.1127 - val_loss: 0.5716\n",
      "Epoch 2048/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 19.5717 - val_loss: 21.1203\n",
      "Epoch 2049/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 27.9904 - val_loss: 28.2473\n",
      "Epoch 2050/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 102.1428 - val_loss: 52.6315\n",
      "Epoch 2051/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 475.2943 - val_loss: 818.0897\n",
      "Epoch 2052/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3004.7217 - val_loss: 137.5577\n",
      "Epoch 2053/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2518.2465 - val_loss: 894.1067\n",
      "Epoch 2054/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 2765.4011 - val_loss: 10484.5936\n",
      "Epoch 2055/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5279.3076 - val_loss: 6284.3562\n",
      "Epoch 2056/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3200.7274 - val_loss: 378.2464\n",
      "Epoch 2057/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 176us/sample - loss: 2221.7565 - val_loss: 4393.0061\n",
      "Epoch 2058/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2808.1990 - val_loss: 2983.5416\n",
      "Epoch 2059/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1441.2243 - val_loss: 1519.3606\n",
      "Epoch 2060/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 1395.7242 - val_loss: 2192.7793\n",
      "Epoch 2061/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 1678.7338 - val_loss: 2598.8230\n",
      "Epoch 2062/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1155.9008 - val_loss: 1172.9771\n",
      "Epoch 2063/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1278.6137 - val_loss: 581.3608\n",
      "Epoch 2064/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 683.3640 - val_loss: 260.8107\n",
      "Epoch 2065/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 444.0005 - val_loss: 88.4617\n",
      "Epoch 2066/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 168.3292 - val_loss: 115.3976\n",
      "Epoch 2067/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 86.7900 - val_loss: 3.3183\n",
      "Epoch 2068/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.3783 - val_loss: 2.2854\n",
      "Epoch 2069/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 12.4024 - val_loss: 11.7413\n",
      "Epoch 2070/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 10.3883 - val_loss: 10.6050\n",
      "Epoch 2071/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.2253 - val_loss: 0.2643\n",
      "Epoch 2072/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 5.3814 - val_loss: 1.1761\n",
      "Epoch 2073/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.6043 - val_loss: 1.3262\n",
      "Epoch 2074/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.2206 - val_loss: 5.1238\n",
      "Epoch 2075/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.2551 - val_loss: 3.8105\n",
      "Epoch 2076/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7504 - val_loss: 1.7025\n",
      "Epoch 2077/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7491 - val_loss: 0.8782\n",
      "Epoch 2078/10000\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 0.4701 - val_loss: 0.7744\n",
      "Epoch 2079/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.3535 - val_loss: 0.4218\n",
      "Epoch 2080/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2801 - val_loss: 0.2612\n",
      "Epoch 2081/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3631 - val_loss: 0.1293\n",
      "Epoch 2082/10000\n",
      "68/68 [==============================] - 0s 706us/sample - loss: 0.7502 - val_loss: 0.2442\n",
      "Epoch 2083/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.7009 - val_loss: 0.4606\n",
      "Epoch 2084/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.8777 - val_loss: 1.5517\n",
      "Epoch 2085/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.6706 - val_loss: 0.7484\n",
      "Epoch 2086/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.3239 - val_loss: 0.1888\n",
      "Epoch 2087/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.3552 - val_loss: 0.1751\n",
      "Epoch 2088/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.3506 - val_loss: 0.4496\n",
      "Epoch 2089/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2739 - val_loss: 0.3156\n",
      "Epoch 2090/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.3231 - val_loss: 1.8242\n",
      "Epoch 2091/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.6035 - val_loss: 0.4290\n",
      "Epoch 2092/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3589 - val_loss: 0.0767\n",
      "Epoch 2093/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1237 - val_loss: 0.0611\n",
      "Epoch 2094/10000\n",
      "68/68 [==============================] - 0s 912us/sample - loss: 0.0907 - val_loss: 0.0683\n",
      "Epoch 2095/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0941 - val_loss: 0.0356\n",
      "Epoch 2096/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2371 - val_loss: 0.1801\n",
      "Epoch 2097/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1654 - val_loss: 0.0484\n",
      "Epoch 2098/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1238 - val_loss: 0.2294\n",
      "Epoch 2099/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1705 - val_loss: 0.2128\n",
      "Epoch 2100/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1072 - val_loss: 0.0967\n",
      "Epoch 2101/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1213 - val_loss: 0.1626\n",
      "Epoch 2102/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1324 - val_loss: 0.1018\n",
      "Epoch 2103/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.0784 - val_loss: 0.0433\n",
      "Epoch 2104/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0576 - val_loss: 0.0667\n",
      "Epoch 2105/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0482 - val_loss: 0.0525\n",
      "Epoch 2106/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0528 - val_loss: 0.0289\n",
      "Epoch 2107/10000\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 0.0412 - val_loss: 0.0575\n",
      "Epoch 2108/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0646 - val_loss: 0.0356\n",
      "Epoch 2109/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0729 - val_loss: 0.0367\n",
      "Epoch 2110/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0819 - val_loss: 0.3251\n",
      "Epoch 2111/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1183 - val_loss: 0.0616\n",
      "Epoch 2112/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0389 - val_loss: 0.0296\n",
      "Epoch 2113/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0416 - val_loss: 0.0438\n",
      "Epoch 2114/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0446 - val_loss: 0.0308\n",
      "Epoch 2115/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0338 - val_loss: 0.0253\n",
      "Epoch 2116/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0340 - val_loss: 0.0607\n",
      "Epoch 2117/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0374 - val_loss: 0.0482\n",
      "Epoch 2118/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0501 - val_loss: 0.2021\n",
      "Epoch 2119/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2292 - val_loss: 0.2284\n",
      "Epoch 2120/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.3135 - val_loss: 0.3382\n",
      "Epoch 2121/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3049 - val_loss: 0.7017\n",
      "Epoch 2122/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5128 - val_loss: 0.4588\n",
      "Epoch 2123/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.3291 - val_loss: 0.0882\n",
      "Epoch 2124/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1397 - val_loss: 0.0811\n",
      "Epoch 2125/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0722 - val_loss: 0.0658\n",
      "Epoch 2126/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0513 - val_loss: 0.0609\n",
      "Epoch 2127/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0445 - val_loss: 0.0605\n",
      "Epoch 2128/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0619 - val_loss: 0.3072\n",
      "Epoch 2129/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1969 - val_loss: 0.0286\n",
      "Epoch 2130/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0757 - val_loss: 0.0190\n",
      "Epoch 2131/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0785 - val_loss: 0.0839\n",
      "Epoch 2132/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0397 - val_loss: 0.0333\n",
      "Epoch 2133/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0440 - val_loss: 0.0527\n",
      "Epoch 2134/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0764 - val_loss: 0.0404\n",
      "Epoch 2135/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1246 - val_loss: 0.3006\n",
      "Epoch 2136/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2515 - val_loss: 0.3876\n",
      "Epoch 2137/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1674 - val_loss: 0.0986\n",
      "Epoch 2138/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1639 - val_loss: 0.4605\n",
      "Epoch 2139/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.9211 - val_loss: 0.6128\n",
      "Epoch 2140/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.2761 - val_loss: 0.6269\n",
      "Epoch 2141/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.7885 - val_loss: 0.0284\n",
      "Epoch 2142/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8888 - val_loss: 1.0211\n",
      "Epoch 2143/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4054 - val_loss: 0.0595\n",
      "Epoch 2144/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.2634 - val_loss: 0.5971\n",
      "Epoch 2145/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.5032 - val_loss: 0.0398\n",
      "Epoch 2146/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2372 - val_loss: 0.3093\n",
      "Epoch 2147/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1159 - val_loss: 0.1011\n",
      "Epoch 2148/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0456 - val_loss: 0.0912\n",
      "Epoch 2149/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0571 - val_loss: 0.0166\n",
      "Epoch 2150/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0378 - val_loss: 0.0279\n",
      "Epoch 2151/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0318 - val_loss: 0.0232\n",
      "Epoch 2152/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0499 - val_loss: 0.1590\n",
      "Epoch 2153/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0657 - val_loss: 0.0200\n",
      "Epoch 2154/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0958 - val_loss: 0.3464\n",
      "Epoch 2155/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.1142 - val_loss: 0.0428\n",
      "Epoch 2156/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0256 - val_loss: 0.0199\n",
      "Epoch 2157/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.1516 - val_loss: 0.1502\n",
      "Epoch 2158/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.9376 - val_loss: 2.9781\n",
      "Epoch 2159/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.6324 - val_loss: 0.1705\n",
      "Epoch 2160/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1.1609 - val_loss: 0.8273\n",
      "Epoch 2161/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 1.0952 - val_loss: 3.7484\n",
      "Epoch 2162/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.1328 - val_loss: 1.3065\n",
      "Epoch 2163/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6830 - val_loss: 3.1744\n",
      "Epoch 2164/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.3517 - val_loss: 3.4232\n",
      "Epoch 2165/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1096 - val_loss: 0.1706\n",
      "Epoch 2166/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1994 - val_loss: 0.0977\n",
      "Epoch 2167/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0628 - val_loss: 0.0968\n",
      "Epoch 2168/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.0767 - val_loss: 0.2903\n",
      "Epoch 2169/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2998 - val_loss: 0.4190\n",
      "Epoch 2170/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.2049 - val_loss: 0.0946\n",
      "Epoch 2171/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.1503 - val_loss: 0.4086\n",
      "Epoch 2172/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.7560 - val_loss: 0.1456\n",
      "Epoch 2173/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.7467 - val_loss: 0.0772\n",
      "Epoch 2174/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1401 - val_loss: 0.7123\n",
      "Epoch 2175/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.3423 - val_loss: 0.2522\n",
      "Epoch 2176/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9513 - val_loss: 1.6995\n",
      "Epoch 2177/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 1.1293 - val_loss: 0.6086\n",
      "Epoch 2178/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.6211 - val_loss: 0.1860\n",
      "Epoch 2179/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2280 - val_loss: 0.1239\n",
      "Epoch 2180/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3895 - val_loss: 0.8010\n",
      "Epoch 2181/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.8898 - val_loss: 1.7644\n",
      "Epoch 2182/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.3753 - val_loss: 0.3327\n",
      "Epoch 2183/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 4.7253 - val_loss: 3.1871\n",
      "Epoch 2184/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 3.3048 - val_loss: 0.5437\n",
      "Epoch 2185/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.8220 - val_loss: 66.6492\n",
      "Epoch 2186/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 60.7800 - val_loss: 49.6326\n",
      "Epoch 2187/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 63.3269 - val_loss: 63.8012\n",
      "Epoch 2188/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 83.1294 - val_loss: 518.4751\n",
      "Epoch 2189/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 164.4320 - val_loss: 453.7265\n",
      "Epoch 2190/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 306.1160 - val_loss: 359.1000\n",
      "Epoch 2191/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 223.5315 - val_loss: 308.9103\n",
      "Epoch 2192/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 140.7326 - val_loss: 649.7361\n",
      "Epoch 2193/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 957.2795 - val_loss: 1752.5921\n",
      "Epoch 2194/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 742.1401 - val_loss: 225.3536\n",
      "Epoch 2195/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 352.0457 - val_loss: 104.6208\n",
      "Epoch 2196/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 171.3970 - val_loss: 96.6473\n",
      "Epoch 2197/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 152.5783 - val_loss: 634.0213\n",
      "Epoch 2198/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 435.9915 - val_loss: 1210.2821\n",
      "Epoch 2199/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 500.2182 - val_loss: 327.4023\n",
      "Epoch 2200/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 163.0893 - val_loss: 179.4633\n",
      "Epoch 2201/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 161.6371 - val_loss: 170.6561\n",
      "Epoch 2202/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 102.4719 - val_loss: 45.8900\n",
      "Epoch 2203/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 51.1491 - val_loss: 11.6367\n",
      "Epoch 2204/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 32.7060 - val_loss: 63.0950\n",
      "Epoch 2205/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 33.0842 - val_loss: 62.3089\n",
      "Epoch 2206/10000\n",
      "68/68 [==============================] - 0s 544us/sample - loss: 42.6584 - val_loss: 85.2482\n",
      "Epoch 2207/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 68.9618 - val_loss: 6.5018\n",
      "Epoch 2208/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 87.3845 - val_loss: 147.2039\n",
      "Epoch 2209/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 389.4845 - val_loss: 943.4368\n",
      "Epoch 2210/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 769.7968 - val_loss: 2485.1965\n",
      "Epoch 2211/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1127.2015 - val_loss: 1485.2507\n",
      "Epoch 2212/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 2404.1129 - val_loss: 539.6205\n",
      "Epoch 2213/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 575.2371 - val_loss: 1056.0526\n",
      "Epoch 2214/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1310.2670 - val_loss: 441.0165\n",
      "Epoch 2215/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 912.5403 - val_loss: 1185.6707\n",
      "Epoch 2216/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 1105.6237 - val_loss: 480.8519\n",
      "Epoch 2217/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 623.9754 - val_loss: 98.3070\n",
      "Epoch 2218/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 312.1132 - val_loss: 398.6730\n",
      "Epoch 2219/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 284.5636 - val_loss: 325.6839\n",
      "Epoch 2220/10000\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 218.5088 - val_loss: 566.5896\n",
      "Epoch 2221/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 306.7146 - val_loss: 18.8704\n",
      "Epoch 2222/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 126.3067 - val_loss: 45.2475\n",
      "Epoch 2223/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 58.1458 - val_loss: 47.4107\n",
      "Epoch 2224/10000\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 55.8949 - val_loss: 28.8185\n",
      "Epoch 2225/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 31.1655 - val_loss: 76.0542\n",
      "Epoch 2226/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 27.8144 - val_loss: 12.1104\n",
      "Epoch 2227/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 14.4202 - val_loss: 15.0487\n",
      "Epoch 2228/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 10.3617 - val_loss: 33.8036\n",
      "Epoch 2229/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 15.2887 - val_loss: 7.0451\n",
      "Epoch 2230/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 22.1520 - val_loss: 61.3332\n",
      "Epoch 2231/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 33.4461 - val_loss: 37.5768\n",
      "Epoch 2232/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26.7174 - val_loss: 10.6187\n",
      "Epoch 2233/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 79.6278 - val_loss: 435.4759\n",
      "Epoch 2234/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 359.9201 - val_loss: 467.8225\n",
      "Epoch 2235/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 466.8926 - val_loss: 207.2623\n",
      "Epoch 2236/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 98.0712 - val_loss: 87.2723\n",
      "Epoch 2237/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 96.7675 - val_loss: 280.6046\n",
      "Epoch 2238/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 197.0255 - val_loss: 34.8939\n",
      "Epoch 2239/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 77.6649 - val_loss: 197.0817\n",
      "Epoch 2240/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 88.0586 - val_loss: 158.3338\n",
      "Epoch 2241/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 74.7001 - val_loss: 40.5196\n",
      "Epoch 2242/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 98.1000 - val_loss: 578.3219\n",
      "Epoch 2243/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 252.6912 - val_loss: 332.5225\n",
      "Epoch 2244/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 242.3633 - val_loss: 376.9375\n",
      "Epoch 2245/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 1048.1863 - val_loss: 947.8627\n",
      "Epoch 2246/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2333.0073 - val_loss: 334.8009\n",
      "Epoch 2247/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1467.7391 - val_loss: 4436.9020\n",
      "Epoch 2248/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4172.5552 - val_loss: 1093.1158\n",
      "Epoch 2249/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3763.2968 - val_loss: 4147.7551\n",
      "Epoch 2250/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1649.6906 - val_loss: 2055.4720\n",
      "Epoch 2251/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1162.6450 - val_loss: 2822.8480\n",
      "Epoch 2252/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1535.3787 - val_loss: 4674.0753\n",
      "Epoch 2253/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2104.5026 - val_loss: 3551.3378\n",
      "Epoch 2254/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1519.6905 - val_loss: 2826.7755\n",
      "Epoch 2255/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1345.1048 - val_loss: 2370.1817\n",
      "Epoch 2256/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1569.2052 - val_loss: 5704.3819\n",
      "Epoch 2257/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1855.1348 - val_loss: 4323.1289\n",
      "Epoch 2258/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 1650.8589 - val_loss: 2084.5160\n",
      "Epoch 2259/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2084.7494 - val_loss: 4655.7671\n",
      "Epoch 2260/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1568.2009 - val_loss: 1964.6072\n",
      "Epoch 2261/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 771.4639 - val_loss: 545.9461\n",
      "Epoch 2262/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 224.1017 - val_loss: 133.8460\n",
      "Epoch 2263/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 93.2661 - val_loss: 66.8228\n",
      "Epoch 2264/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 69.1229 - val_loss: 65.9357\n",
      "Epoch 2265/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 48.1652 - val_loss: 47.9866\n",
      "Epoch 2266/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 66.0694 - val_loss: 49.1448\n",
      "Epoch 2267/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 78.8852 - val_loss: 68.2371\n",
      "Epoch 2268/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 168.0251 - val_loss: 284.6751\n",
      "Epoch 2269/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 160.5446 - val_loss: 89.8269\n",
      "Epoch 2270/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 31.7125 - val_loss: 24.0016\n",
      "Epoch 2271/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 12.0845 - val_loss: 12.1436\n",
      "Epoch 2272/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 6.8877 - val_loss: 3.1713\n",
      "Epoch 2273/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.2783 - val_loss: 5.1384\n",
      "Epoch 2274/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.1898 - val_loss: 1.9439\n",
      "Epoch 2275/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 0.6354 - val_loss: 0.1559\n",
      "Epoch 2276/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3641 - val_loss: 0.0175\n",
      "Epoch 2277/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1850 - val_loss: 0.0785\n",
      "Epoch 2278/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0760 - val_loss: 0.0028\n",
      "Epoch 2279/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0721 - val_loss: 0.0846\n",
      "Epoch 2280/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 0.0680 - val_loss: 0.0896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2281/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0538 - val_loss: 0.0639\n",
      "Epoch 2282/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0578 - val_loss: 0.0662\n",
      "Epoch 2283/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0922 - val_loss: 0.0482\n",
      "Epoch 2284/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0534 - val_loss: 0.0744\n",
      "Epoch 2285/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0723 - val_loss: 0.0949\n",
      "Epoch 2286/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1147 - val_loss: 0.1750\n",
      "Epoch 2287/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0982 - val_loss: 0.1379\n",
      "Epoch 2288/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0652 - val_loss: 0.1152\n",
      "Epoch 2289/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0554 - val_loss: 0.0722\n",
      "Epoch 2290/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.066 - 0s 162us/sample - loss: 0.0408 - val_loss: 0.0923\n",
      "Epoch 2291/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0480 - val_loss: 0.1110\n",
      "Epoch 2292/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1231 - val_loss: 0.2681\n",
      "Epoch 2293/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1421 - val_loss: 0.1695\n",
      "Epoch 2294/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0645 - val_loss: 0.0920\n",
      "Epoch 2295/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1041 - val_loss: 0.0286\n",
      "Epoch 2296/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0350 - val_loss: 0.0209\n",
      "Epoch 2297/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0348 - val_loss: 0.0259\n",
      "Epoch 2298/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0140 - val_loss: 0.0091\n",
      "Epoch 2299/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0108 - val_loss: 0.0253\n",
      "Epoch 2300/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0120 - val_loss: 0.0134\n",
      "Epoch 2301/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0083 - val_loss: 0.0037\n",
      "Epoch 2302/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0034 - val_loss: 6.4380e-05\n",
      "Epoch 2303/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.0023 - val_loss: 0.0038\n",
      "Epoch 2304/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0027 - val_loss: 0.0053\n",
      "Epoch 2305/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0046 - val_loss: 0.0072\n",
      "Epoch 2306/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0033 - val_loss: 0.0045\n",
      "Epoch 2307/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 2308/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0012 - val_loss: 3.3424e-04\n",
      "Epoch 2309/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 7.0918e-04 - val_loss: 0.0014\n",
      "Epoch 2310/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.4684e-04 - val_loss: 0.0013\n",
      "Epoch 2311/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.5449e-04 - val_loss: 2.0440e-04\n",
      "Epoch 2312/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.8552e-04 - val_loss: 6.9580e-04\n",
      "Epoch 2313/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.1871e-04 - val_loss: 0.0012\n",
      "Epoch 2314/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.7503e-04 - val_loss: 0.0018\n",
      "Epoch 2315/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.9172e-04 - val_loss: 0.0014\n",
      "Epoch 2316/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.3046e-04 - val_loss: 0.0011\n",
      "Epoch 2317/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.4734e-04 - val_loss: 1.7842e-04\n",
      "Epoch 2318/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1.1745e-04 - val_loss: 3.5596e-05\n",
      "Epoch 2319/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.0019e-04 - val_loss: 3.7866e-05\n",
      "Epoch 2320/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.4759e-04 - val_loss: 4.4117e-05\n",
      "Epoch 2321/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.8202e-05 - val_loss: 7.2237e-05\n",
      "Epoch 2322/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.5749e-05 - val_loss: 8.7742e-05\n",
      "Epoch 2323/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 8.1650e-05 - val_loss: 3.2260e-05\n",
      "Epoch 2324/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.9392e-05 - val_loss: 1.3468e-04\n",
      "Epoch 2325/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 7.4637e-05 - val_loss: 3.8327e-05\n",
      "Epoch 2326/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.8249e-05 - val_loss: 3.1079e-05\n",
      "Epoch 2327/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.4623e-05 - val_loss: 1.0961e-04\n",
      "Epoch 2328/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.2327e-05 - val_loss: 4.9358e-05\n",
      "Epoch 2329/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1772e-04 - val_loss: 4.5887e-05\n",
      "Epoch 2330/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 3.9497e-05 - val_loss: 4.1245e-05\n",
      "Epoch 2331/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 3.0973e-05 - val_loss: 7.6138e-05\n",
      "Epoch 2332/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 4.7050e-05 - val_loss: 2.2800e-05\n",
      "Epoch 2333/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.3924e-05 - val_loss: 7.2223e-05\n",
      "Epoch 2334/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 9.5338e-05 - val_loss: 2.0030e-05\n",
      "Epoch 2335/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.2132e-04 - val_loss: 1.6794e-04\n",
      "Epoch 2336/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0753e-04 - val_loss: 5.1576e-05\n",
      "Epoch 2337/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 6.5652e-05 - val_loss: 3.9403e-05\n",
      "Epoch 2338/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.3056e-05 - val_loss: 3.1541e-04\n",
      "Epoch 2339/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.5437e-04 - val_loss: 2.6986e-05\n",
      "Epoch 2340/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.0027e-05 - val_loss: 3.8542e-05\n",
      "Epoch 2341/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.6392e-05 - val_loss: 2.2238e-05\n",
      "Epoch 2342/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.8573e-05 - val_loss: 2.3404e-04\n",
      "Epoch 2343/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.0056e-04 - val_loss: 1.2926e-04\n",
      "Epoch 2344/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 3.2797e-04 - val_loss: 1.8672e-04\n",
      "Epoch 2345/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 7.7629e-04 - val_loss: 0.0016\n",
      "Epoch 2346/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0011 - val_loss: 3.0506e-04\n",
      "Epoch 2347/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.5625e-04 - val_loss: 3.5654e-04\n",
      "Epoch 2348/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.5266e-05 - val_loss: 1.5202e-04\n",
      "Epoch 2349/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.5094e-05 - val_loss: 7.9492e-05\n",
      "Epoch 2350/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 8.9421e-05 - val_loss: 6.8608e-05\n",
      "Epoch 2351/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2857e-04 - val_loss: 3.8209e-04\n",
      "Epoch 2352/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.6621e-04 - val_loss: 3.5174e-05\n",
      "Epoch 2353/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 9.7882e-05 - val_loss: 3.1347e-05\n",
      "Epoch 2354/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.2690e-05 - val_loss: 1.0273e-04\n",
      "Epoch 2355/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.7051e-05 - val_loss: 1.5640e-04\n",
      "Epoch 2356/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6370e-04 - val_loss: 1.7752e-04\n",
      "Epoch 2357/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.9730e-04 - val_loss: 9.3169e-05\n",
      "Epoch 2358/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 2359/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.5373e-04 - val_loss: 9.0042e-05\n",
      "Epoch 2360/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 4.3507e-04 - val_loss: 4.4123e-04\n",
      "Epoch 2361/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.5341e-04 - val_loss: 4.6112e-05\n",
      "Epoch 2362/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.9145e-04 - val_loss: 2.3966e-04\n",
      "Epoch 2363/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.5175e-04 - val_loss: 0.0012\n",
      "Epoch 2364/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.0876e-04 - val_loss: 0.0010\n",
      "Epoch 2365/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.8404e-04 - val_loss: 1.8401e-04\n",
      "Epoch 2366/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.3981e-04 - val_loss: 0.0017\n",
      "Epoch 2367/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.9712e-04 - val_loss: 6.0430e-04\n",
      "Epoch 2368/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.4874e-04 - val_loss: 2.3138e-04\n",
      "Epoch 2369/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.7221e-04 - val_loss: 2.0869e-04\n",
      "Epoch 2370/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.6218e-04 - val_loss: 5.9950e-04\n",
      "Epoch 2371/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.7347e-04 - val_loss: 1.4430e-04\n",
      "Epoch 2372/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.5264e-04 - val_loss: 0.0013\n",
      "Epoch 2373/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0017 - val_loss: 0.0042\n",
      "Epoch 2374/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0017 - val_loss: 6.8363e-04\n",
      "Epoch 2375/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0014 - val_loss: 9.0225e-04\n",
      "Epoch 2376/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0014 - val_loss: 9.7785e-04\n",
      "Epoch 2377/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0027 - val_loss: 0.0018\n",
      "Epoch 2378/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 5.9393e-04 - val_loss: 8.1873e-04\n",
      "Epoch 2379/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 3.8738e-04 - val_loss: 1.8865e-04\n",
      "Epoch 2380/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 8.7178e-05 - val_loss: 1.7240e-05\n",
      "Epoch 2381/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 4.9704e-05 - val_loss: 9.2832e-05\n",
      "Epoch 2382/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 2.7361e-05 - val_loss: 1.7291e-04\n",
      "Epoch 2383/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 1.6000e-04 - val_loss: 4.3310e-04\n",
      "Epoch 2384/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.3751e-04 - val_loss: 4.1825e-04\n",
      "Epoch 2385/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5.5805e-04 - val_loss: 4.3900e-04\n",
      "Epoch 2386/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0014 - val_loss: 7.9652e-04\n",
      "Epoch 2387/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 8.0926e-04 - val_loss: 3.7919e-04\n",
      "Epoch 2388/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.3438e-04 - val_loss: 0.0015\n",
      "Epoch 2389/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0021 - val_loss: 0.0036\n",
      "Epoch 2390/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.4583e-04 - val_loss: 9.8965e-04\n",
      "Epoch 2391/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0010 - val_loss: 3.3261e-05\n",
      "Epoch 2392/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.1476e-04 - val_loss: 1.0721e-04\n",
      "Epoch 2393/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3302e-04 - val_loss: 2.9718e-04\n",
      "Epoch 2394/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4285e-04 - val_loss: 1.5795e-04\n",
      "Epoch 2395/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.7044e-05 - val_loss: 1.3445e-05\n",
      "Epoch 2396/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 3.9790e-05 - val_loss: 7.1612e-05\n",
      "Epoch 2397/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1246e-04 - val_loss: 9.5418e-05\n",
      "Epoch 2398/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.6128e-04 - val_loss: 1.2948e-04\n",
      "Epoch 2399/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.2124e-05 - val_loss: 2.1796e-05\n",
      "Epoch 2400/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.5498e-05 - val_loss: 4.1788e-05\n",
      "Epoch 2401/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.1301e-05 - val_loss: 8.2684e-05\n",
      "Epoch 2402/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 7.0900e-05 - val_loss: 3.0605e-04\n",
      "Epoch 2403/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.1236e-04 - val_loss: 4.4188e-04\n",
      "Epoch 2404/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 2.9545e-04 - val_loss: 1.8592e-05\n",
      "Epoch 2405/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.6444e-05 - val_loss: 6.1847e-04\n",
      "Epoch 2406/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 3.6740e-04 - val_loss: 0.0011\n",
      "Epoch 2407/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.7809e-04 - val_loss: 5.2561e-04\n",
      "Epoch 2408/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 4.2348e-04 - val_loss: 8.6337e-04\n",
      "Epoch 2409/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.7315e-04 - val_loss: 2.3736e-04\n",
      "Epoch 2410/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.3020e-04 - val_loss: 2.3401e-04\n",
      "Epoch 2411/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 7.7615e-04 - val_loss: 9.4171e-04\n",
      "Epoch 2412/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 2.3172e-04 - val_loss: 1.1575e-04\n",
      "Epoch 2413/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 2.5096e-04 - val_loss: 2.2066e-04\n",
      "Epoch 2414/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6447e-04 - val_loss: 2.0376e-04\n",
      "Epoch 2415/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.0902e-04 - val_loss: 8.6681e-06\n",
      "Epoch 2416/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 4.8141e-04 - val_loss: 2.7734e-04\n",
      "Epoch 2417/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1.8418e-04 - val_loss: 1.2074e-04\n",
      "Epoch 2418/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3.2413e-04 - val_loss: 3.6226e-05\n",
      "Epoch 2419/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.6809e-04 - val_loss: 0.0014\n",
      "Epoch 2420/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.8995e-04 - val_loss: 0.0012\n",
      "Epoch 2421/10000\n",
      "68/68 [==============================] - 0s 588us/sample - loss: 0.0011 - val_loss: 0.0073\n",
      "Epoch 2422/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0023 - val_loss: 0.0071\n",
      "Epoch 2423/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0118 - val_loss: 0.0286\n",
      "Epoch 2424/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0088 - val_loss: 0.0042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2425/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0042 - val_loss: 0.0034\n",
      "Epoch 2426/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0029 - val_loss: 0.0084\n",
      "Epoch 2427/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0114 - val_loss: 0.0119\n",
      "Epoch 2428/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0266 - val_loss: 0.1362\n",
      "Epoch 2429/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.2615 - val_loss: 0.7815\n",
      "Epoch 2430/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2783 - val_loss: 0.4962\n",
      "Epoch 2431/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.414 - 0s 147us/sample - loss: 0.5336 - val_loss: 0.4062\n",
      "Epoch 2432/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3273 - val_loss: 0.0271\n",
      "Epoch 2433/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1264 - val_loss: 0.0673\n",
      "Epoch 2434/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 1.3793 - val_loss: 0.2432\n",
      "Epoch 2435/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9175 - val_loss: 0.5086\n",
      "Epoch 2436/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6001 - val_loss: 2.3405\n",
      "Epoch 2437/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.9533 - val_loss: 3.6831\n",
      "Epoch 2438/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0459 - val_loss: 0.3087\n",
      "Epoch 2439/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6456 - val_loss: 1.2872\n",
      "Epoch 2440/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.0106 - val_loss: 5.6358\n",
      "Epoch 2441/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.6909 - val_loss: 28.1277\n",
      "Epoch 2442/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.1711 - val_loss: 16.3474\n",
      "Epoch 2443/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 26.9674 - val_loss: 120.2954\n",
      "Epoch 2444/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 385.5768 - val_loss: 227.5538\n",
      "Epoch 2445/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 863.7207 - val_loss: 1371.9138\n",
      "Epoch 2446/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 1805.5093 - val_loss: 287.2048\n",
      "Epoch 2447/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1430.2281 - val_loss: 977.8262\n",
      "Epoch 2448/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2919.7663 - val_loss: 1659.0633\n",
      "Epoch 2449/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4662.4950 - val_loss: 1768.3511\n",
      "Epoch 2450/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1923.2123 - val_loss: 2706.4947\n",
      "Epoch 2451/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2343.0907 - val_loss: 2491.4490\n",
      "Epoch 2452/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3328.8157 - val_loss: 7201.5397\n",
      "Epoch 2453/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5397.7896 - val_loss: 13370.1147\n",
      "Epoch 2454/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 8876.1544 - val_loss: 8348.3154\n",
      "Epoch 2455/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 4197.0533 - val_loss: 2949.0398\n",
      "Epoch 2456/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 954.9211 - val_loss: 737.2827\n",
      "Epoch 2457/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 193.4602 - val_loss: 249.1757\n",
      "Epoch 2458/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 113.3615 - val_loss: 226.2789\n",
      "Epoch 2459/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 117.1466 - val_loss: 102.8265\n",
      "Epoch 2460/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 46.2619 - val_loss: 63.0267\n",
      "Epoch 2461/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 64.1375 - val_loss: 162.4680\n",
      "Epoch 2462/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 99.6433 - val_loss: 256.3122\n",
      "Epoch 2463/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 111.0348 - val_loss: 127.0746\n",
      "Epoch 2464/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 60.3404 - val_loss: 113.1308\n",
      "Epoch 2465/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 66.2301 - val_loss: 61.3090\n",
      "Epoch 2466/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 25.5620 - val_loss: 3.4436\n",
      "Epoch 2467/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.0950 - val_loss: 2.0938\n",
      "Epoch 2468/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 6.8323 - val_loss: 9.4486\n",
      "Epoch 2469/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.2356 - val_loss: 4.1821\n",
      "Epoch 2470/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8942 - val_loss: 8.4877\n",
      "Epoch 2471/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.4655 - val_loss: 3.2831\n",
      "Epoch 2472/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.4260 - val_loss: 11.8818\n",
      "Epoch 2473/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.0394 - val_loss: 10.0504\n",
      "Epoch 2474/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.5366 - val_loss: 5.9792\n",
      "Epoch 2475/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.7957 - val_loss: 2.2597\n",
      "Epoch 2476/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8235 - val_loss: 0.5075\n",
      "Epoch 2477/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.5178 - val_loss: 0.6455\n",
      "Epoch 2478/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2635 - val_loss: 0.4092\n",
      "Epoch 2479/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2291 - val_loss: 0.6394\n",
      "Epoch 2480/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1528 - val_loss: 0.2787\n",
      "Epoch 2481/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0815 - val_loss: 0.3027\n",
      "Epoch 2482/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1027 - val_loss: 0.0445\n",
      "Epoch 2483/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0423 - val_loss: 0.0181\n",
      "Epoch 2484/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0656 - val_loss: 0.1444\n",
      "Epoch 2485/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0674 - val_loss: 0.0407\n",
      "Epoch 2486/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0352 - val_loss: 0.0427\n",
      "Epoch 2487/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0316 - val_loss: 0.0227\n",
      "Epoch 2488/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0349 - val_loss: 0.0341\n",
      "Epoch 2489/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0246 - val_loss: 0.0150\n",
      "Epoch 2490/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 2491/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0147 - val_loss: 0.0095\n",
      "Epoch 2492/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0553 - val_loss: 0.0274\n",
      "Epoch 2493/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0636 - val_loss: 0.0402\n",
      "Epoch 2494/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0518 - val_loss: 0.0740\n",
      "Epoch 2495/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0520 - val_loss: 0.0303\n",
      "Epoch 2496/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0249 - val_loss: 0.0094\n",
      "Epoch 2497/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0317 - val_loss: 0.0127\n",
      "Epoch 2498/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0145 - val_loss: 0.0269\n",
      "Epoch 2499/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0136 - val_loss: 0.0097\n",
      "Epoch 2500/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0119 - val_loss: 0.0044\n",
      "Epoch 2501/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0117 - val_loss: 0.0224\n",
      "Epoch 2502/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0194 - val_loss: 0.0268\n",
      "Epoch 2503/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0177 - val_loss: 0.0338\n",
      "Epoch 2504/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 2505/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0160 - val_loss: 0.0159\n",
      "Epoch 2506/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0202 - val_loss: 0.0093\n",
      "Epoch 2507/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0111 - val_loss: 0.0036\n",
      "Epoch 2508/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0058 - val_loss: 0.0045\n",
      "Epoch 2509/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0081 - val_loss: 0.0218\n",
      "Epoch 2510/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0148 - val_loss: 0.0089\n",
      "Epoch 2511/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0554 - val_loss: 0.1299\n",
      "Epoch 2512/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0500 - val_loss: 0.0513\n",
      "Epoch 2513/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0237 - val_loss: 0.0175\n",
      "Epoch 2514/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0167 - val_loss: 0.0095\n",
      "Epoch 2515/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0087 - val_loss: 0.0111\n",
      "Epoch 2516/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 2517/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0109 - val_loss: 0.0066\n",
      "Epoch 2518/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0108 - val_loss: 0.0098\n",
      "Epoch 2519/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0247 - val_loss: 0.0374\n",
      "Epoch 2520/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0183 - val_loss: 0.0126\n",
      "Epoch 2521/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0103 - val_loss: 0.0026\n",
      "Epoch 2522/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0117 - val_loss: 0.0025\n",
      "Epoch 2523/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 2524/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0058 - val_loss: 0.0070\n",
      "Epoch 2525/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0047 - val_loss: 0.0342\n",
      "Epoch 2526/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0208 - val_loss: 0.0064\n",
      "Epoch 2527/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0134 - val_loss: 0.0357\n",
      "Epoch 2528/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0263 - val_loss: 0.0239\n",
      "Epoch 2529/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0127 - val_loss: 0.0080\n",
      "Epoch 2530/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0065 - val_loss: 0.0314\n",
      "Epoch 2531/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0134 - val_loss: 0.0049\n",
      "Epoch 2532/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0045 - val_loss: 0.0130\n",
      "Epoch 2533/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0081 - val_loss: 0.0126\n",
      "Epoch 2534/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0153 - val_loss: 0.0547\n",
      "Epoch 2535/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0568 - val_loss: 0.0305\n",
      "Epoch 2536/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0404 - val_loss: 0.0943\n",
      "Epoch 2537/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1765 - val_loss: 0.0892\n",
      "Epoch 2538/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3910 - val_loss: 0.3698\n",
      "Epoch 2539/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3513 - val_loss: 0.1585\n",
      "Epoch 2540/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0988 - val_loss: 0.0803\n",
      "Epoch 2541/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0371 - val_loss: 0.0078\n",
      "Epoch 2542/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0165 - val_loss: 0.0134\n",
      "Epoch 2543/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0126 - val_loss: 0.0144\n",
      "Epoch 2544/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0339 - val_loss: 0.0836\n",
      "Epoch 2545/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0636 - val_loss: 0.0627\n",
      "Epoch 2546/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0706 - val_loss: 0.0374\n",
      "Epoch 2547/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0503 - val_loss: 0.1655\n",
      "Epoch 2548/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0465 - val_loss: 0.1060\n",
      "Epoch 2549/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1007 - val_loss: 0.0425\n",
      "Epoch 2550/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1064 - val_loss: 0.8437\n",
      "Epoch 2551/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1387 - val_loss: 0.8413\n",
      "Epoch 2552/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.7291 - val_loss: 0.9683\n",
      "Epoch 2553/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2475 - val_loss: 0.2201\n",
      "Epoch 2554/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3007 - val_loss: 0.0131\n",
      "Epoch 2555/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2617 - val_loss: 0.2299\n",
      "Epoch 2556/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2639 - val_loss: 0.0156\n",
      "Epoch 2557/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0877 - val_loss: 0.2407\n",
      "Epoch 2558/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2233 - val_loss: 0.2368\n",
      "Epoch 2559/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0616 - val_loss: 0.0050\n",
      "Epoch 2560/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0507 - val_loss: 0.3879\n",
      "Epoch 2561/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3185 - val_loss: 0.3166\n",
      "Epoch 2562/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1417 - val_loss: 0.2292\n",
      "Epoch 2563/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2136 - val_loss: 0.0126\n",
      "Epoch 2564/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0816 - val_loss: 0.0787\n",
      "Epoch 2565/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1043 - val_loss: 0.1784\n",
      "Epoch 2566/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2725 - val_loss: 0.6366\n",
      "Epoch 2567/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1998 - val_loss: 0.0811\n",
      "Epoch 2568/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0856 - val_loss: 0.0019\n",
      "Epoch 2569/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0600 - val_loss: 0.1017\n",
      "Epoch 2570/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0524 - val_loss: 0.2900\n",
      "Epoch 2571/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5060 - val_loss: 2.1323\n",
      "Epoch 2572/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6134 - val_loss: 9.7518\n",
      "Epoch 2573/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.0499 - val_loss: 0.2070\n",
      "Epoch 2574/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.6263 - val_loss: 4.6792\n",
      "Epoch 2575/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 12.4660 - val_loss: 41.6506\n",
      "Epoch 2576/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 27.3643 - val_loss: 63.6723\n",
      "Epoch 2577/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 39.5099 - val_loss: 47.9973\n",
      "Epoch 2578/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 34.2965 - val_loss: 67.4108\n",
      "Epoch 2579/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 30.8187 - val_loss: 2.8797\n",
      "Epoch 2580/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 41.5745 - val_loss: 122.7387\n",
      "Epoch 2581/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 40.1858 - val_loss: 10.6501\n",
      "Epoch 2582/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.3544 - val_loss: 38.6587\n",
      "Epoch 2583/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 29.4117 - val_loss: 41.5722\n",
      "Epoch 2584/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 26.8928 - val_loss: 67.8537\n",
      "Epoch 2585/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 26.2481 - val_loss: 88.3939\n",
      "Epoch 2586/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 99.0571 - val_loss: 38.1129\n",
      "Epoch 2587/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 21.8682 - val_loss: 36.4074\n",
      "Epoch 2588/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 32.8526 - val_loss: 37.6721\n",
      "Epoch 2589/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.4660 - val_loss: 7.7165\n",
      "Epoch 2590/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 16.6337 - val_loss: 14.7387\n",
      "Epoch 2591/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 51.9308 - val_loss: 41.1683\n",
      "Epoch 2592/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 33.3931 - val_loss: 192.7611\n",
      "Epoch 2593/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 72.4995 - val_loss: 121.0439\n",
      "Epoch 2594/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 46.3956 - val_loss: 38.8148\n",
      "Epoch 2595/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 42.8617 - val_loss: 79.1196\n",
      "Epoch 2596/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 37.9505 - val_loss: 223.3462\n",
      "Epoch 2597/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 89.1760 - val_loss: 30.1986\n",
      "Epoch 2598/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 23.0880 - val_loss: 31.8363\n",
      "Epoch 2599/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 74.6422 - val_loss: 33.9020\n",
      "Epoch 2600/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 33.3494 - val_loss: 40.1922\n",
      "Epoch 2601/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 23.8383 - val_loss: 23.6467\n",
      "Epoch 2602/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 10.4998 - val_loss: 20.5365\n",
      "Epoch 2603/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.7993 - val_loss: 2.9247\n",
      "Epoch 2604/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.4395 - val_loss: 3.6911\n",
      "Epoch 2605/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 13.9136 - val_loss: 8.5917\n",
      "Epoch 2606/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 11.1527 - val_loss: 15.7893\n",
      "Epoch 2607/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 22.2118 - val_loss: 3.7468\n",
      "Epoch 2608/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.7917 - val_loss: 1.7551\n",
      "Epoch 2609/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 21.9405 - val_loss: 6.3072\n",
      "Epoch 2610/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.7839 - val_loss: 12.7719\n",
      "Epoch 2611/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 5.3665 - val_loss: 0.2341\n",
      "Epoch 2612/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3551 - val_loss: 0.0571\n",
      "Epoch 2613/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.8542 - val_loss: 1.9202\n",
      "Epoch 2614/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.9542 - val_loss: 0.2209\n",
      "Epoch 2615/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.7971 - val_loss: 6.4457\n",
      "Epoch 2616/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.4662 - val_loss: 8.7698\n",
      "Epoch 2617/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.0051 - val_loss: 23.8065\n",
      "Epoch 2618/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 18.1628 - val_loss: 112.0640\n",
      "Epoch 2619/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 35.6304 - val_loss: 43.3738\n",
      "Epoch 2620/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 36.2769 - val_loss: 47.3333\n",
      "Epoch 2621/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 25.0631 - val_loss: 14.8220\n",
      "Epoch 2622/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 41.1283 - val_loss: 78.1581\n",
      "Epoch 2623/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 25.8010 - val_loss: 6.1285\n",
      "Epoch 2624/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 8.9580 - val_loss: 4.3158\n",
      "Epoch 2625/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.8096 - val_loss: 11.9752\n",
      "Epoch 2626/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.5372 - val_loss: 17.4680\n",
      "Epoch 2627/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 10.7764 - val_loss: 47.6594\n",
      "Epoch 2628/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 10.9633 - val_loss: 16.0176\n",
      "Epoch 2629/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 12.3342 - val_loss: 2.3078\n",
      "Epoch 2630/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 23.6314 - val_loss: 10.4933\n",
      "Epoch 2631/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.8054 - val_loss: 4.6391\n",
      "Epoch 2632/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.4433 - val_loss: 4.0601\n",
      "Epoch 2633/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.6189 - val_loss: 1.1843\n",
      "Epoch 2634/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.2457 - val_loss: 1.6697\n",
      "Epoch 2635/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6659 - val_loss: 2.4348\n",
      "Epoch 2636/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 3.0411 - val_loss: 1.7349\n",
      "Epoch 2637/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.5430 - val_loss: 0.4691\n",
      "Epoch 2638/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.4598 - val_loss: 7.5728\n",
      "Epoch 2639/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15.3860 - val_loss: 102.6810\n",
      "Epoch 2640/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 91.7080 - val_loss: 30.1679\n",
      "Epoch 2641/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 43.2305 - val_loss: 60.4796\n",
      "Epoch 2642/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 43.4840 - val_loss: 119.0699\n",
      "Epoch 2643/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 54.2134 - val_loss: 10.2166\n",
      "Epoch 2644/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 30.8201 - val_loss: 226.6479\n",
      "Epoch 2645/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 435.7739 - val_loss: 905.2962\n",
      "Epoch 2646/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1453.5403 - val_loss: 1184.0544\n",
      "Epoch 2647/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 902.7341 - val_loss: 2875.5376\n",
      "Epoch 2648/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1419.5886 - val_loss: 1839.3123\n",
      "Epoch 2649/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1107.7852 - val_loss: 2695.3576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2650/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 2800.0025 - val_loss: 7210.9605\n",
      "Epoch 2651/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4620.5065 - val_loss: 6113.0959\n",
      "Epoch 2652/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 2464.9347 - val_loss: 1007.1858\n",
      "Epoch 2653/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 750.0123 - val_loss: 377.6991\n",
      "Epoch 2654/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1044.7062 - val_loss: 148.8007\n",
      "Epoch 2655/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1114.9480 - val_loss: 11635.3218\n",
      "Epoch 2656/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4282.2679 - val_loss: 1828.0138\n",
      "Epoch 2657/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 662.4018 - val_loss: 256.4993\n",
      "Epoch 2658/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 839.9698 - val_loss: 48.9857\n",
      "Epoch 2659/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 982.7158 - val_loss: 720.0467\n",
      "Epoch 2660/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1062.2182 - val_loss: 953.5481\n",
      "Epoch 2661/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 976.7984 - val_loss: 1657.7318\n",
      "Epoch 2662/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 1054.6075 - val_loss: 668.9420\n",
      "Epoch 2663/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 682.5140 - val_loss: 26.6222\n",
      "Epoch 2664/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 244.0046 - val_loss: 99.3535\n",
      "Epoch 2665/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 146.0913 - val_loss: 254.2218\n",
      "Epoch 2666/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 162.8582 - val_loss: 5.0892\n",
      "Epoch 2667/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 54.7590 - val_loss: 19.4348\n",
      "Epoch 2668/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.3166 - val_loss: 9.0621\n",
      "Epoch 2669/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.6717 - val_loss: 8.8488\n",
      "Epoch 2670/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.4573 - val_loss: 5.0862\n",
      "Epoch 2671/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.1907 - val_loss: 9.8452\n",
      "Epoch 2672/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.6082 - val_loss: 7.7472\n",
      "Epoch 2673/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.3941 - val_loss: 7.7644\n",
      "Epoch 2674/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.0644 - val_loss: 6.6450\n",
      "Epoch 2675/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.5972 - val_loss: 3.6739\n",
      "Epoch 2676/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.5298 - val_loss: 0.2551\n",
      "Epoch 2677/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4178 - val_loss: 0.2207\n",
      "Epoch 2678/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.7788 - val_loss: 1.3541\n",
      "Epoch 2679/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 2.6261 - val_loss: 2.8329\n",
      "Epoch 2680/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.8971 - val_loss: 0.8481\n",
      "Epoch 2681/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9932 - val_loss: 1.0912\n",
      "Epoch 2682/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.868 - 0s 206us/sample - loss: 1.4648 - val_loss: 0.2818\n",
      "Epoch 2683/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3513 - val_loss: 0.2433\n",
      "Epoch 2684/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1115 - val_loss: 0.0645\n",
      "Epoch 2685/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0267 - val_loss: 0.0023\n",
      "Epoch 2686/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0051 - val_loss: 0.0108\n",
      "Epoch 2687/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 0.0075 - val_loss: 0.0189\n",
      "Epoch 2688/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0081 - val_loss: 0.0072\n",
      "Epoch 2689/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0062 - val_loss: 0.0191\n",
      "Epoch 2690/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0172 - val_loss: 0.0081\n",
      "Epoch 2691/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 0.0057 - val_loss: 0.0033\n",
      "Epoch 2692/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0029 - val_loss: 0.0046\n",
      "Epoch 2693/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0024 - val_loss: 0.0043\n",
      "Epoch 2694/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 2695/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 0.0032 - val_loss: 0.0019\n",
      "Epoch 2696/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 2697/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0028 - val_loss: 0.0063\n",
      "Epoch 2698/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0042 - val_loss: 0.0058\n",
      "Epoch 2699/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0047 - val_loss: 0.0060\n",
      "Epoch 2700/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0038 - val_loss: 0.0018\n",
      "Epoch 2701/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0059 - val_loss: 0.0051\n",
      "Epoch 2702/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0050 - val_loss: 0.0066\n",
      "Epoch 2703/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0051 - val_loss: 0.0079\n",
      "Epoch 2704/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 0.0078 - val_loss: 0.0124\n",
      "Epoch 2705/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0054 - val_loss: 0.0019\n",
      "Epoch 2706/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0030 - val_loss: 0.0026\n",
      "Epoch 2707/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 2708/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 0.0038 - val_loss: 0.0025\n",
      "Epoch 2709/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0031 - val_loss: 0.0014\n",
      "Epoch 2710/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0026 - val_loss: 0.0037\n",
      "Epoch 2711/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0031 - val_loss: 0.0087\n",
      "Epoch 2712/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0065 - val_loss: 0.0042\n",
      "Epoch 2713/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0028 - val_loss: 0.0151\n",
      "Epoch 2714/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0078 - val_loss: 0.0042\n",
      "Epoch 2715/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0094 - val_loss: 0.0083\n",
      "Epoch 2716/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0072 - val_loss: 8.0948e-04\n",
      "Epoch 2717/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.0057 - val_loss: 0.0038\n",
      "Epoch 2718/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 2719/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0030 - val_loss: 6.3154e-04\n",
      "Epoch 2720/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.0019 - val_loss: 0.0029\n",
      "Epoch 2721/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 0.0036 - val_loss: 0.0024\n",
      "Epoch 2722/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0011 - val_loss: 6.9329e-04\n",
      "Epoch 2723/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.3534e-04 - val_loss: 6.2254e-04\n",
      "Epoch 2724/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 6.7603e-04 - val_loss: 7.6964e-04\n",
      "Epoch 2725/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.3261e-04 - val_loss: 4.4110e-04\n",
      "Epoch 2726/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0014 - val_loss: 0.0041\n",
      "Epoch 2727/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0032 - val_loss: 0.0114\n",
      "Epoch 2728/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0039 - val_loss: 0.0033\n",
      "Epoch 2729/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0024 - val_loss: 6.1667e-04\n",
      "Epoch 2730/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0028 - val_loss: 0.0050\n",
      "Epoch 2731/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0080 - val_loss: 0.0174\n",
      "Epoch 2732/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0149 - val_loss: 0.0094\n",
      "Epoch 2733/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0045 - val_loss: 0.0042\n",
      "Epoch 2734/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0074 - val_loss: 0.0089\n",
      "Epoch 2735/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0307 - val_loss: 0.0431\n",
      "Epoch 2736/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0313 - val_loss: 0.0367\n",
      "Epoch 2737/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0553 - val_loss: 0.0888\n",
      "Epoch 2738/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1759 - val_loss: 0.0484\n",
      "Epoch 2739/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4444 - val_loss: 0.0072\n",
      "Epoch 2740/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4924 - val_loss: 0.7898\n",
      "Epoch 2741/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8855 - val_loss: 0.2156\n",
      "Epoch 2742/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8943 - val_loss: 0.7371\n",
      "Epoch 2743/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.7227 - val_loss: 4.3093\n",
      "Epoch 2744/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 22.1755 - val_loss: 143.3758\n",
      "Epoch 2745/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 176.4628 - val_loss: 2.0996\n",
      "Epoch 2746/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 175.5788 - val_loss: 45.2572\n",
      "Epoch 2747/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 49.4874 - val_loss: 29.3597\n",
      "Epoch 2748/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14.9040 - val_loss: 10.2693\n",
      "Epoch 2749/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26.9461 - val_loss: 126.2641\n",
      "Epoch 2750/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 62.1372 - val_loss: 31.2980\n",
      "Epoch 2751/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 101.6347 - val_loss: 36.5332\n",
      "Epoch 2752/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 87.0627 - val_loss: 21.1398\n",
      "Epoch 2753/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16.4941 - val_loss: 18.1357\n",
      "Epoch 2754/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 14.5363 - val_loss: 27.9534\n",
      "Epoch 2755/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.8936 - val_loss: 12.7446\n",
      "Epoch 2756/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.6302 - val_loss: 16.4242\n",
      "Epoch 2757/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.0732 - val_loss: 9.4631\n",
      "Epoch 2758/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.5858 - val_loss: 3.5278\n",
      "Epoch 2759/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.5735 - val_loss: 0.3838\n",
      "Epoch 2760/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1365 - val_loss: 0.3074\n",
      "Epoch 2761/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7402 - val_loss: 0.0424\n",
      "Epoch 2762/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0254 - val_loss: 2.6795\n",
      "Epoch 2763/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.7554 - val_loss: 2.1861\n",
      "Epoch 2764/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.5341 - val_loss: 0.1920\n",
      "Epoch 2765/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1231 - val_loss: 0.1266\n",
      "Epoch 2766/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3562 - val_loss: 0.4460\n",
      "Epoch 2767/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4045 - val_loss: 0.5870\n",
      "Epoch 2768/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2311 - val_loss: 0.3580\n",
      "Epoch 2769/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5120 - val_loss: 0.3526\n",
      "Epoch 2770/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1686 - val_loss: 0.1541\n",
      "Epoch 2771/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2056 - val_loss: 0.0684\n",
      "Epoch 2772/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1327 - val_loss: 0.1816\n",
      "Epoch 2773/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.2397 - val_loss: 0.3520\n",
      "Epoch 2774/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.1598 - val_loss: 0.1512\n",
      "Epoch 2775/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0451 - val_loss: 0.0433\n",
      "Epoch 2776/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0201 - val_loss: 0.0251\n",
      "Epoch 2777/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0249 - val_loss: 0.1015\n",
      "Epoch 2778/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0622 - val_loss: 0.2398\n",
      "Epoch 2779/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1584 - val_loss: 0.5593\n",
      "Epoch 2780/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5131 - val_loss: 1.4306\n",
      "Epoch 2781/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.5749 - val_loss: 5.0333\n",
      "Epoch 2782/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.6722 - val_loss: 6.3227\n",
      "Epoch 2783/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.1750 - val_loss: 27.6292\n",
      "Epoch 2784/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.6731 - val_loss: 31.5224\n",
      "Epoch 2785/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 33.2654 - val_loss: 23.7729\n",
      "Epoch 2786/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 70.9209 - val_loss: 49.9924\n",
      "Epoch 2787/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 25.8362 - val_loss: 2.8565\n",
      "Epoch 2788/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 31.5286 - val_loss: 3.9056\n",
      "Epoch 2789/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 76.8374 - val_loss: 0.5889\n",
      "Epoch 2790/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 114.3773 - val_loss: 98.4228\n",
      "Epoch 2791/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 266.6792 - val_loss: 448.8023\n",
      "Epoch 2792/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 347.0974 - val_loss: 879.0273\n",
      "Epoch 2793/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 290.4092 - val_loss: 48.3545\n",
      "Epoch 2794/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 21.0476 - val_loss: 88.1281\n",
      "Epoch 2795/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 46.1253 - val_loss: 120.2296\n",
      "Epoch 2796/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 53.3688 - val_loss: 437.9712\n",
      "Epoch 2797/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 292.5840 - val_loss: 381.9253\n",
      "Epoch 2798/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 521.3879 - val_loss: 427.1283\n",
      "Epoch 2799/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 176us/sample - loss: 614.8532 - val_loss: 1814.9208\n",
      "Epoch 2800/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1565.7394 - val_loss: 3797.3277\n",
      "Epoch 2801/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2101.4238 - val_loss: 4362.0124\n",
      "Epoch 2802/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1825.6781 - val_loss: 3589.4074\n",
      "Epoch 2803/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1264.3169 - val_loss: 829.2519\n",
      "Epoch 2804/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 789.1302 - val_loss: 285.9926\n",
      "Epoch 2805/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 615.2945 - val_loss: 131.8470\n",
      "Epoch 2806/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 238.4000 - val_loss: 333.5892\n",
      "Epoch 2807/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 343.4429 - val_loss: 997.8495\n",
      "Epoch 2808/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2394.1348 - val_loss: 10420.3896\n",
      "Epoch 2809/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5911.2315 - val_loss: 4526.2333\n",
      "Epoch 2810/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3717.5012 - val_loss: 1126.5476\n",
      "Epoch 2811/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2725.0952 - val_loss: 667.8414\n",
      "Epoch 2812/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 727.7552 - val_loss: 1079.6488\n",
      "Epoch 2813/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1058.4892 - val_loss: 2928.2642\n",
      "Epoch 2814/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1760.2509 - val_loss: 1146.2096\n",
      "Epoch 2815/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 401.4343 - val_loss: 524.3726\n",
      "Epoch 2816/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 420.4070 - val_loss: 377.9798\n",
      "Epoch 2817/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 578.2697 - val_loss: 1445.5618\n",
      "Epoch 2818/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1239.5187 - val_loss: 267.1266\n",
      "Epoch 2819/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 387.9559 - val_loss: 57.7550\n",
      "Epoch 2820/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 57.3495 - val_loss: 8.7605\n",
      "Epoch 2821/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 18.6588 - val_loss: 1.5400\n",
      "Epoch 2822/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.0507 - val_loss: 4.2753\n",
      "Epoch 2823/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 11.9507 - val_loss: 2.4909\n",
      "Epoch 2824/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.4879 - val_loss: 0.1500\n",
      "Epoch 2825/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1271 - val_loss: 0.0344\n",
      "Epoch 2826/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3507 - val_loss: 0.3687\n",
      "Epoch 2827/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3085 - val_loss: 0.3416\n",
      "Epoch 2828/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2380 - val_loss: 0.2560\n",
      "Epoch 2829/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2785 - val_loss: 0.5342\n",
      "Epoch 2830/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 0.5824 - val_loss: 0.8887\n",
      "Epoch 2831/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.6347 - val_loss: 0.0075\n",
      "Epoch 2832/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.1994 - val_loss: 0.0979\n",
      "Epoch 2833/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1368 - val_loss: 0.2942\n",
      "Epoch 2834/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0911 - val_loss: 0.0891\n",
      "Epoch 2835/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0444 - val_loss: 0.0117\n",
      "Epoch 2836/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0183 - val_loss: 0.0378\n",
      "Epoch 2837/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1291 - val_loss: 0.1895\n",
      "Epoch 2838/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0930 - val_loss: 0.1431\n",
      "Epoch 2839/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0721 - val_loss: 0.2229\n",
      "Epoch 2840/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0951 - val_loss: 0.0784\n",
      "Epoch 2841/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3340 - val_loss: 0.0103\n",
      "Epoch 2842/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3311 - val_loss: 0.3556\n",
      "Epoch 2843/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3354 - val_loss: 0.3454\n",
      "Epoch 2844/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1466 - val_loss: 0.0059\n",
      "Epoch 2845/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0414 - val_loss: 0.0349\n",
      "Epoch 2846/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0209 - val_loss: 0.0307\n",
      "Epoch 2847/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0117 - val_loss: 0.0043\n",
      "Epoch 2848/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0067 - val_loss: 0.0024\n",
      "Epoch 2849/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0024 - val_loss: 0.0010\n",
      "Epoch 2850/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0036 - val_loss: 0.0053\n",
      "Epoch 2851/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0028 - val_loss: 0.0016\n",
      "Epoch 2852/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0024 - val_loss: 4.1554e-04\n",
      "Epoch 2853/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 2854/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0024 - val_loss: 0.0043\n",
      "Epoch 2855/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0017 - val_loss: 2.1022e-04\n",
      "Epoch 2856/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 6.4774e-04 - val_loss: 4.0215e-04\n",
      "Epoch 2857/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0017 - val_loss: 0.0066\n",
      "Epoch 2858/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0072 - val_loss: 0.0041\n",
      "Epoch 2859/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0062 - val_loss: 0.0210\n",
      "Epoch 2860/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0104 - val_loss: 0.0021\n",
      "Epoch 2861/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0157 - val_loss: 0.0417\n",
      "Epoch 2862/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0348 - val_loss: 0.0459\n",
      "Epoch 2863/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0194 - val_loss: 0.0235\n",
      "Epoch 2864/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0316 - val_loss: 0.0395\n",
      "Epoch 2865/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0417 - val_loss: 0.0552\n",
      "Epoch 2866/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0331 - val_loss: 0.0465\n",
      "Epoch 2867/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0169 - val_loss: 0.0060\n",
      "Epoch 2868/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 0.0025 - val_loss: 3.2349e-04\n",
      "Epoch 2869/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0010 - val_loss: 0.0023\n",
      "Epoch 2870/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 4.0430e-04\n",
      "Epoch 2871/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.0678e-04 - val_loss: 6.3998e-04\n",
      "Epoch 2872/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 2.9988e-04 - val_loss: 2.0649e-04\n",
      "Epoch 2873/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2600e-04 - val_loss: 8.6879e-05\n",
      "Epoch 2874/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1225e-04 - val_loss: 1.1395e-04\n",
      "Epoch 2875/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.5341e-04 - val_loss: 2.2061e-04\n",
      "Epoch 2876/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8183e-04 - val_loss: 4.5162e-04\n",
      "Epoch 2877/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 2.0876e-04 - val_loss: 9.4642e-05\n",
      "Epoch 2878/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.6637e-04 - val_loss: 2.7967e-04\n",
      "Epoch 2879/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.1579e-05 - val_loss: 9.0387e-05\n",
      "Epoch 2880/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.8377e-05 - val_loss: 1.0633e-05\n",
      "Epoch 2881/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6673e-05 - val_loss: 7.0585e-05\n",
      "Epoch 2882/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.1118e-05 - val_loss: 2.4074e-04\n",
      "Epoch 2883/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0232e-04 - val_loss: 8.8838e-05\n",
      "Epoch 2884/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1154e-04 - val_loss: 1.6792e-04\n",
      "Epoch 2885/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 9.7082e-05 - val_loss: 8.2417e-06\n",
      "Epoch 2886/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0867e-04 - val_loss: 1.8236e-04\n",
      "Epoch 2887/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.9500e-05 - val_loss: 3.4901e-05\n",
      "Epoch 2888/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.5817e-05 - val_loss: 9.5880e-05\n",
      "Epoch 2889/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.8948e-05 - val_loss: 3.0217e-05\n",
      "Epoch 2890/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 4.2397e-05 - val_loss: 3.7229e-05\n",
      "Epoch 2891/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.1504e-05 - val_loss: 3.6346e-05\n",
      "Epoch 2892/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.8791e-05 - val_loss: 3.7790e-05\n",
      "Epoch 2893/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.3358e-04 - val_loss: 7.0188e-04\n",
      "Epoch 2894/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 3.7273e-04 - val_loss: 6.7818e-06\n",
      "Epoch 2895/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.5246e-04 - val_loss: 1.6895e-04\n",
      "Epoch 2896/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6197e-04 - val_loss: 1.4903e-04\n",
      "Epoch 2897/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.7386e-04 - val_loss: 3.4473e-04\n",
      "Epoch 2898/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 4.0744e-04 - val_loss: 6.7341e-04\n",
      "Epoch 2899/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0013 - val_loss: 0.0059\n",
      "Epoch 2900/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0037 - val_loss: 0.0046\n",
      "Epoch 2901/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 2902/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.6086e-04 - val_loss: 4.8711e-05\n",
      "Epoch 2903/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 1.4468e-04 - val_loss: 4.5808e-04\n",
      "Epoch 2904/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9283e-04 - val_loss: 9.7145e-05\n",
      "Epoch 2905/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.7517e-05 - val_loss: 1.8575e-04\n",
      "Epoch 2906/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.3863e-05 - val_loss: 7.9831e-05\n",
      "Epoch 2907/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.1385e-05 - val_loss: 3.5845e-05\n",
      "Epoch 2908/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.6944e-04 - val_loss: 1.7928e-04\n",
      "Epoch 2909/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1095e-04 - val_loss: 8.2136e-04\n",
      "Epoch 2910/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.0433e-04 - val_loss: 6.1464e-04\n",
      "Epoch 2911/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.6751e-04 - val_loss: 8.0744e-04\n",
      "Epoch 2912/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.2915e-04 - val_loss: 3.5049e-04\n",
      "Epoch 2913/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 7.7285e-04\n",
      "Epoch 2914/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 5.7986e-04 - val_loss: 0.0010\n",
      "Epoch 2915/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0014 - val_loss: 4.4493e-04\n",
      "Epoch 2916/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.1977e-04 - val_loss: 2.7539e-04\n",
      "Epoch 2917/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1493e-04 - val_loss: 2.4199e-04\n",
      "Epoch 2918/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0011 - val_loss: 4.0795e-04\n",
      "Epoch 2919/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0022 - val_loss: 7.0718e-05\n",
      "Epoch 2920/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0017 - val_loss: 0.0033\n",
      "Epoch 2921/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0017 - val_loss: 0.0126\n",
      "Epoch 2922/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0064 - val_loss: 0.0022\n",
      "Epoch 2923/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0191 - val_loss: 0.0662\n",
      "Epoch 2924/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0513 - val_loss: 0.1172\n",
      "Epoch 2925/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0668 - val_loss: 0.6220\n",
      "Epoch 2926/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.1448 - val_loss: 0.3044\n",
      "Epoch 2927/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.8330 - val_loss: 0.2042\n",
      "Epoch 2928/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5501 - val_loss: 0.6470\n",
      "Epoch 2929/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.8516 - val_loss: 0.4739\n",
      "Epoch 2930/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.371 - 0s 162us/sample - loss: 0.1930 - val_loss: 0.4152\n",
      "Epoch 2931/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2025 - val_loss: 0.1638\n",
      "Epoch 2932/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1031 - val_loss: 0.1831\n",
      "Epoch 2933/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 0.1500 - val_loss: 0.4424\n",
      "Epoch 2934/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3762 - val_loss: 0.4775\n",
      "Epoch 2935/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3227 - val_loss: 0.5937\n",
      "Epoch 2936/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.5379 - val_loss: 1.2251\n",
      "Epoch 2937/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5118 - val_loss: 0.8560\n",
      "Epoch 2938/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3079 - val_loss: 0.1856\n",
      "Epoch 2939/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1501 - val_loss: 0.0043\n",
      "Epoch 2940/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0703 - val_loss: 0.1787\n",
      "Epoch 2941/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0600 - val_loss: 0.0556\n",
      "Epoch 2942/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0231 - val_loss: 0.0254\n",
      "Epoch 2943/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0424 - val_loss: 0.0229\n",
      "Epoch 2944/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1084 - val_loss: 0.1210\n",
      "Epoch 2945/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0967 - val_loss: 0.2870\n",
      "Epoch 2946/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1131 - val_loss: 0.3847\n",
      "Epoch 2947/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2095 - val_loss: 0.0933\n",
      "Epoch 2948/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.9291 - val_loss: 0.7694\n",
      "Epoch 2949/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3.7684 - val_loss: 3.9314\n",
      "Epoch 2950/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.0241 - val_loss: 6.1019\n",
      "Epoch 2951/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.4416 - val_loss: 0.4804\n",
      "Epoch 2952/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4398 - val_loss: 0.1861\n",
      "Epoch 2953/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2626 - val_loss: 2.5108\n",
      "Epoch 2954/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8452 - val_loss: 0.7994\n",
      "Epoch 2955/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1862 - val_loss: 0.2585\n",
      "Epoch 2956/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3191 - val_loss: 1.0965\n",
      "Epoch 2957/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.3962 - val_loss: 0.4955\n",
      "Epoch 2958/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2378 - val_loss: 0.0553\n",
      "Epoch 2959/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1583 - val_loss: 0.1187\n",
      "Epoch 2960/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0931 - val_loss: 0.3368\n",
      "Epoch 2961/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1397 - val_loss: 0.4316\n",
      "Epoch 2962/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5630 - val_loss: 0.2170\n",
      "Epoch 2963/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4619 - val_loss: 0.0779\n",
      "Epoch 2964/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.3554 - val_loss: 0.2257\n",
      "Epoch 2965/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3240 - val_loss: 0.2534\n",
      "Epoch 2966/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.5266 - val_loss: 0.8519\n",
      "Epoch 2967/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.4387 - val_loss: 0.6144\n",
      "Epoch 2968/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.6263 - val_loss: 0.6812\n",
      "Epoch 2969/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2042 - val_loss: 0.0609\n",
      "Epoch 2970/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.1508 - val_loss: 1.0110\n",
      "Epoch 2971/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.0687 - val_loss: 0.2541\n",
      "Epoch 2972/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.2369 - val_loss: 0.3852\n",
      "Epoch 2973/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.7859 - val_loss: 1.0106\n",
      "Epoch 2974/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.9978 - val_loss: 0.5279\n",
      "Epoch 2975/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7978 - val_loss: 2.2295\n",
      "Epoch 2976/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.4131 - val_loss: 0.0144\n",
      "Epoch 2977/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8490 - val_loss: 0.0869\n",
      "Epoch 2978/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.2725 - val_loss: 1.7880\n",
      "Epoch 2979/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1849 - val_loss: 3.7081\n",
      "Epoch 2980/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2645 - val_loss: 0.3333\n",
      "Epoch 2981/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.0878 - val_loss: 15.9678\n",
      "Epoch 2982/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 10.1566 - val_loss: 3.6571\n",
      "Epoch 2983/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.9192 - val_loss: 12.4905\n",
      "Epoch 2984/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.0392 - val_loss: 8.9438\n",
      "Epoch 2985/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.8081 - val_loss: 4.0758\n",
      "Epoch 2986/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.0216 - val_loss: 3.0389\n",
      "Epoch 2987/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7721 - val_loss: 4.4746\n",
      "Epoch 2988/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1280 - val_loss: 2.0515\n",
      "Epoch 2989/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.6836 - val_loss: 55.9771\n",
      "Epoch 2990/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.8332 - val_loss: 0.8031\n",
      "Epoch 2991/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.0572 - val_loss: 17.1370\n",
      "Epoch 2992/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.3569 - val_loss: 2.5920\n",
      "Epoch 2993/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.0890 - val_loss: 10.5816\n",
      "Epoch 2994/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.5123 - val_loss: 5.7573\n",
      "Epoch 2995/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 19.5806 - val_loss: 13.6592\n",
      "Epoch 2996/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 50.2168 - val_loss: 42.6616\n",
      "Epoch 2997/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 129.9374 - val_loss: 452.8945\n",
      "Epoch 2998/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 338.4730 - val_loss: 1215.6053\n",
      "Epoch 2999/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 780.9880 - val_loss: 546.9433\n",
      "Epoch 3000/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 805.0710 - val_loss: 2251.1955\n",
      "Epoch 3001/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6138.4311 - val_loss: 5507.3874\n",
      "Epoch 3002/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3466.1970 - val_loss: 5300.9771\n",
      "Epoch 3003/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9437.8825 - val_loss: 14794.4412\n",
      "Epoch 3004/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 19615.5710 - val_loss: 32212.9475\n",
      "Epoch 3005/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 11865.6927 - val_loss: 1500.7334\n",
      "Epoch 3006/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4574.5420 - val_loss: 3722.5759\n",
      "Epoch 3007/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2437.7525 - val_loss: 2100.2451\n",
      "Epoch 3008/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1131.1916 - val_loss: 434.4701\n",
      "Epoch 3009/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 425.1698 - val_loss: 6.6165\n",
      "Epoch 3010/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 321.5814 - val_loss: 816.0427\n",
      "Epoch 3011/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 463.3789 - val_loss: 72.8186\n",
      "Epoch 3012/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 238.9931 - val_loss: 286.4901\n",
      "Epoch 3013/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 193.7002 - val_loss: 403.6241\n",
      "Epoch 3014/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 158.6604 - val_loss: 51.2253\n",
      "Epoch 3015/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 113.7778 - val_loss: 13.4409\n",
      "Epoch 3016/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 43.3863 - val_loss: 39.6749\n",
      "Epoch 3017/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 26.1329 - val_loss: 33.0170\n",
      "Epoch 3018/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 22.5910 - val_loss: 44.5619\n",
      "Epoch 3019/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 29.0431 - val_loss: 18.9533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3020/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 22.2776 - val_loss: 26.8545\n",
      "Epoch 3021/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19.7965 - val_loss: 33.7229\n",
      "Epoch 3022/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.9770 - val_loss: 6.4261\n",
      "Epoch 3023/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.4986 - val_loss: 0.9756\n",
      "Epoch 3024/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.7036 - val_loss: 27.9036\n",
      "Epoch 3025/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.7137 - val_loss: 14.2734\n",
      "Epoch 3026/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.7450 - val_loss: 2.2562\n",
      "Epoch 3027/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.9887 - val_loss: 11.9783\n",
      "Epoch 3028/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.7003 - val_loss: 2.9888\n",
      "Epoch 3029/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.1286 - val_loss: 1.0158\n",
      "Epoch 3030/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.6766 - val_loss: 0.8629\n",
      "Epoch 3031/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1201 - val_loss: 1.4647\n",
      "Epoch 3032/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.4292 - val_loss: 0.4716\n",
      "Epoch 3033/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8937 - val_loss: 0.7185\n",
      "Epoch 3034/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8484 - val_loss: 0.0252\n",
      "Epoch 3035/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6966 - val_loss: 0.5876\n",
      "Epoch 3036/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2398 - val_loss: 2.0292\n",
      "Epoch 3037/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8486 - val_loss: 0.6676\n",
      "Epoch 3038/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5847 - val_loss: 0.0631\n",
      "Epoch 3039/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6130 - val_loss: 0.4838\n",
      "Epoch 3040/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3070 - val_loss: 0.7267\n",
      "Epoch 3041/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8647 - val_loss: 2.4137\n",
      "Epoch 3042/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4085 - val_loss: 1.1401\n",
      "Epoch 3043/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4682 - val_loss: 0.1577\n",
      "Epoch 3044/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3731 - val_loss: 1.4683\n",
      "Epoch 3045/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4189 - val_loss: 0.4298\n",
      "Epoch 3046/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2195 - val_loss: 0.0028\n",
      "Epoch 3047/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1034 - val_loss: 0.0466\n",
      "Epoch 3048/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0654 - val_loss: 0.0453\n",
      "Epoch 3049/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0342 - val_loss: 0.0164\n",
      "Epoch 3050/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0170 - val_loss: 0.0195\n",
      "Epoch 3051/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0238 - val_loss: 0.0166\n",
      "Epoch 3052/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0133 - val_loss: 0.0059\n",
      "Epoch 3053/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 3054/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0128 - val_loss: 0.0035\n",
      "Epoch 3055/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0047 - val_loss: 0.0051\n",
      "Epoch 3056/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 3057/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 3058/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0064 - val_loss: 0.0041\n",
      "Epoch 3059/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0040 - val_loss: 0.0025\n",
      "Epoch 3060/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 3061/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0024 - val_loss: 0.0028\n",
      "Epoch 3062/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0056 - val_loss: 0.0012\n",
      "Epoch 3063/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0036 - val_loss: 0.0017\n",
      "Epoch 3064/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0025 - val_loss: 0.0074\n",
      "Epoch 3065/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0025 - val_loss: 0.0017\n",
      "Epoch 3066/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 3067/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0014 - val_loss: 8.2922e-04\n",
      "Epoch 3068/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 3069/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 8.0929e-04\n",
      "Epoch 3070/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0017 - val_loss: 8.5494e-04\n",
      "Epoch 3071/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0029 - val_loss: 8.7075e-04\n",
      "Epoch 3072/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0020 - val_loss: 9.1864e-04\n",
      "Epoch 3073/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0017 - val_loss: 9.1166e-04\n",
      "Epoch 3074/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 8.0547e-04\n",
      "Epoch 3075/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 3076/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 3077/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0023 - val_loss: 6.7190e-04\n",
      "Epoch 3078/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0013 - val_loss: 6.7513e-04\n",
      "Epoch 3079/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0021 - val_loss: 0.0038\n",
      "Epoch 3080/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0037 - val_loss: 0.0024\n",
      "Epoch 3081/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0030 - val_loss: 0.0092\n",
      "Epoch 3082/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0044 - val_loss: 0.0031\n",
      "Epoch 3083/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0060 - val_loss: 0.0017\n",
      "Epoch 3084/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0024 - val_loss: 0.0046\n",
      "Epoch 3085/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0025 - val_loss: 8.5504e-04\n",
      "Epoch 3086/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0017 - val_loss: 5.8783e-04\n",
      "Epoch 3087/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0014 - val_loss: 5.5842e-04\n",
      "Epoch 3088/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.6524e-04 - val_loss: 6.3193e-04\n",
      "Epoch 3089/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0012 - val_loss: 8.4473e-04\n",
      "Epoch 3090/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.5798e-04 - val_loss: 7.5428e-04\n",
      "Epoch 3091/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0013 - val_loss: 8.9979e-04\n",
      "Epoch 3092/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.6183e-04 - val_loss: 8.7712e-04\n",
      "Epoch 3093/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 3094/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.3798e-04 - val_loss: 4.3921e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3095/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 9.8802e-04 - val_loss: 0.0012\n",
      "Epoch 3096/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0012 - val_loss: 9.2544e-04\n",
      "Epoch 3097/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.8103e-04 - val_loss: 4.6981e-04\n",
      "Epoch 3098/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0011 - val_loss: 8.5589e-04\n",
      "Epoch 3099/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.8902e-04 - val_loss: 8.9035e-04\n",
      "Epoch 3100/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 3101/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0011 - val_loss: 8.7214e-04\n",
      "Epoch 3102/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0027 - val_loss: 0.0146\n",
      "Epoch 3103/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0075 - val_loss: 0.0064\n",
      "Epoch 3104/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0044 - val_loss: 0.0092\n",
      "Epoch 3105/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0083 - val_loss: 0.0015\n",
      "Epoch 3106/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0076 - val_loss: 0.0021\n",
      "Epoch 3107/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0043 - val_loss: 0.0076\n",
      "Epoch 3108/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0062 - val_loss: 0.0105\n",
      "Epoch 3109/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0043 - val_loss: 0.0050\n",
      "Epoch 3110/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0055 - val_loss: 0.0016\n",
      "Epoch 3111/10000\n",
      "68/68 [==============================] - 0s 559us/sample - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 3112/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0020 - val_loss: 0.0068\n",
      "Epoch 3113/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0048 - val_loss: 0.0166\n",
      "Epoch 3114/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0086 - val_loss: 0.0012\n",
      "Epoch 3115/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.0027 - val_loss: 0.0011\n",
      "Epoch 3116/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0029 - val_loss: 0.0045\n",
      "Epoch 3117/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0052 - val_loss: 0.0016\n",
      "Epoch 3118/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.3700e-04 - val_loss: 0.0026\n",
      "Epoch 3119/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0023 - val_loss: 0.0068\n",
      "Epoch 3120/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0065 - val_loss: 0.0040\n",
      "Epoch 3121/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0064 - val_loss: 0.0075\n",
      "Epoch 3122/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0076 - val_loss: 0.0088\n",
      "Epoch 3123/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0278 - val_loss: 0.0085\n",
      "Epoch 3124/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0196 - val_loss: 0.0402\n",
      "Epoch 3125/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0137 - val_loss: 0.0097\n",
      "Epoch 3126/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0187 - val_loss: 0.0123\n",
      "Epoch 3127/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0278 - val_loss: 0.0674\n",
      "Epoch 3128/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0371 - val_loss: 0.0438\n",
      "Epoch 3129/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0419 - val_loss: 0.0043\n",
      "Epoch 3130/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0201 - val_loss: 0.0179\n",
      "Epoch 3131/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0082 - val_loss: 0.0031\n",
      "Epoch 3132/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0030 - val_loss: 0.0047\n",
      "Epoch 3133/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 0.0026 - val_loss: 0.0019\n",
      "Epoch 3134/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0022 - val_loss: 3.3348e-04\n",
      "Epoch 3135/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0011 - val_loss: 0.0037\n",
      "Epoch 3136/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 7.0864e-04\n",
      "Epoch 3137/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 4.7566e-04 - val_loss: 3.7274e-04\n",
      "Epoch 3138/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.2282e-04 - val_loss: 4.9489e-04\n",
      "Epoch 3139/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.2490e-04 - val_loss: 1.9640e-04\n",
      "Epoch 3140/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 5.3035e-04 - val_loss: 0.0016\n",
      "Epoch 3141/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.4045e-04 - val_loss: 2.3197e-04\n",
      "Epoch 3142/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 4.1946e-04 - val_loss: 5.7894e-04\n",
      "Epoch 3143/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.6920e-04 - val_loss: 0.0044\n",
      "Epoch 3144/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.7633e-04 - val_loss: 0.0013\n",
      "Epoch 3145/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0032 - val_loss: 0.0059\n",
      "Epoch 3146/10000\n",
      "68/68 [==============================] - 0s 588us/sample - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 3147/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0012 - val_loss: 2.4570e-04\n",
      "Epoch 3148/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.2784e-04 - val_loss: 0.0064\n",
      "Epoch 3149/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0069 - val_loss: 0.0042\n",
      "Epoch 3150/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 0.0050 - val_loss: 0.0281\n",
      "Epoch 3151/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0226 - val_loss: 0.0286\n",
      "Epoch 3152/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1886 - val_loss: 0.1239\n",
      "Epoch 3153/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1532 - val_loss: 0.2421\n",
      "Epoch 3154/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1326 - val_loss: 0.0362\n",
      "Epoch 3155/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.0269 - val_loss: 0.0100\n",
      "Epoch 3156/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0022 - val_loss: 9.0531e-04\n",
      "Epoch 3157/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 6.4218e-04 - val_loss: 9.3238e-05\n",
      "Epoch 3158/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 3159/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0022 - val_loss: 0.0053\n",
      "Epoch 3160/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0055 - val_loss: 0.0099\n",
      "Epoch 3161/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0027 - val_loss: 0.0041\n",
      "Epoch 3162/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0011 - val_loss: 3.2006e-04\n",
      "Epoch 3163/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 2.8425e-04 - val_loss: 7.3188e-05\n",
      "Epoch 3164/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.8630e-04 - val_loss: 1.9529e-04\n",
      "Epoch 3165/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5484e-04 - val_loss: 1.0476e-04\n",
      "Epoch 3166/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3116e-04 - val_loss: 6.3055e-05\n",
      "Epoch 3167/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.8006e-04 - val_loss: 1.9965e-04\n",
      "Epoch 3168/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 0.0011 - val_loss: 0.0029\n",
      "Epoch 3169/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 8.8742e-04\n",
      "Epoch 3170/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.1729e-04 - val_loss: 1.8420e-04\n",
      "Epoch 3171/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.0221e-04 - val_loss: 2.1410e-04\n",
      "Epoch 3172/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 3.0120e-04 - val_loss: 4.2914e-04\n",
      "Epoch 3173/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.7757e-04 - val_loss: 7.5039e-04\n",
      "Epoch 3174/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.7856e-04 - val_loss: 1.9567e-04\n",
      "Epoch 3175/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.2961e-04 - val_loss: 1.0128e-04\n",
      "Epoch 3176/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 4.3960e-04 - val_loss: 9.1524e-04\n",
      "Epoch 3177/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0011 - val_loss: 0.0024\n",
      "Epoch 3178/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0052 - val_loss: 0.0159\n",
      "Epoch 3179/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0078 - val_loss: 0.0069\n",
      "Epoch 3180/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 3181/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0024 - val_loss: 0.0042\n",
      "Epoch 3182/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0056 - val_loss: 0.0051\n",
      "Epoch 3183/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0024 - val_loss: 0.0020\n",
      "Epoch 3184/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.3650e-04 - val_loss: 2.6103e-04\n",
      "Epoch 3185/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 8.0636e-04 - val_loss: 2.0601e-04\n",
      "Epoch 3186/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0026 - val_loss: 2.2974e-04\n",
      "Epoch 3187/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0017 - val_loss: 0.0038\n",
      "Epoch 3188/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0063 - val_loss: 1.5557e-04\n",
      "Epoch 3189/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0076 - val_loss: 4.5131e-04\n",
      "Epoch 3190/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0047 - val_loss: 0.0040\n",
      "Epoch 3191/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0097 - val_loss: 0.0098\n",
      "Epoch 3192/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0124 - val_loss: 0.0138\n",
      "Epoch 3193/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0084 - val_loss: 0.0027\n",
      "Epoch 3194/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0060 - val_loss: 0.0048\n",
      "Epoch 3195/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0166 - val_loss: 0.0056\n",
      "Epoch 3196/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0033 - val_loss: 0.0021\n",
      "Epoch 3197/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0015 - val_loss: 0.0041\n",
      "Epoch 3198/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0022 - val_loss: 0.0068\n",
      "Epoch 3199/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0048 - val_loss: 0.0157\n",
      "Epoch 3200/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0046 - val_loss: 0.0054\n",
      "Epoch 3201/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0021 - val_loss: 5.9778e-04\n",
      "Epoch 3202/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 3203/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.0823e-04 - val_loss: 0.0017\n",
      "Epoch 3204/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0031 - val_loss: 9.3232e-04\n",
      "Epoch 3205/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0012 - val_loss: 9.4279e-04\n",
      "Epoch 3206/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0060 - val_loss: 0.0330\n",
      "Epoch 3207/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0187 - val_loss: 0.0140\n",
      "Epoch 3208/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0060 - val_loss: 0.0101\n",
      "Epoch 3209/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0345 - val_loss: 0.2098\n",
      "Epoch 3210/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2525 - val_loss: 0.0318\n",
      "Epoch 3211/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0514 - val_loss: 0.0749\n",
      "Epoch 3212/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0191 - val_loss: 0.0371\n",
      "Epoch 3213/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0309 - val_loss: 0.0082\n",
      "Epoch 3214/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0113 - val_loss: 0.0057\n",
      "Epoch 3215/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0074 - val_loss: 0.0065\n",
      "Epoch 3216/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0115 - val_loss: 0.0086\n",
      "Epoch 3217/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0054 - val_loss: 0.0012\n",
      "Epoch 3218/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0037 - val_loss: 0.0066\n",
      "Epoch 3219/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0050 - val_loss: 0.0375\n",
      "Epoch 3220/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0114 - val_loss: 0.0143\n",
      "Epoch 3221/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0217 - val_loss: 0.0180\n",
      "Epoch 3222/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1336 - val_loss: 0.4250\n",
      "Epoch 3223/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1143 - val_loss: 0.1994\n",
      "Epoch 3224/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1046 - val_loss: 0.0145\n",
      "Epoch 3225/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0466 - val_loss: 0.0238\n",
      "Epoch 3226/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0363 - val_loss: 0.0117\n",
      "Epoch 3227/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0125 - val_loss: 0.0264\n",
      "Epoch 3228/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0344 - val_loss: 0.0046\n",
      "Epoch 3229/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0256 - val_loss: 0.0022\n",
      "Epoch 3230/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0620 - val_loss: 0.1202\n",
      "Epoch 3231/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0357 - val_loss: 0.1015\n",
      "Epoch 3232/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0789 - val_loss: 0.1180\n",
      "Epoch 3233/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2526 - val_loss: 0.3613\n",
      "Epoch 3234/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3446 - val_loss: 1.2551\n",
      "Epoch 3235/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4787 - val_loss: 1.1654\n",
      "Epoch 3236/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2947 - val_loss: 0.0624\n",
      "Epoch 3237/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0322 - val_loss: 0.0821\n",
      "Epoch 3238/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0851 - val_loss: 0.0279\n",
      "Epoch 3239/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1175 - val_loss: 0.1824\n",
      "Epoch 3240/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.153 - 0s 147us/sample - loss: 0.0800 - val_loss: 0.0288\n",
      "Epoch 3241/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0223 - val_loss: 0.0167\n",
      "Epoch 3242/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0274 - val_loss: 0.0502\n",
      "Epoch 3243/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0168 - val_loss: 0.0262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3244/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1088 - val_loss: 0.0851\n",
      "Epoch 3245/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1521 - val_loss: 0.3343\n",
      "Epoch 3246/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3497 - val_loss: 1.3018\n",
      "Epoch 3247/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.0155 - val_loss: 0.9273\n",
      "Epoch 3248/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.3065 - val_loss: 30.8460\n",
      "Epoch 3249/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 24.90 - 0s 162us/sample - loss: 18.4784 - val_loss: 4.2190\n",
      "Epoch 3250/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.5303 - val_loss: 23.5495\n",
      "Epoch 3251/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 56.4697 - val_loss: 45.5817\n",
      "Epoch 3252/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 29.6278 - val_loss: 34.8238\n",
      "Epoch 3253/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 27.6950 - val_loss: 13.1660\n",
      "Epoch 3254/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 40.7728 - val_loss: 18.2898\n",
      "Epoch 3255/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 31.6761 - val_loss: 58.2944\n",
      "Epoch 3256/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 29.6424 - val_loss: 22.8258\n",
      "Epoch 3257/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.8818 - val_loss: 47.4605\n",
      "Epoch 3258/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 163.3441 - val_loss: 15.5040\n",
      "Epoch 3259/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 324.3579 - val_loss: 1005.1738\n",
      "Epoch 3260/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 605.4909 - val_loss: 159.8431\n",
      "Epoch 3261/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 111.0871 - val_loss: 1320.2154\n",
      "Epoch 3262/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 594.6117 - val_loss: 154.7411\n",
      "Epoch 3263/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 108.3248 - val_loss: 56.1655\n",
      "Epoch 3264/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 96.7218 - val_loss: 144.6981\n",
      "Epoch 3265/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 95.9326 - val_loss: 19.7498\n",
      "Epoch 3266/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 55.6452 - val_loss: 198.3315\n",
      "Epoch 3267/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 222.1650 - val_loss: 207.0235\n",
      "Epoch 3268/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 232.7193 - val_loss: 997.7046\n",
      "Epoch 3269/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1303.6088 - val_loss: 175.5856\n",
      "Epoch 3270/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1211.4130 - val_loss: 666.9102\n",
      "Epoch 3271/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 271.3771 - val_loss: 179.2294\n",
      "Epoch 3272/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 230.344 - 0s 162us/sample - loss: 209.3943 - val_loss: 568.2526\n",
      "Epoch 3273/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 101.2419 - val_loss: 76.5732\n",
      "Epoch 3274/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 237.1618 - val_loss: 34.4192\n",
      "Epoch 3275/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1018.5477 - val_loss: 1051.4583\n",
      "Epoch 3276/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 269.5240 - val_loss: 348.4607\n",
      "Epoch 3277/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 103.8243 - val_loss: 28.3903\n",
      "Epoch 3278/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 48.3672 - val_loss: 128.1036\n",
      "Epoch 3279/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 71.0781 - val_loss: 55.3579\n",
      "Epoch 3280/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.1612 - val_loss: 8.3435\n",
      "Epoch 3281/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.1196 - val_loss: 4.6100\n",
      "Epoch 3282/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.1267 - val_loss: 9.2756\n",
      "Epoch 3283/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.8986 - val_loss: 2.4408\n",
      "Epoch 3284/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.5573 - val_loss: 4.3559\n",
      "Epoch 3285/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.9195 - val_loss: 4.2164\n",
      "Epoch 3286/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15.6544 - val_loss: 21.4069\n",
      "Epoch 3287/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.3861 - val_loss: 18.5157\n",
      "Epoch 3288/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 32.9411 - val_loss: 61.8474\n",
      "Epoch 3289/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 45.2765 - val_loss: 16.8393\n",
      "Epoch 3290/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 89.9786 - val_loss: 272.3721\n",
      "Epoch 3291/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 170.4962 - val_loss: 438.1304\n",
      "Epoch 3292/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 170.3261 - val_loss: 22.6514\n",
      "Epoch 3293/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 50.9911 - val_loss: 14.9732\n",
      "Epoch 3294/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 23.7982 - val_loss: 49.2433\n",
      "Epoch 3295/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 22.0374 - val_loss: 5.8619\n",
      "Epoch 3296/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 9.6620 - val_loss: 3.9468\n",
      "Epoch 3297/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16.9880 - val_loss: 13.3970\n",
      "Epoch 3298/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.7209 - val_loss: 0.4258\n",
      "Epoch 3299/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.4491 - val_loss: 2.3639\n",
      "Epoch 3300/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.7656 - val_loss: 18.7537\n",
      "Epoch 3301/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 29.9876 - val_loss: 53.3490\n",
      "Epoch 3302/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 33.5098 - val_loss: 15.9294\n",
      "Epoch 3303/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 30.6625 - val_loss: 18.8456\n",
      "Epoch 3304/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.3559 - val_loss: 4.1422\n",
      "Epoch 3305/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 22.6582 - val_loss: 30.2273\n",
      "Epoch 3306/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 37.8930 - val_loss: 3.8987\n",
      "Epoch 3307/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12.7915 - val_loss: 20.8778\n",
      "Epoch 3308/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 23.1056 - val_loss: 15.7646\n",
      "Epoch 3309/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15.8521 - val_loss: 23.9149\n",
      "Epoch 3310/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 19.0535 - val_loss: 10.6308\n",
      "Epoch 3311/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 46.4053 - val_loss: 11.8807\n",
      "Epoch 3312/10000\n",
      "68/68 [==============================] - 0s 588us/sample - loss: 53.1588 - val_loss: 130.4328\n",
      "Epoch 3313/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 45.5045 - val_loss: 5.1475\n",
      "Epoch 3314/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 33.6822 - val_loss: 38.3797\n",
      "Epoch 3315/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 33.6773 - val_loss: 95.2958\n",
      "Epoch 3316/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 241.5043 - val_loss: 160.2489\n",
      "Epoch 3317/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 224.2512 - val_loss: 1253.9973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3318/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1044.7810 - val_loss: 830.8693\n",
      "Epoch 3319/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3161.7494 - val_loss: 3334.9268\n",
      "Epoch 3320/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 12115.6881 - val_loss: 417.4716\n",
      "Epoch 3321/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 5206.9910 - val_loss: 7393.0356\n",
      "Epoch 3322/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2552.1150 - val_loss: 2528.2431\n",
      "Epoch 3323/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2587.4378 - val_loss: 4865.3410\n",
      "Epoch 3324/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1790.1572 - val_loss: 2628.6285\n",
      "Epoch 3325/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1402.6392 - val_loss: 960.6329\n",
      "Epoch 3326/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 933.6609 - val_loss: 81.1429\n",
      "Epoch 3327/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 275.8469 - val_loss: 50.9574\n",
      "Epoch 3328/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 149.6614 - val_loss: 7.2002\n",
      "Epoch 3329/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 112.7119 - val_loss: 63.0965\n",
      "Epoch 3330/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 173.0002 - val_loss: 41.7408\n",
      "Epoch 3331/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 260.8720 - val_loss: 384.6560\n",
      "Epoch 3332/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 555.7587 - val_loss: 1081.4519\n",
      "Epoch 3333/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 538.0865 - val_loss: 353.2707\n",
      "Epoch 3334/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 190.9124 - val_loss: 77.0511\n",
      "Epoch 3335/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 49.4200 - val_loss: 16.2498\n",
      "Epoch 3336/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 15.6736 - val_loss: 7.8780\n",
      "Epoch 3337/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.7277 - val_loss: 3.4110\n",
      "Epoch 3338/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 13.5531 - val_loss: 7.1067\n",
      "Epoch 3339/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.2169 - val_loss: 5.6691\n",
      "Epoch 3340/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.4166 - val_loss: 1.1907\n",
      "Epoch 3341/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2081 - val_loss: 3.0727\n",
      "Epoch 3342/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.5157 - val_loss: 1.8124\n",
      "Epoch 3343/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.7488 - val_loss: 0.6055\n",
      "Epoch 3344/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.7334 - val_loss: 2.0197\n",
      "Epoch 3345/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5885 - val_loss: 0.7048\n",
      "Epoch 3346/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.2945 - val_loss: 0.2204\n",
      "Epoch 3347/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2333 - val_loss: 0.5206\n",
      "Epoch 3348/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2404 - val_loss: 0.4464\n",
      "Epoch 3349/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2123 - val_loss: 0.1746\n",
      "Epoch 3350/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1221 - val_loss: 0.0238\n",
      "Epoch 3351/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0334 - val_loss: 0.0451\n",
      "Epoch 3352/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0857 - val_loss: 0.1018\n",
      "Epoch 3353/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1436 - val_loss: 0.0654\n",
      "Epoch 3354/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2120 - val_loss: 0.0763\n",
      "Epoch 3355/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0778 - val_loss: 0.0294\n",
      "Epoch 3356/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0550 - val_loss: 0.0852\n",
      "Epoch 3357/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0280 - val_loss: 0.0344\n",
      "Epoch 3358/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0240 - val_loss: 0.0326\n",
      "Epoch 3359/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0204 - val_loss: 0.0514\n",
      "Epoch 3360/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0601 - val_loss: 0.1579\n",
      "Epoch 3361/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0476 - val_loss: 0.0660\n",
      "Epoch 3362/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0604 - val_loss: 0.0320\n",
      "Epoch 3363/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0319 - val_loss: 0.0443\n",
      "Epoch 3364/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0399 - val_loss: 0.0270\n",
      "Epoch 3365/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0212 - val_loss: 0.0202\n",
      "Epoch 3366/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0137 - val_loss: 0.0064\n",
      "Epoch 3367/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.004 - 0s 162us/sample - loss: 0.0157 - val_loss: 0.0950\n",
      "Epoch 3368/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0708 - val_loss: 0.1202\n",
      "Epoch 3369/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0534 - val_loss: 0.0073\n",
      "Epoch 3370/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0166 - val_loss: 0.0408\n",
      "Epoch 3371/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0444 - val_loss: 0.1112\n",
      "Epoch 3372/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0337 - val_loss: 0.0388\n",
      "Epoch 3373/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0267 - val_loss: 0.0224\n",
      "Epoch 3374/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0192 - val_loss: 0.0133\n",
      "Epoch 3375/10000\n",
      "68/68 [==============================] - 0s 647us/sample - loss: 0.0117 - val_loss: 0.0085\n",
      "Epoch 3376/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 0.0208 - val_loss: 0.0180\n",
      "Epoch 3377/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0125 - val_loss: 0.0096\n",
      "Epoch 3378/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0121 - val_loss: 0.0415\n",
      "Epoch 3379/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0265 - val_loss: 0.0280\n",
      "Epoch 3380/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.0348 - val_loss: 0.1201\n",
      "Epoch 3381/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0545 - val_loss: 0.0226\n",
      "Epoch 3382/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0321 - val_loss: 0.0247\n",
      "Epoch 3383/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0372 - val_loss: 0.0395\n",
      "Epoch 3384/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0661 - val_loss: 0.1411\n",
      "Epoch 3385/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1602 - val_loss: 0.2067\n",
      "Epoch 3386/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0044 - val_loss: 0.5560\n",
      "Epoch 3387/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3523 - val_loss: 0.4306\n",
      "Epoch 3388/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4113 - val_loss: 0.1076\n",
      "Epoch 3389/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8321 - val_loss: 0.9297\n",
      "Epoch 3390/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.4626 - val_loss: 0.1146\n",
      "Epoch 3391/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4055 - val_loss: 0.3514\n",
      "Epoch 3392/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2430 - val_loss: 0.0346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3393/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2506 - val_loss: 0.2415\n",
      "Epoch 3394/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1767 - val_loss: 0.3062\n",
      "Epoch 3395/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1428 - val_loss: 0.1163\n",
      "Epoch 3396/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0619 - val_loss: 0.0190\n",
      "Epoch 3397/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0477 - val_loss: 0.0366\n",
      "Epoch 3398/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1247 - val_loss: 0.1720\n",
      "Epoch 3399/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0546 - val_loss: 0.0478\n",
      "Epoch 3400/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1182 - val_loss: 0.0841\n",
      "Epoch 3401/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2058 - val_loss: 0.3223\n",
      "Epoch 3402/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5019 - val_loss: 0.4102\n",
      "Epoch 3403/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7278 - val_loss: 0.3290\n",
      "Epoch 3404/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4383 - val_loss: 0.2842\n",
      "Epoch 3405/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1149 - val_loss: 0.0953\n",
      "Epoch 3406/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0638 - val_loss: 0.0234\n",
      "Epoch 3407/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0385 - val_loss: 0.0761\n",
      "Epoch 3408/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0364 - val_loss: 0.0146\n",
      "Epoch 3409/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0309 - val_loss: 0.0069\n",
      "Epoch 3410/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0243 - val_loss: 0.0062\n",
      "Epoch 3411/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0071 - val_loss: 0.0099\n",
      "Epoch 3412/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0236 - val_loss: 0.0049\n",
      "Epoch 3413/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0344 - val_loss: 0.0067\n",
      "Epoch 3414/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0364 - val_loss: 0.0134\n",
      "Epoch 3415/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0225 - val_loss: 0.0127\n",
      "Epoch 3416/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0066 - val_loss: 0.0098\n",
      "Epoch 3417/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0049 - val_loss: 0.0055\n",
      "Epoch 3418/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 3419/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.0026 - val_loss: 0.0013\n",
      "Epoch 3420/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0036 - val_loss: 0.0017\n",
      "Epoch 3421/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0135 - val_loss: 0.0164\n",
      "Epoch 3422/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0099 - val_loss: 0.0266\n",
      "Epoch 3423/10000\n",
      "68/68 [==============================] - 0s 721us/sample - loss: 0.0336 - val_loss: 0.0059\n",
      "Epoch 3424/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0163 - val_loss: 0.0296\n",
      "Epoch 3425/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0344 - val_loss: 0.0323\n",
      "Epoch 3426/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0316 - val_loss: 0.0141\n",
      "Epoch 3427/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0476 - val_loss: 0.0049\n",
      "Epoch 3428/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0492 - val_loss: 0.0713\n",
      "Epoch 3429/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0123 - val_loss: 0.0151\n",
      "Epoch 3430/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0116 - val_loss: 0.0651\n",
      "Epoch 3431/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0396 - val_loss: 0.0389\n",
      "Epoch 3432/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 0.0445 - val_loss: 0.0456\n",
      "Epoch 3433/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0246 - val_loss: 0.0386\n",
      "Epoch 3434/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0461 - val_loss: 0.0272\n",
      "Epoch 3435/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0344 - val_loss: 0.0088\n",
      "Epoch 3436/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0393 - val_loss: 0.0356\n",
      "Epoch 3437/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0169 - val_loss: 0.0732\n",
      "Epoch 3438/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0386 - val_loss: 0.1382\n",
      "Epoch 3439/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1629 - val_loss: 0.1765\n",
      "Epoch 3440/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3849 - val_loss: 0.1340\n",
      "Epoch 3441/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3427 - val_loss: 0.4405\n",
      "Epoch 3442/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.3248 - val_loss: 0.7153\n",
      "Epoch 3443/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4114 - val_loss: 0.7972\n",
      "Epoch 3444/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4683 - val_loss: 0.8887\n",
      "Epoch 3445/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2412 - val_loss: 0.1016\n",
      "Epoch 3446/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0592 - val_loss: 0.0763\n",
      "Epoch 3447/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1461 - val_loss: 0.4987\n",
      "Epoch 3448/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8971 - val_loss: 0.3630\n",
      "Epoch 3449/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1701 - val_loss: 1.4482\n",
      "Epoch 3450/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.3853 - val_loss: 0.0977\n",
      "Epoch 3451/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.1433 - val_loss: 0.4060\n",
      "Epoch 3452/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2503 - val_loss: 0.1174\n",
      "Epoch 3453/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1455 - val_loss: 0.4964\n",
      "Epoch 3454/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2310 - val_loss: 0.3363\n",
      "Epoch 3455/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3553 - val_loss: 0.2628\n",
      "Epoch 3456/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6104 - val_loss: 1.9177\n",
      "Epoch 3457/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7492 - val_loss: 0.2212\n",
      "Epoch 3458/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1683 - val_loss: 0.0271\n",
      "Epoch 3459/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0986 - val_loss: 0.0688\n",
      "Epoch 3460/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0398 - val_loss: 0.0336\n",
      "Epoch 3461/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0453 - val_loss: 0.1158\n",
      "Epoch 3462/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1816 - val_loss: 0.7144\n",
      "Epoch 3463/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2771 - val_loss: 2.7342e-04\n",
      "Epoch 3464/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1088 - val_loss: 0.1589\n",
      "Epoch 3465/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0863 - val_loss: 1.1363e-04\n",
      "Epoch 3466/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0773 - val_loss: 0.1926\n",
      "Epoch 3467/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0758 - val_loss: 0.3965\n",
      "Epoch 3468/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1130 - val_loss: 0.0027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3469/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0095 - val_loss: 0.0141\n",
      "Epoch 3470/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0550 - val_loss: 0.4413\n",
      "Epoch 3471/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.1473 - val_loss: 3.5246\n",
      "Epoch 3472/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.7038 - val_loss: 0.8038\n",
      "Epoch 3473/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5181 - val_loss: 0.3827\n",
      "Epoch 3474/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8695 - val_loss: 4.4922\n",
      "Epoch 3475/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6701 - val_loss: 1.7224\n",
      "Epoch 3476/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.2241 - val_loss: 3.8855\n",
      "Epoch 3477/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5701 - val_loss: 3.7557\n",
      "Epoch 3478/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.7803 - val_loss: 3.8336\n",
      "Epoch 3479/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14.3644 - val_loss: 20.7817\n",
      "Epoch 3480/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 29.4374 - val_loss: 159.4068\n",
      "Epoch 3481/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 205.9896 - val_loss: 57.3671\n",
      "Epoch 3482/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 324.1110 - val_loss: 1844.4262\n",
      "Epoch 3483/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2506.0401 - val_loss: 2660.2531\n",
      "Epoch 3484/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 742.6245 - val_loss: 463.9812\n",
      "Epoch 3485/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 129.8741 - val_loss: 44.6824\n",
      "Epoch 3486/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 107.0052 - val_loss: 104.9212\n",
      "Epoch 3487/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 39.3671 - val_loss: 33.5664\n",
      "Epoch 3488/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 25.08 - 0s 147us/sample - loss: 17.9124 - val_loss: 1.3584\n",
      "Epoch 3489/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.1594 - val_loss: 1.9617\n",
      "Epoch 3490/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.9948 - val_loss: 2.2868\n",
      "Epoch 3491/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.4731 - val_loss: 2.4441\n",
      "Epoch 3492/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.6562 - val_loss: 0.8139\n",
      "Epoch 3493/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1227 - val_loss: 0.4675\n",
      "Epoch 3494/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.2745 - val_loss: 0.4147\n",
      "Epoch 3495/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.6520 - val_loss: 2.0503\n",
      "Epoch 3496/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.8299 - val_loss: 4.2978\n",
      "Epoch 3497/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5958 - val_loss: 1.8923\n",
      "Epoch 3498/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.1590 - val_loss: 5.9297\n",
      "Epoch 3499/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.6944 - val_loss: 12.6177\n",
      "Epoch 3500/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 24.0933 - val_loss: 6.4579\n",
      "Epoch 3501/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 53.3996 - val_loss: 47.5340\n",
      "Epoch 3502/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 22.7353 - val_loss: 3.7752\n",
      "Epoch 3503/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.2320 - val_loss: 3.6000\n",
      "Epoch 3504/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.0227 - val_loss: 16.5351\n",
      "Epoch 3505/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 14.9579 - val_loss: 11.1675\n",
      "Epoch 3506/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.9068 - val_loss: 28.1825\n",
      "Epoch 3507/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.1535 - val_loss: 40.3225\n",
      "Epoch 3508/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 47.1432 - val_loss: 88.3264\n",
      "Epoch 3509/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 34.3097 - val_loss: 32.7611\n",
      "Epoch 3510/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 36.0539 - val_loss: 80.8659\n",
      "Epoch 3511/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 48.5033 - val_loss: 281.6045\n",
      "Epoch 3512/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 877.1306 - val_loss: 782.7253\n",
      "Epoch 3513/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1320.4959 - val_loss: 386.9419\n",
      "Epoch 3514/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 436.9638 - val_loss: 763.0964\n",
      "Epoch 3515/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 344.1405 - val_loss: 625.4602\n",
      "Epoch 3516/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 957.1901 - val_loss: 335.1730\n",
      "Epoch 3517/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1109.5908 - val_loss: 623.7461\n",
      "Epoch 3518/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1151.2667 - val_loss: 91.9197\n",
      "Epoch 3519/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1076.2453 - val_loss: 5220.6971\n",
      "Epoch 3520/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2117.7094 - val_loss: 3182.9046\n",
      "Epoch 3521/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1909.7540 - val_loss: 3078.6218\n",
      "Epoch 3522/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 987.3783 - val_loss: 55.3170\n",
      "Epoch 3523/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 535.9216 - val_loss: 998.3400\n",
      "Epoch 3524/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 359.7207 - val_loss: 45.4636\n",
      "Epoch 3525/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 269.9355 - val_loss: 80.4703\n",
      "Epoch 3526/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 55.0485 - val_loss: 53.6781\n",
      "Epoch 3527/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 34.2825 - val_loss: 18.7213\n",
      "Epoch 3528/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 12.7645 - val_loss: 21.3298\n",
      "Epoch 3529/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 30.0648 - val_loss: 34.3538\n",
      "Epoch 3530/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 14.3943 - val_loss: 17.3122\n",
      "Epoch 3531/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 26.1979 - val_loss: 48.8249\n",
      "Epoch 3532/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 22.5257 - val_loss: 0.6215\n",
      "Epoch 3533/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.6406 - val_loss: 15.0430\n",
      "Epoch 3534/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.9794 - val_loss: 4.7078\n",
      "Epoch 3535/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.1118 - val_loss: 0.1476\n",
      "Epoch 3536/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.6101 - val_loss: 1.2900\n",
      "Epoch 3537/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.5731 - val_loss: 0.1116\n",
      "Epoch 3538/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3338 - val_loss: 0.4453\n",
      "Epoch 3539/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3070 - val_loss: 0.5584\n",
      "Epoch 3540/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2959 - val_loss: 0.4608\n",
      "Epoch 3541/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5360 - val_loss: 0.4408\n",
      "Epoch 3542/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2043 - val_loss: 0.0780\n",
      "Epoch 3543/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 221us/sample - loss: 0.3507 - val_loss: 0.1571\n",
      "Epoch 3544/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.1884 - val_loss: 0.0242\n",
      "Epoch 3545/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0376 - val_loss: 0.0319\n",
      "Epoch 3546/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0270 - val_loss: 0.0789\n",
      "Epoch 3547/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1088 - val_loss: 0.3250\n",
      "Epoch 3548/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1032 - val_loss: 0.0299\n",
      "Epoch 3549/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0250 - val_loss: 0.0190\n",
      "Epoch 3550/10000\n",
      "68/68 [==============================] - 0s 809us/sample - loss: 0.0127 - val_loss: 0.0152\n",
      "Epoch 3551/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0162 - val_loss: 0.0241\n",
      "Epoch 3552/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0139 - val_loss: 0.0068\n",
      "Epoch 3553/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0077 - val_loss: 0.0042\n",
      "Epoch 3554/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0166 - val_loss: 0.0208\n",
      "Epoch 3555/10000\n",
      "68/68 [==============================] - 0s 647us/sample - loss: 0.0219 - val_loss: 0.0060\n",
      "Epoch 3556/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0246 - val_loss: 0.0039\n",
      "Epoch 3557/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0173 - val_loss: 0.0046\n",
      "Epoch 3558/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0080 - val_loss: 0.0084\n",
      "Epoch 3559/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0051 - val_loss: 0.0056\n",
      "Epoch 3560/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.0057 - val_loss: 0.0036\n",
      "Epoch 3561/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0056 - val_loss: 0.0041\n",
      "Epoch 3562/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0061 - val_loss: 0.0098\n",
      "Epoch 3563/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 3564/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.0068 - val_loss: 0.0045\n",
      "Epoch 3565/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0097 - val_loss: 0.0133\n",
      "Epoch 3566/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0095 - val_loss: 0.0088\n",
      "Epoch 3567/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0060 - val_loss: 0.0068\n",
      "Epoch 3568/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0090 - val_loss: 0.0129\n",
      "Epoch 3569/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.0092 - val_loss: 0.0058\n",
      "Epoch 3570/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0048 - val_loss: 0.0034\n",
      "Epoch 3571/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0056 - val_loss: 0.0157\n",
      "Epoch 3572/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.0098 - val_loss: 0.0089\n",
      "Epoch 3573/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0090 - val_loss: 0.0227\n",
      "Epoch 3574/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0471 - val_loss: 0.0532\n",
      "Epoch 3575/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0621 - val_loss: 0.1111\n",
      "Epoch 3576/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.3567 - val_loss: 1.2240\n",
      "Epoch 3577/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.2713 - val_loss: 3.3623\n",
      "Epoch 3578/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.7544 - val_loss: 10.0493\n",
      "Epoch 3579/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.0188 - val_loss: 19.4615\n",
      "Epoch 3580/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 28.6475 - val_loss: 146.1303\n",
      "Epoch 3581/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 121.1227 - val_loss: 112.7014\n",
      "Epoch 3582/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 115.8155 - val_loss: 74.7042\n",
      "Epoch 3583/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 207.8791 - val_loss: 217.1710\n",
      "Epoch 3584/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 77.2219 - val_loss: 186.4269\n",
      "Epoch 3585/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 122.8047 - val_loss: 524.0077\n",
      "Epoch 3586/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 261.7458 - val_loss: 210.0619\n",
      "Epoch 3587/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 585.2620 - val_loss: 301.2155\n",
      "Epoch 3588/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 651.7193 - val_loss: 662.0353\n",
      "Epoch 3589/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 384.5863 - val_loss: 1365.2712\n",
      "Epoch 3590/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1039.3197 - val_loss: 1554.2278\n",
      "Epoch 3591/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1191.4078 - val_loss: 1009.4840\n",
      "Epoch 3592/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1070.9275 - val_loss: 2243.3603\n",
      "Epoch 3593/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 778.0747 - val_loss: 295.8291\n",
      "Epoch 3594/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 102.2157 - val_loss: 185.1066\n",
      "Epoch 3595/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 55.9064 - val_loss: 88.2409\n",
      "Epoch 3596/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 181.4094 - val_loss: 4.3606\n",
      "Epoch 3597/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 97.2596 - val_loss: 51.4252\n",
      "Epoch 3598/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 78.4391 - val_loss: 115.0943\n",
      "Epoch 3599/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 93.0099 - val_loss: 44.8804\n",
      "Epoch 3600/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 44.7941 - val_loss: 49.1476\n",
      "Epoch 3601/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 76.9185 - val_loss: 100.0443\n",
      "Epoch 3602/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 109.7238 - val_loss: 323.8898\n",
      "Epoch 3603/10000\n",
      "68/68 [==============================] - 0s 559us/sample - loss: 104.4550 - val_loss: 67.9039\n",
      "Epoch 3604/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 33.9610 - val_loss: 3.4652\n",
      "Epoch 3605/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26.7850 - val_loss: 23.5822\n",
      "Epoch 3606/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 18.6996 - val_loss: 38.0964\n",
      "Epoch 3607/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16.0437 - val_loss: 11.1612\n",
      "Epoch 3608/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 4.3972 - val_loss: 0.2172\n",
      "Epoch 3609/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.5898 - val_loss: 0.7844\n",
      "Epoch 3610/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2039 - val_loss: 0.1846\n",
      "Epoch 3611/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0816 - val_loss: 0.0531\n",
      "Epoch 3612/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0219 - val_loss: 0.0306\n",
      "Epoch 3613/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0288 - val_loss: 0.0169\n",
      "Epoch 3614/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0155 - val_loss: 0.0520\n",
      "Epoch 3615/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0249 - val_loss: 0.0177\n",
      "Epoch 3616/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0323 - val_loss: 0.0126\n",
      "Epoch 3617/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0108 - val_loss: 0.0053\n",
      "Epoch 3618/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0128 - val_loss: 0.0109\n",
      "Epoch 3619/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0123 - val_loss: 0.0324\n",
      "Epoch 3620/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0196 - val_loss: 0.0372\n",
      "Epoch 3621/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0827 - val_loss: 0.0298\n",
      "Epoch 3622/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0626 - val_loss: 0.1678\n",
      "Epoch 3623/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1508 - val_loss: 0.2118\n",
      "Epoch 3624/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1692 - val_loss: 0.1680\n",
      "Epoch 3625/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1421 - val_loss: 0.1295\n",
      "Epoch 3626/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3010 - val_loss: 0.0224\n",
      "Epoch 3627/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4519 - val_loss: 0.3716\n",
      "Epoch 3628/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1470 - val_loss: 1.0936\n",
      "Epoch 3629/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3355 - val_loss: 0.1055\n",
      "Epoch 3630/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.2321 - val_loss: 0.4590\n",
      "Epoch 3631/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9441 - val_loss: 0.8426\n",
      "Epoch 3632/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.888 - 0s 147us/sample - loss: 0.3443 - val_loss: 0.1597\n",
      "Epoch 3633/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2066 - val_loss: 1.3813\n",
      "Epoch 3634/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4129 - val_loss: 0.9005\n",
      "Epoch 3635/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3961 - val_loss: 0.4966\n",
      "Epoch 3636/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.3563 - val_loss: 0.5708\n",
      "Epoch 3637/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2762 - val_loss: 0.3383\n",
      "Epoch 3638/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2592 - val_loss: 0.9074\n",
      "Epoch 3639/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5448 - val_loss: 1.9040\n",
      "Epoch 3640/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.1809 - val_loss: 32.1963\n",
      "Epoch 3641/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 75.9003 - val_loss: 94.4854\n",
      "Epoch 3642/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 265.0062 - val_loss: 146.9187\n",
      "Epoch 3643/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 54.8850 - val_loss: 22.5694\n",
      "Epoch 3644/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 43.7554 - val_loss: 22.0745\n",
      "Epoch 3645/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 54.1646 - val_loss: 54.2252\n",
      "Epoch 3646/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 108.5287 - val_loss: 325.9428\n",
      "Epoch 3647/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 126.0128 - val_loss: 116.0304\n",
      "Epoch 3648/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 55.3809 - val_loss: 42.6850\n",
      "Epoch 3649/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 18.1443 - val_loss: 76.3864\n",
      "Epoch 3650/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 410.6320 - val_loss: 262.2664\n",
      "Epoch 3651/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1001.6721 - val_loss: 111.9311\n",
      "Epoch 3652/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 959.9769 - val_loss: 1085.3242\n",
      "Epoch 3653/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1059.5556 - val_loss: 609.8287\n",
      "Epoch 3654/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1620.8216 - val_loss: 5.2910\n",
      "Epoch 3655/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 567.9280 - val_loss: 869.0934\n",
      "Epoch 3656/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1928.8938 - val_loss: 2295.5003\n",
      "Epoch 3657/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2047.2151 - val_loss: 7967.2489\n",
      "Epoch 3658/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3455.0873 - val_loss: 4254.2554\n",
      "Epoch 3659/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3775.6860 - val_loss: 549.7560\n",
      "Epoch 3660/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2729.3769 - val_loss: 4613.3549\n",
      "Epoch 3661/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4708.7668 - val_loss: 9532.5136\n",
      "Epoch 3662/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3229.2524 - val_loss: 794.7588\n",
      "Epoch 3663/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 957.8778 - val_loss: 1590.0213\n",
      "Epoch 3664/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 2667.3363 - val_loss: 1159.1748\n",
      "Epoch 3665/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1274.0877 - val_loss: 989.7052\n",
      "Epoch 3666/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 462.8075 - val_loss: 922.4642\n",
      "Epoch 3667/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 464.3703 - val_loss: 676.4527\n",
      "Epoch 3668/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 440.5026 - val_loss: 434.6132\n",
      "Epoch 3669/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 252.3056 - val_loss: 590.6159\n",
      "Epoch 3670/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 284.7325 - val_loss: 573.0056\n",
      "Epoch 3671/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 207.1806 - val_loss: 154.4517\n",
      "Epoch 3672/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 110.2215 - val_loss: 80.0088\n",
      "Epoch 3673/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 27.8296 - val_loss: 14.5118\n",
      "Epoch 3674/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.3725 - val_loss: 6.6076\n",
      "Epoch 3675/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0798 - val_loss: 5.2215\n",
      "Epoch 3676/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.6526 - val_loss: 0.3554\n",
      "Epoch 3677/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4172 - val_loss: 0.0676\n",
      "Epoch 3678/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3931 - val_loss: 0.0644\n",
      "Epoch 3679/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2702 - val_loss: 0.5249\n",
      "Epoch 3680/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4637 - val_loss: 0.1670\n",
      "Epoch 3681/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2924 - val_loss: 0.3044\n",
      "Epoch 3682/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2914 - val_loss: 0.2717\n",
      "Epoch 3683/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1767 - val_loss: 0.1966\n",
      "Epoch 3684/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0847 - val_loss: 0.1467\n",
      "Epoch 3685/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0374 - val_loss: 0.0554\n",
      "Epoch 3686/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0199 - val_loss: 0.0284\n",
      "Epoch 3687/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0192 - val_loss: 0.0647\n",
      "Epoch 3688/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0521 - val_loss: 0.0590\n",
      "Epoch 3689/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0389 - val_loss: 0.0027\n",
      "Epoch 3690/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0225 - val_loss: 0.0347\n",
      "Epoch 3691/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0194 - val_loss: 0.0222\n",
      "Epoch 3692/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0143 - val_loss: 0.0519\n",
      "Epoch 3693/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0185 - val_loss: 0.0332\n",
      "Epoch 3694/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0375 - val_loss: 0.0096\n",
      "Epoch 3695/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0355 - val_loss: 0.0215\n",
      "Epoch 3696/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0062 - val_loss: 0.0052\n",
      "Epoch 3697/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0073 - val_loss: 0.0034\n",
      "Epoch 3698/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0026 - val_loss: 0.0049\n",
      "Epoch 3699/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0071 - val_loss: 0.0063\n",
      "Epoch 3700/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0071 - val_loss: 0.0048\n",
      "Epoch 3701/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0081 - val_loss: 0.0187\n",
      "Epoch 3702/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0079 - val_loss: 0.0095\n",
      "Epoch 3703/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0059 - val_loss: 0.0038\n",
      "Epoch 3704/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0069 - val_loss: 0.0078\n",
      "Epoch 3705/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0081 - val_loss: 0.0023\n",
      "Epoch 3706/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0049 - val_loss: 0.0075\n",
      "Epoch 3707/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0039 - val_loss: 9.0339e-04\n",
      "Epoch 3708/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 3709/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0014 - val_loss: 6.4205e-04\n",
      "Epoch 3710/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.6390e-04 - val_loss: 5.6842e-04\n",
      "Epoch 3711/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 8.6679e-04 - val_loss: 7.1310e-04\n",
      "Epoch 3712/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0015 - val_loss: 0.0041\n",
      "Epoch 3713/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0059 - val_loss: 0.0053\n",
      "Epoch 3714/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0035 - val_loss: 0.0030\n",
      "Epoch 3715/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0042 - val_loss: 7.7827e-04\n",
      "Epoch 3716/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0037 - val_loss: 0.0066\n",
      "Epoch 3717/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0021 - val_loss: 6.8073e-04\n",
      "Epoch 3718/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9.0252e-04 - val_loss: 0.0011\n",
      "Epoch 3719/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 0.0028\n",
      "Epoch 3720/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 3721/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0100 - val_loss: 0.0238\n",
      "Epoch 3722/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0120 - val_loss: 0.0038\n",
      "Epoch 3723/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0058 - val_loss: 0.0056\n",
      "Epoch 3724/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0054 - val_loss: 0.0064\n",
      "Epoch 3725/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0041 - val_loss: 0.0068\n",
      "Epoch 3726/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0053 - val_loss: 0.0039\n",
      "Epoch 3727/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0034 - val_loss: 0.0048\n",
      "Epoch 3728/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 3729/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0019 - val_loss: 0.0051\n",
      "Epoch 3730/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0043 - val_loss: 0.0037\n",
      "Epoch 3731/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0020 - val_loss: 0.0037\n",
      "Epoch 3732/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 3733/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0022 - val_loss: 0.0197\n",
      "Epoch 3734/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0115 - val_loss: 0.0068\n",
      "Epoch 3735/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0057 - val_loss: 0.0114\n",
      "Epoch 3736/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0315 - val_loss: 0.0851\n",
      "Epoch 3737/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0469 - val_loss: 0.0279\n",
      "Epoch 3738/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0167 - val_loss: 0.0222\n",
      "Epoch 3739/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0130 - val_loss: 0.0100\n",
      "Epoch 3740/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0104 - val_loss: 0.0333\n",
      "Epoch 3741/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0274 - val_loss: 0.0083\n",
      "Epoch 3742/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0136 - val_loss: 0.0096\n",
      "Epoch 3743/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0090 - val_loss: 0.0058\n",
      "Epoch 3744/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0247 - val_loss: 0.0385\n",
      "Epoch 3745/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0136 - val_loss: 0.0208\n",
      "Epoch 3746/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0102 - val_loss: 0.0018\n",
      "Epoch 3747/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 3748/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0032 - val_loss: 0.0070\n",
      "Epoch 3749/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0022 - val_loss: 0.0016\n",
      "Epoch 3750/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0054 - val_loss: 0.0055\n",
      "Epoch 3751/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0124 - val_loss: 0.0043\n",
      "Epoch 3752/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0097 - val_loss: 0.0146\n",
      "Epoch 3753/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0068 - val_loss: 0.0203\n",
      "Epoch 3754/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0092 - val_loss: 0.0027\n",
      "Epoch 3755/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0069 - val_loss: 0.0025\n",
      "Epoch 3756/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0159 - val_loss: 0.0043\n",
      "Epoch 3757/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0319 - val_loss: 0.0402\n",
      "Epoch 3758/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0460 - val_loss: 0.0801\n",
      "Epoch 3759/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1328 - val_loss: 0.5214\n",
      "Epoch 3760/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4931 - val_loss: 1.4443\n",
      "Epoch 3761/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3578 - val_loss: 0.0744\n",
      "Epoch 3762/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5293 - val_loss: 2.0374\n",
      "Epoch 3763/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.7332 - val_loss: 7.3115\n",
      "Epoch 3764/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.2021 - val_loss: 1.8866\n",
      "Epoch 3765/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7526 - val_loss: 0.5455\n",
      "Epoch 3766/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.8284 - val_loss: 2.2271\n",
      "Epoch 3767/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 10.3015 - val_loss: 40.5427\n",
      "Epoch 3768/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.9545 - val_loss: 3.1668\n",
      "Epoch 3769/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 56.1089 - val_loss: 69.3589\n",
      "Epoch 3770/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 45.8127 - val_loss: 0.7634\n",
      "Epoch 3771/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 27.8894 - val_loss: 103.4247\n",
      "Epoch 3772/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 91.0268 - val_loss: 130.0515\n",
      "Epoch 3773/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 115.7525 - val_loss: 31.0183\n",
      "Epoch 3774/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 192.3276 - val_loss: 11.1899\n",
      "Epoch 3775/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 279.7727 - val_loss: 174.8399\n",
      "Epoch 3776/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 46.4409 - val_loss: 36.1815\n",
      "Epoch 3777/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 15.1184 - val_loss: 20.6422\n",
      "Epoch 3778/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.0985 - val_loss: 3.1898\n",
      "Epoch 3779/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.8030 - val_loss: 1.1632\n",
      "Epoch 3780/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.9775 - val_loss: 1.4862\n",
      "Epoch 3781/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 12.3628 - val_loss: 0.1923\n",
      "Epoch 3782/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 17.9976 - val_loss: 15.5994\n",
      "Epoch 3783/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 31.6843 - val_loss: 12.6434\n",
      "Epoch 3784/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 48.8024 - val_loss: 2.8765\n",
      "Epoch 3785/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 36.0288 - val_loss: 73.2297\n",
      "Epoch 3786/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 32.7089 - val_loss: 4.1948\n",
      "Epoch 3787/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 78.3024 - val_loss: 54.0883\n",
      "Epoch 3788/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 67.7247 - val_loss: 97.3367\n",
      "Epoch 3789/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 455.8244 - val_loss: 699.2427\n",
      "Epoch 3790/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 783.6273 - val_loss: 4217.5116\n",
      "Epoch 3791/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3379.2290 - val_loss: 8564.2854\n",
      "Epoch 3792/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3628.9764 - val_loss: 60.1687\n",
      "Epoch 3793/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 671.6090 - val_loss: 1378.6216\n",
      "Epoch 3794/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 513.9470 - val_loss: 153.6318\n",
      "Epoch 3795/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 218.0915 - val_loss: 714.7457\n",
      "Epoch 3796/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 481.7899 - val_loss: 643.7225\n",
      "Epoch 3797/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1071.8861 - val_loss: 4469.4270\n",
      "Epoch 3798/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2274.3131 - val_loss: 727.9399\n",
      "Epoch 3799/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1735.2749 - val_loss: 952.6904\n",
      "Epoch 3800/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1676.1283 - val_loss: 1856.7764\n",
      "Epoch 3801/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3039.6241 - val_loss: 1140.0710\n",
      "Epoch 3802/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1471.0998 - val_loss: 29.6003\n",
      "Epoch 3803/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 734.6694 - val_loss: 119.0434\n",
      "Epoch 3804/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 146.6429 - val_loss: 30.1903\n",
      "Epoch 3805/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 20.2542 - val_loss: 41.4841\n",
      "Epoch 3806/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 13.3700 - val_loss: 15.3548\n",
      "Epoch 3807/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 12.5629 - val_loss: 3.1512\n",
      "Epoch 3808/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.9695 - val_loss: 6.5740\n",
      "Epoch 3809/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.3826 - val_loss: 14.3381\n",
      "Epoch 3810/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 9.6331 - val_loss: 8.0215\n",
      "Epoch 3811/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 3.0128 - val_loss: 1.3202\n",
      "Epoch 3812/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9449 - val_loss: 0.6650\n",
      "Epoch 3813/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5571 - val_loss: 0.5686\n",
      "Epoch 3814/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3961 - val_loss: 0.9453\n",
      "Epoch 3815/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 0.3851 - val_loss: 0.5773\n",
      "Epoch 3816/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2429 - val_loss: 0.1462\n",
      "Epoch 3817/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.1390 - val_loss: 0.0251\n",
      "Epoch 3818/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0688 - val_loss: 0.0214\n",
      "Epoch 3819/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0327 - val_loss: 0.1097\n",
      "Epoch 3820/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.0614 - val_loss: 0.0407\n",
      "Epoch 3821/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0395 - val_loss: 0.0176\n",
      "Epoch 3822/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0246 - val_loss: 0.0207\n",
      "Epoch 3823/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0132 - val_loss: 0.0105\n",
      "Epoch 3824/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.0195 - val_loss: 0.0236\n",
      "Epoch 3825/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0359 - val_loss: 0.0445\n",
      "Epoch 3826/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0505 - val_loss: 0.0975\n",
      "Epoch 3827/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0423 - val_loss: 0.0246\n",
      "Epoch 3828/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0826 - val_loss: 0.0590\n",
      "Epoch 3829/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 0.0614 - val_loss: 0.1183\n",
      "Epoch 3830/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0539 - val_loss: 0.0181\n",
      "Epoch 3831/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0626 - val_loss: 0.1513\n",
      "Epoch 3832/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0615 - val_loss: 0.2521\n",
      "Epoch 3833/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.1869 - val_loss: 0.1649\n",
      "Epoch 3834/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3037 - val_loss: 0.5145\n",
      "Epoch 3835/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3312 - val_loss: 0.0347\n",
      "Epoch 3836/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2347 - val_loss: 0.3676\n",
      "Epoch 3837/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3283 - val_loss: 0.0740\n",
      "Epoch 3838/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 0.1637 - val_loss: 0.1905\n",
      "Epoch 3839/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0985 - val_loss: 0.0137\n",
      "Epoch 3840/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0265 - val_loss: 0.0974\n",
      "Epoch 3841/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0565 - val_loss: 0.0086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3842/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.0364 - val_loss: 0.0283\n",
      "Epoch 3843/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0473 - val_loss: 0.0210\n",
      "Epoch 3844/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0671 - val_loss: 0.0541\n",
      "Epoch 3845/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0285 - val_loss: 0.0082\n",
      "Epoch 3846/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0446 - val_loss: 0.0340\n",
      "Epoch 3847/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0178 - val_loss: 0.0244\n",
      "Epoch 3848/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0151 - val_loss: 0.0084\n",
      "Epoch 3849/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0138 - val_loss: 0.0115\n",
      "Epoch 3850/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0162 - val_loss: 0.0181\n",
      "Epoch 3851/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0189 - val_loss: 0.0199\n",
      "Epoch 3852/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0196 - val_loss: 0.0401\n",
      "Epoch 3853/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0290 - val_loss: 0.0531\n",
      "Epoch 3854/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0462 - val_loss: 0.0101\n",
      "Epoch 3855/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0302 - val_loss: 0.0461\n",
      "Epoch 3856/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0564 - val_loss: 0.0659\n",
      "Epoch 3857/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0465 - val_loss: 0.0535\n",
      "Epoch 3858/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0370 - val_loss: 0.0579\n",
      "Epoch 3859/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.052 - 0s 191us/sample - loss: 0.0365 - val_loss: 0.0502\n",
      "Epoch 3860/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 0.0290 - val_loss: 0.0558\n",
      "Epoch 3861/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0187 - val_loss: 0.0402\n",
      "Epoch 3862/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0126 - val_loss: 0.0093\n",
      "Epoch 3863/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0116 - val_loss: 0.0118\n",
      "Epoch 3864/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0168 - val_loss: 0.0068\n",
      "Epoch 3865/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0130 - val_loss: 0.0317\n",
      "Epoch 3866/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0190 - val_loss: 0.0133\n",
      "Epoch 3867/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0072 - val_loss: 0.0059\n",
      "Epoch 3868/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0061 - val_loss: 0.0128\n",
      "Epoch 3869/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0099 - val_loss: 0.0034\n",
      "Epoch 3870/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0074 - val_loss: 0.0033\n",
      "Epoch 3871/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0073 - val_loss: 0.0378\n",
      "Epoch 3872/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0201 - val_loss: 0.0122\n",
      "Epoch 3873/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0208 - val_loss: 0.0339\n",
      "Epoch 3874/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0186 - val_loss: 0.0379\n",
      "Epoch 3875/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0389 - val_loss: 0.0657\n",
      "Epoch 3876/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0326 - val_loss: 0.0427\n",
      "Epoch 3877/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0266 - val_loss: 0.0552\n",
      "Epoch 3878/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1305 - val_loss: 0.0210\n",
      "Epoch 3879/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1638 - val_loss: 0.1076\n",
      "Epoch 3880/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0948 - val_loss: 0.0860\n",
      "Epoch 3881/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0645 - val_loss: 0.1171\n",
      "Epoch 3882/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1064 - val_loss: 0.4331\n",
      "Epoch 3883/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1277 - val_loss: 0.0435\n",
      "Epoch 3884/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0925 - val_loss: 0.2432\n",
      "Epoch 3885/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1322 - val_loss: 0.0500\n",
      "Epoch 3886/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1329 - val_loss: 0.0159\n",
      "Epoch 3887/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0737 - val_loss: 0.1284\n",
      "Epoch 3888/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2117 - val_loss: 0.9968\n",
      "Epoch 3889/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5864 - val_loss: 0.1321\n",
      "Epoch 3890/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.8974 - val_loss: 0.0613\n",
      "Epoch 3891/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8287 - val_loss: 1.5231\n",
      "Epoch 3892/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.1851 - val_loss: 4.6097\n",
      "Epoch 3893/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 3.878 - 0s 147us/sample - loss: 2.0922 - val_loss: 1.6624\n",
      "Epoch 3894/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.2520 - val_loss: 0.2486\n",
      "Epoch 3895/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.2375 - val_loss: 1.3783\n",
      "Epoch 3896/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.9993 - val_loss: 1.5817\n",
      "Epoch 3897/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8744 - val_loss: 3.3226\n",
      "Epoch 3898/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8388 - val_loss: 1.9063\n",
      "Epoch 3899/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.0233 - val_loss: 5.5256\n",
      "Epoch 3900/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.6258 - val_loss: 15.2448\n",
      "Epoch 3901/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 22.6201 - val_loss: 37.3173\n",
      "Epoch 3902/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 23.9804 - val_loss: 98.9146\n",
      "Epoch 3903/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 632.9929 - val_loss: 2002.4382\n",
      "Epoch 3904/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1566.4213 - val_loss: 1774.1861\n",
      "Epoch 3905/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 2752.3912 - val_loss: 25404.5250\n",
      "Epoch 3906/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 15741.6765 - val_loss: 21518.6187\n",
      "Epoch 3907/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6578.6155 - val_loss: 12491.9837\n",
      "Epoch 3908/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4029.9750 - val_loss: 3725.7509\n",
      "Epoch 3909/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2043.8081 - val_loss: 1886.0207\n",
      "Epoch 3910/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2527.2991 - val_loss: 3488.0661\n",
      "Epoch 3911/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2649.9814 - val_loss: 3438.6464\n",
      "Epoch 3912/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 3932.93 - 0s 176us/sample - loss: 2131.3818 - val_loss: 116.3310\n",
      "Epoch 3913/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1083.8264 - val_loss: 1082.4067\n",
      "Epoch 3914/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 701.2690 - val_loss: 1200.6785\n",
      "Epoch 3915/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 544.8701 - val_loss: 206.4171\n",
      "Epoch 3916/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 206us/sample - loss: 423.8460 - val_loss: 32.1109\n",
      "Epoch 3917/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 385.5706 - val_loss: 265.8119\n",
      "Epoch 3918/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 288.0657 - val_loss: 174.4501\n",
      "Epoch 3919/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 379.0442 - val_loss: 81.0809\n",
      "Epoch 3920/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 248.3510 - val_loss: 35.3761\n",
      "Epoch 3921/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 109.5574 - val_loss: 109.4836\n",
      "Epoch 3922/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 97.9726 - val_loss: 155.0251\n",
      "Epoch 3923/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 52.4905 - val_loss: 36.1932\n",
      "Epoch 3924/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.5528 - val_loss: 12.9688\n",
      "Epoch 3925/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 11.3178 - val_loss: 7.6881\n",
      "Epoch 3926/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.8768 - val_loss: 3.6552\n",
      "Epoch 3927/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.6168 - val_loss: 5.2866\n",
      "Epoch 3928/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.3918 - val_loss: 2.4987\n",
      "Epoch 3929/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.5148 - val_loss: 0.9645\n",
      "Epoch 3930/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4196 - val_loss: 1.3699\n",
      "Epoch 3931/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8629 - val_loss: 1.5471\n",
      "Epoch 3932/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6337 - val_loss: 0.8340\n",
      "Epoch 3933/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3205 - val_loss: 0.6354\n",
      "Epoch 3934/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2428 - val_loss: 0.2460\n",
      "Epoch 3935/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2044 - val_loss: 0.0995\n",
      "Epoch 3936/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0652 - val_loss: 0.0693\n",
      "Epoch 3937/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0450 - val_loss: 0.0625\n",
      "Epoch 3938/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0284 - val_loss: 0.0374\n",
      "Epoch 3939/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0156 - val_loss: 0.0468\n",
      "Epoch 3940/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0200 - val_loss: 0.0419\n",
      "Epoch 3941/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0174 - val_loss: 0.0081\n",
      "Epoch 3942/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0104 - val_loss: 0.0127\n",
      "Epoch 3943/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0139 - val_loss: 0.0517\n",
      "Epoch 3944/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0363 - val_loss: 0.0311\n",
      "Epoch 3945/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0188 - val_loss: 0.0245\n",
      "Epoch 3946/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0084 - val_loss: 0.0031\n",
      "Epoch 3947/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0057 - val_loss: 0.0059\n",
      "Epoch 3948/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0046 - val_loss: 0.0022\n",
      "Epoch 3949/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0076 - val_loss: 0.0086\n",
      "Epoch 3950/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0072 - val_loss: 0.0049\n",
      "Epoch 3951/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0051 - val_loss: 0.0219\n",
      "Epoch 3952/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0074 - val_loss: 0.0094\n",
      "Epoch 3953/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0063 - val_loss: 0.0096\n",
      "Epoch 3954/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0052 - val_loss: 0.0080\n",
      "Epoch 3955/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0037 - val_loss: 0.0056\n",
      "Epoch 3956/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 3957/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 3958/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.001 - 0s 162us/sample - loss: 0.0028 - val_loss: 0.0018\n",
      "Epoch 3959/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 3960/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0048 - val_loss: 0.0020\n",
      "Epoch 3961/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0049 - val_loss: 0.0016\n",
      "Epoch 3962/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0040 - val_loss: 0.0060\n",
      "Epoch 3963/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0071 - val_loss: 0.0242\n",
      "Epoch 3964/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0130 - val_loss: 0.0245\n",
      "Epoch 3965/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0183 - val_loss: 0.0049\n",
      "Epoch 3966/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0049 - val_loss: 0.0022\n",
      "Epoch 3967/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 3968/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 3969/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0043 - val_loss: 0.0047\n",
      "Epoch 3970/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0063 - val_loss: 0.0012\n",
      "Epoch 3971/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0075 - val_loss: 0.0084\n",
      "Epoch 3972/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0114 - val_loss: 0.0269\n",
      "Epoch 3973/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0133 - val_loss: 0.0388\n",
      "Epoch 3974/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0156 - val_loss: 0.0073\n",
      "Epoch 3975/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0049 - val_loss: 0.0038\n",
      "Epoch 3976/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0033 - val_loss: 0.0078\n",
      "Epoch 3977/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0053 - val_loss: 0.0033\n",
      "Epoch 3978/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0024 - val_loss: 0.0010\n",
      "Epoch 3979/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 3980/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0019 - val_loss: 0.0026\n",
      "Epoch 3981/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0025 - val_loss: 0.0013\n",
      "Epoch 3982/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0021 - val_loss: 0.0016\n",
      "Epoch 3983/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 3984/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 3985/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0028 - val_loss: 0.0150\n",
      "Epoch 3986/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0054 - val_loss: 0.0048\n",
      "Epoch 3987/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0074 - val_loss: 0.0090\n",
      "Epoch 3988/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0106 - val_loss: 0.0013\n",
      "Epoch 3989/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0034 - val_loss: 0.0095\n",
      "Epoch 3990/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0060 - val_loss: 0.0036\n",
      "Epoch 3991/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0037 - val_loss: 0.0022\n",
      "Epoch 3992/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0031 - val_loss: 0.0013\n",
      "Epoch 3993/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 3994/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 3995/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0011 - val_loss: 8.1511e-04\n",
      "Epoch 3996/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 3997/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0024 - val_loss: 0.0013\n",
      "Epoch 3998/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 3999/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0010 - val_loss: 5.9325e-04\n",
      "Epoch 4000/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 4001/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0019 - val_loss: 5.8614e-04\n",
      "Epoch 4002/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 4003/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0030 - val_loss: 0.0034\n",
      "Epoch 4004/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0024 - val_loss: 6.2738e-04\n",
      "Epoch 4005/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0018 - val_loss: 0.0121\n",
      "Epoch 4006/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0070 - val_loss: 0.0021\n",
      "Epoch 4007/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0016 - val_loss: 9.9735e-04\n",
      "Epoch 4008/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 0.0031\n",
      "Epoch 4009/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0024 - val_loss: 0.0056\n",
      "Epoch 4010/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0047 - val_loss: 0.0010\n",
      "Epoch 4011/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0038 - val_loss: 0.0105\n",
      "Epoch 4012/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0109 - val_loss: 0.0041\n",
      "Epoch 4013/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0046 - val_loss: 0.0090\n",
      "Epoch 4014/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0050 - val_loss: 0.0043\n",
      "Epoch 4015/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0033 - val_loss: 0.0011\n",
      "Epoch 4016/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 4017/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0066 - val_loss: 0.0112\n",
      "Epoch 4018/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0105 - val_loss: 3.5841e-04\n",
      "Epoch 4019/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0029 - val_loss: 0.0026\n",
      "Epoch 4020/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0034 - val_loss: 0.0051\n",
      "Epoch 4021/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0036 - val_loss: 0.0028\n",
      "Epoch 4022/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0050 - val_loss: 0.0095\n",
      "Epoch 4023/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0030 - val_loss: 0.0016\n",
      "Epoch 4024/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0028 - val_loss: 0.0036\n",
      "Epoch 4025/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.0020 - val_loss: 6.5768e-04\n",
      "Epoch 4026/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.8401e-04 - val_loss: 0.0016\n",
      "Epoch 4027/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0023 - val_loss: 0.0053\n",
      "Epoch 4028/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0034 - val_loss: 0.0022\n",
      "Epoch 4029/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0030 - val_loss: 6.7242e-04\n",
      "Epoch 4030/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 4031/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 4032/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.5682e-04 - val_loss: 3.3274e-04\n",
      "Epoch 4033/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 5.2185e-04\n",
      "Epoch 4034/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0015 - val_loss: 7.1161e-04\n",
      "Epoch 4035/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 5.5724e-04 - val_loss: 2.8808e-04\n",
      "Epoch 4036/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.2457e-04 - val_loss: 7.7116e-04\n",
      "Epoch 4037/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.1163e-04 - val_loss: 4.0212e-04\n",
      "Epoch 4038/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 7.8661e-04 - val_loss: 3.2255e-04\n",
      "Epoch 4039/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.6116e-04 - val_loss: 0.0041\n",
      "Epoch 4040/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0021 - val_loss: 3.5261e-04\n",
      "Epoch 4041/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.7770e-04 - val_loss: 7.6460e-04\n",
      "Epoch 4042/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0016 - val_loss: 0.0032\n",
      "Epoch 4043/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0037 - val_loss: 0.0109\n",
      "Epoch 4044/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0064 - val_loss: 0.0035\n",
      "Epoch 4045/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 4046/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0013 - val_loss: 6.3574e-04\n",
      "Epoch 4047/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.3030e-04 - val_loss: 0.0010\n",
      "Epoch 4048/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0026 - val_loss: 0.0096\n",
      "Epoch 4049/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0078 - val_loss: 0.0146\n",
      "Epoch 4050/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0134 - val_loss: 0.0410\n",
      "Epoch 4051/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0139 - val_loss: 0.0016\n",
      "Epoch 4052/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0076 - val_loss: 0.0079\n",
      "Epoch 4053/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0149 - val_loss: 0.0147\n",
      "Epoch 4054/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0140 - val_loss: 0.0485\n",
      "Epoch 4055/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0471 - val_loss: 0.0249\n",
      "Epoch 4056/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1015 - val_loss: 0.2564\n",
      "Epoch 4057/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2433 - val_loss: 0.0947\n",
      "Epoch 4058/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2860 - val_loss: 0.1165\n",
      "Epoch 4059/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0602 - val_loss: 0.0139\n",
      "Epoch 4060/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0207 - val_loss: 0.1065\n",
      "Epoch 4061/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0383 - val_loss: 0.0301\n",
      "Epoch 4062/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0975 - val_loss: 0.1404\n",
      "Epoch 4063/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1726 - val_loss: 0.0437\n",
      "Epoch 4064/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1947 - val_loss: 0.0825\n",
      "Epoch 4065/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0723 - val_loss: 0.2321\n",
      "Epoch 4066/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 176us/sample - loss: 0.7011 - val_loss: 0.7087\n",
      "Epoch 4067/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3610 - val_loss: 0.8161\n",
      "Epoch 4068/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.1134 - val_loss: 4.9268\n",
      "Epoch 4069/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.3465 - val_loss: 4.1985\n",
      "Epoch 4070/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.0629 - val_loss: 0.1499\n",
      "Epoch 4071/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0581 - val_loss: 0.0544\n",
      "Epoch 4072/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0213 - val_loss: 0.0065\n",
      "Epoch 4073/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0063 - val_loss: 0.0076\n",
      "Epoch 4074/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0020 - val_loss: 9.5890e-04\n",
      "Epoch 4075/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0016 - val_loss: 0.0068\n",
      "Epoch 4076/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0030 - val_loss: 0.0070\n",
      "Epoch 4077/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0041 - val_loss: 0.0115\n",
      "Epoch 4078/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0061 - val_loss: 0.0136\n",
      "Epoch 4079/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0085 - val_loss: 0.0046\n",
      "Epoch 4080/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0038 - val_loss: 0.0093\n",
      "Epoch 4081/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0106 - val_loss: 0.0178\n",
      "Epoch 4082/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0075 - val_loss: 0.0329\n",
      "Epoch 4083/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0282 - val_loss: 0.1198\n",
      "Epoch 4084/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1009 - val_loss: 0.0070\n",
      "Epoch 4085/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1073 - val_loss: 0.1255\n",
      "Epoch 4086/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1602 - val_loss: 0.0468\n",
      "Epoch 4087/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0602 - val_loss: 0.1933\n",
      "Epoch 4088/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3381 - val_loss: 0.3377\n",
      "Epoch 4089/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7199 - val_loss: 2.2754\n",
      "Epoch 4090/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.6457 - val_loss: 10.6723\n",
      "Epoch 4091/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 25.6767 - val_loss: 7.0529\n",
      "Epoch 4092/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 29.0787 - val_loss: 13.7323\n",
      "Epoch 4093/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 133.8473 - val_loss: 41.5994\n",
      "Epoch 4094/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 49.4741 - val_loss: 113.6917\n",
      "Epoch 4095/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 42.7755 - val_loss: 235.1884\n",
      "Epoch 4096/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 97.0165 - val_loss: 193.3601\n",
      "Epoch 4097/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 173.4883 - val_loss: 912.8827\n",
      "Epoch 4098/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1060.0353 - val_loss: 816.6890\n",
      "Epoch 4099/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 600.4786 - val_loss: 533.8304\n",
      "Epoch 4100/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 311.9474 - val_loss: 198.7309\n",
      "Epoch 4101/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 123.9885 - val_loss: 25.6533\n",
      "Epoch 4102/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 28.4545 - val_loss: 40.5287\n",
      "Epoch 4103/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 21.4581 - val_loss: 24.3769\n",
      "Epoch 4104/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 51.0639 - val_loss: 28.9570\n",
      "Epoch 4105/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 125.0043 - val_loss: 269.5444\n",
      "Epoch 4106/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 50.3452 - val_loss: 108.8457\n",
      "Epoch 4107/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 101.2715 - val_loss: 24.4826\n",
      "Epoch 4108/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 56.5302 - val_loss: 20.9375\n",
      "Epoch 4109/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 28.8074 - val_loss: 11.5956\n",
      "Epoch 4110/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 11.4882 - val_loss: 11.8691\n",
      "Epoch 4111/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.5253 - val_loss: 20.6021\n",
      "Epoch 4112/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.3582 - val_loss: 9.5372\n",
      "Epoch 4113/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.6083 - val_loss: 3.0488\n",
      "Epoch 4114/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 5.4327 - val_loss: 10.5818\n",
      "Epoch 4115/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 19.9759 - val_loss: 1.3284\n",
      "Epoch 4116/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26.6752 - val_loss: 4.9299\n",
      "Epoch 4117/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 14.6785 - val_loss: 19.5781\n",
      "Epoch 4118/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 12.9497 - val_loss: 30.9841\n",
      "Epoch 4119/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 17.6726 - val_loss: 12.3422\n",
      "Epoch 4120/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 25.6534 - val_loss: 1.0407\n",
      "Epoch 4121/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 5.9777 - val_loss: 0.8442\n",
      "Epoch 4122/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 2.1667 - val_loss: 3.7680\n",
      "Epoch 4123/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.1229 - val_loss: 0.1323\n",
      "Epoch 4124/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.2086 - val_loss: 21.1733\n",
      "Epoch 4125/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16.2963 - val_loss: 14.1937\n",
      "Epoch 4126/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 34.0341 - val_loss: 92.1907\n",
      "Epoch 4127/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 58.1036 - val_loss: 17.3599\n",
      "Epoch 4128/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 46.3595 - val_loss: 57.9888\n",
      "Epoch 4129/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 46.1521 - val_loss: 65.4947\n",
      "Epoch 4130/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 76.9297 - val_loss: 8.7888\n",
      "Epoch 4131/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 129.6725 - val_loss: 423.9522\n",
      "Epoch 4132/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 172.0552 - val_loss: 95.9077\n",
      "Epoch 4133/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 90.9593 - val_loss: 123.8598\n",
      "Epoch 4134/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 204.8741 - val_loss: 137.8191\n",
      "Epoch 4135/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 637.7250 - val_loss: 5073.9397\n",
      "Epoch 4136/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5952.7804 - val_loss: 10662.2571\n",
      "Epoch 4137/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9799.6579 - val_loss: 2012.2637\n",
      "Epoch 4138/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8503.4941 - val_loss: 11033.3991\n",
      "Epoch 4139/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17839.9093 - val_loss: 27901.2703\n",
      "Epoch 4140/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 21450.3844 - val_loss: 17821.0863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4141/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 12350.6533 - val_loss: 13069.0958\n",
      "Epoch 4142/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 8539.4977 - val_loss: 1946.4319\n",
      "Epoch 4143/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2836.7562 - val_loss: 1433.8529\n",
      "Epoch 4144/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2144.0903 - val_loss: 5106.7162\n",
      "Epoch 4145/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 2389.0063 - val_loss: 1197.5504\n",
      "Epoch 4146/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 1187.9661 - val_loss: 2529.6249\n",
      "Epoch 4147/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 982.0566 - val_loss: 928.9380\n",
      "Epoch 4148/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 344.2397 - val_loss: 451.6720\n",
      "Epoch 4149/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 164.8855 - val_loss: 14.3664\n",
      "Epoch 4150/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 106.5253 - val_loss: 166.7254\n",
      "Epoch 4151/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 100.8236 - val_loss: 44.5687\n",
      "Epoch 4152/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 47.3753 - val_loss: 13.8201\n",
      "Epoch 4153/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15.5335 - val_loss: 33.6711\n",
      "Epoch 4154/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 8.5973 - val_loss: 5.2605\n",
      "Epoch 4155/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 4.5149 - val_loss: 3.3309\n",
      "Epoch 4156/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.5862 - val_loss: 1.9499\n",
      "Epoch 4157/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3877 - val_loss: 0.7678\n",
      "Epoch 4158/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8087 - val_loss: 0.5751\n",
      "Epoch 4159/10000\n",
      "68/68 [==============================] - 0s 559us/sample - loss: 0.5098 - val_loss: 0.1240\n",
      "Epoch 4160/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1917 - val_loss: 0.1193\n",
      "Epoch 4161/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0662 - val_loss: 0.0395\n",
      "Epoch 4162/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0141 - val_loss: 0.0318\n",
      "Epoch 4163/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0237 - val_loss: 0.0625\n",
      "Epoch 4164/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 0.0324 - val_loss: 0.0087\n",
      "Epoch 4165/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0257 - val_loss: 0.0253\n",
      "Epoch 4166/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0176 - val_loss: 0.0225\n",
      "Epoch 4167/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0042 - val_loss: 0.0032\n",
      "Epoch 4168/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 0.0027 - val_loss: 0.0066\n",
      "Epoch 4169/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0028 - val_loss: 0.0011\n",
      "Epoch 4170/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0014 - val_loss: 4.7198e-05\n",
      "Epoch 4171/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.5542e-04 - val_loss: 1.5539e-04\n",
      "Epoch 4172/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 2.7502e-04 - val_loss: 0.0010\n",
      "Epoch 4173/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8886e-04 - val_loss: 5.7900e-04\n",
      "Epoch 4174/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.6090e-04 - val_loss: 3.0553e-04\n",
      "Epoch 4175/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 4.1621e-04 - val_loss: 3.0542e-04\n",
      "Epoch 4176/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 3.2682e-04 - val_loss: 8.5948e-04\n",
      "Epoch 4177/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 4.2203e-04 - val_loss: 8.7933e-05\n",
      "Epoch 4178/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.4573e-04 - val_loss: 4.7642e-04\n",
      "Epoch 4179/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.2339e-04 - val_loss: 5.3634e-04\n",
      "Epoch 4180/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.3170e-04 - val_loss: 1.0345e-04\n",
      "Epoch 4181/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 1.6611e-04 - val_loss: 6.6975e-05\n",
      "Epoch 4182/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 3.4180e-0 - 0s 176us/sample - loss: 2.0902e-04 - val_loss: 2.7339e-04\n",
      "Epoch 4183/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.5255e-04 - val_loss: 3.1263e-04\n",
      "Epoch 4184/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.2063e-04 - val_loss: 3.5914e-04\n",
      "Epoch 4185/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 4.9219e-04 - val_loss: 2.5572e-04\n",
      "Epoch 4186/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.4929e-04 - val_loss: 9.2923e-04\n",
      "Epoch 4187/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 3.1803e-04 - val_loss: 2.5483e-04\n",
      "Epoch 4188/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.8420e-04 - val_loss: 5.7377e-04\n",
      "Epoch 4189/10000\n",
      "68/68 [==============================] - 0s 544us/sample - loss: 4.2656e-04 - val_loss: 0.0010\n",
      "Epoch 4190/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 4.9996e-04 - val_loss: 8.7857e-05\n",
      "Epoch 4191/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.7570e-04 - val_loss: 4.6401e-04\n",
      "Epoch 4192/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.4115e-05 - val_loss: 6.7408e-05\n",
      "Epoch 4193/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 8.7476e-05 - val_loss: 1.0923e-04\n",
      "Epoch 4194/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1107e-04 - val_loss: 6.1503e-05\n",
      "Epoch 4195/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.5726e-05 - val_loss: 3.3454e-05\n",
      "Epoch 4196/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.2801e-05 - val_loss: 2.4583e-05\n",
      "Epoch 4197/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.2340e-05 - val_loss: 4.7246e-05\n",
      "Epoch 4198/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.5662e-05 - val_loss: 2.4414e-05\n",
      "Epoch 4199/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.9202e-05 - val_loss: 8.3437e-05\n",
      "Epoch 4200/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.3720e-05 - val_loss: 4.9869e-05\n",
      "Epoch 4201/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 5.3291e-05 - val_loss: 3.4097e-05\n",
      "Epoch 4202/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 5.4331e-05 - val_loss: 2.0127e-04\n",
      "Epoch 4203/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 2.2239e-04 - val_loss: 5.4363e-05\n",
      "Epoch 4204/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.9817e-04 - val_loss: 2.5968e-04\n",
      "Epoch 4205/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6459e-04 - val_loss: 2.6692e-04\n",
      "Epoch 4206/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.0861e-04 - val_loss: 2.2141e-05\n",
      "Epoch 4207/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.8369e-05 - val_loss: 1.5584e-05\n",
      "Epoch 4208/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.9409e-05 - val_loss: 8.5350e-05\n",
      "Epoch 4209/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.9955e-05 - val_loss: 1.0048e-04\n",
      "Epoch 4210/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3961e-04 - val_loss: 1.2362e-04\n",
      "Epoch 4211/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.6617e-04 - val_loss: 8.3294e-05\n",
      "Epoch 4212/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7490e-04 - val_loss: 9.1323e-05\n",
      "Epoch 4213/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 2.7359e-05 - val_loss: 1.5275e-05\n",
      "Epoch 4214/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 7.1729e-05 - val_loss: 2.9784e-04\n",
      "Epoch 4215/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.7749e-04 - val_loss: 2.2117e-04\n",
      "Epoch 4216/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.5587e-04 - val_loss: 9.7765e-06\n",
      "Epoch 4217/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.7194e-05 - val_loss: 6.8771e-05\n",
      "Epoch 4218/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2163e-04 - val_loss: 1.6269e-04\n",
      "Epoch 4219/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 9.5521e-05 - val_loss: 1.1110e-04\n",
      "Epoch 4220/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9.3244e-05 - val_loss: 1.3845e-04\n",
      "Epoch 4221/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.8659e-05 - val_loss: 4.9098e-05\n",
      "Epoch 4222/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 6.8454e-05 - val_loss: 2.5687e-05\n",
      "Epoch 4223/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 5.6620e-05 - val_loss: 4.3786e-05\n",
      "Epoch 4224/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.7723e-05 - val_loss: 1.1974e-04\n",
      "Epoch 4225/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3651e-04 - val_loss: 2.0128e-04\n",
      "Epoch 4226/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9792e-04 - val_loss: 8.7515e-05\n",
      "Epoch 4227/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.6854e-05 - val_loss: 2.1851e-05\n",
      "Epoch 4228/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.2214e-05 - val_loss: 6.0770e-05\n",
      "Epoch 4229/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0389e-04 - val_loss: 2.3697e-04\n",
      "Epoch 4230/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0711e-04 - val_loss: 2.7215e-05\n",
      "Epoch 4231/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9100e-04 - val_loss: 3.8225e-05\n",
      "Epoch 4232/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7272e-04 - val_loss: 3.7583e-04\n",
      "Epoch 4233/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3185e-04 - val_loss: 7.9541e-05\n",
      "Epoch 4234/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.8377e-05 - val_loss: 1.1566e-04\n",
      "Epoch 4235/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.0474e-04 - val_loss: 1.1789e-04\n",
      "Epoch 4236/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2016e-04 - val_loss: 3.7544e-05\n",
      "Epoch 4237/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6480e-04 - val_loss: 8.3545e-05\n",
      "Epoch 4238/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.5459e-05 - val_loss: 8.5957e-05\n",
      "Epoch 4239/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9078e-04 - val_loss: 9.7163e-05\n",
      "Epoch 4240/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.9329e-05 - val_loss: 2.2625e-04\n",
      "Epoch 4241/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4166e-04 - val_loss: 4.6379e-04\n",
      "Epoch 4242/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.0714e-04 - val_loss: 2.6147e-04\n",
      "Epoch 4243/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.0761e-04 - val_loss: 0.0013\n",
      "Epoch 4244/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.2091e-04 - val_loss: 3.3244e-04\n",
      "Epoch 4245/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2650e-04 - val_loss: 8.9982e-05\n",
      "Epoch 4246/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.7869e-04 - val_loss: 4.4475e-04\n",
      "Epoch 4247/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.7731e-04 - val_loss: 2.4964e-04\n",
      "Epoch 4248/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.4756e-04 - val_loss: 4.9260e-04\n",
      "Epoch 4249/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9204e-04 - val_loss: 2.0128e-04\n",
      "Epoch 4250/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2788e-04 - val_loss: 6.2248e-05\n",
      "Epoch 4251/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.5386e-05 - val_loss: 1.9658e-04\n",
      "Epoch 4252/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.2907e-04 - val_loss: 4.1485e-05\n",
      "Epoch 4253/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.7872e-05 - val_loss: 8.2360e-05\n",
      "Epoch 4254/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.9151e-05 - val_loss: 4.1036e-05\n",
      "Epoch 4255/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 4.8719e-05 - val_loss: 4.7123e-05\n",
      "Epoch 4256/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.4471e-05 - val_loss: 1.2747e-04\n",
      "Epoch 4257/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4935e-04 - val_loss: 3.9065e-04\n",
      "Epoch 4258/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9521e-04 - val_loss: 1.7541e-04\n",
      "Epoch 4259/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.0944e-05 - val_loss: 2.2148e-04\n",
      "Epoch 4260/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.8152e-05 - val_loss: 5.7515e-05\n",
      "Epoch 4261/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.7956e-05 - val_loss: 1.6734e-04\n",
      "Epoch 4262/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2976e-04 - val_loss: 1.4981e-04\n",
      "Epoch 4263/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.6758e-05 - val_loss: 1.4072e-04\n",
      "Epoch 4264/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.8236e-05 - val_loss: 1.1503e-04\n",
      "Epoch 4265/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.7582e-05 - val_loss: 9.7387e-05\n",
      "Epoch 4266/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.9799e-05 - val_loss: 1.1809e-04\n",
      "Epoch 4267/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.2864e-04 - val_loss: 8.7787e-04\n",
      "Epoch 4268/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 5.8862e-04 - val_loss: 0.0015\n",
      "Epoch 4269/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 4270/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.0865e-04 - val_loss: 0.0011\n",
      "Epoch 4271/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.4659e-04 - val_loss: 0.0021\n",
      "Epoch 4272/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0012 - val_loss: 0.0027\n",
      "Epoch 4273/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.4442e-04 - val_loss: 0.0014\n",
      "Epoch 4274/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.3124e-04 - val_loss: 3.0404e-04\n",
      "Epoch 4275/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3171e-04 - val_loss: 7.4416e-05\n",
      "Epoch 4276/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.6137e-04 - val_loss: 3.8486e-04\n",
      "Epoch 4277/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.4057e-04 - val_loss: 9.1175e-05\n",
      "Epoch 4278/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7061e-04 - val_loss: 4.3162e-04\n",
      "Epoch 4279/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.5121e-04 - val_loss: 2.2208e-04\n",
      "Epoch 4280/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.4061e-05 - val_loss: 1.2285e-04\n",
      "Epoch 4281/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1019e-04 - val_loss: 1.8237e-04\n",
      "Epoch 4282/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9319e-04 - val_loss: 1.6276e-04\n",
      "Epoch 4283/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.4775e-05 - val_loss: 2.0376e-04\n",
      "Epoch 4284/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1903e-04 - val_loss: 2.8502e-04\n",
      "Epoch 4285/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2862e-04 - val_loss: 5.6864e-05\n",
      "Epoch 4286/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 7.3845e-05 - val_loss: 1.3835e-04\n",
      "Epoch 4287/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.9459e-05 - val_loss: 4.5385e-05\n",
      "Epoch 4288/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2427e-05 - val_loss: 8.8069e-05\n",
      "Epoch 4289/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.2080e-05 - val_loss: 1.5934e-04\n",
      "Epoch 4290/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.4396e-05 - val_loss: 4.3254e-05\n",
      "Epoch 4291/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4166e-04 - val_loss: 4.1624e-04\n",
      "Epoch 4292/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1606e-04 - val_loss: 3.3372e-04\n",
      "Epoch 4293/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.5553e-04 - val_loss: 8.0475e-05\n",
      "Epoch 4294/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.1151e-04 - val_loss: 1.1948e-04\n",
      "Epoch 4295/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8113e-04 - val_loss: 1.8623e-04\n",
      "Epoch 4296/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.5203e-04 - val_loss: 6.7867e-05\n",
      "Epoch 4297/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.9918e-05 - val_loss: 8.1639e-05\n",
      "Epoch 4298/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0086e-04 - val_loss: 6.8699e-05\n",
      "Epoch 4299/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.5760e-05 - val_loss: 2.6530e-05\n",
      "Epoch 4300/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.5898e-04 - val_loss: 4.2165e-04\n",
      "Epoch 4301/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.3034e-04 - val_loss: 3.2589e-05\n",
      "Epoch 4302/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1982e-04 - val_loss: 9.5566e-05\n",
      "Epoch 4303/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 9.3320e-0 - 0s 162us/sample - loss: 5.4760e-05 - val_loss: 4.0527e-05\n",
      "Epoch 4304/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9633e-05 - val_loss: 2.2032e-05\n",
      "Epoch 4305/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.7312e-05 - val_loss: 2.7519e-05\n",
      "Epoch 4306/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5391e-04 - val_loss: 3.3931e-04\n",
      "Epoch 4307/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.4066e-04 - val_loss: 3.5846e-05\n",
      "Epoch 4308/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 7.6076e-05 - val_loss: 6.2666e-05\n",
      "Epoch 4309/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2071e-04 - val_loss: 3.7814e-05\n",
      "Epoch 4310/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.4523e-05 - val_loss: 6.4853e-05\n",
      "Epoch 4311/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.9433e-05 - val_loss: 1.3720e-05\n",
      "Epoch 4312/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.9890e-05 - val_loss: 8.3240e-05\n",
      "Epoch 4313/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.3698e-04 - val_loss: 5.5705e-04\n",
      "Epoch 4314/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.0302e-04 - val_loss: 3.8516e-04\n",
      "Epoch 4315/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3.6814e-04 - val_loss: 0.0010\n",
      "Epoch 4316/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0012 - val_loss: 4.1446e-05\n",
      "Epoch 4317/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 4318/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 4319/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0010 - val_loss: 4.2233e-04\n",
      "Epoch 4320/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.9091e-04 - val_loss: 4.8298e-05\n",
      "Epoch 4321/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.9191e-04 - val_loss: 0.0022\n",
      "Epoch 4322/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0046 - val_loss: 0.0022\n",
      "Epoch 4323/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0058 - val_loss: 0.0159\n",
      "Epoch 4324/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0075 - val_loss: 6.2255e-04\n",
      "Epoch 4325/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0045 - val_loss: 0.0040\n",
      "Epoch 4326/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0023 - val_loss: 4.2467e-04\n",
      "Epoch 4327/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0022 - val_loss: 0.0034\n",
      "Epoch 4328/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 4329/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0033 - val_loss: 0.0092\n",
      "Epoch 4330/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0027 - val_loss: 2.8765e-04\n",
      "Epoch 4331/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.9191e-04 - val_loss: 0.0029\n",
      "Epoch 4332/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0013 - val_loss: 1.3819e-04\n",
      "Epoch 4333/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.9728e-04 - val_loss: 4.2466e-04\n",
      "Epoch 4334/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 4.8881e-04\n",
      "Epoch 4335/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 5.2344e-04 - val_loss: 0.0026\n",
      "Epoch 4336/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.8370e-04 - val_loss: 6.5971e-04\n",
      "Epoch 4337/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 2.7501e-04 - val_loss: 6.0207e-04\n",
      "Epoch 4338/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.6732e-04 - val_loss: 3.7401e-04\n",
      "Epoch 4339/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.2852e-04 - val_loss: 0.0012\n",
      "Epoch 4340/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 3.7171e-04\n",
      "Epoch 4341/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 4342/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 4343/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 4344/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0103 - val_loss: 0.0146\n",
      "Epoch 4345/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0165 - val_loss: 0.0198\n",
      "Epoch 4346/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0211 - val_loss: 0.0330\n",
      "Epoch 4347/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0193 - val_loss: 8.8996e-04\n",
      "Epoch 4348/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0123 - val_loss: 5.9342e-04\n",
      "Epoch 4349/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0054 - val_loss: 0.0121\n",
      "Epoch 4350/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0106 - val_loss: 0.0125\n",
      "Epoch 4351/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0210 - val_loss: 7.4076e-04\n",
      "Epoch 4352/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0152 - val_loss: 0.0337\n",
      "Epoch 4353/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0252 - val_loss: 0.0241\n",
      "Epoch 4354/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0292 - val_loss: 0.0016\n",
      "Epoch 4355/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0110 - val_loss: 0.0030\n",
      "Epoch 4356/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0020 - val_loss: 0.0056\n",
      "Epoch 4357/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0039 - val_loss: 9.6787e-04\n",
      "Epoch 4358/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0022 - val_loss: 5.8224e-04\n",
      "Epoch 4359/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0026 - val_loss: 0.0081\n",
      "Epoch 4360/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 4361/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.004 - 0s 132us/sample - loss: 0.0090 - val_loss: 0.0360\n",
      "Epoch 4362/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0099 - val_loss: 0.0055\n",
      "Epoch 4363/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0127 - val_loss: 0.0144\n",
      "Epoch 4364/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0089 - val_loss: 0.0015\n",
      "Epoch 4365/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0031 - val_loss: 0.0024\n",
      "Epoch 4366/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0045 - val_loss: 0.0099\n",
      "Epoch 4367/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0045 - val_loss: 0.0013\n",
      "Epoch 4368/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0043 - val_loss: 0.0018\n",
      "Epoch 4369/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0047 - val_loss: 0.0084\n",
      "Epoch 4370/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0040 - val_loss: 0.0115\n",
      "Epoch 4371/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0125 - val_loss: 0.0069\n",
      "Epoch 4372/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.0201 - val_loss: 0.0024\n",
      "Epoch 4373/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0121 - val_loss: 0.0086\n",
      "Epoch 4374/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0035 - val_loss: 0.0103\n",
      "Epoch 4375/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0060 - val_loss: 0.0026\n",
      "Epoch 4376/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 4377/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0034 - val_loss: 0.0013\n",
      "Epoch 4378/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 4379/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.0899e-04 - val_loss: 3.1275e-04\n",
      "Epoch 4380/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.5675e-04 - val_loss: 1.9465e-04\n",
      "Epoch 4381/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.3803e-04 - val_loss: 1.7018e-04\n",
      "Epoch 4382/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.3063e-04 - val_loss: 0.0027\n",
      "Epoch 4383/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 4384/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.6504e-04 - val_loss: 0.0018\n",
      "Epoch 4385/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 8.4034e-04 - val_loss: 6.7099e-04\n",
      "Epoch 4386/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0038 - val_loss: 7.8398e-04\n",
      "Epoch 4387/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0014 - val_loss: 0.0029\n",
      "Epoch 4388/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 4389/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.0014 - val_loss: 3.4192e-04\n",
      "Epoch 4390/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0011 - val_loss: 4.1796e-04\n",
      "Epoch 4391/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.6597e-04 - val_loss: 0.0013\n",
      "Epoch 4392/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0010 - val_loss: 0.0023\n",
      "Epoch 4393/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.0010 - val_loss: 9.2759e-04\n",
      "Epoch 4394/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.6556e-04 - val_loss: 9.6975e-04\n",
      "Epoch 4395/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.9139e-04 - val_loss: 5.7835e-04\n",
      "Epoch 4396/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.6733e-04 - val_loss: 1.9439e-04\n",
      "Epoch 4397/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 1.0355e-04 - val_loss: 3.7311e-05\n",
      "Epoch 4398/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.8438e-05 - val_loss: 2.6775e-05\n",
      "Epoch 4399/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.0692e-05 - val_loss: 2.5787e-05\n",
      "Epoch 4400/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.4318e-05 - val_loss: 4.5400e-05\n",
      "Epoch 4401/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.5107e-05 - val_loss: 3.1365e-05\n",
      "Epoch 4402/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 1.6519e-05 - val_loss: 1.3084e-04\n",
      "Epoch 4403/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.6107e-05 - val_loss: 4.4976e-05\n",
      "Epoch 4404/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1200e-04 - val_loss: 3.5014e-04\n",
      "Epoch 4405/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1807e-04 - val_loss: 2.6411e-04\n",
      "Epoch 4406/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 6.4315e-04 - val_loss: 0.0015\n",
      "Epoch 4407/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4930e-04 - val_loss: 0.0013\n",
      "Epoch 4408/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0010 - val_loss: 0.0042\n",
      "Epoch 4409/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 4410/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.3818e-04 - val_loss: 3.2098e-04\n",
      "Epoch 4411/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 1.3960e-04 - val_loss: 1.1698e-04\n",
      "Epoch 4412/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.6013e-05 - val_loss: 4.5056e-06\n",
      "Epoch 4413/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1083e-05 - val_loss: 4.4094e-04\n",
      "Epoch 4414/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.2652e-04 - val_loss: 1.2560e-04\n",
      "Epoch 4415/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.1847e-04 - val_loss: 4.6829e-04\n",
      "Epoch 4416/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.2046e-04 - val_loss: 3.6530e-04\n",
      "Epoch 4417/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.3384e-04 - val_loss: 0.0014\n",
      "Epoch 4418/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.5753e-04 - val_loss: 5.9567e-04\n",
      "Epoch 4419/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0015 - val_loss: 2.0924e-04\n",
      "Epoch 4420/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.2594e-04 - val_loss: 3.2301e-04\n",
      "Epoch 4421/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.7070e-04 - val_loss: 4.7729e-04\n",
      "Epoch 4422/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.4920e-04 - val_loss: 0.0012\n",
      "Epoch 4423/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 5.0363e-04 - val_loss: 0.0010\n",
      "Epoch 4424/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0010 - val_loss: 0.0091\n",
      "Epoch 4425/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0041 - val_loss: 0.0013\n",
      "Epoch 4426/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0038 - val_loss: 3.7231e-04\n",
      "Epoch 4427/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 0.0038 - val_loss: 0.0053\n",
      "Epoch 4428/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0076 - val_loss: 0.0090\n",
      "Epoch 4429/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0236 - val_loss: 0.0839\n",
      "Epoch 4430/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1468 - val_loss: 0.0762\n",
      "Epoch 4431/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2914 - val_loss: 0.3347\n",
      "Epoch 4432/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2293 - val_loss: 0.1841\n",
      "Epoch 4433/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.6385 - val_loss: 0.2641\n",
      "Epoch 4434/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.8181 - val_loss: 4.3569\n",
      "Epoch 4435/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9.0165 - val_loss: 22.7809\n",
      "Epoch 4436/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.2204 - val_loss: 3.5193\n",
      "Epoch 4437/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.0075 - val_loss: 8.6441\n",
      "Epoch 4438/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 10.3002 - val_loss: 16.6893\n",
      "Epoch 4439/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.5809 - val_loss: 1.5748\n",
      "Epoch 4440/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.6301 - val_loss: 6.8793\n",
      "Epoch 4441/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.6846 - val_loss: 21.3766\n",
      "Epoch 4442/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.6858 - val_loss: 8.1513\n",
      "Epoch 4443/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 33.1749 - val_loss: 10.5414\n",
      "Epoch 4444/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 159.7870 - val_loss: 411.3562\n",
      "Epoch 4445/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 360.6927 - val_loss: 38.7560\n",
      "Epoch 4446/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 226.6120 - val_loss: 323.9782\n",
      "Epoch 4447/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 55.0549 - val_loss: 55.0695\n",
      "Epoch 4448/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.5636 - val_loss: 1.9246\n",
      "Epoch 4449/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.3437 - val_loss: 20.5206\n",
      "Epoch 4450/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.7068 - val_loss: 4.1667\n",
      "Epoch 4451/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 72.8570 - val_loss: 388.8318\n",
      "Epoch 4452/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 241.6160 - val_loss: 471.4726\n",
      "Epoch 4453/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 304.8734 - val_loss: 171.1542\n",
      "Epoch 4454/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 63.8997 - val_loss: 4.4547\n",
      "Epoch 4455/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 75.2003 - val_loss: 422.2423\n",
      "Epoch 4456/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 454.0398 - val_loss: 1192.1839\n",
      "Epoch 4457/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 572.6184 - val_loss: 1222.3660\n",
      "Epoch 4458/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 558.3197 - val_loss: 530.2134\n",
      "Epoch 4459/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 785.5907 - val_loss: 6834.7256\n",
      "Epoch 4460/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 6771.2665 - val_loss: 2867.3443\n",
      "Epoch 4461/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5588.5091 - val_loss: 5635.2260\n",
      "Epoch 4462/10000\n",
      "68/68 [==============================] - 0s 103us/sample - loss: 2586.7747 - val_loss: 3383.7739\n",
      "Epoch 4463/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3970.4103 - val_loss: 8535.0714\n",
      "Epoch 4464/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11186.9338 - val_loss: 20741.7823\n",
      "Epoch 4465/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8662.4220 - val_loss: 849.9825\n",
      "Epoch 4466/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4923.0552 - val_loss: 288.8802\n",
      "Epoch 4467/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1954.0125 - val_loss: 2009.6002\n",
      "Epoch 4468/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 618.4972 - val_loss: 596.0320\n",
      "Epoch 4469/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 491.0524 - val_loss: 365.3325\n",
      "Epoch 4470/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 292.9214 - val_loss: 332.3533\n",
      "Epoch 4471/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 486.0454 - val_loss: 109.5586\n",
      "Epoch 4472/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 338.1220 - val_loss: 17.3091\n",
      "Epoch 4473/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 80.6853 - val_loss: 23.9806\n",
      "Epoch 4474/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 23.6827 - val_loss: 6.4676\n",
      "Epoch 4475/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 14.6843 - val_loss: 11.2144\n",
      "Epoch 4476/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 6.9345 - val_loss: 10.0116\n",
      "Epoch 4477/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.2367 - val_loss: 11.5038\n",
      "Epoch 4478/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.4746 - val_loss: 1.3988\n",
      "Epoch 4479/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.5493 - val_loss: 0.2412\n",
      "Epoch 4480/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.1909 - val_loss: 2.3803\n",
      "Epoch 4481/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.4063 - val_loss: 6.5306\n",
      "Epoch 4482/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.6625 - val_loss: 51.5710\n",
      "Epoch 4483/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 30.1832 - val_loss: 8.2492\n",
      "Epoch 4484/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.8227 - val_loss: 6.2477\n",
      "Epoch 4485/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.4814 - val_loss: 10.2948\n",
      "Epoch 4486/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3.9683 - val_loss: 1.9320\n",
      "Epoch 4487/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4617 - val_loss: 1.5633\n",
      "Epoch 4488/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5568 - val_loss: 0.1833\n",
      "Epoch 4489/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1759 - val_loss: 0.0740\n",
      "Epoch 4490/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1491 - val_loss: 0.1970\n",
      "Epoch 4491/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1152 - val_loss: 0.1140\n",
      "Epoch 4492/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0914 - val_loss: 0.0748\n",
      "Epoch 4493/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0698 - val_loss: 0.0520\n",
      "Epoch 4494/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1107 - val_loss: 0.1048\n",
      "Epoch 4495/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0974 - val_loss: 0.1142\n",
      "Epoch 4496/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1303 - val_loss: 0.1435\n",
      "Epoch 4497/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1308 - val_loss: 0.0747\n",
      "Epoch 4498/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0899 - val_loss: 0.0531\n",
      "Epoch 4499/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0743 - val_loss: 0.0503\n",
      "Epoch 4500/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1242 - val_loss: 0.0427\n",
      "Epoch 4501/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0813 - val_loss: 0.0331\n",
      "Epoch 4502/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0536 - val_loss: 0.0265\n",
      "Epoch 4503/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0372 - val_loss: 0.0255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4504/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0354 - val_loss: 0.0509\n",
      "Epoch 4505/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0582 - val_loss: 0.0278\n",
      "Epoch 4506/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0562 - val_loss: 0.1094\n",
      "Epoch 4507/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1419 - val_loss: 0.2166\n",
      "Epoch 4508/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3396 - val_loss: 0.0896\n",
      "Epoch 4509/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1610 - val_loss: 0.0613\n",
      "Epoch 4510/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1526 - val_loss: 0.1700\n",
      "Epoch 4511/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1669 - val_loss: 0.2888\n",
      "Epoch 4512/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1923 - val_loss: 0.2715\n",
      "Epoch 4513/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1837 - val_loss: 0.2985\n",
      "Epoch 4514/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1203 - val_loss: 0.1495\n",
      "Epoch 4515/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0882 - val_loss: 0.0658\n",
      "Epoch 4516/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0493 - val_loss: 0.0875\n",
      "Epoch 4517/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0807 - val_loss: 0.0500\n",
      "Epoch 4518/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0848 - val_loss: 0.0440\n",
      "Epoch 4519/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0771 - val_loss: 0.0367\n",
      "Epoch 4520/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0957 - val_loss: 0.0675\n",
      "Epoch 4521/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1879 - val_loss: 0.3194\n",
      "Epoch 4522/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2798 - val_loss: 0.0212\n",
      "Epoch 4523/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1442 - val_loss: 0.2752\n",
      "Epoch 4524/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1390 - val_loss: 0.1729\n",
      "Epoch 4525/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3005 - val_loss: 0.1242\n",
      "Epoch 4526/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1905 - val_loss: 0.1794\n",
      "Epoch 4527/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1479 - val_loss: 0.1661\n",
      "Epoch 4528/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0663 - val_loss: 0.1298\n",
      "Epoch 4529/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0837 - val_loss: 0.1224\n",
      "Epoch 4530/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0617 - val_loss: 0.0320\n",
      "Epoch 4531/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0310 - val_loss: 0.0278\n",
      "Epoch 4532/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0310 - val_loss: 0.0308\n",
      "Epoch 4533/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0404 - val_loss: 0.0561\n",
      "Epoch 4534/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0356 - val_loss: 0.0594\n",
      "Epoch 4535/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0283 - val_loss: 0.0268\n",
      "Epoch 4536/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0326 - val_loss: 0.0560\n",
      "Epoch 4537/10000\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 0.0495 - val_loss: 0.0701\n",
      "Epoch 4538/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0313 - val_loss: 0.0260\n",
      "Epoch 4539/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0261 - val_loss: 0.0308\n",
      "Epoch 4540/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0462 - val_loss: 0.0225\n",
      "Epoch 4541/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0331 - val_loss: 0.0620\n",
      "Epoch 4542/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0267 - val_loss: 0.0290\n",
      "Epoch 4543/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0693 - val_loss: 0.0406\n",
      "Epoch 4544/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1314 - val_loss: 0.1111\n",
      "Epoch 4545/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0584 - val_loss: 0.0767\n",
      "Epoch 4546/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0668 - val_loss: 0.1308\n",
      "Epoch 4547/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0711 - val_loss: 0.0327\n",
      "Epoch 4548/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0582 - val_loss: 0.0174\n",
      "Epoch 4549/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0364 - val_loss: 0.0675\n",
      "Epoch 4550/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.0361 - val_loss: 0.0539\n",
      "Epoch 4551/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0802 - val_loss: 0.1168\n",
      "Epoch 4552/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1746 - val_loss: 0.0549\n",
      "Epoch 4553/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1367 - val_loss: 0.3436\n",
      "Epoch 4554/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2473 - val_loss: 0.1218\n",
      "Epoch 4555/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1158 - val_loss: 0.2030\n",
      "Epoch 4556/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0675 - val_loss: 0.0654\n",
      "Epoch 4557/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0306 - val_loss: 0.0083\n",
      "Epoch 4558/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0235 - val_loss: 0.0144\n",
      "Epoch 4559/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 0.0202 - val_loss: 0.0092\n",
      "Epoch 4560/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0185 - val_loss: 0.0441\n",
      "Epoch 4561/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0270 - val_loss: 0.0090\n",
      "Epoch 4562/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0320 - val_loss: 0.0152\n",
      "Epoch 4563/10000\n",
      "68/68 [==============================] - 0s 779us/sample - loss: 0.0147 - val_loss: 0.0242\n",
      "Epoch 4564/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0148 - val_loss: 0.0071\n",
      "Epoch 4565/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0165 - val_loss: 0.0523\n",
      "Epoch 4566/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0264 - val_loss: 0.0111\n",
      "Epoch 4567/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0431 - val_loss: 0.0385\n",
      "Epoch 4568/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0600 - val_loss: 0.0192\n",
      "Epoch 4569/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0565 - val_loss: 0.0356\n",
      "Epoch 4570/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1177 - val_loss: 0.1027\n",
      "Epoch 4571/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0997 - val_loss: 0.0352\n",
      "Epoch 4572/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.0179 - val_loss: 0.0056\n",
      "Epoch 4573/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0098 - val_loss: 0.0085\n",
      "Epoch 4574/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0112 - val_loss: 0.0412\n",
      "Epoch 4575/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0206 - val_loss: 0.0090\n",
      "Epoch 4576/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0103 - val_loss: 0.0123\n",
      "Epoch 4577/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 0.0231 - val_loss: 0.0158\n",
      "Epoch 4578/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0107 - val_loss: 0.0079\n",
      "Epoch 4579/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0143 - val_loss: 0.0119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4580/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0308 - val_loss: 0.0601\n",
      "Epoch 4581/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0290 - val_loss: 0.0242\n",
      "Epoch 4582/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0453 - val_loss: 0.1874\n",
      "Epoch 4583/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0961 - val_loss: 0.0590\n",
      "Epoch 4584/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0570 - val_loss: 0.0143\n",
      "Epoch 4585/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0339 - val_loss: 0.0392\n",
      "Epoch 4586/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.0247 - val_loss: 0.0138\n",
      "Epoch 4587/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.0227 - val_loss: 0.0642\n",
      "Epoch 4588/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0330 - val_loss: 0.0163\n",
      "Epoch 4589/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0138 - val_loss: 0.0183\n",
      "Epoch 4590/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0166 - val_loss: 0.0428\n",
      "Epoch 4591/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0216 - val_loss: 0.0121\n",
      "Epoch 4592/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0269 - val_loss: 0.0457\n",
      "Epoch 4593/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0213 - val_loss: 0.0041\n",
      "Epoch 4594/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0079 - val_loss: 0.0208\n",
      "Epoch 4595/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0181 - val_loss: 0.0046\n",
      "Epoch 4596/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0122 - val_loss: 0.0059\n",
      "Epoch 4597/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.0093 - val_loss: 0.0238\n",
      "Epoch 4598/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0086 - val_loss: 0.0030\n",
      "Epoch 4599/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0040 - val_loss: 0.0024\n",
      "Epoch 4600/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0049 - val_loss: 0.0026\n",
      "Epoch 4601/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0112 - val_loss: 0.0671\n",
      "Epoch 4602/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.0473 - val_loss: 0.1013\n",
      "Epoch 4603/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0367 - val_loss: 0.0207\n",
      "Epoch 4604/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0280 - val_loss: 0.0283\n",
      "Epoch 4605/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 4606/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.0114 - val_loss: 0.0259\n",
      "Epoch 4607/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0134 - val_loss: 0.0066\n",
      "Epoch 4608/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0113 - val_loss: 0.0074\n",
      "Epoch 4609/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0191 - val_loss: 0.0304\n",
      "Epoch 4610/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0175 - val_loss: 0.0032\n",
      "Epoch 4611/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0118 - val_loss: 0.0066\n",
      "Epoch 4612/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0085 - val_loss: 0.0025\n",
      "Epoch 4613/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0159 - val_loss: 0.0149\n",
      "Epoch 4614/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0327 - val_loss: 0.0117\n",
      "Epoch 4615/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0196 - val_loss: 0.0203\n",
      "Epoch 4616/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0142 - val_loss: 0.0229\n",
      "Epoch 4617/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0313 - val_loss: 0.0087\n",
      "Epoch 4618/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1026 - val_loss: 0.0072\n",
      "Epoch 4619/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1981 - val_loss: 0.1827\n",
      "Epoch 4620/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2665 - val_loss: 0.0409\n",
      "Epoch 4621/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0469 - val_loss: 0.0355\n",
      "Epoch 4622/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0373 - val_loss: 0.0501\n",
      "Epoch 4623/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4169 - val_loss: 0.8912\n",
      "Epoch 4624/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3385 - val_loss: 0.0834\n",
      "Epoch 4625/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1919 - val_loss: 0.1712\n",
      "Epoch 4626/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2264 - val_loss: 0.2919\n",
      "Epoch 4627/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8100 - val_loss: 0.7083\n",
      "Epoch 4628/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8396 - val_loss: 0.9015\n",
      "Epoch 4629/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2774 - val_loss: 1.3702\n",
      "Epoch 4630/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.6619 - val_loss: 3.5416\n",
      "Epoch 4631/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.5220 - val_loss: 11.9553\n",
      "Epoch 4632/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 7.9529 - val_loss: 16.8178\n",
      "Epoch 4633/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.5957 - val_loss: 152.9769\n",
      "Epoch 4634/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 99.1691 - val_loss: 82.6025\n",
      "Epoch 4635/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 62.2567 - val_loss: 192.9999\n",
      "Epoch 4636/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 83.8711 - val_loss: 56.1014\n",
      "Epoch 4637/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 22.6589 - val_loss: 55.8830\n",
      "Epoch 4638/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 44.0089 - val_loss: 13.5994\n",
      "Epoch 4639/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 34.9664 - val_loss: 114.9040\n",
      "Epoch 4640/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 61.3345 - val_loss: 338.0866\n",
      "Epoch 4641/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 361.7811 - val_loss: 1146.7372\n",
      "Epoch 4642/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 489.3661 - val_loss: 454.0390\n",
      "Epoch 4643/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1462.7735 - val_loss: 8984.3885\n",
      "Epoch 4644/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3053.2049 - val_loss: 3571.9599\n",
      "Epoch 4645/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2361.5784 - val_loss: 1825.7321\n",
      "Epoch 4646/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1446.6715 - val_loss: 1431.6170\n",
      "Epoch 4647/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 482.7188 - val_loss: 109.9921\n",
      "Epoch 4648/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 366.7408 - val_loss: 276.6446\n",
      "Epoch 4649/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 326.1064 - val_loss: 87.1197\n",
      "Epoch 4650/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 57.4914 - val_loss: 16.6119\n",
      "Epoch 4651/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 20.2496 - val_loss: 39.5166\n",
      "Epoch 4652/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 48.6566 - val_loss: 56.4574\n",
      "Epoch 4653/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 131.5198 - val_loss: 18.2042\n",
      "Epoch 4654/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 445.1472 - val_loss: 213.0729\n",
      "Epoch 4655/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 221.4820 - val_loss: 564.7418\n",
      "Epoch 4656/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 510.5279 - val_loss: 1428.0877\n",
      "Epoch 4657/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 555.8827 - val_loss: 14.0159\n",
      "Epoch 4658/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 171.3562 - val_loss: 205.9818\n",
      "Epoch 4659/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 195.6645 - val_loss: 2.4629\n",
      "Epoch 4660/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 70.2042 - val_loss: 16.6140\n",
      "Epoch 4661/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 22.0792 - val_loss: 6.0860\n",
      "Epoch 4662/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.1726 - val_loss: 11.5055\n",
      "Epoch 4663/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.2212 - val_loss: 2.8541\n",
      "Epoch 4664/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.7937 - val_loss: 3.4125\n",
      "Epoch 4665/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.9439 - val_loss: 1.3840\n",
      "Epoch 4666/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.3660 - val_loss: 3.3927\n",
      "Epoch 4667/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.9370 - val_loss: 3.9773\n",
      "Epoch 4668/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 13.4646 - val_loss: 7.4315\n",
      "Epoch 4669/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.6050 - val_loss: 84.9521\n",
      "Epoch 4670/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 50.4438 - val_loss: 6.5777\n",
      "Epoch 4671/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.8516 - val_loss: 90.5871\n",
      "Epoch 4672/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 155.7824 - val_loss: 10.4677\n",
      "Epoch 4673/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 115.0795 - val_loss: 513.6931\n",
      "Epoch 4674/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 322.8357 - val_loss: 220.9695\n",
      "Epoch 4675/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 214.0995 - val_loss: 186.6278\n",
      "Epoch 4676/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 65.5285 - val_loss: 59.6481\n",
      "Epoch 4677/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 102.3354 - val_loss: 94.2329\n",
      "Epoch 4678/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 130.6075 - val_loss: 316.6758\n",
      "Epoch 4679/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 177.8059 - val_loss: 114.2957\n",
      "Epoch 4680/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 94.2930 - val_loss: 292.4301\n",
      "Epoch 4681/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 637.2909 - val_loss: 2926.0265\n",
      "Epoch 4682/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2125.3132 - val_loss: 2372.2353\n",
      "Epoch 4683/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1039.0851 - val_loss: 351.6273\n",
      "Epoch 4684/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 402.1762 - val_loss: 298.2862\n",
      "Epoch 4685/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 422.9085 - val_loss: 1093.5466\n",
      "Epoch 4686/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 831.8750 - val_loss: 115.7262\n",
      "Epoch 4687/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 124.3503 - val_loss: 151.6479\n",
      "Epoch 4688/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 132.9882 - val_loss: 406.1853\n",
      "Epoch 4689/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 270.7058 - val_loss: 175.8604\n",
      "Epoch 4690/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 227.4890 - val_loss: 298.9880\n",
      "Epoch 4691/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 112.4238 - val_loss: 111.1524\n",
      "Epoch 4692/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 146.3788 - val_loss: 211.2505\n",
      "Epoch 4693/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 291.2935 - val_loss: 743.7691\n",
      "Epoch 4694/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 486.7443 - val_loss: 527.7897\n",
      "Epoch 4695/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 312.0090 - val_loss: 300.0558\n",
      "Epoch 4696/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 373.5540 - val_loss: 327.9032\n",
      "Epoch 4697/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 394.9960 - val_loss: 330.2690\n",
      "Epoch 4698/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 107.9833 - val_loss: 54.1174\n",
      "Epoch 4699/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 64.4199 - val_loss: 95.4021\n",
      "Epoch 4700/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 84.7978 - val_loss: 18.8149\n",
      "Epoch 4701/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 32.3443 - val_loss: 29.1514\n",
      "Epoch 4702/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 48.2371 - val_loss: 30.4379\n",
      "Epoch 4703/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 49.4822 - val_loss: 35.5164\n",
      "Epoch 4704/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 29.5332 - val_loss: 23.8435\n",
      "Epoch 4705/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 16.3595 - val_loss: 22.1476\n",
      "Epoch 4706/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.1134 - val_loss: 12.6898\n",
      "Epoch 4707/10000\n",
      "68/68 [==============================] - 0s 941us/sample - loss: 5.9845 - val_loss: 9.7958\n",
      "Epoch 4708/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 22.5046 - val_loss: 37.3490\n",
      "Epoch 4709/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.3580 - val_loss: 3.3812\n",
      "Epoch 4710/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.9304 - val_loss: 4.1432\n",
      "Epoch 4711/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.5115 - val_loss: 0.0433\n",
      "Epoch 4712/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4177 - val_loss: 1.7778\n",
      "Epoch 4713/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 2.2321 - val_loss: 0.9585\n",
      "Epoch 4714/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7424 - val_loss: 0.6217\n",
      "Epoch 4715/10000\n",
      "68/68 [==============================] - 0s 647us/sample - loss: 0.6166 - val_loss: 0.8668\n",
      "Epoch 4716/10000\n",
      "68/68 [==============================] - 0s 824us/sample - loss: 0.2443 - val_loss: 0.2839\n",
      "Epoch 4717/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0986 - val_loss: 0.0350\n",
      "Epoch 4718/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0408 - val_loss: 0.0402\n",
      "Epoch 4719/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0545 - val_loss: 0.0466\n",
      "Epoch 4720/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0260 - val_loss: 0.0265\n",
      "Epoch 4721/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0378 - val_loss: 0.0701\n",
      "Epoch 4722/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0360 - val_loss: 0.0286\n",
      "Epoch 4723/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.0288 - val_loss: 0.0122\n",
      "Epoch 4724/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 0.0147 - val_loss: 0.0249\n",
      "Epoch 4725/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0158 - val_loss: 0.0128\n",
      "Epoch 4726/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0151 - val_loss: 0.0154\n",
      "Epoch 4727/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0131 - val_loss: 0.0103\n",
      "Epoch 4728/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0099 - val_loss: 0.0210\n",
      "Epoch 4729/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0144 - val_loss: 0.0048\n",
      "Epoch 4730/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.0068 - val_loss: 0.0235\n",
      "Epoch 4731/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0074 - val_loss: 0.0047\n",
      "Epoch 4732/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.0059 - val_loss: 0.0081\n",
      "Epoch 4733/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0128 - val_loss: 0.0210\n",
      "Epoch 4734/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0127 - val_loss: 0.0103\n",
      "Epoch 4735/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0067 - val_loss: 0.0038\n",
      "Epoch 4736/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.0049 - val_loss: 0.0032\n",
      "Epoch 4737/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0052 - val_loss: 0.0046\n",
      "Epoch 4738/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0051 - val_loss: 0.0029\n",
      "Epoch 4739/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0084 - val_loss: 0.0029\n",
      "Epoch 4740/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0077 - val_loss: 0.0042\n",
      "Epoch 4741/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0089 - val_loss: 0.0123\n",
      "Epoch 4742/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0109 - val_loss: 0.0182\n",
      "Epoch 4743/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0206 - val_loss: 0.0258\n",
      "Epoch 4744/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0087 - val_loss: 0.0070\n",
      "Epoch 4745/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.0141 - val_loss: 0.0134\n",
      "Epoch 4746/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0349 - val_loss: 0.0727\n",
      "Epoch 4747/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0443 - val_loss: 0.0888\n",
      "Epoch 4748/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1471 - val_loss: 0.8198\n",
      "Epoch 4749/10000\n",
      "68/68 [==============================] - 0s 882us/sample - loss: 0.4765 - val_loss: 1.2415\n",
      "Epoch 4750/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7418 - val_loss: 0.1119\n",
      "Epoch 4751/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2786 - val_loss: 1.4564\n",
      "Epoch 4752/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2809 - val_loss: 0.7944\n",
      "Epoch 4753/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.6354 - val_loss: 2.4937\n",
      "Epoch 4754/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0064 - val_loss: 2.0298\n",
      "Epoch 4755/10000\n",
      "68/68 [==============================] - 0s 559us/sample - loss: 1.0374 - val_loss: 0.7654\n",
      "Epoch 4756/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.818 - 0s 118us/sample - loss: 1.2643 - val_loss: 0.6462\n",
      "Epoch 4757/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3.2336 - val_loss: 4.2026\n",
      "Epoch 4758/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.3095 - val_loss: 24.0213\n",
      "Epoch 4759/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 35.9850 - val_loss: 19.8821\n",
      "Epoch 4760/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 15.0385 - val_loss: 22.5601\n",
      "Epoch 4761/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 16.7383 - val_loss: 59.4349\n",
      "Epoch 4762/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 17.0002 - val_loss: 16.5364\n",
      "Epoch 4763/10000\n",
      "68/68 [==============================] - 0s 706us/sample - loss: 28.4406 - val_loss: 6.3482\n",
      "Epoch 4764/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 156.1315 - val_loss: 242.4331\n",
      "Epoch 4765/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 274.3611 - val_loss: 69.7212\n",
      "Epoch 4766/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 334.5977 - val_loss: 19.2673\n",
      "Epoch 4767/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 447.6997 - val_loss: 158.7912\n",
      "Epoch 4768/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 273.4043 - val_loss: 4.0751\n",
      "Epoch 4769/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 23.5257 - val_loss: 74.1034\n",
      "Epoch 4770/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 46.3655 - val_loss: 230.9237\n",
      "Epoch 4771/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 168.7971 - val_loss: 93.2817\n",
      "Epoch 4772/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 117.9446 - val_loss: 34.3769\n",
      "Epoch 4773/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 61.1154 - val_loss: 102.9243\n",
      "Epoch 4774/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 99.9832 - val_loss: 255.9905\n",
      "Epoch 4775/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 202.7244 - val_loss: 237.7383\n",
      "Epoch 4776/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 757.8180 - val_loss: 1203.6965\n",
      "Epoch 4777/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 818.2971 - val_loss: 890.8113\n",
      "Epoch 4778/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 400.6629 - val_loss: 215.1080\n",
      "Epoch 4779/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 251.0849 - val_loss: 104.8198\n",
      "Epoch 4780/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 89.1165 - val_loss: 247.2119\n",
      "Epoch 4781/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 155.8189 - val_loss: 62.0364\n",
      "Epoch 4782/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 161.2278 - val_loss: 77.5700\n",
      "Epoch 4783/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 121.0343 - val_loss: 200.3996\n",
      "Epoch 4784/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 79.9554 - val_loss: 5.9734\n",
      "Epoch 4785/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 26.9515 - val_loss: 35.2428\n",
      "Epoch 4786/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15.3220 - val_loss: 7.1538\n",
      "Epoch 4787/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 2.5023 - val_loss: 2.0358\n",
      "Epoch 4788/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5160 - val_loss: 0.3250\n",
      "Epoch 4789/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 1.1962 - val_loss: 2.3566\n",
      "Epoch 4790/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.5258 - val_loss: 0.3495\n",
      "Epoch 4791/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2125 - val_loss: 0.0218\n",
      "Epoch 4792/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0645 - val_loss: 0.1732\n",
      "Epoch 4793/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1502 - val_loss: 0.1525\n",
      "Epoch 4794/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2986 - val_loss: 0.0552\n",
      "Epoch 4795/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0796 - val_loss: 0.0213\n",
      "Epoch 4796/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0547 - val_loss: 0.1761\n",
      "Epoch 4797/10000\n",
      "68/68 [==============================] - 0s 544us/sample - loss: 0.0481 - val_loss: 0.0833\n",
      "Epoch 4798/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0327 - val_loss: 0.0092\n",
      "Epoch 4799/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0059 - val_loss: 0.0030\n",
      "Epoch 4800/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 4801/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0033 - val_loss: 0.0095\n",
      "Epoch 4802/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0165 - val_loss: 0.0031\n",
      "Epoch 4803/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.0254 - val_loss: 0.0981\n",
      "Epoch 4804/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0676 - val_loss: 0.1728\n",
      "Epoch 4805/10000\n",
      "68/68 [==============================] - 0s 750us/sample - loss: 0.0582 - val_loss: 0.0502\n",
      "Epoch 4806/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0917 - val_loss: 0.0690\n",
      "Epoch 4807/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0571 - val_loss: 0.1484\n",
      "Epoch 4808/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.2300 - val_loss: 1.1706\n",
      "Epoch 4809/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3564 - val_loss: 0.4278\n",
      "Epoch 4810/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1357 - val_loss: 0.3057\n",
      "Epoch 4811/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1246 - val_loss: 0.5350\n",
      "Epoch 4812/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.2640 - val_loss: 0.1381\n",
      "Epoch 4813/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.1338 - val_loss: 0.1453\n",
      "Epoch 4814/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3192 - val_loss: 0.2389\n",
      "Epoch 4815/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5239 - val_loss: 1.4806\n",
      "Epoch 4816/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.720 - 0s 221us/sample - loss: 2.9225 - val_loss: 5.4118\n",
      "Epoch 4817/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.6232 - val_loss: 18.0357\n",
      "Epoch 4818/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 27.7766 - val_loss: 23.3349\n",
      "Epoch 4819/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.5418 - val_loss: 6.3527\n",
      "Epoch 4820/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.9224 - val_loss: 1.0702\n",
      "Epoch 4821/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 7.1415 - val_loss: 43.4344\n",
      "Epoch 4822/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 40.7794 - val_loss: 480.1997\n",
      "Epoch 4823/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 552.2787 - val_loss: 230.3424\n",
      "Epoch 4824/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 545.6913 - val_loss: 117.2706\n",
      "Epoch 4825/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 387.5973 - val_loss: 33.9623\n",
      "Epoch 4826/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 97.4632 - val_loss: 58.5514\n",
      "Epoch 4827/10000\n",
      "68/68 [==============================] - 0s 765us/sample - loss: 70.2362 - val_loss: 25.9161\n",
      "Epoch 4828/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 13.3386 - val_loss: 0.7311\n",
      "Epoch 4829/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 7.2275 - val_loss: 11.0030\n",
      "Epoch 4830/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.4580 - val_loss: 1.5030\n",
      "Epoch 4831/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 4.0663 - val_loss: 4.8816\n",
      "Epoch 4832/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1715 - val_loss: 12.0258\n",
      "Epoch 4833/10000\n",
      "68/68 [==============================] - 0s 853us/sample - loss: 3.8186 - val_loss: 4.7979\n",
      "Epoch 4834/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.4852 - val_loss: 1.2487\n",
      "Epoch 4835/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 21.2931 - val_loss: 1.0306\n",
      "Epoch 4836/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 7.2934 - val_loss: 4.5035\n",
      "Epoch 4837/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 3.9101 - val_loss: 8.4730\n",
      "Epoch 4838/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 13.8747 - val_loss: 41.4137\n",
      "Epoch 4839/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 62.1402 - val_loss: 109.6374\n",
      "Epoch 4840/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 31.2369 - val_loss: 35.4329\n",
      "Epoch 4841/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 17.7278 - val_loss: 9.4945\n",
      "Epoch 4842/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 5.9330 - val_loss: 9.6956\n",
      "Epoch 4843/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 17.8924 - val_loss: 31.1753\n",
      "Epoch 4844/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 37.4011 - val_loss: 4.3445\n",
      "Epoch 4845/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 35.4127 - val_loss: 11.7218\n",
      "Epoch 4846/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 25.5205 - val_loss: 10.5788\n",
      "Epoch 4847/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 71.7991 - val_loss: 128.2517\n",
      "Epoch 4848/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 214.2853 - val_loss: 63.8444\n",
      "Epoch 4849/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 210.3997 - val_loss: 65.9614\n",
      "Epoch 4850/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 30.6426 - val_loss: 73.9200\n",
      "Epoch 4851/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 101.4702 - val_loss: 68.9556\n",
      "Epoch 4852/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 74.4145 - val_loss: 265.9893\n",
      "Epoch 4853/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 772.5550 - val_loss: 1135.1873\n",
      "Epoch 4854/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 993.0587 - val_loss: 178.6020\n",
      "Epoch 4855/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 762.0493 - val_loss: 601.8436\n",
      "Epoch 4856/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 390.5931 - val_loss: 587.1850\n",
      "Epoch 4857/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 167.1842 - val_loss: 496.8270\n",
      "Epoch 4858/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 273.0384 - val_loss: 564.5272\n",
      "Epoch 4859/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 214.4318 - val_loss: 167.2316\n",
      "Epoch 4860/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 314.1017 - val_loss: 1222.0072\n",
      "Epoch 4861/10000\n",
      "68/68 [==============================] - 0s 897us/sample - loss: 395.1173 - val_loss: 186.7923\n",
      "Epoch 4862/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 64.3245 - val_loss: 93.6269\n",
      "Epoch 4863/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 202.8248 - val_loss: 890.3707\n",
      "Epoch 4864/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 905.8760 - val_loss: 543.4135\n",
      "Epoch 4865/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 763.6503 - val_loss: 152.8700\n",
      "Epoch 4866/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1386.9401 - val_loss: 1375.9032\n",
      "Epoch 4867/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 601.5650 - val_loss: 346.8709\n",
      "Epoch 4868/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 247.6840 - val_loss: 313.0194\n",
      "Epoch 4869/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 487.2731 - val_loss: 329.4189\n",
      "Epoch 4870/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 104.8104 - val_loss: 91.1509\n",
      "Epoch 4871/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 40.0936 - val_loss: 3.9802\n",
      "Epoch 4872/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 29.9959 - val_loss: 27.4129\n",
      "Epoch 4873/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 37.6382 - val_loss: 20.6526\n",
      "Epoch 4874/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 8.3755 - val_loss: 9.1808\n",
      "Epoch 4875/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 5.1819 - val_loss: 10.0497\n",
      "Epoch 4876/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.1669 - val_loss: 3.1565\n",
      "Epoch 4877/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.2064 - val_loss: 0.4385\n",
      "Epoch 4878/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 191us/sample - loss: 0.7284 - val_loss: 0.8689\n",
      "Epoch 4879/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 6.5114 - val_loss: 32.7730\n",
      "Epoch 4880/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 12.5857 - val_loss: 22.3435\n",
      "Epoch 4881/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 20.1925 - val_loss: 40.8282\n",
      "Epoch 4882/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 27.8344 - val_loss: 65.9727\n",
      "Epoch 4883/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 99.8946 - val_loss: 10.6927\n",
      "Epoch 4884/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 119.0406 - val_loss: 300.7854\n",
      "Epoch 4885/10000\n",
      "68/68 [==============================] - 0s 721us/sample - loss: 275.2721 - val_loss: 253.5415\n",
      "Epoch 4886/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 134.6697 - val_loss: 212.3212\n",
      "Epoch 4887/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 66.5244 - val_loss: 174.7031\n",
      "Epoch 4888/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 92.1851 - val_loss: 187.9946\n",
      "Epoch 4889/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 117.8811 - val_loss: 69.3466\n",
      "Epoch 4890/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 71.5740 - val_loss: 67.9858\n",
      "Epoch 4891/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 22.0005 - val_loss: 3.7073\n",
      "Epoch 4892/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 6.8532 - val_loss: 3.5745\n",
      "Epoch 4893/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.1770 - val_loss: 15.0673\n",
      "Epoch 4894/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 16.2720 - val_loss: 78.7727\n",
      "Epoch 4895/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 28.8840 - val_loss: 5.2485\n",
      "Epoch 4896/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 21.9531 - val_loss: 38.9391\n",
      "Epoch 4897/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 21.9160 - val_loss: 10.5103\n",
      "Epoch 4898/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 89.3718 - val_loss: 19.1241\n",
      "Epoch 4899/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 48.8499 - val_loss: 22.3896\n",
      "Epoch 4900/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 36.9196 - val_loss: 80.2644\n",
      "Epoch 4901/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 96.0298 - val_loss: 6.7992\n",
      "Epoch 4902/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 36.1161 - val_loss: 5.2534\n",
      "Epoch 4903/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 51.8063 - val_loss: 184.8766\n",
      "Epoch 4904/10000\n",
      "68/68 [==============================] - 0s 794us/sample - loss: 121.3831 - val_loss: 28.6227\n",
      "Epoch 4905/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 42.3005 - val_loss: 103.2188\n",
      "Epoch 4906/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 64.9667 - val_loss: 1.7249\n",
      "Epoch 4907/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 51.2748 - val_loss: 88.9463\n",
      "Epoch 4908/10000\n",
      "68/68 [==============================] - 0s 824us/sample - loss: 71.2727 - val_loss: 6.4741\n",
      "Epoch 4909/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 21.0423 - val_loss: 39.2750\n",
      "Epoch 4910/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 113.1836 - val_loss: 290.2128\n",
      "Epoch 4911/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 319.7678 - val_loss: 1210.2469\n",
      "Epoch 4912/10000\n",
      "68/68 [==============================] - 0s 765us/sample - loss: 560.5825 - val_loss: 957.5255\n",
      "Epoch 4913/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 518.7011 - val_loss: 643.9678\n",
      "Epoch 4914/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 231.2486 - val_loss: 158.4190\n",
      "Epoch 4915/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 110.8353 - val_loss: 88.3589\n",
      "Epoch 4916/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 63.6632 - val_loss: 1.7004\n",
      "Epoch 4917/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 38.5544 - val_loss: 18.0799\n",
      "Epoch 4918/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 110.1970 - val_loss: 84.7155\n",
      "Epoch 4919/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 52.2908 - val_loss: 7.9884\n",
      "Epoch 4920/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 25.8994 - val_loss: 54.1121\n",
      "Epoch 4921/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 32.0755 - val_loss: 17.1031\n",
      "Epoch 4922/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 20.0601 - val_loss: 17.9159\n",
      "Epoch 4923/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 12.3084 - val_loss: 24.8665\n",
      "Epoch 4924/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 29.7397 - val_loss: 20.7266\n",
      "Epoch 4925/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 64.5978 - val_loss: 24.6347\n",
      "Epoch 4926/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 18.2348 - val_loss: 17.5323\n",
      "Epoch 4927/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.0953 - val_loss: 3.0653\n",
      "Epoch 4928/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5647 - val_loss: 2.3114\n",
      "Epoch 4929/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 1.3909 - val_loss: 6.0162\n",
      "Epoch 4930/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.1769 - val_loss: 1.2305\n",
      "Epoch 4931/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7611 - val_loss: 8.4660\n",
      "Epoch 4932/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.4447 - val_loss: 26.0853\n",
      "Epoch 4933/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 28.8434 - val_loss: 94.0766\n",
      "Epoch 4934/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 54.8252 - val_loss: 75.9017\n",
      "Epoch 4935/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 149.9151 - val_loss: 318.3256\n",
      "Epoch 4936/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 777.5576 - val_loss: 2149.1030\n",
      "Epoch 4937/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2202.9750 - val_loss: 6921.7565\n",
      "Epoch 4938/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 6129.3315 - val_loss: 9088.8992\n",
      "Epoch 4939/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6135.9479 - val_loss: 30907.7830\n",
      "Epoch 4940/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 8468.3895 - val_loss: 11912.4220\n",
      "Epoch 4941/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4695.9558 - val_loss: 3027.0367\n",
      "Epoch 4942/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 802.9687 - val_loss: 163.9511\n",
      "Epoch 4943/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 169.1722 - val_loss: 251.1364\n",
      "Epoch 4944/10000\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 154.5425 - val_loss: 315.9091\n",
      "Epoch 4945/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 180.8781 - val_loss: 359.1048\n",
      "Epoch 4946/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 124.5725 - val_loss: 100.3046\n",
      "Epoch 4947/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 57.4698 - val_loss: 1.8711\n",
      "Epoch 4948/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 63.4983 - val_loss: 20.3508\n",
      "Epoch 4949/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 119.5875 - val_loss: 27.2717\n",
      "Epoch 4950/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 109.0450 - val_loss: 198.0146\n",
      "Epoch 4951/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 173.7770 - val_loss: 35.5591\n",
      "Epoch 4952/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 50.3724 - val_loss: 20.0528\n",
      "Epoch 4953/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 59.9068 - val_loss: 4.2534\n",
      "Epoch 4954/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 37.6746 - val_loss: 4.7549\n",
      "Epoch 4955/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.9265 - val_loss: 2.7618\n",
      "Epoch 4956/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.2603 - val_loss: 3.3434\n",
      "Epoch 4957/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 1.9865 - val_loss: 1.9335\n",
      "Epoch 4958/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1.2516 - val_loss: 0.9890\n",
      "Epoch 4959/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4410 - val_loss: 0.4854\n",
      "Epoch 4960/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4396 - val_loss: 0.2898\n",
      "Epoch 4961/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1937 - val_loss: 0.0540\n",
      "Epoch 4962/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1347 - val_loss: 0.1148\n",
      "Epoch 4963/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0989 - val_loss: 0.1124\n",
      "Epoch 4964/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0630 - val_loss: 0.0654\n",
      "Epoch 4965/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0474 - val_loss: 0.0525\n",
      "Epoch 4966/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0556 - val_loss: 0.0365\n",
      "Epoch 4967/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0840 - val_loss: 0.0544\n",
      "Epoch 4968/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1215 - val_loss: 0.0320\n",
      "Epoch 4969/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0724 - val_loss: 0.0230\n",
      "Epoch 4970/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0617 - val_loss: 0.0326\n",
      "Epoch 4971/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0348 - val_loss: 0.0394\n",
      "Epoch 4972/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0339 - val_loss: 0.0299\n",
      "Epoch 4973/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0358 - val_loss: 0.0293\n",
      "Epoch 4974/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0297 - val_loss: 0.0230\n",
      "Epoch 4975/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0312 - val_loss: 0.0342\n",
      "Epoch 4976/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0351 - val_loss: 0.0254\n",
      "Epoch 4977/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0350 - val_loss: 0.0233\n",
      "Epoch 4978/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0332 - val_loss: 0.0425\n",
      "Epoch 4979/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0448 - val_loss: 0.0239\n",
      "Epoch 4980/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1594 - val_loss: 0.3575\n",
      "Epoch 4981/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1718 - val_loss: 0.0454\n",
      "Epoch 4982/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0514 - val_loss: 0.0953\n",
      "Epoch 4983/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0659 - val_loss: 0.0518\n",
      "Epoch 4984/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0414 - val_loss: 0.1022\n",
      "Epoch 4985/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0782 - val_loss: 0.0440\n",
      "Epoch 4986/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0475 - val_loss: 0.0884\n",
      "Epoch 4987/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0549 - val_loss: 0.0228\n",
      "Epoch 4988/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0331 - val_loss: 0.0166\n",
      "Epoch 4989/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0468 - val_loss: 0.0442\n",
      "Epoch 4990/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0369 - val_loss: 0.0283\n",
      "Epoch 4991/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0353 - val_loss: 0.0272\n",
      "Epoch 4992/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0235 - val_loss: 0.0273\n",
      "Epoch 4993/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0349 - val_loss: 0.0297\n",
      "Epoch 4994/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0514 - val_loss: 0.0510\n",
      "Epoch 4995/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0240 - val_loss: 0.0222\n",
      "Epoch 4996/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0989 - val_loss: 0.0188\n",
      "Epoch 4997/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1193 - val_loss: 0.1279\n",
      "Epoch 4998/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0530 - val_loss: 0.0190\n",
      "Epoch 4999/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0454 - val_loss: 0.1013\n",
      "Epoch 5000/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0828 - val_loss: 0.1452\n",
      "Epoch 5001/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1176 - val_loss: 0.0820\n",
      "Epoch 5002/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0634 - val_loss: 0.0590\n",
      "Epoch 5003/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0780 - val_loss: 0.0556\n",
      "Epoch 5004/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0971 - val_loss: 0.1540\n",
      "Epoch 5005/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1659 - val_loss: 0.4796\n",
      "Epoch 5006/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2665 - val_loss: 0.3359\n",
      "Epoch 5007/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1752 - val_loss: 0.3578\n",
      "Epoch 5008/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3925 - val_loss: 0.7705\n",
      "Epoch 5009/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5351 - val_loss: 0.0940\n",
      "Epoch 5010/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.5378 - val_loss: 3.3889\n",
      "Epoch 5011/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.0106 - val_loss: 0.4369\n",
      "Epoch 5012/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6726 - val_loss: 0.1255\n",
      "Epoch 5013/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3205 - val_loss: 0.5611\n",
      "Epoch 5014/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2574 - val_loss: 0.4873\n",
      "Epoch 5015/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.6343 - val_loss: 1.8246\n",
      "Epoch 5016/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3579 - val_loss: 0.8428\n",
      "Epoch 5017/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6144 - val_loss: 0.5646\n",
      "Epoch 5018/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9013 - val_loss: 1.1232\n",
      "Epoch 5019/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.5012 - val_loss: 0.4730\n",
      "Epoch 5020/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1284 - val_loss: 0.1542\n",
      "Epoch 5021/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0755 - val_loss: 0.0341\n",
      "Epoch 5022/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0247 - val_loss: 0.0087\n",
      "Epoch 5023/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0209 - val_loss: 0.0505\n",
      "Epoch 5024/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0622 - val_loss: 0.0426\n",
      "Epoch 5025/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0533 - val_loss: 0.0795\n",
      "Epoch 5026/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0490 - val_loss: 0.2607\n",
      "Epoch 5027/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2729 - val_loss: 0.4386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5028/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3143 - val_loss: 0.6599\n",
      "Epoch 5029/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4122 - val_loss: 0.5162\n",
      "Epoch 5030/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1415 - val_loss: 0.0734\n",
      "Epoch 5031/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0271 - val_loss: 0.0414\n",
      "Epoch 5032/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0326 - val_loss: 0.0161\n",
      "Epoch 5033/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0245 - val_loss: 0.0084\n",
      "Epoch 5034/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0162 - val_loss: 0.0162\n",
      "Epoch 5035/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0302 - val_loss: 0.1189\n",
      "Epoch 5036/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0873 - val_loss: 0.2605\n",
      "Epoch 5037/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2769 - val_loss: 0.3367\n",
      "Epoch 5038/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0934 - val_loss: 0.0322\n",
      "Epoch 5039/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0186 - val_loss: 0.0453\n",
      "Epoch 5040/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0208 - val_loss: 0.0119\n",
      "Epoch 5041/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0509 - val_loss: 0.0296\n",
      "Epoch 5042/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0462 - val_loss: 0.0635\n",
      "Epoch 5043/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0208 - val_loss: 0.0236\n",
      "Epoch 5044/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0275 - val_loss: 0.0305\n",
      "Epoch 5045/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0201 - val_loss: 0.0272\n",
      "Epoch 5046/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0145 - val_loss: 0.0044\n",
      "Epoch 5047/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0117 - val_loss: 0.0110\n",
      "Epoch 5048/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0114 - val_loss: 0.0065\n",
      "Epoch 5049/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0265 - val_loss: 0.0031\n",
      "Epoch 5050/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0224 - val_loss: 0.0586\n",
      "Epoch 5051/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0100 - val_loss: 0.0091\n",
      "Epoch 5052/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0303 - val_loss: 0.0088\n",
      "Epoch 5053/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0271 - val_loss: 0.0057\n",
      "Epoch 5054/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0322 - val_loss: 0.0467\n",
      "Epoch 5055/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1286 - val_loss: 0.1895\n",
      "Epoch 5056/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0895 - val_loss: 0.1350\n",
      "Epoch 5057/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0608 - val_loss: 0.1212\n",
      "Epoch 5058/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0323 - val_loss: 0.0071\n",
      "Epoch 5059/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0082 - val_loss: 0.0023\n",
      "Epoch 5060/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0109 - val_loss: 0.0236\n",
      "Epoch 5061/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0534 - val_loss: 0.0099\n",
      "Epoch 5062/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0545 - val_loss: 0.0093\n",
      "Epoch 5063/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0285 - val_loss: 0.0069\n",
      "Epoch 5064/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0076 - val_loss: 0.0068\n",
      "Epoch 5065/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0079 - val_loss: 0.0065\n",
      "Epoch 5066/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0262 - val_loss: 0.1100\n",
      "Epoch 5067/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1558 - val_loss: 0.4199\n",
      "Epoch 5068/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.483 - 0s 118us/sample - loss: 0.6097 - val_loss: 4.1012\n",
      "Epoch 5069/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8675 - val_loss: 10.0873\n",
      "Epoch 5070/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.8300 - val_loss: 6.7417\n",
      "Epoch 5071/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.5574 - val_loss: 8.9663\n",
      "Epoch 5072/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 19.6372 - val_loss: 24.9512\n",
      "Epoch 5073/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 23.9630 - val_loss: 3.2022\n",
      "Epoch 5074/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.3567 - val_loss: 43.8423\n",
      "Epoch 5075/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 32.9752 - val_loss: 111.9149\n",
      "Epoch 5076/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 64.4122 - val_loss: 68.7099\n",
      "Epoch 5077/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 34.1683 - val_loss: 67.7112\n",
      "Epoch 5078/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 56.5426 - val_loss: 16.1189\n",
      "Epoch 5079/10000\n",
      "68/68 [==============================] - 0s 853us/sample - loss: 16.8704 - val_loss: 25.3484\n",
      "Epoch 5080/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 26.2564 - val_loss: 20.8313\n",
      "Epoch 5081/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.5949 - val_loss: 75.9617\n",
      "Epoch 5082/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 25.6260 - val_loss: 2.7830\n",
      "Epoch 5083/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 9.1626 - val_loss: 7.1459\n",
      "Epoch 5084/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 3.8541 - val_loss: 17.5236\n",
      "Epoch 5085/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.4948 - val_loss: 4.8107\n",
      "Epoch 5086/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 42.4901 - val_loss: 7.1006\n",
      "Epoch 5087/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 197.9727 - val_loss: 835.8440\n",
      "Epoch 5088/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 790.3430 - val_loss: 1043.0620\n",
      "Epoch 5089/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1402.3669 - val_loss: 4836.7395\n",
      "Epoch 5090/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8940.3933 - val_loss: 8975.6384\n",
      "Epoch 5091/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 17240.7192 - val_loss: 34710.9577\n",
      "Epoch 5092/10000\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 12433.2428 - val_loss: 5624.8426\n",
      "Epoch 5093/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3699.3132 - val_loss: 2797.1514\n",
      "Epoch 5094/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1325.9077 - val_loss: 2289.7885\n",
      "Epoch 5095/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 941.0559 - val_loss: 763.6498\n",
      "Epoch 5096/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 714.1274 - val_loss: 308.8898\n",
      "Epoch 5097/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 156.3216 - val_loss: 225.8598\n",
      "Epoch 5098/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 366.6619 - val_loss: 469.4039\n",
      "Epoch 5099/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 210.7489 - val_loss: 86.5506\n",
      "Epoch 5100/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 136.7106 - val_loss: 151.7697\n",
      "Epoch 5101/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 103.5038 - val_loss: 126.4036\n",
      "Epoch 5102/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 28.2204 - val_loss: 84.0399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5103/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 37.6837 - val_loss: 10.5626\n",
      "Epoch 5104/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 40.7091 - val_loss: 15.0544\n",
      "Epoch 5105/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 14.8997 - val_loss: 40.4402\n",
      "Epoch 5106/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 26.8120 - val_loss: 42.0225\n",
      "Epoch 5107/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 13.8603 - val_loss: 5.1925\n",
      "Epoch 5108/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.7165 - val_loss: 4.8827\n",
      "Epoch 5109/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.0870 - val_loss: 5.4027\n",
      "Epoch 5110/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6256 - val_loss: 0.6584\n",
      "Epoch 5111/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.5308 - val_loss: 0.0833\n",
      "Epoch 5112/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.4220 - val_loss: 0.1203\n",
      "Epoch 5113/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.1674 - val_loss: 0.1802\n",
      "Epoch 5114/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1132 - val_loss: 0.2008\n",
      "Epoch 5115/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0962 - val_loss: 0.0799\n",
      "Epoch 5116/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0385 - val_loss: 0.0137\n",
      "Epoch 5117/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0148 - val_loss: 0.0103\n",
      "Epoch 5118/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.0157 - val_loss: 0.0210\n",
      "Epoch 5119/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0178 - val_loss: 0.0709\n",
      "Epoch 5120/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0419 - val_loss: 0.0560\n",
      "Epoch 5121/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0195 - val_loss: 0.0146\n",
      "Epoch 5122/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0204 - val_loss: 0.0176\n",
      "Epoch 5123/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0135 - val_loss: 0.0095\n",
      "Epoch 5124/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 0.0168 - val_loss: 0.0245\n",
      "Epoch 5125/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0205 - val_loss: 0.0094\n",
      "Epoch 5126/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0154 - val_loss: 0.0340\n",
      "Epoch 5127/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0308 - val_loss: 0.0117\n",
      "Epoch 5128/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 0.0199 - val_loss: 0.0187\n",
      "Epoch 5129/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0224 - val_loss: 0.0100\n",
      "Epoch 5130/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0130 - val_loss: 0.0439\n",
      "Epoch 5131/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0855 - val_loss: 0.2224\n",
      "Epoch 5132/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1283 - val_loss: 0.1204\n",
      "Epoch 5133/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.0761 - val_loss: 0.0173\n",
      "Epoch 5134/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0211 - val_loss: 0.0384\n",
      "Epoch 5135/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0269 - val_loss: 0.0450\n",
      "Epoch 5136/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0345 - val_loss: 0.1136\n",
      "Epoch 5137/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 0.0504 - val_loss: 0.0253\n",
      "Epoch 5138/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0158 - val_loss: 0.0153\n",
      "Epoch 5139/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0139 - val_loss: 0.0064\n",
      "Epoch 5140/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0099 - val_loss: 0.0063\n",
      "Epoch 5141/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0108 - val_loss: 0.0066\n",
      "Epoch 5142/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0141 - val_loss: 0.0184\n",
      "Epoch 5143/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0136 - val_loss: 0.0085\n",
      "Epoch 5144/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0203 - val_loss: 0.0171\n",
      "Epoch 5145/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0197 - val_loss: 0.0149\n",
      "Epoch 5146/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0209 - val_loss: 0.0111\n",
      "Epoch 5147/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0156 - val_loss: 0.0084\n",
      "Epoch 5148/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0082 - val_loss: 0.0067\n",
      "Epoch 5149/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0098 - val_loss: 0.0134\n",
      "Epoch 5150/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0115 - val_loss: 0.0064\n",
      "Epoch 5151/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0119 - val_loss: 0.0061\n",
      "Epoch 5152/10000\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 0.0187 - val_loss: 0.0417\n",
      "Epoch 5153/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0290 - val_loss: 0.0936\n",
      "Epoch 5154/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1399 - val_loss: 0.3011\n",
      "Epoch 5155/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1512 - val_loss: 0.0946\n",
      "Epoch 5156/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0514 - val_loss: 0.0259\n",
      "Epoch 5157/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0754 - val_loss: 0.0189\n",
      "Epoch 5158/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0696 - val_loss: 0.0721\n",
      "Epoch 5159/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0666 - val_loss: 0.1043\n",
      "Epoch 5160/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1281 - val_loss: 0.0944\n",
      "Epoch 5161/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3885 - val_loss: 0.1316\n",
      "Epoch 5162/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2393 - val_loss: 0.1806\n",
      "Epoch 5163/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0534 - val_loss: 0.0649\n",
      "Epoch 5164/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0611 - val_loss: 0.1762\n",
      "Epoch 5165/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0854 - val_loss: 0.0226\n",
      "Epoch 5166/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0480 - val_loss: 0.0248\n",
      "Epoch 5167/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0354 - val_loss: 0.0562\n",
      "Epoch 5168/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0302 - val_loss: 0.0209\n",
      "Epoch 5169/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0182 - val_loss: 0.0049\n",
      "Epoch 5170/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0237 - val_loss: 0.0082\n",
      "Epoch 5171/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0091 - val_loss: 0.0064\n",
      "Epoch 5172/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0076 - val_loss: 0.0100\n",
      "Epoch 5173/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0112 - val_loss: 0.0066\n",
      "Epoch 5174/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0140 - val_loss: 0.0043\n",
      "Epoch 5175/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0070 - val_loss: 0.0032\n",
      "Epoch 5176/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0052 - val_loss: 0.0067\n",
      "Epoch 5177/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0108 - val_loss: 0.0190\n",
      "Epoch 5178/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0143 - val_loss: 0.0069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5179/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0097 - val_loss: 0.0075\n",
      "Epoch 5180/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0093 - val_loss: 0.0148\n",
      "Epoch 5181/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0175 - val_loss: 0.0207\n",
      "Epoch 5182/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0182 - val_loss: 0.0087\n",
      "Epoch 5183/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0144 - val_loss: 0.0136\n",
      "Epoch 5184/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0147 - val_loss: 0.0152\n",
      "Epoch 5185/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0113 - val_loss: 0.0136\n",
      "Epoch 5186/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0118 - val_loss: 0.0146\n",
      "Epoch 5187/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0098 - val_loss: 0.0022\n",
      "Epoch 5188/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0114 - val_loss: 0.0208\n",
      "Epoch 5189/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0147 - val_loss: 0.0143\n",
      "Epoch 5190/10000\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 0.0092 - val_loss: 0.0024\n",
      "Epoch 5191/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0045 - val_loss: 0.0058\n",
      "Epoch 5192/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 5193/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 5194/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0036 - val_loss: 0.0020\n",
      "Epoch 5195/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0072 - val_loss: 0.0043\n",
      "Epoch 5196/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0044 - val_loss: 0.0137\n",
      "Epoch 5197/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0064 - val_loss: 0.0184\n",
      "Epoch 5198/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 0.0102 - val_loss: 0.0058\n",
      "Epoch 5199/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0219 - val_loss: 0.0720\n",
      "Epoch 5200/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0580 - val_loss: 0.0047\n",
      "Epoch 5201/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0245 - val_loss: 0.0239\n",
      "Epoch 5202/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0119 - val_loss: 0.0021\n",
      "Epoch 5203/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.003 - 0s 1ms/sample - loss: 0.0043 - val_loss: 0.0054\n",
      "Epoch 5204/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0080 - val_loss: 0.0048\n",
      "Epoch 5205/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0058 - val_loss: 0.0094\n",
      "Epoch 5206/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 5207/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0059 - val_loss: 0.0112\n",
      "Epoch 5208/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0081 - val_loss: 0.0046\n",
      "Epoch 5209/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0169 - val_loss: 0.0231\n",
      "Epoch 5210/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0129 - val_loss: 0.0133\n",
      "Epoch 5211/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0067 - val_loss: 0.0017\n",
      "Epoch 5212/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0185 - val_loss: 0.0476\n",
      "Epoch 5213/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0169 - val_loss: 0.0036\n",
      "Epoch 5214/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0155 - val_loss: 0.0205\n",
      "Epoch 5215/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0098 - val_loss: 0.0064\n",
      "Epoch 5216/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0107 - val_loss: 0.0230\n",
      "Epoch 5217/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0110 - val_loss: 0.0026\n",
      "Epoch 5218/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0098 - val_loss: 0.0960\n",
      "Epoch 5219/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1277 - val_loss: 0.0944\n",
      "Epoch 5220/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3513 - val_loss: 1.7439\n",
      "Epoch 5221/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8542 - val_loss: 0.6298\n",
      "Epoch 5222/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2544 - val_loss: 2.2577\n",
      "Epoch 5223/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.9022 - val_loss: 0.7067\n",
      "Epoch 5224/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1577 - val_loss: 1.4398\n",
      "Epoch 5225/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6201 - val_loss: 0.1474\n",
      "Epoch 5226/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5953 - val_loss: 2.1603\n",
      "Epoch 5227/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.9172 - val_loss: 0.3169\n",
      "Epoch 5228/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3141 - val_loss: 0.0082\n",
      "Epoch 5229/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1140 - val_loss: 0.1467\n",
      "Epoch 5230/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1021 - val_loss: 0.1325\n",
      "Epoch 5231/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0472 - val_loss: 0.0426\n",
      "Epoch 5232/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0443 - val_loss: 0.0485\n",
      "Epoch 5233/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1818 - val_loss: 0.0326\n",
      "Epoch 5234/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.3256 - val_loss: 1.6193\n",
      "Epoch 5235/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.6185 - val_loss: 0.4972\n",
      "Epoch 5236/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.5584 - val_loss: 1.1726\n",
      "Epoch 5237/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.9875 - val_loss: 2.0942\n",
      "Epoch 5238/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.9163 - val_loss: 0.0120\n",
      "Epoch 5239/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1565 - val_loss: 0.2735\n",
      "Epoch 5240/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2217 - val_loss: 0.2595\n",
      "Epoch 5241/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4254 - val_loss: 1.3933\n",
      "Epoch 5242/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2056 - val_loss: 2.0088\n",
      "Epoch 5243/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.5017 - val_loss: 7.7745\n",
      "Epoch 5244/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 10.8222 - val_loss: 20.3337\n",
      "Epoch 5245/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.5366 - val_loss: 2.1921\n",
      "Epoch 5246/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.2253 - val_loss: 2.4923\n",
      "Epoch 5247/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.8820 - val_loss: 0.1488\n",
      "Epoch 5248/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.7085 - val_loss: 2.1646\n",
      "Epoch 5249/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.1648 - val_loss: 1.5059\n",
      "Epoch 5250/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.9231 - val_loss: 0.4367\n",
      "Epoch 5251/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.7254 - val_loss: 3.3201\n",
      "Epoch 5252/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 14.7826 - val_loss: 67.5578\n",
      "Epoch 5253/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.8301 - val_loss: 37.3677\n",
      "Epoch 5254/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 191us/sample - loss: 30.5837 - val_loss: 38.4257\n",
      "Epoch 5255/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 67.9495 - val_loss: 42.0262\n",
      "Epoch 5256/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.7626 - val_loss: 24.1266\n",
      "Epoch 5257/10000\n",
      "68/68 [==============================] - 0s 721us/sample - loss: 10.2862 - val_loss: 18.5915\n",
      "Epoch 5258/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.3199 - val_loss: 2.4309\n",
      "Epoch 5259/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 9.0881 - val_loss: 2.8435\n",
      "Epoch 5260/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 16.2597 - val_loss: 2.8992\n",
      "Epoch 5261/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 10.5179 - val_loss: 7.1980\n",
      "Epoch 5262/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.5067 - val_loss: 2.4124\n",
      "Epoch 5263/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.5173 - val_loss: 2.2058\n",
      "Epoch 5264/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.1339 - val_loss: 7.9855\n",
      "Epoch 5265/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.6598 - val_loss: 1.1162\n",
      "Epoch 5266/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.5515 - val_loss: 3.5519\n",
      "Epoch 5267/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7418 - val_loss: 1.0189\n",
      "Epoch 5268/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.6674 - val_loss: 0.4441\n",
      "Epoch 5269/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4087 - val_loss: 0.0060\n",
      "Epoch 5270/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1072 - val_loss: 0.0653\n",
      "Epoch 5271/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1529 - val_loss: 0.2716\n",
      "Epoch 5272/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3436 - val_loss: 1.6562\n",
      "Epoch 5273/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0570 - val_loss: 1.2676\n",
      "Epoch 5274/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9982 - val_loss: 1.2937\n",
      "Epoch 5275/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.8603 - val_loss: 0.0043\n",
      "Epoch 5276/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7792 - val_loss: 0.9823\n",
      "Epoch 5277/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7299 - val_loss: 0.1159\n",
      "Epoch 5278/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6327 - val_loss: 1.1990\n",
      "Epoch 5279/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6745 - val_loss: 3.8138\n",
      "Epoch 5280/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.0254 - val_loss: 17.8368\n",
      "Epoch 5281/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 52.3960 - val_loss: 43.8792\n",
      "Epoch 5282/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 220.8097 - val_loss: 84.5523\n",
      "Epoch 5283/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 173.8442 - val_loss: 258.7167\n",
      "Epoch 5284/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 113.5761 - val_loss: 108.0688\n",
      "Epoch 5285/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 206.8763 - val_loss: 583.9292\n",
      "Epoch 5286/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 594.0154 - val_loss: 1283.1010\n",
      "Epoch 5287/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 469.6973 - val_loss: 826.7464\n",
      "Epoch 5288/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 828.660 - 0s 147us/sample - loss: 489.2583 - val_loss: 424.6745\n",
      "Epoch 5289/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 250.1208 - val_loss: 86.1401\n",
      "Epoch 5290/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 156.5722 - val_loss: 63.3087\n",
      "Epoch 5291/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 39.4195 - val_loss: 30.4573\n",
      "Epoch 5292/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 39.4829 - val_loss: 7.6492\n",
      "Epoch 5293/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.3497 - val_loss: 9.6511\n",
      "Epoch 5294/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 8.2436 - val_loss: 8.9544\n",
      "Epoch 5295/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.6735 - val_loss: 1.5472\n",
      "Epoch 5296/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1032 - val_loss: 8.0316\n",
      "Epoch 5297/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 5.769 - 0s 162us/sample - loss: 11.1116 - val_loss: 24.9953\n",
      "Epoch 5298/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.3079 - val_loss: 1.5442\n",
      "Epoch 5299/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.8384 - val_loss: 13.4819\n",
      "Epoch 5300/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.7292 - val_loss: 23.5984\n",
      "Epoch 5301/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 13.8354 - val_loss: 18.4143\n",
      "Epoch 5302/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 64.8968 - val_loss: 210.7699\n",
      "Epoch 5303/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 317.4132 - val_loss: 1180.8993\n",
      "Epoch 5304/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 287.0954 - val_loss: 426.1546\n",
      "Epoch 5305/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 347.5407 - val_loss: 124.1555\n",
      "Epoch 5306/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 188.9576 - val_loss: 22.2910\n",
      "Epoch 5307/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 77.8604 - val_loss: 71.3790\n",
      "Epoch 5308/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 48.6743 - val_loss: 84.4580\n",
      "Epoch 5309/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 285.8681 - val_loss: 1275.8697\n",
      "Epoch 5310/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 850.5723 - val_loss: 135.2276\n",
      "Epoch 5311/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 557.0010 - val_loss: 218.1131\n",
      "Epoch 5312/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 126.9623 - val_loss: 106.0739\n",
      "Epoch 5313/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 98.4268 - val_loss: 1.2462\n",
      "Epoch 5314/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 341.1320 - val_loss: 929.7211\n",
      "Epoch 5315/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 263.9698 - val_loss: 17.6790\n",
      "Epoch 5316/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.3022 - val_loss: 7.7424\n",
      "Epoch 5317/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 40.3938 - val_loss: 15.7141\n",
      "Epoch 5318/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 14.35 - 0s 191us/sample - loss: 15.1704 - val_loss: 19.1087\n",
      "Epoch 5319/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 40.3585 - val_loss: 8.1959\n",
      "Epoch 5320/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 43.4681 - val_loss: 172.1098\n",
      "Epoch 5321/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 106.5555 - val_loss: 696.7540\n",
      "Epoch 5322/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 220.4310 - val_loss: 605.1500\n",
      "Epoch 5323/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 321.6687 - val_loss: 1162.4642\n",
      "Epoch 5324/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1034.2136 - val_loss: 649.1990\n",
      "Epoch 5325/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 244.0301 - val_loss: 159.9231\n",
      "Epoch 5326/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 114.2852 - val_loss: 307.5605\n",
      "Epoch 5327/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 137.5094 - val_loss: 133.7465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5328/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 41.0771 - val_loss: 39.9065\n",
      "Epoch 5329/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 28.9974 - val_loss: 5.5274\n",
      "Epoch 5330/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 31.2701 - val_loss: 7.7774\n",
      "Epoch 5331/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 45.4467 - val_loss: 21.8188\n",
      "Epoch 5332/10000\n",
      "68/68 [==============================] - 0s 794us/sample - loss: 25.6831 - val_loss: 17.2906\n",
      "Epoch 5333/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 11.2215 - val_loss: 27.5591\n",
      "Epoch 5334/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 31.7354 - val_loss: 114.5838\n",
      "Epoch 5335/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 275.8864 - val_loss: 678.9176\n",
      "Epoch 5336/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 181.0902 - val_loss: 117.2652\n",
      "Epoch 5337/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 137.1791 - val_loss: 125.3689\n",
      "Epoch 5338/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 92.6262 - val_loss: 31.8702\n",
      "Epoch 5339/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 561.2862 - val_loss: 313.0541\n",
      "Epoch 5340/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 239.2799 - val_loss: 353.0147\n",
      "Epoch 5341/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 266.2853 - val_loss: 201.7075\n",
      "Epoch 5342/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 112.5253 - val_loss: 60.7748\n",
      "Epoch 5343/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 33.9589 - val_loss: 25.4610\n",
      "Epoch 5344/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 22.8878 - val_loss: 161.7830\n",
      "Epoch 5345/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 67.4555 - val_loss: 12.3328\n",
      "Epoch 5346/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 63.4504 - val_loss: 69.0883\n",
      "Epoch 5347/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 140.7510 - val_loss: 58.4900\n",
      "Epoch 5348/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 211.6455 - val_loss: 116.4161\n",
      "Epoch 5349/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 163.1744 - val_loss: 123.1888\n",
      "Epoch 5350/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 82.4833 - val_loss: 193.7381\n",
      "Epoch 5351/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 163.2622 - val_loss: 314.0736\n",
      "Epoch 5352/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 207.1732 - val_loss: 148.5021\n",
      "Epoch 5353/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 431.5783 - val_loss: 589.8402\n",
      "Epoch 5354/10000\n",
      "68/68 [==============================] - 0s 956us/sample - loss: 454.5976 - val_loss: 310.1781\n",
      "Epoch 5355/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 248.8076 - val_loss: 847.7354\n",
      "Epoch 5356/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 294.8496 - val_loss: 89.9131\n",
      "Epoch 5357/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 57.4481 - val_loss: 1.3684\n",
      "Epoch 5358/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 18.9670 - val_loss: 163.7758\n",
      "Epoch 5359/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 183.7003 - val_loss: 182.5652\n",
      "Epoch 5360/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 96.6928 - val_loss: 51.9802\n",
      "Epoch 5361/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 268.1925 - val_loss: 79.6273\n",
      "Epoch 5362/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 763.0908 - val_loss: 288.7058\n",
      "Epoch 5363/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 431.0670 - val_loss: 624.6130\n",
      "Epoch 5364/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 349.2357 - val_loss: 508.3519\n",
      "Epoch 5365/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 954.5242 - val_loss: 640.5161\n",
      "Epoch 5366/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 441.1298 - val_loss: 278.0319\n",
      "Epoch 5367/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 623.1035 - val_loss: 1390.1877\n",
      "Epoch 5368/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 379.0331 - val_loss: 88.6257\n",
      "Epoch 5369/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 225.1901 - val_loss: 501.3099\n",
      "Epoch 5370/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 952.2363 - val_loss: 269.8669\n",
      "Epoch 5371/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 718.1703 - val_loss: 375.5070\n",
      "Epoch 5372/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 224.9827 - val_loss: 285.4294\n",
      "Epoch 5373/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 88.5138 - val_loss: 25.5037\n",
      "Epoch 5374/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 14.0517 - val_loss: 10.8238\n",
      "Epoch 5375/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.0758 - val_loss: 8.0938\n",
      "Epoch 5376/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.2054 - val_loss: 6.7776\n",
      "Epoch 5377/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.8193 - val_loss: 7.7758\n",
      "Epoch 5378/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.9053 - val_loss: 6.9823\n",
      "Epoch 5379/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.0076 - val_loss: 5.0928\n",
      "Epoch 5380/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.5895 - val_loss: 12.5298\n",
      "Epoch 5381/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.7366 - val_loss: 104.4831\n",
      "Epoch 5382/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 24.0515 - val_loss: 6.9536\n",
      "Epoch 5383/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.6998 - val_loss: 3.6350\n",
      "Epoch 5384/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8901 - val_loss: 2.9195\n",
      "Epoch 5385/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.8133 - val_loss: 0.6219\n",
      "Epoch 5386/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.6507 - val_loss: 0.8800\n",
      "Epoch 5387/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5298 - val_loss: 0.2286\n",
      "Epoch 5388/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1260 - val_loss: 0.4237\n",
      "Epoch 5389/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1697 - val_loss: 0.1873\n",
      "Epoch 5390/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1303 - val_loss: 0.2639\n",
      "Epoch 5391/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.3309 - val_loss: 0.4785\n",
      "Epoch 5392/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.9113 - val_loss: 4.7158\n",
      "Epoch 5393/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.7201 - val_loss: 24.8562\n",
      "Epoch 5394/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 17.8429 - val_loss: 30.3454\n",
      "Epoch 5395/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 26.2547 - val_loss: 27.6816\n",
      "Epoch 5396/10000\n",
      "68/68 [==============================] - 0s 544us/sample - loss: 68.5567 - val_loss: 112.8998\n",
      "Epoch 5397/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 75.0241 - val_loss: 38.5906\n",
      "Epoch 5398/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 23.2166 - val_loss: 11.8010\n",
      "Epoch 5399/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.4732 - val_loss: 22.2965\n",
      "Epoch 5400/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 25.2552 - val_loss: 8.6302\n",
      "Epoch 5401/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 16.1751 - val_loss: 44.2977\n",
      "Epoch 5402/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 13.7876 - val_loss: 21.3638\n",
      "Epoch 5403/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.4238 - val_loss: 9.0521\n",
      "Epoch 5404/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.5916 - val_loss: 11.4056\n",
      "Epoch 5405/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.1446 - val_loss: 21.3992\n",
      "Epoch 5406/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 14.2021 - val_loss: 36.3936\n",
      "Epoch 5407/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 28.2687 - val_loss: 56.9430\n",
      "Epoch 5408/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 17.8357 - val_loss: 30.2376\n",
      "Epoch 5409/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 30.7284 - val_loss: 73.1316\n",
      "Epoch 5410/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 27.1822 - val_loss: 34.0606\n",
      "Epoch 5411/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 32.2042 - val_loss: 152.1262\n",
      "Epoch 5412/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 120.7646 - val_loss: 153.1538\n",
      "Epoch 5413/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 38.2485 - val_loss: 19.3949\n",
      "Epoch 5414/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 13.3797 - val_loss: 18.6944\n",
      "Epoch 5415/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 23.1287 - val_loss: 0.4587\n",
      "Epoch 5416/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.0539 - val_loss: 11.7377\n",
      "Epoch 5417/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.8507 - val_loss: 1.4188\n",
      "Epoch 5418/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 21.1302 - val_loss: 19.0257\n",
      "Epoch 5419/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 46.7936 - val_loss: 19.0407\n",
      "Epoch 5420/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 34.2512 - val_loss: 81.7317\n",
      "Epoch 5421/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 129.3164 - val_loss: 45.8704\n",
      "Epoch 5422/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 225.0630 - val_loss: 48.5596\n",
      "Epoch 5423/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 140.5750 - val_loss: 15.2791\n",
      "Epoch 5424/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 59.1522 - val_loss: 60.9532\n",
      "Epoch 5425/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 122.4683 - val_loss: 160.5911\n",
      "Epoch 5426/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 69.9798 - val_loss: 107.8167\n",
      "Epoch 5427/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 64.3018 - val_loss: 54.5392\n",
      "Epoch 5428/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 93.0563 - val_loss: 128.2614\n",
      "Epoch 5429/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 88.1310 - val_loss: 98.4386\n",
      "Epoch 5430/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 42.6364 - val_loss: 92.9588\n",
      "Epoch 5431/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 175.3492 - val_loss: 460.5477\n",
      "Epoch 5432/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 507.8778 - val_loss: 1422.3111\n",
      "Epoch 5433/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1099.7785 - val_loss: 2450.8704\n",
      "Epoch 5434/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 680.4781 - val_loss: 2626.6645\n",
      "Epoch 5435/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1360.4589 - val_loss: 1089.4556\n",
      "Epoch 5436/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1143.1812 - val_loss: 2897.5286\n",
      "Epoch 5437/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1192.2581 - val_loss: 666.0974\n",
      "Epoch 5438/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 332.1916 - val_loss: 1374.6549\n",
      "Epoch 5439/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1001.2627 - val_loss: 681.7851\n",
      "Epoch 5440/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 1708.2731 - val_loss: 1293.9638\n",
      "Epoch 5441/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2253.7834 - val_loss: 5422.3080\n",
      "Epoch 5442/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2498.7094 - val_loss: 720.5946\n",
      "Epoch 5443/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1974.2942 - val_loss: 2830.6785\n",
      "Epoch 5444/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2548.1434 - val_loss: 3579.2717\n",
      "Epoch 5445/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 2660.7738 - val_loss: 11653.6200\n",
      "Epoch 5446/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4010.7091 - val_loss: 2146.7672\n",
      "Epoch 5447/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2974.0129 - val_loss: 49.7522\n",
      "Epoch 5448/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1928.4992 - val_loss: 3602.8761\n",
      "Epoch 5449/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 1868.5533 - val_loss: 1752.8041\n",
      "Epoch 5450/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 800.6486 - val_loss: 1225.3501\n",
      "Epoch 5451/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 757.9548 - val_loss: 748.3709\n",
      "Epoch 5452/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1610.1171 - val_loss: 3242.9312\n",
      "Epoch 5453/10000\n",
      "68/68 [==============================] - 0s 838us/sample - loss: 1108.1713 - val_loss: 1087.2270\n",
      "Epoch 5454/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 481.3627 - val_loss: 727.0786\n",
      "Epoch 5455/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 239.3090 - val_loss: 626.1436\n",
      "Epoch 5456/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 290.7270 - val_loss: 843.2486\n",
      "Epoch 5457/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 555.1893 - val_loss: 1207.9883\n",
      "Epoch 5458/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1005.1717 - val_loss: 2047.4682\n",
      "Epoch 5459/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1993.3241 - val_loss: 4016.5723\n",
      "Epoch 5460/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1452.0254 - val_loss: 1798.1455\n",
      "Epoch 5461/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1563.5783 - val_loss: 2741.8848\n",
      "Epoch 5462/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 845.1584 - val_loss: 719.0178\n",
      "Epoch 5463/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 346.7307 - val_loss: 453.5119\n",
      "Epoch 5464/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 168.6556 - val_loss: 110.8063\n",
      "Epoch 5465/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 115.4261 - val_loss: 138.4514\n",
      "Epoch 5466/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 130.9039 - val_loss: 213.5381\n",
      "Epoch 5467/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 79.9478 - val_loss: 12.3339\n",
      "Epoch 5468/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 34.0847 - val_loss: 38.3355\n",
      "Epoch 5469/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16.2787 - val_loss: 8.1555\n",
      "Epoch 5470/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.0997 - val_loss: 3.5449\n",
      "Epoch 5471/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.7110 - val_loss: 0.3353\n",
      "Epoch 5472/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.5414 - val_loss: 0.6488\n",
      "Epoch 5473/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.8138 - val_loss: 2.5836\n",
      "Epoch 5474/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.5452 - val_loss: 7.4147\n",
      "Epoch 5475/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 5.5796 - val_loss: 15.0589\n",
      "Epoch 5476/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.2749 - val_loss: 3.6328\n",
      "Epoch 5477/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0620 - val_loss: 1.9102\n",
      "Epoch 5478/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.6157 - val_loss: 0.7569\n",
      "Epoch 5479/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.6549 - val_loss: 1.4012\n",
      "Epoch 5480/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.7597 - val_loss: 0.7410\n",
      "Epoch 5481/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7647 - val_loss: 0.9881\n",
      "Epoch 5482/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8799 - val_loss: 1.5600\n",
      "Epoch 5483/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5603 - val_loss: 0.9204\n",
      "Epoch 5484/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2749 - val_loss: 0.1262\n",
      "Epoch 5485/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1209 - val_loss: 0.0810\n",
      "Epoch 5486/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0999 - val_loss: 0.0229\n",
      "Epoch 5487/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0378 - val_loss: 0.0309\n",
      "Epoch 5488/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0219 - val_loss: 0.0093\n",
      "Epoch 5489/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0152 - val_loss: 0.0371\n",
      "Epoch 5490/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0372 - val_loss: 0.0997\n",
      "Epoch 5491/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0355 - val_loss: 0.0679\n",
      "Epoch 5492/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0299 - val_loss: 0.0334\n",
      "Epoch 5493/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0135 - val_loss: 0.0063\n",
      "Epoch 5494/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0133 - val_loss: 0.0106\n",
      "Epoch 5495/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0178 - val_loss: 0.0072\n",
      "Epoch 5496/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0073 - val_loss: 0.0045\n",
      "Epoch 5497/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0065 - val_loss: 0.0041\n",
      "Epoch 5498/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0058 - val_loss: 0.0111\n",
      "Epoch 5499/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0069 - val_loss: 0.0077\n",
      "Epoch 5500/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0059 - val_loss: 0.0105\n",
      "Epoch 5501/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0066 - val_loss: 0.0040\n",
      "Epoch 5502/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0094 - val_loss: 0.0208\n",
      "Epoch 5503/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0100 - val_loss: 0.0122\n",
      "Epoch 5504/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0104 - val_loss: 0.0532\n",
      "Epoch 5505/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0251 - val_loss: 0.0605\n",
      "Epoch 5506/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0379 - val_loss: 0.0219\n",
      "Epoch 5507/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0169 - val_loss: 0.0037\n",
      "Epoch 5508/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0242 - val_loss: 0.0296\n",
      "Epoch 5509/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0302 - val_loss: 0.0181\n",
      "Epoch 5510/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0102 - val_loss: 0.0132\n",
      "Epoch 5511/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0217 - val_loss: 0.1005\n",
      "Epoch 5512/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0418 - val_loss: 0.0064\n",
      "Epoch 5513/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0079 - val_loss: 0.0048\n",
      "Epoch 5514/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 5515/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0065 - val_loss: 0.0138\n",
      "Epoch 5516/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0095 - val_loss: 0.0054\n",
      "Epoch 5517/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0070 - val_loss: 0.0120\n",
      "Epoch 5518/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0120 - val_loss: 0.0023\n",
      "Epoch 5519/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0077 - val_loss: 0.0072\n",
      "Epoch 5520/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0054 - val_loss: 0.0041\n",
      "Epoch 5521/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0042 - val_loss: 0.0062\n",
      "Epoch 5522/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0042 - val_loss: 0.0021\n",
      "Epoch 5523/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0093 - val_loss: 0.0073\n",
      "Epoch 5524/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0062 - val_loss: 0.0083\n",
      "Epoch 5525/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0069 - val_loss: 0.0022\n",
      "Epoch 5526/10000\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 0.0038 - val_loss: 0.0020\n",
      "Epoch 5527/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0095 - val_loss: 0.0034\n",
      "Epoch 5528/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0076 - val_loss: 0.0386\n",
      "Epoch 5529/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0223 - val_loss: 0.0350\n",
      "Epoch 5530/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0524 - val_loss: 0.0219\n",
      "Epoch 5531/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.0787 - val_loss: 0.6272\n",
      "Epoch 5532/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3754 - val_loss: 0.0070\n",
      "Epoch 5533/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1306 - val_loss: 0.1773\n",
      "Epoch 5534/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0968 - val_loss: 0.0333\n",
      "Epoch 5535/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.0212 - val_loss: 0.0101\n",
      "Epoch 5536/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0097 - val_loss: 0.0025\n",
      "Epoch 5537/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0059 - val_loss: 0.0060\n",
      "Epoch 5538/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0059 - val_loss: 0.0101\n",
      "Epoch 5539/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0054 - val_loss: 0.0018\n",
      "Epoch 5540/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0030 - val_loss: 0.0034\n",
      "Epoch 5541/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 5542/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0082 - val_loss: 0.0097\n",
      "Epoch 5543/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0058 - val_loss: 0.0038\n",
      "Epoch 5544/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.0086 - val_loss: 0.0250\n",
      "Epoch 5545/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0135 - val_loss: 0.0362\n",
      "Epoch 5546/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0280 - val_loss: 0.0169\n",
      "Epoch 5547/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0467 - val_loss: 0.0130\n",
      "Epoch 5548/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0765 - val_loss: 0.0635\n",
      "Epoch 5549/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0487 - val_loss: 0.0629\n",
      "Epoch 5550/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0333 - val_loss: 0.0131\n",
      "Epoch 5551/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0277 - val_loss: 0.0193\n",
      "Epoch 5552/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0195 - val_loss: 0.0377\n",
      "Epoch 5553/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0132 - val_loss: 0.0079\n",
      "Epoch 5554/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0055 - val_loss: 0.0142\n",
      "Epoch 5555/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0154 - val_loss: 0.0433\n",
      "Epoch 5556/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0727 - val_loss: 0.0486\n",
      "Epoch 5557/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0336 - val_loss: 0.0236\n",
      "Epoch 5558/10000\n",
      "68/68 [==============================] - 0s 735us/sample - loss: 0.0342 - val_loss: 0.0385\n",
      "Epoch 5559/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0439 - val_loss: 0.0869\n",
      "Epoch 5560/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0360 - val_loss: 0.0343\n",
      "Epoch 5561/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0161 - val_loss: 0.0058\n",
      "Epoch 5562/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0211 - val_loss: 0.0227\n",
      "Epoch 5563/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0170 - val_loss: 0.0101\n",
      "Epoch 5564/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0076 - val_loss: 0.0020\n",
      "Epoch 5565/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0043 - val_loss: 0.0071\n",
      "Epoch 5566/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0050 - val_loss: 0.0023\n",
      "Epoch 5567/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 5568/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0025 - val_loss: 0.0046\n",
      "Epoch 5569/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0054 - val_loss: 0.0058\n",
      "Epoch 5570/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0052 - val_loss: 0.0041\n",
      "Epoch 5571/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0026 - val_loss: 0.0010\n",
      "Epoch 5572/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0043 - val_loss: 0.0041\n",
      "Epoch 5573/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0019 - val_loss: 0.0032\n",
      "Epoch 5574/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0023 - val_loss: 7.8122e-04\n",
      "Epoch 5575/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0016 - val_loss: 6.7855e-04\n",
      "Epoch 5576/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0030 - val_loss: 0.0021\n",
      "Epoch 5577/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0162 - val_loss: 0.2056\n",
      "Epoch 5578/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0957 - val_loss: 0.0321\n",
      "Epoch 5579/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1153 - val_loss: 0.1554\n",
      "Epoch 5580/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2257 - val_loss: 0.3238\n",
      "Epoch 5581/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3831 - val_loss: 0.3735\n",
      "Epoch 5582/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2525 - val_loss: 0.3822\n",
      "Epoch 5583/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3436 - val_loss: 0.3447\n",
      "Epoch 5584/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0161 - val_loss: 1.6978\n",
      "Epoch 5585/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6527 - val_loss: 0.1954\n",
      "Epoch 5586/10000\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 1.6757 - val_loss: 10.3909\n",
      "Epoch 5587/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.4035 - val_loss: 1.9723\n",
      "Epoch 5588/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7305 - val_loss: 2.0818\n",
      "Epoch 5589/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 5.3152 - val_loss: 0.2601\n",
      "Epoch 5590/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.7727 - val_loss: 1.3134\n",
      "Epoch 5591/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 6.8668 - val_loss: 1.1917\n",
      "Epoch 5592/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.0514 - val_loss: 9.7383\n",
      "Epoch 5593/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 3.4314 - val_loss: 9.1572\n",
      "Epoch 5594/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.5126 - val_loss: 7.8708\n",
      "Epoch 5595/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.6796 - val_loss: 15.5474\n",
      "Epoch 5596/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 29.1504 - val_loss: 14.2246\n",
      "Epoch 5597/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12.0935 - val_loss: 4.1908\n",
      "Epoch 5598/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 24.2721 - val_loss: 5.2397\n",
      "Epoch 5599/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 21.3510 - val_loss: 1.8905\n",
      "Epoch 5600/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.2811 - val_loss: 0.4984\n",
      "Epoch 5601/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.0645 - val_loss: 0.0587\n",
      "Epoch 5602/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 3.1000 - val_loss: 5.2782\n",
      "Epoch 5603/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.5679 - val_loss: 2.0614\n",
      "Epoch 5604/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0801 - val_loss: 2.9841\n",
      "Epoch 5605/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.0862 - val_loss: 6.2088\n",
      "Epoch 5606/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.3853 - val_loss: 1.5319\n",
      "Epoch 5607/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 1.3467 - val_loss: 0.9661\n",
      "Epoch 5608/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3230 - val_loss: 0.0597\n",
      "Epoch 5609/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4197 - val_loss: 1.0438\n",
      "Epoch 5610/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4302 - val_loss: 0.8299\n",
      "Epoch 5611/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 2.3888 - val_loss: 12.5345\n",
      "Epoch 5612/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.8717 - val_loss: 26.8880\n",
      "Epoch 5613/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.8511 - val_loss: 24.8190\n",
      "Epoch 5614/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 74.4783 - val_loss: 43.3879\n",
      "Epoch 5615/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 68.2207 - val_loss: 3.3244\n",
      "Epoch 5616/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 27.0029 - val_loss: 29.2016\n",
      "Epoch 5617/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 158.5593 - val_loss: 604.0085\n",
      "Epoch 5618/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 328.6628 - val_loss: 755.4701\n",
      "Epoch 5619/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 251.2602 - val_loss: 318.9597\n",
      "Epoch 5620/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 201.8939 - val_loss: 124.4244\n",
      "Epoch 5621/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 170.8336 - val_loss: 27.1460\n",
      "Epoch 5622/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 32.5539 - val_loss: 17.8871\n",
      "Epoch 5623/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 28.7158 - val_loss: 48.8203\n",
      "Epoch 5624/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 18.3443 - val_loss: 34.3073\n",
      "Epoch 5625/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 18.5383 - val_loss: 91.3600\n",
      "Epoch 5626/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 176us/sample - loss: 46.2761 - val_loss: 171.2638\n",
      "Epoch 5627/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 104.9640 - val_loss: 664.7011\n",
      "Epoch 5628/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 338.1970 - val_loss: 786.6229\n",
      "Epoch 5629/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 478.0608 - val_loss: 1468.0677\n",
      "Epoch 5630/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 362.0666 - val_loss: 145.6058\n",
      "Epoch 5631/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 55.3618 - val_loss: 215.6111\n",
      "Epoch 5632/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 149.9588 - val_loss: 149.3381\n",
      "Epoch 5633/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 59.5070 - val_loss: 86.1238\n",
      "Epoch 5634/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 87.6473 - val_loss: 102.7459\n",
      "Epoch 5635/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 626.9935 - val_loss: 103.0931\n",
      "Epoch 5636/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3499.1290 - val_loss: 4136.5655\n",
      "Epoch 5637/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 2843.7591 - val_loss: 6290.2361\n",
      "Epoch 5638/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 5638.4675 - val_loss: 368.6270\n",
      "Epoch 5639/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3425.5977 - val_loss: 1687.4437\n",
      "Epoch 5640/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1644.9569 - val_loss: 3386.6338\n",
      "Epoch 5641/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2771.9441 - val_loss: 1425.8138\n",
      "Epoch 5642/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 468.7974 - val_loss: 1698.2878\n",
      "Epoch 5643/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 897.5783 - val_loss: 313.4754\n",
      "Epoch 5644/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 154.2743 - val_loss: 47.9288\n",
      "Epoch 5645/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 78.4022 - val_loss: 67.5027\n",
      "Epoch 5646/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 85.2446 - val_loss: 65.7467\n",
      "Epoch 5647/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 79.9916 - val_loss: 270.0011\n",
      "Epoch 5648/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 166.6254 - val_loss: 495.2264\n",
      "Epoch 5649/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 339.2069 - val_loss: 282.7963\n",
      "Epoch 5650/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 266.8892 - val_loss: 472.5337\n",
      "Epoch 5651/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 311.6954 - val_loss: 216.3269\n",
      "Epoch 5652/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 264.7307 - val_loss: 378.7729\n",
      "Epoch 5653/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 200.7238 - val_loss: 100.6694\n",
      "Epoch 5654/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 101.9900 - val_loss: 46.3266\n",
      "Epoch 5655/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 66.1442 - val_loss: 68.7067\n",
      "Epoch 5656/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 79.2900 - val_loss: 66.1192\n",
      "Epoch 5657/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 26.7979 - val_loss: 1.0314\n",
      "Epoch 5658/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 5.4422 - val_loss: 8.5744\n",
      "Epoch 5659/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.4298 - val_loss: 1.9176\n",
      "Epoch 5660/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.0982 - val_loss: 9.5515\n",
      "Epoch 5661/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.4946 - val_loss: 2.2137\n",
      "Epoch 5662/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.5784 - val_loss: 1.1435\n",
      "Epoch 5663/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 2.7168 - val_loss: 1.3921\n",
      "Epoch 5664/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.6156 - val_loss: 0.4933\n",
      "Epoch 5665/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4194 - val_loss: 2.2877\n",
      "Epoch 5666/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.2028 - val_loss: 11.6983\n",
      "Epoch 5667/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 6.9550 - val_loss: 1.2513\n",
      "Epoch 5668/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 7.3330 - val_loss: 10.5323\n",
      "Epoch 5669/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 11.7134 - val_loss: 7.1182\n",
      "Epoch 5670/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.5351 - val_loss: 15.4687\n",
      "Epoch 5671/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 5.2135 - val_loss: 2.1611\n",
      "Epoch 5672/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.3301 - val_loss: 3.0686\n",
      "Epoch 5673/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.2975 - val_loss: 7.3474\n",
      "Epoch 5674/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.2894 - val_loss: 1.8242\n",
      "Epoch 5675/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.5859 - val_loss: 4.8570\n",
      "Epoch 5676/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.0223 - val_loss: 1.0640\n",
      "Epoch 5677/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.0757 - val_loss: 9.3691\n",
      "Epoch 5678/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.8141 - val_loss: 5.1558\n",
      "Epoch 5679/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 9.2382 - val_loss: 26.2010\n",
      "Epoch 5680/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 21.0755 - val_loss: 1.7199\n",
      "Epoch 5681/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 6.5904 - val_loss: 15.3878\n",
      "Epoch 5682/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 8.1424 - val_loss: 8.6098\n",
      "Epoch 5683/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16.7551 - val_loss: 28.5408\n",
      "Epoch 5684/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 126.9456 - val_loss: 309.4693\n",
      "Epoch 5685/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 673.1580 - val_loss: 1069.2955\n",
      "Epoch 5686/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 930.6230 - val_loss: 1219.1322\n",
      "Epoch 5687/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 864.5162 - val_loss: 339.5886\n",
      "Epoch 5688/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 959.5294 - val_loss: 1233.3883\n",
      "Epoch 5689/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1410.6215 - val_loss: 1319.9046\n",
      "Epoch 5690/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 999.5024 - val_loss: 631.0458\n",
      "Epoch 5691/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 655.8893 - val_loss: 1543.0472\n",
      "Epoch 5692/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 2318.4783 - val_loss: 3066.0851\n",
      "Epoch 5693/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2400.5915 - val_loss: 168.1768\n",
      "Epoch 5694/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1416.9680 - val_loss: 1957.1247\n",
      "Epoch 5695/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1414.9086 - val_loss: 2434.6424\n",
      "Epoch 5696/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 970.9011 - val_loss: 857.4855\n",
      "Epoch 5697/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 614.2709 - val_loss: 176.0930\n",
      "Epoch 5698/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 278.6910 - val_loss: 30.7998\n",
      "Epoch 5699/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 203.9873 - val_loss: 9.0079\n",
      "Epoch 5700/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 89.0638 - val_loss: 8.8275\n",
      "Epoch 5701/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 18.0714 - val_loss: 14.7482\n",
      "Epoch 5702/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 20.8898 - val_loss: 14.0629\n",
      "Epoch 5703/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15.3591 - val_loss: 48.3718\n",
      "Epoch 5704/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19.7748 - val_loss: 25.9346\n",
      "Epoch 5705/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.6463 - val_loss: 13.5868\n",
      "Epoch 5706/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.4160 - val_loss: 0.8381\n",
      "Epoch 5707/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.9497 - val_loss: 4.7472\n",
      "Epoch 5708/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.4554 - val_loss: 4.5876\n",
      "Epoch 5709/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.9786 - val_loss: 16.5758\n",
      "Epoch 5710/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 10.9842 - val_loss: 2.6879\n",
      "Epoch 5711/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.1656 - val_loss: 2.4139\n",
      "Epoch 5712/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.2907 - val_loss: 3.1527\n",
      "Epoch 5713/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 4.2875 - val_loss: 3.0040\n",
      "Epoch 5714/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.8321 - val_loss: 2.8827\n",
      "Epoch 5715/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 1.147 - 0s 147us/sample - loss: 2.5160 - val_loss: 3.7739\n",
      "Epoch 5716/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.0187 - val_loss: 0.3189\n",
      "Epoch 5717/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3290 - val_loss: 0.1955\n",
      "Epoch 5718/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1607 - val_loss: 0.1473\n",
      "Epoch 5719/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0968 - val_loss: 0.1308\n",
      "Epoch 5720/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1277 - val_loss: 0.0881\n",
      "Epoch 5721/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0978 - val_loss: 0.0600\n",
      "Epoch 5722/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0917 - val_loss: 0.1079\n",
      "Epoch 5723/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1036 - val_loss: 0.1011\n",
      "Epoch 5724/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1462 - val_loss: 0.3149\n",
      "Epoch 5725/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3253 - val_loss: 1.1261\n",
      "Epoch 5726/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6162 - val_loss: 0.1215\n",
      "Epoch 5727/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5560 - val_loss: 1.1050\n",
      "Epoch 5728/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5326 - val_loss: 1.0591\n",
      "Epoch 5729/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5570 - val_loss: 1.2844\n",
      "Epoch 5730/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5507 - val_loss: 0.2233\n",
      "Epoch 5731/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2562 - val_loss: 0.2362\n",
      "Epoch 5732/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4093 - val_loss: 0.1730\n",
      "Epoch 5733/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.6237 - val_loss: 1.1344\n",
      "Epoch 5734/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.7210 - val_loss: 3.4465\n",
      "Epoch 5735/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.4595 - val_loss: 19.2134\n",
      "Epoch 5736/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.8944 - val_loss: 14.3818\n",
      "Epoch 5737/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.4633 - val_loss: 0.6142\n",
      "Epoch 5738/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.0250 - val_loss: 13.4690\n",
      "Epoch 5739/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 4.0360 - val_loss: 0.8129\n",
      "Epoch 5740/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9936 - val_loss: 0.2783\n",
      "Epoch 5741/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5190 - val_loss: 0.4956\n",
      "Epoch 5742/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2688 - val_loss: 0.2373\n",
      "Epoch 5743/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2656 - val_loss: 0.1773\n",
      "Epoch 5744/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2551 - val_loss: 0.4654\n",
      "Epoch 5745/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4812 - val_loss: 0.0569\n",
      "Epoch 5746/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3296 - val_loss: 0.7566\n",
      "Epoch 5747/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3940 - val_loss: 0.1870\n",
      "Epoch 5748/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5106 - val_loss: 0.3071\n",
      "Epoch 5749/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3224 - val_loss: 0.2188\n",
      "Epoch 5750/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2090 - val_loss: 0.1906\n",
      "Epoch 5751/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2707 - val_loss: 0.2871\n",
      "Epoch 5752/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3144 - val_loss: 1.6624\n",
      "Epoch 5753/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8503 - val_loss: 0.2107\n",
      "Epoch 5754/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1162 - val_loss: 0.0420\n",
      "Epoch 5755/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1176 - val_loss: 0.2003\n",
      "Epoch 5756/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.154 - 0s 132us/sample - loss: 0.1154 - val_loss: 0.1171\n",
      "Epoch 5757/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0923 - val_loss: 0.0314\n",
      "Epoch 5758/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0521 - val_loss: 0.0383\n",
      "Epoch 5759/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0499 - val_loss: 0.0381\n",
      "Epoch 5760/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0818 - val_loss: 0.0636\n",
      "Epoch 5761/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0777 - val_loss: 0.0273\n",
      "Epoch 5762/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0357 - val_loss: 0.0257\n",
      "Epoch 5763/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0859 - val_loss: 0.0478\n",
      "Epoch 5764/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0526 - val_loss: 0.0286\n",
      "Epoch 5765/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0460 - val_loss: 0.1159\n",
      "Epoch 5766/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0781 - val_loss: 0.0547\n",
      "Epoch 5767/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0380 - val_loss: 0.1185\n",
      "Epoch 5768/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0983 - val_loss: 0.0276\n",
      "Epoch 5769/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1564 - val_loss: 0.1158\n",
      "Epoch 5770/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0911 - val_loss: 0.0444\n",
      "Epoch 5771/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0612 - val_loss: 0.0465\n",
      "Epoch 5772/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0510 - val_loss: 0.0988\n",
      "Epoch 5773/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0991 - val_loss: 0.0943\n",
      "Epoch 5774/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1020 - val_loss: 0.0972\n",
      "Epoch 5775/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0646 - val_loss: 0.0573\n",
      "Epoch 5776/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0739 - val_loss: 0.0896\n",
      "Epoch 5777/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1278 - val_loss: 0.0759\n",
      "Epoch 5778/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1316 - val_loss: 0.1546\n",
      "Epoch 5779/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1169 - val_loss: 0.0488\n",
      "Epoch 5780/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0426 - val_loss: 0.0389\n",
      "Epoch 5781/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0478 - val_loss: 0.0156\n",
      "Epoch 5782/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0626 - val_loss: 0.1154\n",
      "Epoch 5783/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2907 - val_loss: 0.4903\n",
      "Epoch 5784/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3405 - val_loss: 0.0599\n",
      "Epoch 5785/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1168 - val_loss: 0.0128\n",
      "Epoch 5786/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0386 - val_loss: 0.0197\n",
      "Epoch 5787/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0333 - val_loss: 0.0532\n",
      "Epoch 5788/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0310 - val_loss: 0.0129\n",
      "Epoch 5789/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0236 - val_loss: 0.0316\n",
      "Epoch 5790/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0348 - val_loss: 0.0931\n",
      "Epoch 5791/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0492 - val_loss: 0.1105\n",
      "Epoch 5792/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0748 - val_loss: 0.1642\n",
      "Epoch 5793/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1590 - val_loss: 0.3378\n",
      "Epoch 5794/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1348 - val_loss: 0.2808\n",
      "Epoch 5795/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1759 - val_loss: 0.0973\n",
      "Epoch 5796/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0530 - val_loss: 0.0234\n",
      "Epoch 5797/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0318 - val_loss: 0.0949\n",
      "Epoch 5798/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0568 - val_loss: 0.0365\n",
      "Epoch 5799/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1427 - val_loss: 0.0584\n",
      "Epoch 5800/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0402 - val_loss: 0.0441\n",
      "Epoch 5801/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0189 - val_loss: 0.0134\n",
      "Epoch 5802/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0260 - val_loss: 0.0207\n",
      "Epoch 5803/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1342 - val_loss: 0.0141\n",
      "Epoch 5804/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.2749 - val_loss: 0.8225\n",
      "Epoch 5805/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.9913 - val_loss: 5.1314\n",
      "Epoch 5806/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.5344 - val_loss: 33.2847\n",
      "Epoch 5807/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 24.6855 - val_loss: 200.1509\n",
      "Epoch 5808/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 93.0513 - val_loss: 76.8539\n",
      "Epoch 5809/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 22.6671 - val_loss: 0.1006\n",
      "Epoch 5810/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11.3941 - val_loss: 8.9979\n",
      "Epoch 5811/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.2780 - val_loss: 29.6663\n",
      "Epoch 5812/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 7.5166 - val_loss: 7.9535\n",
      "Epoch 5813/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.2807 - val_loss: 3.1088\n",
      "Epoch 5814/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6115 - val_loss: 0.0206\n",
      "Epoch 5815/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0918 - val_loss: 0.1240\n",
      "Epoch 5816/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4991 - val_loss: 0.5947\n",
      "Epoch 5817/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5016 - val_loss: 0.0082\n",
      "Epoch 5818/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2811 - val_loss: 0.5273\n",
      "Epoch 5819/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2492 - val_loss: 0.7154\n",
      "Epoch 5820/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.6420 - val_loss: 1.3868\n",
      "Epoch 5821/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7445 - val_loss: 4.2684\n",
      "Epoch 5822/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.4524 - val_loss: 1.6896\n",
      "Epoch 5823/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 2.3381 - val_loss: 0.3005\n",
      "Epoch 5824/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1511 - val_loss: 5.2274\n",
      "Epoch 5825/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.0755 - val_loss: 2.8992\n",
      "Epoch 5826/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7543 - val_loss: 0.7972\n",
      "Epoch 5827/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3632 - val_loss: 2.1414\n",
      "Epoch 5828/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8038 - val_loss: 1.9805\n",
      "Epoch 5829/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.3550 - val_loss: 0.4115\n",
      "Epoch 5830/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.9292 - val_loss: 0.9045\n",
      "Epoch 5831/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.7724 - val_loss: 0.3003\n",
      "Epoch 5832/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.9904 - val_loss: 0.4898\n",
      "Epoch 5833/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3781 - val_loss: 3.7297\n",
      "Epoch 5834/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.5067 - val_loss: 3.6918\n",
      "Epoch 5835/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9454 - val_loss: 10.6578\n",
      "Epoch 5836/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 23.3688 - val_loss: 24.6555\n",
      "Epoch 5837/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 19.3556 - val_loss: 9.8860\n",
      "Epoch 5838/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 43.4746 - val_loss: 11.7099\n",
      "Epoch 5839/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 130.7673 - val_loss: 123.2501\n",
      "Epoch 5840/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 131.1869 - val_loss: 20.6524\n",
      "Epoch 5841/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 109.0299 - val_loss: 331.5129\n",
      "Epoch 5842/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 173.7301 - val_loss: 321.5798\n",
      "Epoch 5843/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 486.6184 - val_loss: 118.4149\n",
      "Epoch 5844/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 696.3112 - val_loss: 1254.1196\n",
      "Epoch 5845/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1965.5184 - val_loss: 4257.9359\n",
      "Epoch 5846/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4785.1096 - val_loss: 16193.9073\n",
      "Epoch 5847/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9926.7038 - val_loss: 1740.5865\n",
      "Epoch 5848/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2770.2960 - val_loss: 2825.4168\n",
      "Epoch 5849/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1987.4362 - val_loss: 1879.5147\n",
      "Epoch 5850/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 176us/sample - loss: 1212.7222 - val_loss: 2787.3887\n",
      "Epoch 5851/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1354.7009 - val_loss: 1368.7726\n",
      "Epoch 5852/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 492.2711 - val_loss: 123.2234\n",
      "Epoch 5853/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 189.4834 - val_loss: 324.1602\n",
      "Epoch 5854/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 623.6385 - val_loss: 94.0279\n",
      "Epoch 5855/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 342.7064 - val_loss: 198.3531\n",
      "Epoch 5856/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 123.9913 - val_loss: 107.9382\n",
      "Epoch 5857/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 68.0509 - val_loss: 86.7198\n",
      "Epoch 5858/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 33.0951 - val_loss: 0.3738\n",
      "Epoch 5859/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.7114 - val_loss: 1.5784\n",
      "Epoch 5860/10000\n",
      "68/68 [==============================] - 0s 588us/sample - loss: 9.1052 - val_loss: 2.8689\n",
      "Epoch 5861/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.3137 - val_loss: 12.5216\n",
      "Epoch 5862/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.7702 - val_loss: 2.2705\n",
      "Epoch 5863/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8231 - val_loss: 0.0999\n",
      "Epoch 5864/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2923 - val_loss: 0.3038\n",
      "Epoch 5865/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1771 - val_loss: 0.1683\n",
      "Epoch 5866/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1204 - val_loss: 0.0682\n",
      "Epoch 5867/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0428 - val_loss: 0.0317\n",
      "Epoch 5868/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0245 - val_loss: 0.0248\n",
      "Epoch 5869/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0138 - val_loss: 0.0064\n",
      "Epoch 5870/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0100 - val_loss: 0.0087\n",
      "Epoch 5871/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0071 - val_loss: 0.0041\n",
      "Epoch 5872/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0055 - val_loss: 0.0146\n",
      "Epoch 5873/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0690 - val_loss: 0.1704\n",
      "Epoch 5874/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0766 - val_loss: 0.0200\n",
      "Epoch 5875/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0452 - val_loss: 0.0097\n",
      "Epoch 5876/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0053 - val_loss: 0.0042\n",
      "Epoch 5877/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0084 - val_loss: 0.0052\n",
      "Epoch 5878/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0100 - val_loss: 0.0550\n",
      "Epoch 5879/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0345 - val_loss: 0.0041\n",
      "Epoch 5880/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0188 - val_loss: 0.0207\n",
      "Epoch 5881/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0141 - val_loss: 0.0123\n",
      "Epoch 5882/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 5883/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0074 - val_loss: 0.0013\n",
      "Epoch 5884/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0035 - val_loss: 0.0083\n",
      "Epoch 5885/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0045 - val_loss: 0.0033\n",
      "Epoch 5886/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0047 - val_loss: 0.0017\n",
      "Epoch 5887/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 5888/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0035 - val_loss: 0.0021\n",
      "Epoch 5889/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0026 - val_loss: 0.0038\n",
      "Epoch 5890/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0021 - val_loss: 0.0010\n",
      "Epoch 5891/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0025 - val_loss: 0.0040\n",
      "Epoch 5892/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0037 - val_loss: 0.0022\n",
      "Epoch 5893/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 5894/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 5895/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0028 - val_loss: 0.0014\n",
      "Epoch 5896/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0030 - val_loss: 0.0065\n",
      "Epoch 5897/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0069 - val_loss: 0.0140\n",
      "Epoch 5898/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0161 - val_loss: 0.0138\n",
      "Epoch 5899/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0187 - val_loss: 0.0169\n",
      "Epoch 5900/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0080 - val_loss: 0.0034\n",
      "Epoch 5901/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 5902/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 5903/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0014 - val_loss: 8.9946e-04\n",
      "Epoch 5904/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0031 - val_loss: 0.0133\n",
      "Epoch 5905/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0136 - val_loss: 0.0150\n",
      "Epoch 5906/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0127 - val_loss: 0.0046\n",
      "Epoch 5907/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0135 - val_loss: 0.0260\n",
      "Epoch 5908/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0114 - val_loss: 0.0126\n",
      "Epoch 5909/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0252 - val_loss: 0.0211\n",
      "Epoch 5910/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 5911/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0031 - val_loss: 0.0027\n",
      "Epoch 5912/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 5913/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0036 - val_loss: 0.0094\n",
      "Epoch 5914/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0078 - val_loss: 0.0033\n",
      "Epoch 5915/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0022 - val_loss: 0.0010\n",
      "Epoch 5916/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0018 - val_loss: 0.0141\n",
      "Epoch 5917/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0127 - val_loss: 0.0058\n",
      "Epoch 5918/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0038 - val_loss: 0.0090\n",
      "Epoch 5919/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0071 - val_loss: 0.0190\n",
      "Epoch 5920/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0166 - val_loss: 0.0016\n",
      "Epoch 5921/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0093 - val_loss: 0.0017\n",
      "Epoch 5922/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0173 - val_loss: 0.0106\n",
      "Epoch 5923/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0153 - val_loss: 0.0022\n",
      "Epoch 5924/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0022 - val_loss: 5.5822e-04\n",
      "Epoch 5925/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0046 - val_loss: 0.0033\n",
      "Epoch 5926/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0066 - val_loss: 0.0019\n",
      "Epoch 5927/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 5928/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0018 - val_loss: 0.0033\n",
      "Epoch 5929/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0030 - val_loss: 0.0035\n",
      "Epoch 5930/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0027 - val_loss: 0.0042\n",
      "Epoch 5931/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0024 - val_loss: 0.0015\n",
      "Epoch 5932/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0012 - val_loss: 6.8274e-04\n",
      "Epoch 5933/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0015 - val_loss: 0.0047\n",
      "Epoch 5934/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0020 - val_loss: 0.0010\n",
      "Epoch 5935/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0012 - val_loss: 3.9550e-04\n",
      "Epoch 5936/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0013 - val_loss: 9.8003e-04\n",
      "Epoch 5937/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.0054 - val_loss: 0.0144\n",
      "Epoch 5938/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0113 - val_loss: 0.0516\n",
      "Epoch 5939/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0536 - val_loss: 0.0539\n",
      "Epoch 5940/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0331 - val_loss: 0.0072\n",
      "Epoch 5941/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0111 - val_loss: 0.0318\n",
      "Epoch 5942/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0858 - val_loss: 0.1889\n",
      "Epoch 5943/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0551 - val_loss: 0.0392\n",
      "Epoch 5944/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0387 - val_loss: 0.0400\n",
      "Epoch 5945/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0881 - val_loss: 0.0201\n",
      "Epoch 5946/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4283 - val_loss: 0.7034\n",
      "Epoch 5947/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3949 - val_loss: 1.6642\n",
      "Epoch 5948/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6765 - val_loss: 1.4907\n",
      "Epoch 5949/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.1320 - val_loss: 2.1461\n",
      "Epoch 5950/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.0301 - val_loss: 2.3024\n",
      "Epoch 5951/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8883 - val_loss: 0.0247\n",
      "Epoch 5952/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2572 - val_loss: 0.5382\n",
      "Epoch 5953/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.0611 - val_loss: 2.7466\n",
      "Epoch 5954/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.0899 - val_loss: 2.3315\n",
      "Epoch 5955/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.2728 - val_loss: 0.9346\n",
      "Epoch 5956/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 2.1400 - val_loss: 2.2939\n",
      "Epoch 5957/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1.7191 - val_loss: 3.2372\n",
      "Epoch 5958/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 3.498 - 0s 162us/sample - loss: 3.2754 - val_loss: 1.0171\n",
      "Epoch 5959/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1.4780 - val_loss: 2.6547\n",
      "Epoch 5960/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.6565 - val_loss: 0.0618\n",
      "Epoch 5961/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7641 - val_loss: 3.5939\n",
      "Epoch 5962/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.9511 - val_loss: 39.3106\n",
      "Epoch 5963/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 34.6729 - val_loss: 223.0872\n",
      "Epoch 5964/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 81.3050 - val_loss: 58.4648\n",
      "Epoch 5965/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 141.8097 - val_loss: 431.0286\n",
      "Epoch 5966/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 296.4836 - val_loss: 488.4111\n",
      "Epoch 5967/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 161.7206 - val_loss: 108.7140\n",
      "Epoch 5968/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 41.6968 - val_loss: 7.0237\n",
      "Epoch 5969/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.9059 - val_loss: 15.7491\n",
      "Epoch 5970/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 12.9536 - val_loss: 4.2484\n",
      "Epoch 5971/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.4298 - val_loss: 8.0264\n",
      "Epoch 5972/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.6731 - val_loss: 2.6229\n",
      "Epoch 5973/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.0507 - val_loss: 2.9159\n",
      "Epoch 5974/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12.2266 - val_loss: 10.9404\n",
      "Epoch 5975/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 162.9261 - val_loss: 402.3934\n",
      "Epoch 5976/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 221.3395 - val_loss: 156.8942\n",
      "Epoch 5977/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 192.8596 - val_loss: 125.6838\n",
      "Epoch 5978/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 290.9560 - val_loss: 804.1073\n",
      "Epoch 5979/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 367.3716 - val_loss: 283.2755\n",
      "Epoch 5980/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 366.9906 - val_loss: 410.9428\n",
      "Epoch 5981/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 477.6263 - val_loss: 776.3898\n",
      "Epoch 5982/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 322.3760 - val_loss: 1384.6659\n",
      "Epoch 5983/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 731.9763 - val_loss: 2764.2740\n",
      "Epoch 5984/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1944.9906 - val_loss: 2717.7435\n",
      "Epoch 5985/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2269.8566 - val_loss: 968.1702\n",
      "Epoch 5986/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 456.1347 - val_loss: 673.2526\n",
      "Epoch 5987/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 177.1543 - val_loss: 475.2802\n",
      "Epoch 5988/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 211.7636 - val_loss: 1386.7313\n",
      "Epoch 5989/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 623.8876 - val_loss: 275.0694\n",
      "Epoch 5990/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 224.7685 - val_loss: 695.1560\n",
      "Epoch 5991/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1066.9453 - val_loss: 4790.8105\n",
      "Epoch 5992/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4905.9082 - val_loss: 3396.1511\n",
      "Epoch 5993/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1804.9992 - val_loss: 4629.2276\n",
      "Epoch 5994/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2564.1348 - val_loss: 2079.2828\n",
      "Epoch 5995/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1827.4008 - val_loss: 2110.7706\n",
      "Epoch 5996/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1656.0703 - val_loss: 1148.4671\n",
      "Epoch 5997/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 453.5785 - val_loss: 428.2130\n",
      "Epoch 5998/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 500.3517 - val_loss: 351.7127\n",
      "Epoch 5999/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 526.9994 - val_loss: 384.8157\n",
      "Epoch 6000/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 433.1238 - val_loss: 834.1649\n",
      "Epoch 6001/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1005.6814 - val_loss: 1153.7123\n",
      "Epoch 6002/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 339.8524 - val_loss: 142.3384\n",
      "Epoch 6003/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 157.9356 - val_loss: 524.5448\n",
      "Epoch 6004/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 633.4534 - val_loss: 1314.0668\n",
      "Epoch 6005/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1371.5474 - val_loss: 1468.2276\n",
      "Epoch 6006/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3451.8331 - val_loss: 875.8664\n",
      "Epoch 6007/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2768.4315 - val_loss: 1508.9897\n",
      "Epoch 6008/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3887.1361 - val_loss: 4508.2399\n",
      "Epoch 6009/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4698.9895 - val_loss: 2046.3468\n",
      "Epoch 6010/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4417.0396 - val_loss: 650.5667\n",
      "Epoch 6011/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2070.6281 - val_loss: 304.4789\n",
      "Epoch 6012/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 613.1795 - val_loss: 1576.3505\n",
      "Epoch 6013/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1005.9768 - val_loss: 1152.7993\n",
      "Epoch 6014/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 849.7463 - val_loss: 1037.3109\n",
      "Epoch 6015/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1417.7054 - val_loss: 1710.8957\n",
      "Epoch 6016/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 863.6049 - val_loss: 133.1819\n",
      "Epoch 6017/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 522.6132 - val_loss: 48.7805\n",
      "Epoch 6018/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 484.7880 - val_loss: 172.6829\n",
      "Epoch 6019/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 530.8399 - val_loss: 1192.8743\n",
      "Epoch 6020/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 672.8888 - val_loss: 554.7436\n",
      "Epoch 6021/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 436.0094 - val_loss: 562.7442\n",
      "Epoch 6022/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 468.5018 - val_loss: 1320.2070\n",
      "Epoch 6023/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 428.9120 - val_loss: 195.0481\n",
      "Epoch 6024/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 534.7127 - val_loss: 313.8694\n",
      "Epoch 6025/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 405.3508 - val_loss: 40.7721\n",
      "Epoch 6026/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 131.6600 - val_loss: 43.1995\n",
      "Epoch 6027/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26.4824 - val_loss: 30.7715\n",
      "Epoch 6028/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 34.9756 - val_loss: 99.1329\n",
      "Epoch 6029/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 34.0593 - val_loss: 15.6481\n",
      "Epoch 6030/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 22.9367 - val_loss: 2.9238\n",
      "Epoch 6031/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 18.5516 - val_loss: 7.9935\n",
      "Epoch 6032/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.5629 - val_loss: 6.6086\n",
      "Epoch 6033/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.0473 - val_loss: 2.6117\n",
      "Epoch 6034/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4157 - val_loss: 0.7399\n",
      "Epoch 6035/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3156 - val_loss: 0.0984\n",
      "Epoch 6036/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1420 - val_loss: 0.3739\n",
      "Epoch 6037/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1352 - val_loss: 0.0280\n",
      "Epoch 6038/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0936 - val_loss: 0.0715\n",
      "Epoch 6039/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0556 - val_loss: 0.0531\n",
      "Epoch 6040/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0348 - val_loss: 0.0231\n",
      "Epoch 6041/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0239 - val_loss: 0.0264\n",
      "Epoch 6042/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0297 - val_loss: 0.0143\n",
      "Epoch 6043/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0195 - val_loss: 0.0138\n",
      "Epoch 6044/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0250 - val_loss: 0.0200\n",
      "Epoch 6045/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0278 - val_loss: 0.0268\n",
      "Epoch 6046/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0314 - val_loss: 0.0339\n",
      "Epoch 6047/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0301 - val_loss: 0.0141\n",
      "Epoch 6048/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0190 - val_loss: 0.0133\n",
      "Epoch 6049/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0205 - val_loss: 0.0137\n",
      "Epoch 6050/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0306 - val_loss: 0.0229\n",
      "Epoch 6051/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0281 - val_loss: 0.0323\n",
      "Epoch 6052/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0257 - val_loss: 0.0386\n",
      "Epoch 6053/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0311 - val_loss: 0.0279\n",
      "Epoch 6054/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0232 - val_loss: 0.0186\n",
      "Epoch 6055/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0326 - val_loss: 0.0370\n",
      "Epoch 6056/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0375 - val_loss: 0.0260\n",
      "Epoch 6057/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0698 - val_loss: 0.0563\n",
      "Epoch 6058/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0600 - val_loss: 0.2000\n",
      "Epoch 6059/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1160 - val_loss: 0.1150\n",
      "Epoch 6060/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0643 - val_loss: 0.0833\n",
      "Epoch 6061/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0694 - val_loss: 0.0301\n",
      "Epoch 6062/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0525 - val_loss: 0.0256\n",
      "Epoch 6063/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0578 - val_loss: 0.0376\n",
      "Epoch 6064/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0455 - val_loss: 0.0283\n",
      "Epoch 6065/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0421 - val_loss: 0.0157\n",
      "Epoch 6066/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0364 - val_loss: 0.0187\n",
      "Epoch 6067/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0202 - val_loss: 0.0271\n",
      "Epoch 6068/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0247 - val_loss: 0.0192\n",
      "Epoch 6069/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0124 - val_loss: 0.0353\n",
      "Epoch 6070/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0168 - val_loss: 0.0102\n",
      "Epoch 6071/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0221 - val_loss: 0.0181\n",
      "Epoch 6072/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0248 - val_loss: 0.0492\n",
      "Epoch 6073/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0295 - val_loss: 0.0166\n",
      "Epoch 6074/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.021 - 0s 162us/sample - loss: 0.0173 - val_loss: 0.0241\n",
      "Epoch 6075/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0169 - val_loss: 0.0130\n",
      "Epoch 6076/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0111 - val_loss: 0.0086\n",
      "Epoch 6077/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0103 - val_loss: 0.0113\n",
      "Epoch 6078/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0165 - val_loss: 0.0362\n",
      "Epoch 6079/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 0.0330 - val_loss: 0.0088\n",
      "Epoch 6080/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0586 - val_loss: 0.0896\n",
      "Epoch 6081/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2180 - val_loss: 0.4072\n",
      "Epoch 6082/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1424 - val_loss: 0.1512\n",
      "Epoch 6083/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.1290 - val_loss: 0.0530\n",
      "Epoch 6084/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0941 - val_loss: 0.0700\n",
      "Epoch 6085/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0484 - val_loss: 0.0171\n",
      "Epoch 6086/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0218 - val_loss: 0.0250\n",
      "Epoch 6087/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.0268 - val_loss: 0.0371\n",
      "Epoch 6088/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0507 - val_loss: 0.0987\n",
      "Epoch 6089/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1267 - val_loss: 0.1569\n",
      "Epoch 6090/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 0.0523 - val_loss: 0.0128\n",
      "Epoch 6091/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0101 - val_loss: 0.0067\n",
      "Epoch 6092/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0079 - val_loss: 0.0100\n",
      "Epoch 6093/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0107 - val_loss: 0.0122\n",
      "Epoch 6094/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.0237 - val_loss: 0.0088\n",
      "Epoch 6095/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0136 - val_loss: 0.0240\n",
      "Epoch 6096/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0154 - val_loss: 0.0137\n",
      "Epoch 6097/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0099 - val_loss: 0.0276\n",
      "Epoch 6098/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 0.0140 - val_loss: 0.0054\n",
      "Epoch 6099/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0075 - val_loss: 0.0046\n",
      "Epoch 6100/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 6101/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0170 - val_loss: 0.0038\n",
      "Epoch 6102/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0076 - val_loss: 0.0038\n",
      "Epoch 6103/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 0.0088 - val_loss: 0.0046\n",
      "Epoch 6104/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0093 - val_loss: 0.0063\n",
      "Epoch 6105/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0131 - val_loss: 0.0218\n",
      "Epoch 6106/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0321 - val_loss: 0.0193\n",
      "Epoch 6107/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 0.0291 - val_loss: 0.0474\n",
      "Epoch 6108/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0146 - val_loss: 0.0313\n",
      "Epoch 6109/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0222 - val_loss: 0.0315\n",
      "Epoch 6110/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0357 - val_loss: 0.0407\n",
      "Epoch 6111/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0157 - val_loss: 0.0784\n",
      "Epoch 6112/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 0.0328 - val_loss: 0.0244\n",
      "Epoch 6113/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0514 - val_loss: 0.0432\n",
      "Epoch 6114/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0953 - val_loss: 0.1135\n",
      "Epoch 6115/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0360 - val_loss: 0.0173\n",
      "Epoch 6116/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0261 - val_loss: 0.0285\n",
      "Epoch 6117/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0267 - val_loss: 0.0114\n",
      "Epoch 6118/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0470 - val_loss: 0.0972\n",
      "Epoch 6119/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0431 - val_loss: 0.0577\n",
      "Epoch 6120/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0821 - val_loss: 0.0541\n",
      "Epoch 6121/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.0583 - val_loss: 0.0945\n",
      "Epoch 6122/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1144 - val_loss: 0.0381\n",
      "Epoch 6123/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0329 - val_loss: 0.0190\n",
      "Epoch 6124/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0266 - val_loss: 0.0104\n",
      "Epoch 6125/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0344 - val_loss: 0.0280\n",
      "Epoch 6126/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0253 - val_loss: 0.0385\n",
      "Epoch 6127/10000\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 0.0267 - val_loss: 0.0315\n",
      "Epoch 6128/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0269 - val_loss: 0.0072\n",
      "Epoch 6129/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0233 - val_loss: 0.0486\n",
      "Epoch 6130/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0230 - val_loss: 0.0343\n",
      "Epoch 6131/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0604 - val_loss: 0.0128\n",
      "Epoch 6132/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0230 - val_loss: 0.0112\n",
      "Epoch 6133/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0090 - val_loss: 0.0018\n",
      "Epoch 6134/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0047 - val_loss: 0.0019\n",
      "Epoch 6135/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 6136/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0048 - val_loss: 0.0162\n",
      "Epoch 6137/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0047 - val_loss: 0.0033\n",
      "Epoch 6138/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0086 - val_loss: 0.0175\n",
      "Epoch 6139/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0131 - val_loss: 0.0123\n",
      "Epoch 6140/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0128 - val_loss: 0.0064\n",
      "Epoch 6141/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0043 - val_loss: 0.0016\n",
      "Epoch 6142/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 6143/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0049 - val_loss: 0.0032\n",
      "Epoch 6144/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0035 - val_loss: 0.0055\n",
      "Epoch 6145/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0027 - val_loss: 0.0014\n",
      "Epoch 6146/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0043 - val_loss: 0.0046\n",
      "Epoch 6147/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0069 - val_loss: 0.0100\n",
      "Epoch 6148/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0075 - val_loss: 0.0014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6149/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0053 - val_loss: 0.0072\n",
      "Epoch 6150/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0110 - val_loss: 0.0231\n",
      "Epoch 6151/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0128 - val_loss: 0.0116\n",
      "Epoch 6152/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0322 - val_loss: 0.0086\n",
      "Epoch 6153/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0291 - val_loss: 0.0325\n",
      "Epoch 6154/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0146 - val_loss: 0.0031\n",
      "Epoch 6155/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0033 - val_loss: 0.0118\n",
      "Epoch 6156/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0084 - val_loss: 0.0239\n",
      "Epoch 6157/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0087 - val_loss: 0.0250\n",
      "Epoch 6158/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0148 - val_loss: 0.0169\n",
      "Epoch 6159/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0170 - val_loss: 0.0301\n",
      "Epoch 6160/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0338 - val_loss: 0.0344\n",
      "Epoch 6161/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0399 - val_loss: 0.1670\n",
      "Epoch 6162/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0819 - val_loss: 0.1829\n",
      "Epoch 6163/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0842 - val_loss: 0.2718\n",
      "Epoch 6164/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3657 - val_loss: 0.4177\n",
      "Epoch 6165/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3266 - val_loss: 0.5576\n",
      "Epoch 6166/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2558 - val_loss: 0.1996\n",
      "Epoch 6167/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.2876 - val_loss: 4.0849\n",
      "Epoch 6168/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.7721 - val_loss: 5.4111\n",
      "Epoch 6169/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.2443 - val_loss: 0.2959\n",
      "Epoch 6170/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1686 - val_loss: 0.1204\n",
      "Epoch 6171/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1047 - val_loss: 0.1149\n",
      "Epoch 6172/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1358 - val_loss: 0.0317\n",
      "Epoch 6173/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0293 - val_loss: 0.0726\n",
      "Epoch 6174/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0497 - val_loss: 0.1197\n",
      "Epoch 6175/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0660 - val_loss: 0.1655\n",
      "Epoch 6176/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1314 - val_loss: 0.1923\n",
      "Epoch 6177/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1121 - val_loss: 0.1689\n",
      "Epoch 6178/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1099 - val_loss: 0.1653\n",
      "Epoch 6179/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0757 - val_loss: 0.0407\n",
      "Epoch 6180/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0917 - val_loss: 0.2299\n",
      "Epoch 6181/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2227 - val_loss: 0.3005\n",
      "Epoch 6182/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6842 - val_loss: 0.6828\n",
      "Epoch 6183/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0456 - val_loss: 0.8099\n",
      "Epoch 6184/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.7054 - val_loss: 0.6321\n",
      "Epoch 6185/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2838 - val_loss: 0.3846\n",
      "Epoch 6186/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.2443 - val_loss: 1.0451\n",
      "Epoch 6187/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.4324 - val_loss: 1.0750\n",
      "Epoch 6188/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.8979 - val_loss: 1.7920\n",
      "Epoch 6189/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2289 - val_loss: 4.7189\n",
      "Epoch 6190/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.8293 - val_loss: 0.8644\n",
      "Epoch 6191/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1151 - val_loss: 1.8929\n",
      "Epoch 6192/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.5987 - val_loss: 5.8841\n",
      "Epoch 6193/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 3.835 - 0s 147us/sample - loss: 3.9961 - val_loss: 3.6605\n",
      "Epoch 6194/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3.2620 - val_loss: 0.8043\n",
      "Epoch 6195/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.8405 - val_loss: 5.3193\n",
      "Epoch 6196/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10.4520 - val_loss: 4.6417\n",
      "Epoch 6197/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 14.5909 - val_loss: 69.0323\n",
      "Epoch 6198/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 41.1310 - val_loss: 113.8581\n",
      "Epoch 6199/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 47.7191 - val_loss: 201.1106\n",
      "Epoch 6200/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 253.8303 - val_loss: 84.3847\n",
      "Epoch 6201/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 50.1361 - val_loss: 54.5092\n",
      "Epoch 6202/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 78.3304 - val_loss: 163.1247\n",
      "Epoch 6203/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 191.7993 - val_loss: 375.9439\n",
      "Epoch 6204/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 508.9579 - val_loss: 179.7317\n",
      "Epoch 6205/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1013.7430 - val_loss: 88.3047\n",
      "Epoch 6206/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1234.3158 - val_loss: 425.8475\n",
      "Epoch 6207/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 590.7667 - val_loss: 418.6096\n",
      "Epoch 6208/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 307.4437 - val_loss: 647.3575\n",
      "Epoch 6209/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 441.8916 - val_loss: 1940.0848\n",
      "Epoch 6210/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1427.6458 - val_loss: 1693.8523\n",
      "Epoch 6211/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1258.7299 - val_loss: 145.7823\n",
      "Epoch 6212/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1124.4854 - val_loss: 3141.8131\n",
      "Epoch 6213/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1343.4143 - val_loss: 1116.1899\n",
      "Epoch 6214/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 961.4522 - val_loss: 1482.4709\n",
      "Epoch 6215/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 362.4166 - val_loss: 130.4860\n",
      "Epoch 6216/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 425.6101 - val_loss: 11.4923\n",
      "Epoch 6217/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 53.2056 - val_loss: 217.4784\n",
      "Epoch 6218/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 164.8472 - val_loss: 135.7055\n",
      "Epoch 6219/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 100.1190 - val_loss: 111.3283\n",
      "Epoch 6220/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 52.7364 - val_loss: 8.0596\n",
      "Epoch 6221/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 30.2907 - val_loss: 68.5261\n",
      "Epoch 6222/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 52.3610 - val_loss: 83.0304\n",
      "Epoch 6223/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 48.4703 - val_loss: 14.0914\n",
      "Epoch 6224/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 61.5206 - val_loss: 94.9380\n",
      "Epoch 6225/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 39.7372 - val_loss: 0.7595\n",
      "Epoch 6226/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.1583 - val_loss: 6.9978\n",
      "Epoch 6227/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.6536 - val_loss: 0.5712\n",
      "Epoch 6228/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1095 - val_loss: 3.5122\n",
      "Epoch 6229/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.3757 - val_loss: 4.5667\n",
      "Epoch 6230/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.9743 - val_loss: 3.1561\n",
      "Epoch 6231/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.8881 - val_loss: 8.0740\n",
      "Epoch 6232/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.4867 - val_loss: 1.0750\n",
      "Epoch 6233/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8978 - val_loss: 1.3034\n",
      "Epoch 6234/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4561 - val_loss: 0.6661\n",
      "Epoch 6235/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.3960 - val_loss: 0.0427\n",
      "Epoch 6236/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2326 - val_loss: 0.8524\n",
      "Epoch 6237/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.6305 - val_loss: 0.8924\n",
      "Epoch 6238/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6147 - val_loss: 1.1565\n",
      "Epoch 6239/10000\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 0.7218 - val_loss: 0.7069\n",
      "Epoch 6240/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.5541 - val_loss: 0.5203\n",
      "Epoch 6241/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2462 - val_loss: 0.3546\n",
      "Epoch 6242/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1482 - val_loss: 0.3694\n",
      "Epoch 6243/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 0.2335 - val_loss: 0.3919\n",
      "Epoch 6244/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.8198 - val_loss: 1.7922\n",
      "Epoch 6245/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.1182 - val_loss: 12.2910\n",
      "Epoch 6246/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.5790 - val_loss: 8.4421\n",
      "Epoch 6247/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 28.7118 - val_loss: 5.6335\n",
      "Epoch 6248/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 11.0670 - val_loss: 29.1064\n",
      "Epoch 6249/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.5109 - val_loss: 23.1731\n",
      "Epoch 6250/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 15.3721 - val_loss: 6.4622\n",
      "Epoch 6251/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 7.5665 - val_loss: 2.0351\n",
      "Epoch 6252/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.4114 - val_loss: 9.5527\n",
      "Epoch 6253/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 10.7439 - val_loss: 31.2109\n",
      "Epoch 6254/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 15.8930 - val_loss: 35.0225\n",
      "Epoch 6255/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 26.1818 - val_loss: 7.1090\n",
      "Epoch 6256/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 38.9691 - val_loss: 38.9040\n",
      "Epoch 6257/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 78.4125 - val_loss: 45.5971\n",
      "Epoch 6258/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 127.2815 - val_loss: 628.8350\n",
      "Epoch 6259/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1472.9402 - val_loss: 1035.4491\n",
      "Epoch 6260/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1723.6269 - val_loss: 1253.9613\n",
      "Epoch 6261/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1222.7911 - val_loss: 2429.9170\n",
      "Epoch 6262/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 1499.6834 - val_loss: 883.8170\n",
      "Epoch 6263/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1007.0572 - val_loss: 38.5217\n",
      "Epoch 6264/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 193.5338 - val_loss: 250.3977\n",
      "Epoch 6265/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 113.6989 - val_loss: 114.2453\n",
      "Epoch 6266/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 173.4126 - val_loss: 305.0943\n",
      "Epoch 6267/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 164.8694 - val_loss: 7.6370\n",
      "Epoch 6268/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 40.4147 - val_loss: 81.0154\n",
      "Epoch 6269/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 32.4448 - val_loss: 2.9834\n",
      "Epoch 6270/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.5418 - val_loss: 9.4340\n",
      "Epoch 6271/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.3809 - val_loss: 1.6093\n",
      "Epoch 6272/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 3.3334 - val_loss: 3.1093\n",
      "Epoch 6273/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.6523 - val_loss: 4.6311\n",
      "Epoch 6274/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.4691 - val_loss: 0.5256\n",
      "Epoch 6275/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6564 - val_loss: 0.1051\n",
      "Epoch 6276/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3489 - val_loss: 0.5137\n",
      "Epoch 6277/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3340 - val_loss: 0.7520\n",
      "Epoch 6278/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5882 - val_loss: 0.9836\n",
      "Epoch 6279/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4170 - val_loss: 0.0614\n",
      "Epoch 6280/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3005 - val_loss: 2.3957\n",
      "Epoch 6281/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1847 - val_loss: 1.4235\n",
      "Epoch 6282/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5781 - val_loss: 3.4298\n",
      "Epoch 6283/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 3.1964 - val_loss: 7.7139\n",
      "Epoch 6284/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.6183 - val_loss: 3.4960\n",
      "Epoch 6285/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5346 - val_loss: 0.8608\n",
      "Epoch 6286/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.8499 - val_loss: 0.3634\n",
      "Epoch 6287/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4039 - val_loss: 0.0204\n",
      "Epoch 6288/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2268 - val_loss: 0.0686\n",
      "Epoch 6289/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1425 - val_loss: 0.0858\n",
      "Epoch 6290/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1712 - val_loss: 0.1861\n",
      "Epoch 6291/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.4477 - val_loss: 0.7583\n",
      "Epoch 6292/10000\n",
      "68/68 [==============================] - 0s 735us/sample - loss: 0.4102 - val_loss: 0.7271\n",
      "Epoch 6293/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4607 - val_loss: 0.7986\n",
      "Epoch 6294/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3414 - val_loss: 0.2823\n",
      "Epoch 6295/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.1423 - val_loss: 0.1268\n",
      "Epoch 6296/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6871 - val_loss: 0.9928\n",
      "Epoch 6297/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5058 - val_loss: 1.3564\n",
      "Epoch 6298/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8470 - val_loss: 0.0719\n",
      "Epoch 6299/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3552 - val_loss: 0.0531\n",
      "Epoch 6300/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2718 - val_loss: 0.1386\n",
      "Epoch 6301/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1059 - val_loss: 0.0798\n",
      "Epoch 6302/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1262 - val_loss: 0.4317\n",
      "Epoch 6303/10000\n",
      "68/68 [==============================] - 0s 588us/sample - loss: 0.2427 - val_loss: 0.2064\n",
      "Epoch 6304/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1686 - val_loss: 0.0671\n",
      "Epoch 6305/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0760 - val_loss: 0.1155\n",
      "Epoch 6306/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1380 - val_loss: 0.3098\n",
      "Epoch 6307/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1030 - val_loss: 0.0663\n",
      "Epoch 6308/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0437 - val_loss: 0.0034\n",
      "Epoch 6309/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1035 - val_loss: 0.1811\n",
      "Epoch 6310/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4407 - val_loss: 0.2346\n",
      "Epoch 6311/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8985 - val_loss: 2.9945\n",
      "Epoch 6312/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.2644 - val_loss: 1.6623\n",
      "Epoch 6313/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6691 - val_loss: 1.1215\n",
      "Epoch 6314/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1465 - val_loss: 1.3982\n",
      "Epoch 6315/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5782 - val_loss: 0.2625\n",
      "Epoch 6316/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1455 - val_loss: 0.4463\n",
      "Epoch 6317/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.9729 - val_loss: 0.5077\n",
      "Epoch 6318/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2846 - val_loss: 0.1823\n",
      "Epoch 6319/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6394 - val_loss: 1.0480\n",
      "Epoch 6320/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8568 - val_loss: 1.1190\n",
      "Epoch 6321/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5451 - val_loss: 2.1555\n",
      "Epoch 6322/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.9179 - val_loss: 2.3083\n",
      "Epoch 6323/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 0.6539 - val_loss: 2.0150\n",
      "Epoch 6324/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.0247 - val_loss: 4.0623\n",
      "Epoch 6325/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 20.2805 - val_loss: 156.0212\n",
      "Epoch 6326/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 51.1010 - val_loss: 8.5320\n",
      "Epoch 6327/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 6.4092 - val_loss: 2.8893\n",
      "Epoch 6328/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 8.3751 - val_loss: 21.9038\n",
      "Epoch 6329/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 34.0684 - val_loss: 1.8384\n",
      "Epoch 6330/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.7275 - val_loss: 2.4334\n",
      "Epoch 6331/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.8930 - val_loss: 0.9552\n",
      "Epoch 6332/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4810 - val_loss: 0.9698\n",
      "Epoch 6333/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.9557 - val_loss: 1.3741\n",
      "Epoch 6334/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.8665 - val_loss: 1.7355\n",
      "Epoch 6335/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.4175 - val_loss: 3.0368\n",
      "Epoch 6336/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8417 - val_loss: 2.4732\n",
      "Epoch 6337/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5788 - val_loss: 2.2830\n",
      "Epoch 6338/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.1553 - val_loss: 4.3113\n",
      "Epoch 6339/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.4499 - val_loss: 5.2548\n",
      "Epoch 6340/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.6940 - val_loss: 0.5589\n",
      "Epoch 6341/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2995 - val_loss: 15.9169\n",
      "Epoch 6342/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.9615 - val_loss: 25.7612\n",
      "Epoch 6343/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 13.4295 - val_loss: 19.4412\n",
      "Epoch 6344/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.0669 - val_loss: 0.2255\n",
      "Epoch 6345/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.0915 - val_loss: 5.7800\n",
      "Epoch 6346/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2356 - val_loss: 2.3935\n",
      "Epoch 6347/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.2022 - val_loss: 0.0510\n",
      "Epoch 6348/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.7405 - val_loss: 19.1336\n",
      "Epoch 6349/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 16.8596 - val_loss: 27.7533\n",
      "Epoch 6350/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 55.3951 - val_loss: 12.9124\n",
      "Epoch 6351/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 53.7553 - val_loss: 673.5648\n",
      "Epoch 6352/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 275.4195 - val_loss: 861.2472\n",
      "Epoch 6353/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 503.8870 - val_loss: 629.9223\n",
      "Epoch 6354/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 274.1920 - val_loss: 328.6425\n",
      "Epoch 6355/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 268.7401 - val_loss: 203.6783\n",
      "Epoch 6356/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 566.1013 - val_loss: 4733.3591\n",
      "Epoch 6357/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7471.1758 - val_loss: 14344.2841\n",
      "Epoch 6358/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5908.3951 - val_loss: 1341.8881\n",
      "Epoch 6359/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4187.6134 - val_loss: 5594.2202\n",
      "Epoch 6360/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 5176.0466 - val_loss: 7124.6731\n",
      "Epoch 6361/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5423.7271 - val_loss: 796.7465\n",
      "Epoch 6362/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2985.8571 - val_loss: 4146.0290\n",
      "Epoch 6363/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 2583.8392 - val_loss: 725.9080\n",
      "Epoch 6364/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 602.9039 - val_loss: 1965.6536\n",
      "Epoch 6365/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 683.9398 - val_loss: 954.6714\n",
      "Epoch 6366/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 396.7738 - val_loss: 417.8943\n",
      "Epoch 6367/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 142.4679 - val_loss: 218.9501\n",
      "Epoch 6368/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 273.8025 - val_loss: 238.0226\n",
      "Epoch 6369/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 163.2248 - val_loss: 109.7733\n",
      "Epoch 6370/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 172.9455 - val_loss: 57.6812\n",
      "Epoch 6371/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 75.0794 - val_loss: 71.1612\n",
      "Epoch 6372/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 85.0561 - val_loss: 84.2364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6373/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 36.7189 - val_loss: 28.0096\n",
      "Epoch 6374/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 36.9727 - val_loss: 59.6563\n",
      "Epoch 6375/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 23.3177 - val_loss: 32.6882\n",
      "Epoch 6376/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 12.4705 - val_loss: 10.3146\n",
      "Epoch 6377/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.2474 - val_loss: 12.3088\n",
      "Epoch 6378/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.9735 - val_loss: 30.3185\n",
      "Epoch 6379/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 14.5303 - val_loss: 17.5920\n",
      "Epoch 6380/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 11.0161 - val_loss: 31.7918\n",
      "Epoch 6381/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 15.7299 - val_loss: 13.3457\n",
      "Epoch 6382/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 15.8961 - val_loss: 31.6515\n",
      "Epoch 6383/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 39.3173 - val_loss: 64.4946\n",
      "Epoch 6384/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 32.4383 - val_loss: 66.0005\n",
      "Epoch 6385/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 22.6836 - val_loss: 33.6304\n",
      "Epoch 6386/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 23.4490 - val_loss: 23.3057\n",
      "Epoch 6387/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 14.5545 - val_loss: 0.1851\n",
      "Epoch 6388/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.0544 - val_loss: 1.1525\n",
      "Epoch 6389/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.9263 - val_loss: 9.2916\n",
      "Epoch 6390/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.9099 - val_loss: 3.4227\n",
      "Epoch 6391/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.0185 - val_loss: 1.4852\n",
      "Epoch 6392/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5583 - val_loss: 0.5481\n",
      "Epoch 6393/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5209 - val_loss: 0.6579\n",
      "Epoch 6394/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3185 - val_loss: 0.2332\n",
      "Epoch 6395/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1838 - val_loss: 0.3466\n",
      "Epoch 6396/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1768 - val_loss: 0.0664\n",
      "Epoch 6397/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0848 - val_loss: 0.1941\n",
      "Epoch 6398/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1340 - val_loss: 0.2882\n",
      "Epoch 6399/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2313 - val_loss: 0.3218\n",
      "Epoch 6400/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2466 - val_loss: 0.3755\n",
      "Epoch 6401/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2306 - val_loss: 0.2334\n",
      "Epoch 6402/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2107 - val_loss: 0.0574\n",
      "Epoch 6403/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1401 - val_loss: 0.2418\n",
      "Epoch 6404/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2352 - val_loss: 0.4193\n",
      "Epoch 6405/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4438 - val_loss: 1.1894\n",
      "Epoch 6406/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.9007 - val_loss: 0.1739\n",
      "Epoch 6407/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2238 - val_loss: 0.0996\n",
      "Epoch 6408/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1219 - val_loss: 0.5134\n",
      "Epoch 6409/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2810 - val_loss: 0.3875\n",
      "Epoch 6410/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3258 - val_loss: 0.0664\n",
      "Epoch 6411/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2400 - val_loss: 0.4096\n",
      "Epoch 6412/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3227 - val_loss: 0.1068\n",
      "Epoch 6413/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1384 - val_loss: 0.3504\n",
      "Epoch 6414/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2905 - val_loss: 0.2506\n",
      "Epoch 6415/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1823 - val_loss: 0.0805\n",
      "Epoch 6416/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2417 - val_loss: 0.2594\n",
      "Epoch 6417/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1759 - val_loss: 0.0579\n",
      "Epoch 6418/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0736 - val_loss: 0.1392\n",
      "Epoch 6419/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1966 - val_loss: 0.5011\n",
      "Epoch 6420/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2517 - val_loss: 0.1530\n",
      "Epoch 6421/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3979 - val_loss: 0.1629\n",
      "Epoch 6422/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4983 - val_loss: 0.9673\n",
      "Epoch 6423/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.9196 - val_loss: 1.5919\n",
      "Epoch 6424/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1550 - val_loss: 0.5395\n",
      "Epoch 6425/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.0503 - val_loss: 1.4010\n",
      "Epoch 6426/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.2222 - val_loss: 5.9771\n",
      "Epoch 6427/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.5205 - val_loss: 1.4457\n",
      "Epoch 6428/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.0990 - val_loss: 7.7162\n",
      "Epoch 6429/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.1619 - val_loss: 8.6351\n",
      "Epoch 6430/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 20.5900 - val_loss: 5.2247\n",
      "Epoch 6431/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 19.9638 - val_loss: 36.8682\n",
      "Epoch 6432/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 21.3544 - val_loss: 4.9334\n",
      "Epoch 6433/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.6382 - val_loss: 5.0054\n",
      "Epoch 6434/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.3311 - val_loss: 10.3554\n",
      "Epoch 6435/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5.9976 - val_loss: 0.5553\n",
      "Epoch 6436/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.5758 - val_loss: 6.7270\n",
      "Epoch 6437/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.8101 - val_loss: 14.0051\n",
      "Epoch 6438/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 26.5131 - val_loss: 28.1079\n",
      "Epoch 6439/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.5062 - val_loss: 12.5928\n",
      "Epoch 6440/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.7117 - val_loss: 13.2166\n",
      "Epoch 6441/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.7678 - val_loss: 15.5444\n",
      "Epoch 6442/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.4547 - val_loss: 16.4009\n",
      "Epoch 6443/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.7914 - val_loss: 0.3327\n",
      "Epoch 6444/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.1472 - val_loss: 3.2923\n",
      "Epoch 6445/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8708 - val_loss: 2.9944\n",
      "Epoch 6446/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.4521 - val_loss: 1.5591\n",
      "Epoch 6447/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.5660 - val_loss: 1.9337\n",
      "Epoch 6448/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7749 - val_loss: 0.9203\n",
      "Epoch 6449/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1148 - val_loss: 0.2051\n",
      "Epoch 6450/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2868 - val_loss: 0.1596\n",
      "Epoch 6451/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.5252 - val_loss: 0.3631\n",
      "Epoch 6452/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2113 - val_loss: 0.3560\n",
      "Epoch 6453/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1794 - val_loss: 0.4561\n",
      "Epoch 6454/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4714 - val_loss: 0.2397\n",
      "Epoch 6455/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1753 - val_loss: 0.2810\n",
      "Epoch 6456/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1280 - val_loss: 0.2771\n",
      "Epoch 6457/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0891 - val_loss: 0.0361\n",
      "Epoch 6458/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0553 - val_loss: 0.2515\n",
      "Epoch 6459/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1050 - val_loss: 0.0206\n",
      "Epoch 6460/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1155 - val_loss: 0.1311\n",
      "Epoch 6461/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2076 - val_loss: 0.4752\n",
      "Epoch 6462/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2063 - val_loss: 0.2236\n",
      "Epoch 6463/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2389 - val_loss: 0.1011\n",
      "Epoch 6464/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.1293 - val_loss: 0.0457\n",
      "Epoch 6465/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1945 - val_loss: 0.2959\n",
      "Epoch 6466/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2541 - val_loss: 0.5085\n",
      "Epoch 6467/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1980 - val_loss: 0.0802\n",
      "Epoch 6468/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0596 - val_loss: 0.0775\n",
      "Epoch 6469/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0914 - val_loss: 0.4707\n",
      "Epoch 6470/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2594 - val_loss: 0.1249\n",
      "Epoch 6471/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0939 - val_loss: 0.1188\n",
      "Epoch 6472/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0659 - val_loss: 0.1231\n",
      "Epoch 6473/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.7250 - val_loss: 0.1589\n",
      "Epoch 6474/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7735 - val_loss: 5.6695\n",
      "Epoch 6475/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.3603 - val_loss: 4.3949\n",
      "Epoch 6476/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.9001 - val_loss: 6.6395\n",
      "Epoch 6477/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.3641 - val_loss: 2.7275\n",
      "Epoch 6478/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.7610 - val_loss: 2.7245\n",
      "Epoch 6479/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3207 - val_loss: 3.4681\n",
      "Epoch 6480/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.9882 - val_loss: 2.0693\n",
      "Epoch 6481/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.5527 - val_loss: 7.9440\n",
      "Epoch 6482/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.2454 - val_loss: 2.4137\n",
      "Epoch 6483/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.6363 - val_loss: 0.9066\n",
      "Epoch 6484/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.9455 - val_loss: 4.4865\n",
      "Epoch 6485/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.8020 - val_loss: 1.7332\n",
      "Epoch 6486/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8878 - val_loss: 0.5600\n",
      "Epoch 6487/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.8307 - val_loss: 0.5372\n",
      "Epoch 6488/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7608 - val_loss: 1.4199\n",
      "Epoch 6489/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7789 - val_loss: 1.0023\n",
      "Epoch 6490/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4185 - val_loss: 0.0752\n",
      "Epoch 6491/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1298 - val_loss: 0.1570\n",
      "Epoch 6492/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0972 - val_loss: 0.0741\n",
      "Epoch 6493/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0797 - val_loss: 0.4375\n",
      "Epoch 6494/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4472 - val_loss: 0.6179\n",
      "Epoch 6495/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3838 - val_loss: 0.3193\n",
      "Epoch 6496/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3920 - val_loss: 0.1962\n",
      "Epoch 6497/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2236 - val_loss: 0.6599\n",
      "Epoch 6498/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9543 - val_loss: 2.4615\n",
      "Epoch 6499/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8765 - val_loss: 3.2007\n",
      "Epoch 6500/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.8752 - val_loss: 4.7824\n",
      "Epoch 6501/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.7858 - val_loss: 15.1253\n",
      "Epoch 6502/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.3243 - val_loss: 6.1856\n",
      "Epoch 6503/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.5693 - val_loss: 22.1384\n",
      "Epoch 6504/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 35.0687 - val_loss: 69.2213\n",
      "Epoch 6505/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 42.7337 - val_loss: 179.9680\n",
      "Epoch 6506/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 84.6676 - val_loss: 39.6193\n",
      "Epoch 6507/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 29.7963 - val_loss: 3.6138\n",
      "Epoch 6508/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.3377 - val_loss: 10.5512\n",
      "Epoch 6509/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 10.4313 - val_loss: 39.5529\n",
      "Epoch 6510/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 26.8873 - val_loss: 39.1633\n",
      "Epoch 6511/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 14.3484 - val_loss: 13.0421\n",
      "Epoch 6512/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.7005 - val_loss: 8.2808\n",
      "Epoch 6513/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 5.1535 - val_loss: 23.5943\n",
      "Epoch 6514/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 11.8184 - val_loss: 35.1196\n",
      "Epoch 6515/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 24.9310 - val_loss: 76.7130\n",
      "Epoch 6516/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 67.1358 - val_loss: 90.1455\n",
      "Epoch 6517/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 109.2006 - val_loss: 55.1257\n",
      "Epoch 6518/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 59.4109 - val_loss: 21.5648\n",
      "Epoch 6519/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 42.9262 - val_loss: 115.3095\n",
      "Epoch 6520/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 58.9984 - val_loss: 19.0834\n",
      "Epoch 6521/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 10.8231 - val_loss: 14.9491\n",
      "Epoch 6522/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 21.8399 - val_loss: 36.9502\n",
      "Epoch 6523/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 191us/sample - loss: 37.0951 - val_loss: 3.1491\n",
      "Epoch 6524/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.6575 - val_loss: 14.8472\n",
      "Epoch 6525/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.2765 - val_loss: 28.6405\n",
      "Epoch 6526/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 37.4298 - val_loss: 50.2259\n",
      "Epoch 6527/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 225.5347 - val_loss: 406.1684\n",
      "Epoch 6528/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 477.3128 - val_loss: 324.9944\n",
      "Epoch 6529/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1169.5501 - val_loss: 104.3979\n",
      "Epoch 6530/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 720.3619 - val_loss: 65.4166\n",
      "Epoch 6531/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 216.9258 - val_loss: 772.8420\n",
      "Epoch 6532/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 335.6434 - val_loss: 159.8579\n",
      "Epoch 6533/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 82.1262 - val_loss: 34.7921\n",
      "Epoch 6534/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 142.3205 - val_loss: 2308.8260\n",
      "Epoch 6535/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1807.6251 - val_loss: 8610.9367\n",
      "Epoch 6536/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2820.2986 - val_loss: 1832.6928\n",
      "Epoch 6537/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1247.5436 - val_loss: 231.8247\n",
      "Epoch 6538/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 466.5279 - val_loss: 1503.1654\n",
      "Epoch 6539/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2851.9416 - val_loss: 730.6215\n",
      "Epoch 6540/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3704.5337 - val_loss: 3622.2431\n",
      "Epoch 6541/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3737.2195 - val_loss: 1633.9404\n",
      "Epoch 6542/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 10936.7897 - val_loss: 7666.8261\n",
      "Epoch 6543/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 14055.9618 - val_loss: 3001.5334\n",
      "Epoch 6544/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6121.5174 - val_loss: 9138.7156\n",
      "Epoch 6545/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7189.6573 - val_loss: 24569.2239\n",
      "Epoch 6546/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 9822.7436 - val_loss: 2859.8178\n",
      "Epoch 6547/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 3578.9698 - val_loss: 3461.9749\n",
      "Epoch 6548/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1689.6486 - val_loss: 1307.3715\n",
      "Epoch 6549/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 833.3967 - val_loss: 1060.5511\n",
      "Epoch 6550/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 736.8210 - val_loss: 655.0187\n",
      "Epoch 6551/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 352.5049 - val_loss: 303.2539\n",
      "Epoch 6552/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 328.3752 - val_loss: 887.1595\n",
      "Epoch 6553/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 470.9895 - val_loss: 118.5942\n",
      "Epoch 6554/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 256.1538 - val_loss: 647.5617\n",
      "Epoch 6555/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 371.3650 - val_loss: 205.3292\n",
      "Epoch 6556/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 206.7645 - val_loss: 172.2219\n",
      "Epoch 6557/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 54.1133 - val_loss: 12.1164\n",
      "Epoch 6558/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 28.9106 - val_loss: 16.5213\n",
      "Epoch 6559/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 13.5616 - val_loss: 2.7996\n",
      "Epoch 6560/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.7590 - val_loss: 5.4537\n",
      "Epoch 6561/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1.9169 - val_loss: 0.4409\n",
      "Epoch 6562/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0156 - val_loss: 0.8385\n",
      "Epoch 6563/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7045 - val_loss: 0.2248\n",
      "Epoch 6564/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3365 - val_loss: 0.6829\n",
      "Epoch 6565/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5827 - val_loss: 0.6523\n",
      "Epoch 6566/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4650 - val_loss: 0.4901\n",
      "Epoch 6567/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3132 - val_loss: 0.2373\n",
      "Epoch 6568/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1813 - val_loss: 0.1825\n",
      "Epoch 6569/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1319 - val_loss: 0.0920\n",
      "Epoch 6570/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1229 - val_loss: 0.1680\n",
      "Epoch 6571/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1731 - val_loss: 0.1110\n",
      "Epoch 6572/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1522 - val_loss: 0.1292\n",
      "Epoch 6573/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1383 - val_loss: 0.1357\n",
      "Epoch 6574/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1575 - val_loss: 0.1217\n",
      "Epoch 6575/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1444 - val_loss: 0.1207\n",
      "Epoch 6576/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1444 - val_loss: 0.1376\n",
      "Epoch 6577/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1086 - val_loss: 0.1038\n",
      "Epoch 6578/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1163 - val_loss: 0.0684\n",
      "Epoch 6579/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1161 - val_loss: 0.0850\n",
      "Epoch 6580/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1515 - val_loss: 0.2729\n",
      "Epoch 6581/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2556 - val_loss: 0.2119\n",
      "Epoch 6582/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3284 - val_loss: 0.1199\n",
      "Epoch 6583/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2188 - val_loss: 0.0891\n",
      "Epoch 6584/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1921 - val_loss: 0.0712\n",
      "Epoch 6585/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 0.2618 - val_loss: 0.3008\n",
      "Epoch 6586/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2308 - val_loss: 0.3656\n",
      "Epoch 6587/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.3771 - val_loss: 0.6292\n",
      "Epoch 6588/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4937 - val_loss: 0.4749\n",
      "Epoch 6589/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5279 - val_loss: 0.5423\n",
      "Epoch 6590/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7176 - val_loss: 0.0701\n",
      "Epoch 6591/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5308 - val_loss: 0.1344\n",
      "Epoch 6592/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6694 - val_loss: 1.7093\n",
      "Epoch 6593/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8783 - val_loss: 0.7542\n",
      "Epoch 6594/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5020 - val_loss: 0.1965\n",
      "Epoch 6595/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7012 - val_loss: 1.3895\n",
      "Epoch 6596/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.6757 - val_loss: 0.2545\n",
      "Epoch 6597/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2247 - val_loss: 0.2917\n",
      "Epoch 6598/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1275 - val_loss: 0.0925\n",
      "Epoch 6599/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0983 - val_loss: 0.0824\n",
      "Epoch 6600/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1321 - val_loss: 0.0630\n",
      "Epoch 6601/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1955 - val_loss: 0.2064\n",
      "Epoch 6602/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1115 - val_loss: 0.0994\n",
      "Epoch 6603/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0848 - val_loss: 0.0583\n",
      "Epoch 6604/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1011 - val_loss: 0.1712\n",
      "Epoch 6605/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1052 - val_loss: 0.1087\n",
      "Epoch 6606/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1341 - val_loss: 0.1041\n",
      "Epoch 6607/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.1989 - val_loss: 0.7333\n",
      "Epoch 6608/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4166 - val_loss: 0.1804\n",
      "Epoch 6609/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1101 - val_loss: 0.1340\n",
      "Epoch 6610/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0899 - val_loss: 0.0620\n",
      "Epoch 6611/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 0.0895 - val_loss: 0.0782\n",
      "Epoch 6612/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.046 - 0s 221us/sample - loss: 0.1486 - val_loss: 0.4403\n",
      "Epoch 6613/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1719 - val_loss: 0.1405\n",
      "Epoch 6614/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1239 - val_loss: 0.0873\n",
      "Epoch 6615/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.1283 - val_loss: 0.2455\n",
      "Epoch 6616/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.1362 - val_loss: 0.1091\n",
      "Epoch 6617/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1174 - val_loss: 0.2044\n",
      "Epoch 6618/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1087 - val_loss: 0.0352\n",
      "Epoch 6619/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0541 - val_loss: 0.0374\n",
      "Epoch 6620/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0475 - val_loss: 0.0332\n",
      "Epoch 6621/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0468 - val_loss: 0.0323\n",
      "Epoch 6622/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0481 - val_loss: 0.1452\n",
      "Epoch 6623/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0641 - val_loss: 0.0325\n",
      "Epoch 6624/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0604 - val_loss: 0.0845\n",
      "Epoch 6625/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0782 - val_loss: 0.1940\n",
      "Epoch 6626/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0875 - val_loss: 0.1433\n",
      "Epoch 6627/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0881 - val_loss: 0.0324\n",
      "Epoch 6628/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0857 - val_loss: 0.0283\n",
      "Epoch 6629/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0705 - val_loss: 0.0383\n",
      "Epoch 6630/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0472 - val_loss: 0.0325\n",
      "Epoch 6631/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0479 - val_loss: 0.0291\n",
      "Epoch 6632/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0380 - val_loss: 0.0334\n",
      "Epoch 6633/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0350 - val_loss: 0.0255\n",
      "Epoch 6634/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0399 - val_loss: 0.0308\n",
      "Epoch 6635/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0418 - val_loss: 0.0277\n",
      "Epoch 6636/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0412 - val_loss: 0.0301\n",
      "Epoch 6637/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0724 - val_loss: 0.0418\n",
      "Epoch 6638/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0596 - val_loss: 0.0272\n",
      "Epoch 6639/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0348 - val_loss: 0.0244\n",
      "Epoch 6640/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0362 - val_loss: 0.0235\n",
      "Epoch 6641/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0644 - val_loss: 0.0277\n",
      "Epoch 6642/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0331 - val_loss: 0.0283\n",
      "Epoch 6643/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0332 - val_loss: 0.0195\n",
      "Epoch 6644/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0273 - val_loss: 0.0240\n",
      "Epoch 6645/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0330 - val_loss: 0.0395\n",
      "Epoch 6646/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0423 - val_loss: 0.0298\n",
      "Epoch 6647/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0482 - val_loss: 0.0758\n",
      "Epoch 6648/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0694 - val_loss: 0.1347\n",
      "Epoch 6649/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1352 - val_loss: 0.0377\n",
      "Epoch 6650/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3433 - val_loss: 0.2152\n",
      "Epoch 6651/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3009 - val_loss: 0.1520\n",
      "Epoch 6652/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1091 - val_loss: 0.0637\n",
      "Epoch 6653/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0613 - val_loss: 0.0446\n",
      "Epoch 6654/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0660 - val_loss: 0.2574\n",
      "Epoch 6655/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2022 - val_loss: 0.1321\n",
      "Epoch 6656/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2112 - val_loss: 0.0659\n",
      "Epoch 6657/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1829 - val_loss: 0.0442\n",
      "Epoch 6658/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2395 - val_loss: 0.0268\n",
      "Epoch 6659/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0891 - val_loss: 0.0973\n",
      "Epoch 6660/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1011 - val_loss: 0.0583\n",
      "Epoch 6661/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.0947 - val_loss: 0.0642\n",
      "Epoch 6662/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0559 - val_loss: 0.0509\n",
      "Epoch 6663/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0387 - val_loss: 0.0454\n",
      "Epoch 6664/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0335 - val_loss: 0.0146\n",
      "Epoch 6665/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0344 - val_loss: 0.0331\n",
      "Epoch 6666/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.0245 - val_loss: 0.0123\n",
      "Epoch 6667/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0240 - val_loss: 0.0441\n",
      "Epoch 6668/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0598 - val_loss: 0.0727\n",
      "Epoch 6669/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.2179 - val_loss: 0.2091\n",
      "Epoch 6670/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1485 - val_loss: 0.0214\n",
      "Epoch 6671/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0501 - val_loss: 0.0152\n",
      "Epoch 6672/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0217 - val_loss: 0.0486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6673/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0352 - val_loss: 0.0124\n",
      "Epoch 6674/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0239 - val_loss: 0.0402\n",
      "Epoch 6675/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0388 - val_loss: 0.0519\n",
      "Epoch 6676/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0280 - val_loss: 0.0641\n",
      "Epoch 6677/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0497 - val_loss: 0.1227\n",
      "Epoch 6678/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0670 - val_loss: 0.0392\n",
      "Epoch 6679/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1177 - val_loss: 0.3490\n",
      "Epoch 6680/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1410 - val_loss: 0.0870\n",
      "Epoch 6681/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0617 - val_loss: 0.0296\n",
      "Epoch 6682/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0416 - val_loss: 0.1484\n",
      "Epoch 6683/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1126 - val_loss: 0.2112\n",
      "Epoch 6684/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0916 - val_loss: 0.0306\n",
      "Epoch 6685/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0401 - val_loss: 0.0178\n",
      "Epoch 6686/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0960 - val_loss: 0.1480\n",
      "Epoch 6687/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.2248 - val_loss: 0.2890\n",
      "Epoch 6688/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1653 - val_loss: 0.2269\n",
      "Epoch 6689/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1168 - val_loss: 0.2462\n",
      "Epoch 6690/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2423 - val_loss: 0.2332\n",
      "Epoch 6691/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2501 - val_loss: 0.4732\n",
      "Epoch 6692/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2363 - val_loss: 0.1401\n",
      "Epoch 6693/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1211 - val_loss: 0.0706\n",
      "Epoch 6694/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0459 - val_loss: 0.0746\n",
      "Epoch 6695/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0579 - val_loss: 0.0511\n",
      "Epoch 6696/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0250 - val_loss: 0.0149\n",
      "Epoch 6697/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0164 - val_loss: 0.0073\n",
      "Epoch 6698/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0135 - val_loss: 0.0230\n",
      "Epoch 6699/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0216 - val_loss: 0.0588\n",
      "Epoch 6700/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.067 - 0s 147us/sample - loss: 0.0474 - val_loss: 0.0088\n",
      "Epoch 6701/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0482 - val_loss: 0.0632\n",
      "Epoch 6702/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0335 - val_loss: 0.0201\n",
      "Epoch 6703/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0164 - val_loss: 0.0142\n",
      "Epoch 6704/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0171 - val_loss: 0.0147\n",
      "Epoch 6705/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0112 - val_loss: 0.0153\n",
      "Epoch 6706/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0102 - val_loss: 0.0290\n",
      "Epoch 6707/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1026 - val_loss: 0.0583\n",
      "Epoch 6708/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1021 - val_loss: 0.1332\n",
      "Epoch 6709/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0549 - val_loss: 0.2577\n",
      "Epoch 6710/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2096 - val_loss: 0.0171\n",
      "Epoch 6711/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0519 - val_loss: 0.1776\n",
      "Epoch 6712/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0878 - val_loss: 0.0116\n",
      "Epoch 6713/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0857 - val_loss: 0.1865\n",
      "Epoch 6714/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1336 - val_loss: 0.0633\n",
      "Epoch 6715/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0296 - val_loss: 0.0532\n",
      "Epoch 6716/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0476 - val_loss: 0.0176\n",
      "Epoch 6717/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0424 - val_loss: 0.0234\n",
      "Epoch 6718/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0170 - val_loss: 0.0099\n",
      "Epoch 6719/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0155 - val_loss: 0.0305\n",
      "Epoch 6720/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0139 - val_loss: 0.0233\n",
      "Epoch 6721/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0151 - val_loss: 0.0086\n",
      "Epoch 6722/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0061 - val_loss: 0.0058\n",
      "Epoch 6723/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0049 - val_loss: 0.0128\n",
      "Epoch 6724/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0270 - val_loss: 0.0027\n",
      "Epoch 6725/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0293 - val_loss: 0.0771\n",
      "Epoch 6726/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0598 - val_loss: 0.0219\n",
      "Epoch 6727/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0356 - val_loss: 0.0625\n",
      "Epoch 6728/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0337 - val_loss: 0.0420\n",
      "Epoch 6729/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0527 - val_loss: 0.1862\n",
      "Epoch 6730/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1436 - val_loss: 0.3486\n",
      "Epoch 6731/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2434 - val_loss: 0.0365\n",
      "Epoch 6732/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0830 - val_loss: 0.0531\n",
      "Epoch 6733/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 0.0372 - val_loss: 0.0217\n",
      "Epoch 6734/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0093 - val_loss: 0.0050\n",
      "Epoch 6735/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0069 - val_loss: 0.0116\n",
      "Epoch 6736/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0108 - val_loss: 0.0447\n",
      "Epoch 6737/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0135 - val_loss: 0.0109\n",
      "Epoch 6738/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0052 - val_loss: 0.0040\n",
      "Epoch 6739/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0036 - val_loss: 0.0020\n",
      "Epoch 6740/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0052 - val_loss: 0.0032\n",
      "Epoch 6741/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0066 - val_loss: 0.0205\n",
      "Epoch 6742/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0276 - val_loss: 0.1706\n",
      "Epoch 6743/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0474 - val_loss: 0.0233\n",
      "Epoch 6744/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0424 - val_loss: 0.0200\n",
      "Epoch 6745/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0594 - val_loss: 0.1425\n",
      "Epoch 6746/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0971 - val_loss: 0.0047\n",
      "Epoch 6747/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0243 - val_loss: 0.0075\n",
      "Epoch 6748/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0038 - val_loss: 0.0017\n",
      "Epoch 6749/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0027 - val_loss: 0.0018\n",
      "Epoch 6750/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0028 - val_loss: 0.0045\n",
      "Epoch 6751/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.0028 - val_loss: 0.0015\n",
      "Epoch 6752/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0033 - val_loss: 0.0075\n",
      "Epoch 6753/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0048 - val_loss: 0.0063\n",
      "Epoch 6754/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0049 - val_loss: 0.0027\n",
      "Epoch 6755/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.0053 - val_loss: 0.0073\n",
      "Epoch 6756/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0089 - val_loss: 0.0168\n",
      "Epoch 6757/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0249 - val_loss: 0.0810\n",
      "Epoch 6758/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0422 - val_loss: 0.0184\n",
      "Epoch 6759/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0649 - val_loss: 0.0270\n",
      "Epoch 6760/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0741 - val_loss: 0.0191\n",
      "Epoch 6761/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0389 - val_loss: 0.0014\n",
      "Epoch 6762/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0326 - val_loss: 0.0348\n",
      "Epoch 6763/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0476 - val_loss: 0.1402\n",
      "Epoch 6764/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0637 - val_loss: 0.0322\n",
      "Epoch 6765/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0483 - val_loss: 0.0703\n",
      "Epoch 6766/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0337 - val_loss: 0.0170\n",
      "Epoch 6767/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0186 - val_loss: 0.0113\n",
      "Epoch 6768/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0122 - val_loss: 0.0208\n",
      "Epoch 6769/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0462 - val_loss: 0.0990\n",
      "Epoch 6770/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0409 - val_loss: 0.0106\n",
      "Epoch 6771/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0393 - val_loss: 0.1103\n",
      "Epoch 6772/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0540 - val_loss: 0.0791\n",
      "Epoch 6773/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0633 - val_loss: 0.3387\n",
      "Epoch 6774/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1784 - val_loss: 0.2171\n",
      "Epoch 6775/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2375 - val_loss: 0.9852\n",
      "Epoch 6776/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3191 - val_loss: 0.1428\n",
      "Epoch 6777/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0872 - val_loss: 0.1007\n",
      "Epoch 6778/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0823 - val_loss: 0.1964\n",
      "Epoch 6779/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0631 - val_loss: 0.3426\n",
      "Epoch 6780/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1091 - val_loss: 0.3491\n",
      "Epoch 6781/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3173 - val_loss: 0.4498\n",
      "Epoch 6782/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2182 - val_loss: 0.2330\n",
      "Epoch 6783/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2443 - val_loss: 0.5598\n",
      "Epoch 6784/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2062 - val_loss: 0.0735\n",
      "Epoch 6785/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0400 - val_loss: 0.0013\n",
      "Epoch 6786/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0945 - val_loss: 0.2691\n",
      "Epoch 6787/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2435 - val_loss: 0.2013\n",
      "Epoch 6788/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2821 - val_loss: 0.2525\n",
      "Epoch 6789/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5590 - val_loss: 2.8018\n",
      "Epoch 6790/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8418 - val_loss: 1.0796\n",
      "Epoch 6791/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4127 - val_loss: 16.5655\n",
      "Epoch 6792/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 11.0976 - val_loss: 9.2768\n",
      "Epoch 6793/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 49.3582 - val_loss: 11.0363\n",
      "Epoch 6794/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 91.3126 - val_loss: 113.0968\n",
      "Epoch 6795/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 53.2599 - val_loss: 82.4904\n",
      "Epoch 6796/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 117.2606 - val_loss: 101.4485\n",
      "Epoch 6797/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 19.8330 - val_loss: 32.9664\n",
      "Epoch 6798/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 40.5192 - val_loss: 19.4735\n",
      "Epoch 6799/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 15.8603 - val_loss: 19.6941\n",
      "Epoch 6800/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.0419 - val_loss: 2.0781\n",
      "Epoch 6801/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.4526 - val_loss: 0.5139\n",
      "Epoch 6802/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6162 - val_loss: 2.7394\n",
      "Epoch 6803/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.9375 - val_loss: 1.9623\n",
      "Epoch 6804/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 10.1382 - val_loss: 3.1167\n",
      "Epoch 6805/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.8980 - val_loss: 9.3948\n",
      "Epoch 6806/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 12.1529 - val_loss: 75.0450\n",
      "Epoch 6807/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 104.6848 - val_loss: 60.2782\n",
      "Epoch 6808/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 225.5939 - val_loss: 642.0505\n",
      "Epoch 6809/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1366.1491 - val_loss: 213.1893\n",
      "Epoch 6810/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 543.9703 - val_loss: 824.5178\n",
      "Epoch 6811/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4616.3492 - val_loss: 5891.4075\n",
      "Epoch 6812/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 9023.0404 - val_loss: 15540.2838\n",
      "Epoch 6813/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10786.0926 - val_loss: 4145.6521\n",
      "Epoch 6814/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 10509.6582 - val_loss: 3823.1744\n",
      "Epoch 6815/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 13143.2774 - val_loss: 4476.9798\n",
      "Epoch 6816/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 5505.5463 - val_loss: 3209.0713\n",
      "Epoch 6817/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 4064.1957 - val_loss: 3261.8687\n",
      "Epoch 6818/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1909.8329 - val_loss: 1028.4234\n",
      "Epoch 6819/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 910.0556 - val_loss: 738.6311\n",
      "Epoch 6820/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 402.2224 - val_loss: 35.2641\n",
      "Epoch 6821/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 137.9393 - val_loss: 53.1238\n",
      "Epoch 6822/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 84.5711 - val_loss: 89.1686\n",
      "Epoch 6823/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 47.0642 - val_loss: 197.6028\n",
      "Epoch 6824/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 78.6824 - val_loss: 103.4326\n",
      "Epoch 6825/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 20.8866 - val_loss: 24.9608\n",
      "Epoch 6826/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 26.0859 - val_loss: 24.9144\n",
      "Epoch 6827/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 11.2097 - val_loss: 12.7157\n",
      "Epoch 6828/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 7.8201 - val_loss: 4.6030\n",
      "Epoch 6829/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.3460 - val_loss: 9.3541\n",
      "Epoch 6830/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.1363 - val_loss: 7.5734\n",
      "Epoch 6831/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.3101 - val_loss: 0.8714\n",
      "Epoch 6832/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1.6761 - val_loss: 2.6727\n",
      "Epoch 6833/10000\n",
      "68/68 [==============================] - 0s 838us/sample - loss: 1.0146 - val_loss: 1.3503\n",
      "Epoch 6834/10000\n",
      "68/68 [==============================] - 0s 647us/sample - loss: 0.8977 - val_loss: 1.6346\n",
      "Epoch 6835/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.9673 - val_loss: 0.4117\n",
      "Epoch 6836/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 0.5833 - val_loss: 0.4651\n",
      "Epoch 6837/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2240 - val_loss: 0.3908\n",
      "Epoch 6838/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1316 - val_loss: 0.0845\n",
      "Epoch 6839/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0599 - val_loss: 0.0244\n",
      "Epoch 6840/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0179 - val_loss: 0.0384\n",
      "Epoch 6841/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 0.0331 - val_loss: 0.0481\n",
      "Epoch 6842/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0327 - val_loss: 0.0185\n",
      "Epoch 6843/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0137 - val_loss: 0.0184\n",
      "Epoch 6844/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0092 - val_loss: 0.0382\n",
      "Epoch 6845/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.0242 - val_loss: 0.0165\n",
      "Epoch 6846/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0111 - val_loss: 0.0021\n",
      "Epoch 6847/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0024 - val_loss: 0.0055\n",
      "Epoch 6848/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 6849/10000\n",
      "68/68 [==============================] - 0s 853us/sample - loss: 0.0013 - val_loss: 7.7821e-04\n",
      "Epoch 6850/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.0435e-04 - val_loss: 5.3059e-04\n",
      "Epoch 6851/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 7.3149e-04 - val_loss: 2.9591e-04\n",
      "Epoch 6852/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.3510e-04 - val_loss: 6.7848e-04\n",
      "Epoch 6853/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.8147e-04 - val_loss: 4.8091e-05\n",
      "Epoch 6854/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.5187e-05 - val_loss: 5.1067e-05\n",
      "Epoch 6855/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.7952e-05 - val_loss: 6.6940e-05\n",
      "Epoch 6856/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.9098e-05 - val_loss: 4.2663e-05\n",
      "Epoch 6857/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.1491e-05 - val_loss: 1.4300e-04\n",
      "Epoch 6858/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 1.0964e-04 - val_loss: 4.8525e-05\n",
      "Epoch 6859/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.1259e-05 - val_loss: 7.2087e-05\n",
      "Epoch 6860/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.8810e-05 - val_loss: 7.7416e-05\n",
      "Epoch 6861/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1689e-04 - val_loss: 7.3643e-05\n",
      "Epoch 6862/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 1.1414e-04 - val_loss: 1.4910e-04\n",
      "Epoch 6863/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.3311e-04 - val_loss: 5.0938e-04\n",
      "Epoch 6864/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.3529e-04 - val_loss: 6.3546e-05\n",
      "Epoch 6865/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 4.7550e-04 - val_loss: 3.0146e-05\n",
      "Epoch 6866/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.6013e-04 - val_loss: 1.0410e-04\n",
      "Epoch 6867/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.6440e-04 - val_loss: 4.0545e-05\n",
      "Epoch 6868/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.3030e-05 - val_loss: 5.8329e-05\n",
      "Epoch 6869/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.5390e-05 - val_loss: 7.7311e-05\n",
      "Epoch 6870/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.4190e-05 - val_loss: 2.5345e-04\n",
      "Epoch 6871/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 2.5617e-04 - val_loss: 2.8324e-04\n",
      "Epoch 6872/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3.0564e-04 - val_loss: 4.3342e-04\n",
      "Epoch 6873/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2921e-04 - val_loss: 3.1327e-04\n",
      "Epoch 6874/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.5784e-04 - val_loss: 2.3814e-04\n",
      "Epoch 6875/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.2908e-04 - val_loss: 3.5973e-04\n",
      "Epoch 6876/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3.4685e-04 - val_loss: 1.4287e-04\n",
      "Epoch 6877/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.2069e-04 - val_loss: 7.6201e-05\n",
      "Epoch 6878/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.5630e-04 - val_loss: 2.0449e-04\n",
      "Epoch 6879/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.6639e-05 - val_loss: 1.1543e-04\n",
      "Epoch 6880/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.0599e-05 - val_loss: 2.8598e-05\n",
      "Epoch 6881/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 6.1629e-05 - val_loss: 6.8779e-05\n",
      "Epoch 6882/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.3301e-04 - val_loss: 6.8723e-05\n",
      "Epoch 6883/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.6179e-05 - val_loss: 4.1879e-05\n",
      "Epoch 6884/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.3341e-05 - val_loss: 2.7799e-05\n",
      "Epoch 6885/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.3456e-05 - val_loss: 3.3918e-05\n",
      "Epoch 6886/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.0658e-05 - val_loss: 8.9827e-05\n",
      "Epoch 6887/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 9.1135e-05 - val_loss: 1.7139e-04\n",
      "Epoch 6888/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.6350e-04 - val_loss: 3.4383e-04\n",
      "Epoch 6889/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1085e-04 - val_loss: 3.5430e-05\n",
      "Epoch 6890/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 6.9034e-05 - val_loss: 1.7346e-04\n",
      "Epoch 6891/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.6160e-04 - val_loss: 8.4348e-05\n",
      "Epoch 6892/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.6389e-05 - val_loss: 2.3495e-05\n",
      "Epoch 6893/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8798e-05 - val_loss: 3.1208e-05\n",
      "Epoch 6894/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.6686e-05 - val_loss: 1.7311e-05\n",
      "Epoch 6895/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0496e-04 - val_loss: 3.3958e-05\n",
      "Epoch 6896/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2877e-04 - val_loss: 1.0288e-04\n",
      "Epoch 6897/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.0236e-05 - val_loss: 3.5150e-05\n",
      "Epoch 6898/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.7659e-05 - val_loss: 2.4285e-05\n",
      "Epoch 6899/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.1051e-05 - val_loss: 7.5991e-05\n",
      "Epoch 6900/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.3092e-05 - val_loss: 5.8516e-05\n",
      "Epoch 6901/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.2886e-05 - val_loss: 2.1222e-04\n",
      "Epoch 6902/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.9163e-04 - val_loss: 2.5077e-04\n",
      "Epoch 6903/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.3541e-05 - val_loss: 3.2509e-05\n",
      "Epoch 6904/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.5126e-05 - val_loss: 3.0745e-05\n",
      "Epoch 6905/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.5175e-05 - val_loss: 8.5206e-05\n",
      "Epoch 6906/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.1566e-05 - val_loss: 4.7312e-05\n",
      "Epoch 6907/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.5538e-05 - val_loss: 4.2010e-05\n",
      "Epoch 6908/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8192e-05 - val_loss: 1.7439e-04\n",
      "Epoch 6909/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0069e-04 - val_loss: 6.7621e-05\n",
      "Epoch 6910/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.4123e-05 - val_loss: 5.0651e-05\n",
      "Epoch 6911/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.5149e-05 - val_loss: 1.7769e-04\n",
      "Epoch 6912/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6189e-04 - val_loss: 8.8461e-05\n",
      "Epoch 6913/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.5366e-04 - val_loss: 1.9809e-05\n",
      "Epoch 6914/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.5208e-05 - val_loss: 4.2381e-05\n",
      "Epoch 6915/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.6913e-05 - val_loss: 3.4720e-05\n",
      "Epoch 6916/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.7866e-05 - val_loss: 1.7281e-05\n",
      "Epoch 6917/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.3176e-05 - val_loss: 2.3073e-05\n",
      "Epoch 6918/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.3190e-05 - val_loss: 1.4878e-04\n",
      "Epoch 6919/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.0696e-04 - val_loss: 2.8734e-04\n",
      "Epoch 6920/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.9205e-04 - val_loss: 7.8406e-04\n",
      "Epoch 6921/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.4937e-04 - val_loss: 5.0553e-04\n",
      "Epoch 6922/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.0470e-04 - val_loss: 5.3557e-05\n",
      "Epoch 6923/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.0114e-04 - val_loss: 0.0011\n",
      "Epoch 6924/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.5491e-04 - val_loss: 0.0036\n",
      "Epoch 6925/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0012 - val_loss: 2.6949e-04\n",
      "Epoch 6926/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.1929e-04 - val_loss: 5.9435e-05\n",
      "Epoch 6927/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.5106e-04 - val_loss: 4.2698e-04\n",
      "Epoch 6928/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0011 - val_loss: 0.0026\n",
      "Epoch 6929/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0011 - val_loss: 3.8827e-04\n",
      "Epoch 6930/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.0774e-04 - val_loss: 5.8494e-04\n",
      "Epoch 6931/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6457e-04 - val_loss: 3.9297e-04\n",
      "Epoch 6932/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8905e-04 - val_loss: 2.8565e-04\n",
      "Epoch 6933/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8463e-04 - val_loss: 1.8524e-04\n",
      "Epoch 6934/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9314e-04 - val_loss: 7.8313e-05\n",
      "Epoch 6935/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.3557e-04 - val_loss: 8.5920e-04\n",
      "Epoch 6936/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.5577e-04 - val_loss: 0.0024\n",
      "Epoch 6937/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0015 - val_loss: 3.5357e-04\n",
      "Epoch 6938/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0027 - val_loss: 0.0120\n",
      "Epoch 6939/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0100 - val_loss: 0.0217\n",
      "Epoch 6940/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0118 - val_loss: 0.0019\n",
      "Epoch 6941/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0090 - val_loss: 0.0203\n",
      "Epoch 6942/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0095 - val_loss: 0.0071\n",
      "Epoch 6943/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0039 - val_loss: 0.0024\n",
      "Epoch 6944/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 6945/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 6946/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0016 - val_loss: 6.8552e-04\n",
      "Epoch 6947/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.9204e-04 - val_loss: 0.0013\n",
      "Epoch 6948/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.6838e-04 - val_loss: 8.2691e-04\n",
      "Epoch 6949/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.0925e-04 - val_loss: 2.5936e-04\n",
      "Epoch 6950/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.6930e-04 - val_loss: 5.3933e-04\n",
      "Epoch 6951/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.7641e-04 - val_loss: 6.4411e-04\n",
      "Epoch 6952/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 6953/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.0389e-04 - val_loss: 1.9225e-04\n",
      "Epoch 6954/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2771e-04 - val_loss: 4.9714e-04\n",
      "Epoch 6955/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6958e-04 - val_loss: 1.9913e-04\n",
      "Epoch 6956/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.9440e-05 - val_loss: 6.4599e-05\n",
      "Epoch 6957/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.8593e-05 - val_loss: 2.3731e-05\n",
      "Epoch 6958/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.2578e-04 - val_loss: 2.2605e-04\n",
      "Epoch 6959/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.0094e-04 - val_loss: 9.2314e-05\n",
      "Epoch 6960/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9633e-04 - val_loss: 3.2393e-04\n",
      "Epoch 6961/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2952e-04 - val_loss: 1.9189e-05\n",
      "Epoch 6962/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.9154e-05 - val_loss: 1.2421e-04\n",
      "Epoch 6963/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.0018e-05 - val_loss: 3.4953e-05\n",
      "Epoch 6964/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.9700e-05 - val_loss: 7.4103e-05\n",
      "Epoch 6965/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.8791e-04 - val_loss: 1.2072e-04\n",
      "Epoch 6966/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9649e-04 - val_loss: 2.9711e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6967/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8688e-04 - val_loss: 2.9187e-05\n",
      "Epoch 6968/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.4498e-05 - val_loss: 1.1451e-04\n",
      "Epoch 6969/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1.6379e-04 - val_loss: 2.2465e-04\n",
      "Epoch 6970/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.0782e-04 - val_loss: 3.1698e-04\n",
      "Epoch 6971/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1441e-04 - val_loss: 5.0733e-05\n",
      "Epoch 6972/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.1443e-04 - val_loss: 0.0010\n",
      "Epoch 6973/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.6226e-04 - val_loss: 4.6387e-05\n",
      "Epoch 6974/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.6318e-05 - val_loss: 1.5362e-04\n",
      "Epoch 6975/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.9129e-05 - val_loss: 8.1253e-05\n",
      "Epoch 6976/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.2877e-05 - val_loss: 3.3877e-05\n",
      "Epoch 6977/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.4600e-05 - val_loss: 1.6865e-05\n",
      "Epoch 6978/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.6704e-05 - val_loss: 3.1030e-05\n",
      "Epoch 6979/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 3.7606e-05 - val_loss: 5.4031e-05\n",
      "Epoch 6980/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.3218e-05 - val_loss: 7.4142e-05\n",
      "Epoch 6981/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 4.5321e-05 - val_loss: 1.8058e-05\n",
      "Epoch 6982/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.3190e-05 - val_loss: 5.1731e-05\n",
      "Epoch 6983/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.3627e-05 - val_loss: 3.7778e-05\n",
      "Epoch 6984/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.2131e-04 - val_loss: 1.6562e-04\n",
      "Epoch 6985/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 1.0164e-04 - val_loss: 3.9633e-05\n",
      "Epoch 6986/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.1492e-05 - val_loss: 5.8433e-05\n",
      "Epoch 6987/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.4444e-04 - val_loss: 1.7570e-04\n",
      "Epoch 6988/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.0429e-04 - val_loss: 2.0536e-04\n",
      "Epoch 6989/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.1021e-04 - val_loss: 3.4216e-05\n",
      "Epoch 6990/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.3717e-04 - val_loss: 5.9506e-05\n",
      "Epoch 6991/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9102e-04 - val_loss: 1.3524e-04\n",
      "Epoch 6992/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.5271e-05 - val_loss: 8.0017e-05\n",
      "Epoch 6993/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 5.1793e-05 - val_loss: 6.0255e-05\n",
      "Epoch 6994/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.9133e-05 - val_loss: 7.9055e-05\n",
      "Epoch 6995/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.5971e-05 - val_loss: 9.9587e-05\n",
      "Epoch 6996/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 9.1948e-05 - val_loss: 1.0827e-04\n",
      "Epoch 6997/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 4.6252e-05 - val_loss: 1.1559e-04\n",
      "Epoch 6998/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2499e-04 - val_loss: 8.1215e-05\n",
      "Epoch 6999/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.0526e-04 - val_loss: 2.1393e-04\n",
      "Epoch 7000/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 1.9725e-04 - val_loss: 6.0020e-04\n",
      "Epoch 7001/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.0542e-04 - val_loss: 0.0012\n",
      "Epoch 7002/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0041 - val_loss: 0.0072\n",
      "Epoch 7003/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0034 - val_loss: 0.0027\n",
      "Epoch 7004/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0010 - val_loss: 5.7852e-05\n",
      "Epoch 7005/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.4736e-04 - val_loss: 5.6684e-04\n",
      "Epoch 7006/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.7588e-04 - val_loss: 8.3510e-04\n",
      "Epoch 7007/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.2910e-04 - val_loss: 2.1224e-04\n",
      "Epoch 7008/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.2850e-04 - val_loss: 1.1221e-04\n",
      "Epoch 7009/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2954e-04 - val_loss: 8.2725e-04\n",
      "Epoch 7010/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 2.7036e-04 - val_loss: 3.4869e-05\n",
      "Epoch 7011/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2370e-04 - val_loss: 8.0926e-05\n",
      "Epoch 7012/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.9526e-05 - val_loss: 2.9651e-04\n",
      "Epoch 7013/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6531e-04 - val_loss: 2.5800e-04\n",
      "Epoch 7014/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 1.9250e-04 - val_loss: 5.8150e-05\n",
      "Epoch 7015/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.7482e-05 - val_loss: 8.2216e-05\n",
      "Epoch 7016/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 8.8518e-05 - val_loss: 6.7521e-05\n",
      "Epoch 7017/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4558e-04 - val_loss: 3.7873e-04\n",
      "Epoch 7018/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.4213e-04 - val_loss: 0.0013\n",
      "Epoch 7019/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.0312e-04 - val_loss: 0.0019\n",
      "Epoch 7020/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.5609e-04 - val_loss: 8.2289e-04\n",
      "Epoch 7021/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 2.8716e-04 - val_loss: 1.2253e-04\n",
      "Epoch 7022/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.5960e-04 - val_loss: 1.0278e-04\n",
      "Epoch 7023/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 1.0203e-04 - val_loss: 1.2369e-05\n",
      "Epoch 7024/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 1.1962e-04 - val_loss: 9.1229e-05\n",
      "Epoch 7025/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.2518e-04 - val_loss: 1.8024e-04\n",
      "Epoch 7026/10000\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 3.2623e-04 - val_loss: 8.2127e-05\n",
      "Epoch 7027/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 1.7506e-04 - val_loss: 4.5425e-05\n",
      "Epoch 7028/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 4.1787e-05 - val_loss: 6.4715e-06\n",
      "Epoch 7029/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 8.2927e-05 - val_loss: 3.4833e-04\n",
      "Epoch 7030/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.7078e-04 - val_loss: 2.2654e-04\n",
      "Epoch 7031/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.6976e-04 - val_loss: 3.3345e-04\n",
      "Epoch 7032/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.8244e-04 - val_loss: 2.7709e-04\n",
      "Epoch 7033/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.5745e-04 - val_loss: 3.7455e-05\n",
      "Epoch 7034/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.2191e-04 - val_loss: 8.7057e-04\n",
      "Epoch 7035/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.2272e-04 - val_loss: 2.5743e-04\n",
      "Epoch 7036/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8386e-04 - val_loss: 5.5448e-05\n",
      "Epoch 7037/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.1298e-04 - val_loss: 0.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7038/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 7039/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 7040/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 7041/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0149 - val_loss: 0.0777\n",
      "Epoch 7042/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0380 - val_loss: 0.0033\n",
      "Epoch 7043/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0179 - val_loss: 0.0131\n",
      "Epoch 7044/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0122 - val_loss: 0.0111\n",
      "Epoch 7045/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0367 - val_loss: 0.0883\n",
      "Epoch 7046/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1598 - val_loss: 0.6473\n",
      "Epoch 7047/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.7077 - val_loss: 0.8359\n",
      "Epoch 7048/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1913 - val_loss: 0.0045\n",
      "Epoch 7049/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1076 - val_loss: 0.3946\n",
      "Epoch 7050/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1167 - val_loss: 0.1905\n",
      "Epoch 7051/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.6712 - val_loss: 0.8487\n",
      "Epoch 7052/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 1.0241 - val_loss: 0.1859\n",
      "Epoch 7053/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4912 - val_loss: 1.0219\n",
      "Epoch 7054/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.0902 - val_loss: 0.4794\n",
      "Epoch 7055/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1626 - val_loss: 0.1076\n",
      "Epoch 7056/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1643 - val_loss: 0.1062\n",
      "Epoch 7057/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0215 - val_loss: 0.0110\n",
      "Epoch 7058/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0208 - val_loss: 0.0023\n",
      "Epoch 7059/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0243 - val_loss: 0.1025\n",
      "Epoch 7060/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0381 - val_loss: 0.0320\n",
      "Epoch 7061/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0179 - val_loss: 0.0362\n",
      "Epoch 7062/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0165 - val_loss: 0.0235\n",
      "Epoch 7063/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0296 - val_loss: 2.1167e-04\n",
      "Epoch 7064/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0359 - val_loss: 0.0377\n",
      "Epoch 7065/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0331 - val_loss: 0.1561\n",
      "Epoch 7066/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0541 - val_loss: 0.0952\n",
      "Epoch 7067/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0962 - val_loss: 0.4708\n",
      "Epoch 7068/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1770 - val_loss: 0.0822\n",
      "Epoch 7069/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4373 - val_loss: 1.2610\n",
      "Epoch 7070/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.8748 - val_loss: 7.7750\n",
      "Epoch 7071/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 20.8011 - val_loss: 3.6210\n",
      "Epoch 7072/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 40.7451 - val_loss: 184.0626\n",
      "Epoch 7073/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 114.6497 - val_loss: 172.0857\n",
      "Epoch 7074/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 367.4793 - val_loss: 100.9545\n",
      "Epoch 7075/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2996.4080 - val_loss: 4417.4834\n",
      "Epoch 7076/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3498.2337 - val_loss: 8464.4221\n",
      "Epoch 7077/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 10382.3190 - val_loss: 4908.5340\n",
      "Epoch 7078/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11033.9826 - val_loss: 13370.5874\n",
      "Epoch 7079/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6271.7645 - val_loss: 6653.2129\n",
      "Epoch 7080/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3272.0792 - val_loss: 3348.7565\n",
      "Epoch 7081/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1249.1107 - val_loss: 1615.0726\n",
      "Epoch 7082/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 486.0794 - val_loss: 184.4564\n",
      "Epoch 7083/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 80.1391 - val_loss: 68.3751\n",
      "Epoch 7084/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 47.3250 - val_loss: 66.7756\n",
      "Epoch 7085/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 53.9018 - val_loss: 25.4297\n",
      "Epoch 7086/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 21.0177 - val_loss: 2.4916\n",
      "Epoch 7087/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 14.2790 - val_loss: 3.7400\n",
      "Epoch 7088/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.8801 - val_loss: 2.7720\n",
      "Epoch 7089/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6081 - val_loss: 5.5743\n",
      "Epoch 7090/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.0659 - val_loss: 1.1204\n",
      "Epoch 7091/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6487 - val_loss: 0.0591\n",
      "Epoch 7092/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1813 - val_loss: 0.0485\n",
      "Epoch 7093/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1168 - val_loss: 0.0522\n",
      "Epoch 7094/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1941 - val_loss: 0.0545\n",
      "Epoch 7095/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3237 - val_loss: 0.0396\n",
      "Epoch 7096/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3829 - val_loss: 0.0802\n",
      "Epoch 7097/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3595 - val_loss: 0.3520\n",
      "Epoch 7098/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2451 - val_loss: 0.2212\n",
      "Epoch 7099/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4564 - val_loss: 0.2038\n",
      "Epoch 7100/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4144 - val_loss: 0.3035\n",
      "Epoch 7101/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3105 - val_loss: 0.4411\n",
      "Epoch 7102/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3880 - val_loss: 0.0655\n",
      "Epoch 7103/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1591 - val_loss: 0.0711\n",
      "Epoch 7104/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1634 - val_loss: 0.3464\n",
      "Epoch 7105/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7440 - val_loss: 0.7435\n",
      "Epoch 7106/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8312 - val_loss: 1.4990\n",
      "Epoch 7107/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6520 - val_loss: 1.4028\n",
      "Epoch 7108/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5010 - val_loss: 0.5507\n",
      "Epoch 7109/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2182 - val_loss: 0.0263\n",
      "Epoch 7110/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0861 - val_loss: 0.0284\n",
      "Epoch 7111/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0785 - val_loss: 0.0753\n",
      "Epoch 7112/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0606 - val_loss: 0.0608\n",
      "Epoch 7113/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0636 - val_loss: 0.1549\n",
      "Epoch 7114/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1649 - val_loss: 0.3365\n",
      "Epoch 7115/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2430 - val_loss: 0.6288\n",
      "Epoch 7116/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5152 - val_loss: 0.9647\n",
      "Epoch 7117/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.7393 - val_loss: 0.3432\n",
      "Epoch 7118/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.3142 - val_loss: 3.6537\n",
      "Epoch 7119/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 6.2565 - val_loss: 10.2537\n",
      "Epoch 7120/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 6.2035 - val_loss: 7.6760\n",
      "Epoch 7121/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.7672 - val_loss: 0.2065\n",
      "Epoch 7122/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.6241 - val_loss: 0.0910\n",
      "Epoch 7123/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2948 - val_loss: 0.4930\n",
      "Epoch 7124/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.8313 - val_loss: 0.6952\n",
      "Epoch 7125/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3733 - val_loss: 0.3686\n",
      "Epoch 7126/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3628 - val_loss: 1.6620\n",
      "Epoch 7127/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.0058 - val_loss: 0.6049\n",
      "Epoch 7128/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5194 - val_loss: 0.1915\n",
      "Epoch 7129/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.7698 - val_loss: 2.1403\n",
      "Epoch 7130/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9871 - val_loss: 0.2653\n",
      "Epoch 7131/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4211 - val_loss: 0.5340\n",
      "Epoch 7132/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2328 - val_loss: 0.3514\n",
      "Epoch 7133/10000\n",
      "68/68 [==============================] - 0s 765us/sample - loss: 0.3952 - val_loss: 0.4893\n",
      "Epoch 7134/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.3878 - val_loss: 0.2102\n",
      "Epoch 7135/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3986 - val_loss: 0.4423\n",
      "Epoch 7136/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5268 - val_loss: 0.1862\n",
      "Epoch 7137/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.5255 - val_loss: 0.1340\n",
      "Epoch 7138/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.3926 - val_loss: 2.9434\n",
      "Epoch 7139/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3.2438 - val_loss: 0.7125\n",
      "Epoch 7140/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7521 - val_loss: 1.1227\n",
      "Epoch 7141/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6851 - val_loss: 0.9169\n",
      "Epoch 7142/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 0.2921 - val_loss: 0.4187\n",
      "Epoch 7143/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2260 - val_loss: 0.3794\n",
      "Epoch 7144/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.371 - 0s 162us/sample - loss: 0.2130 - val_loss: 0.1019\n",
      "Epoch 7145/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9002 - val_loss: 0.7929\n",
      "Epoch 7146/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.6266 - val_loss: 0.1995\n",
      "Epoch 7147/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0437 - val_loss: 1.4316\n",
      "Epoch 7148/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.3179 - val_loss: 2.5887\n",
      "Epoch 7149/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.3089 - val_loss: 1.7427\n",
      "Epoch 7150/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6495 - val_loss: 0.1828\n",
      "Epoch 7151/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1342 - val_loss: 0.2506\n",
      "Epoch 7152/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3811 - val_loss: 0.1705\n",
      "Epoch 7153/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2966 - val_loss: 0.3785\n",
      "Epoch 7154/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2324 - val_loss: 0.5176\n",
      "Epoch 7155/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3501 - val_loss: 1.1715\n",
      "Epoch 7156/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.5807 - val_loss: 0.2107\n",
      "Epoch 7157/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.0585 - val_loss: 0.5959\n",
      "Epoch 7158/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.3554 - val_loss: 0.3336\n",
      "Epoch 7159/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.2438 - val_loss: 0.5291\n",
      "Epoch 7160/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.2142 - val_loss: 0.0243\n",
      "Epoch 7161/10000\n",
      "68/68 [==============================] - 0s 647us/sample - loss: 0.0681 - val_loss: 0.1204\n",
      "Epoch 7162/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1878 - val_loss: 0.0197\n",
      "Epoch 7163/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1269 - val_loss: 0.2148\n",
      "Epoch 7164/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0698 - val_loss: 0.0111\n",
      "Epoch 7165/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.0760 - val_loss: 0.0939\n",
      "Epoch 7166/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0891 - val_loss: 0.0126\n",
      "Epoch 7167/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0311 - val_loss: 0.0469\n",
      "Epoch 7168/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0309 - val_loss: 0.0160\n",
      "Epoch 7169/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0229 - val_loss: 0.0288\n",
      "Epoch 7170/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0230 - val_loss: 0.0054\n",
      "Epoch 7171/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0164 - val_loss: 0.0062\n",
      "Epoch 7172/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0082 - val_loss: 0.0069\n",
      "Epoch 7173/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0109 - val_loss: 0.0140\n",
      "Epoch 7174/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.0150 - val_loss: 0.0519\n",
      "Epoch 7175/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0353 - val_loss: 0.0486\n",
      "Epoch 7176/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0426 - val_loss: 0.0393\n",
      "Epoch 7177/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0450 - val_loss: 0.0378\n",
      "Epoch 7178/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.0672 - val_loss: 0.0935\n",
      "Epoch 7179/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0399 - val_loss: 0.0329\n",
      "Epoch 7180/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0125 - val_loss: 0.0104\n",
      "Epoch 7181/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0235 - val_loss: 0.0066\n",
      "Epoch 7182/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0292 - val_loss: 0.0263\n",
      "Epoch 7183/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0189 - val_loss: 0.0567\n",
      "Epoch 7184/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0169 - val_loss: 0.0085\n",
      "Epoch 7185/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0075 - val_loss: 0.0063\n",
      "Epoch 7186/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0051 - val_loss: 0.0037\n",
      "Epoch 7187/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0098 - val_loss: 0.0304\n",
      "Epoch 7188/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0333 - val_loss: 0.0100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7189/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0313 - val_loss: 0.0171\n",
      "Epoch 7190/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0243 - val_loss: 0.0103\n",
      "Epoch 7191/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0195 - val_loss: 0.0176\n",
      "Epoch 7192/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0353 - val_loss: 0.0407\n",
      "Epoch 7193/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0456 - val_loss: 0.3265\n",
      "Epoch 7194/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2214 - val_loss: 0.2143\n",
      "Epoch 7195/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2229 - val_loss: 0.2127\n",
      "Epoch 7196/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3603 - val_loss: 0.1948\n",
      "Epoch 7197/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0845 - val_loss: 0.1312\n",
      "Epoch 7198/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0582 - val_loss: 0.1187\n",
      "Epoch 7199/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0584 - val_loss: 0.1697\n",
      "Epoch 7200/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0854 - val_loss: 0.0096\n",
      "Epoch 7201/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.0669 - val_loss: 0.2520\n",
      "Epoch 7202/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1383 - val_loss: 0.3983\n",
      "Epoch 7203/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1570 - val_loss: 0.4659\n",
      "Epoch 7204/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2320 - val_loss: 0.0465\n",
      "Epoch 7205/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1267 - val_loss: 0.0302\n",
      "Epoch 7206/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0770 - val_loss: 0.3884\n",
      "Epoch 7207/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2359 - val_loss: 0.1665\n",
      "Epoch 7208/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 0.0783 - val_loss: 0.1973\n",
      "Epoch 7209/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0742 - val_loss: 0.1411\n",
      "Epoch 7210/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1010 - val_loss: 0.2280\n",
      "Epoch 7211/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.1901 - val_loss: 0.5195\n",
      "Epoch 7212/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.1870 - val_loss: 0.0891\n",
      "Epoch 7213/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0393 - val_loss: 0.0236\n",
      "Epoch 7214/10000\n",
      "68/68 [==============================] - 0s 750us/sample - loss: 0.0383 - val_loss: 0.1522\n",
      "Epoch 7215/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0509 - val_loss: 0.1235\n",
      "Epoch 7216/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1059 - val_loss: 0.1686\n",
      "Epoch 7217/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.1088 - val_loss: 0.0829\n",
      "Epoch 7218/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.1135 - val_loss: 0.2025\n",
      "Epoch 7219/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1733 - val_loss: 0.1708\n",
      "Epoch 7220/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.2457 - val_loss: 0.0161\n",
      "Epoch 7221/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1803 - val_loss: 0.2723\n",
      "Epoch 7222/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.2852 - val_loss: 1.2478\n",
      "Epoch 7223/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1913 - val_loss: 0.0485\n",
      "Epoch 7224/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1049 - val_loss: 0.3652\n",
      "Epoch 7225/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2046 - val_loss: 0.0350\n",
      "Epoch 7226/10000\n",
      "68/68 [==============================] - 0s 706us/sample - loss: 0.0403 - val_loss: 0.0795\n",
      "Epoch 7227/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0341 - val_loss: 0.0967\n",
      "Epoch 7228/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0379 - val_loss: 0.0630\n",
      "Epoch 7229/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0295 - val_loss: 0.0317\n",
      "Epoch 7230/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 0.0450 - val_loss: 0.0204\n",
      "Epoch 7231/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0399 - val_loss: 0.0480\n",
      "Epoch 7232/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0217 - val_loss: 0.0274\n",
      "Epoch 7233/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0337 - val_loss: 0.1573\n",
      "Epoch 7234/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0866 - val_loss: 0.0563\n",
      "Epoch 7235/10000\n",
      "68/68 [==============================] - 0s 559us/sample - loss: 0.0437 - val_loss: 0.0022\n",
      "Epoch 7236/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0467 - val_loss: 0.1811\n",
      "Epoch 7237/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0740 - val_loss: 0.2576\n",
      "Epoch 7238/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1053 - val_loss: 0.0907\n",
      "Epoch 7239/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.0337 - val_loss: 0.0046\n",
      "Epoch 7240/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 7241/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 7242/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 7243/10000\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 7244/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0042 - val_loss: 0.0015\n",
      "Epoch 7245/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0096 - val_loss: 0.0424\n",
      "Epoch 7246/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0136 - val_loss: 0.0063\n",
      "Epoch 7247/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0063 - val_loss: 0.0103\n",
      "Epoch 7248/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0123 - val_loss: 0.0363\n",
      "Epoch 7249/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0208 - val_loss: 0.0052\n",
      "Epoch 7250/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0091 - val_loss: 0.0080\n",
      "Epoch 7251/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0103 - val_loss: 0.0115\n",
      "Epoch 7252/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0213 - val_loss: 0.0932\n",
      "Epoch 7253/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1014 - val_loss: 0.5233\n",
      "Epoch 7254/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1755 - val_loss: 0.0093\n",
      "Epoch 7255/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1095 - val_loss: 0.0281\n",
      "Epoch 7256/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1386 - val_loss: 2.0188\n",
      "Epoch 7257/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.8570 - val_loss: 2.4399\n",
      "Epoch 7258/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.9943 - val_loss: 4.6937\n",
      "Epoch 7259/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.1471 - val_loss: 8.6164\n",
      "Epoch 7260/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.1355 - val_loss: 2.5507\n",
      "Epoch 7261/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.6763 - val_loss: 16.3271\n",
      "Epoch 7262/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 94.7279 - val_loss: 13.5876\n",
      "Epoch 7263/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 503.5993 - val_loss: 743.0226\n",
      "Epoch 7264/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2075.0595 - val_loss: 2271.2917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7265/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5721.6631 - val_loss: 5132.0701\n",
      "Epoch 7266/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1890.2192 - val_loss: 1467.7238\n",
      "Epoch 7267/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2859.6784 - val_loss: 7714.2808\n",
      "Epoch 7268/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6786.2153 - val_loss: 3175.9579\n",
      "Epoch 7269/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5979.6670 - val_loss: 8983.2331\n",
      "Epoch 7270/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3529.6309 - val_loss: 6026.1448\n",
      "Epoch 7271/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4629.2758 - val_loss: 13070.5107\n",
      "Epoch 7272/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5473.5776 - val_loss: 4745.8447\n",
      "Epoch 7273/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2997.7336 - val_loss: 5038.3987\n",
      "Epoch 7274/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2577.7146 - val_loss: 1714.5105\n",
      "Epoch 7275/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 871.2880 - val_loss: 588.5547\n",
      "Epoch 7276/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 370.5243 - val_loss: 39.2020\n",
      "Epoch 7277/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 131.3401 - val_loss: 13.3736\n",
      "Epoch 7278/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 32.8391 - val_loss: 3.2627\n",
      "Epoch 7279/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 24.1872 - val_loss: 15.7107\n",
      "Epoch 7280/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 24.1307 - val_loss: 8.4572\n",
      "Epoch 7281/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16.2299 - val_loss: 8.3228\n",
      "Epoch 7282/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.8509 - val_loss: 2.4773\n",
      "Epoch 7283/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.5315 - val_loss: 5.9250\n",
      "Epoch 7284/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.5545 - val_loss: 2.2613\n",
      "Epoch 7285/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2157 - val_loss: 1.5060\n",
      "Epoch 7286/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1746 - val_loss: 2.5514\n",
      "Epoch 7287/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.8024 - val_loss: 0.5135\n",
      "Epoch 7288/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4045 - val_loss: 0.6407\n",
      "Epoch 7289/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2784 - val_loss: 0.3098\n",
      "Epoch 7290/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4262 - val_loss: 0.1301\n",
      "Epoch 7291/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2383 - val_loss: 0.5825\n",
      "Epoch 7292/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2175 - val_loss: 0.1838\n",
      "Epoch 7293/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0830 - val_loss: 0.1259\n",
      "Epoch 7294/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0388 - val_loss: 0.0174\n",
      "Epoch 7295/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0126 - val_loss: 0.0179\n",
      "Epoch 7296/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 7297/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0063 - val_loss: 0.0056\n",
      "Epoch 7298/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0079 - val_loss: 0.0019\n",
      "Epoch 7299/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0043 - val_loss: 0.0067\n",
      "Epoch 7300/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0091 - val_loss: 0.0271\n",
      "Epoch 7301/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0131 - val_loss: 0.0301\n",
      "Epoch 7302/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0142 - val_loss: 0.0188\n",
      "Epoch 7303/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0091 - val_loss: 0.0046\n",
      "Epoch 7304/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0046 - val_loss: 0.0020\n",
      "Epoch 7305/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0032 - val_loss: 0.0015\n",
      "Epoch 7306/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0020 - val_loss: 0.0034\n",
      "Epoch 7307/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 7308/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 7309/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.9780e-04 - val_loss: 0.0015\n",
      "Epoch 7310/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.2120e-04 - val_loss: 0.0010\n",
      "Epoch 7311/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 0.0059\n",
      "Epoch 7312/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0022 - val_loss: 9.7128e-04\n",
      "Epoch 7313/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 7314/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0025 - val_loss: 0.0044\n",
      "Epoch 7315/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0024 - val_loss: 0.0041\n",
      "Epoch 7316/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0033 - val_loss: 0.0079\n",
      "Epoch 7317/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0036 - val_loss: 0.0061\n",
      "Epoch 7318/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0037 - val_loss: 0.0058\n",
      "Epoch 7319/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0048 - val_loss: 0.0056\n",
      "Epoch 7320/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0102 - val_loss: 0.0190\n",
      "Epoch 7321/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0203 - val_loss: 0.0126\n",
      "Epoch 7322/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 7323/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 7324/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 7325/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0012 - val_loss: 7.3269e-04\n",
      "Epoch 7326/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 6.1206e-04\n",
      "Epoch 7327/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.9605e-04 - val_loss: 9.4547e-04\n",
      "Epoch 7328/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0013 - val_loss: 9.8531e-04\n",
      "Epoch 7329/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.2245e-04 - val_loss: 0.0016\n",
      "Epoch 7330/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 7331/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 7332/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0014 - val_loss: 0.0033\n",
      "Epoch 7333/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.5919e-04 - val_loss: 8.4775e-04\n",
      "Epoch 7334/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.4610e-04 - val_loss: 7.4663e-04\n",
      "Epoch 7335/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0021 - val_loss: 0.0053\n",
      "Epoch 7336/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0026 - val_loss: 4.2910e-04\n",
      "Epoch 7337/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0011 - val_loss: 5.7588e-04\n",
      "Epoch 7338/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0020 - val_loss: 7.1326e-04\n",
      "Epoch 7339/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0014 - val_loss: 7.6639e-04\n",
      "Epoch 7340/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.6163e-04 - val_loss: 4.5803e-04\n",
      "Epoch 7341/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 6.1537e-04 - val_loss: 0.0022\n",
      "Epoch 7342/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 7343/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 7344/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 7.1228e-04 - val_loss: 2.6420e-04\n",
      "Epoch 7345/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.5909e-04 - val_loss: 0.0014\n",
      "Epoch 7346/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.6751e-04 - val_loss: 5.7852e-04\n",
      "Epoch 7347/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0011 - val_loss: 7.3324e-04\n",
      "Epoch 7348/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0020 - val_loss: 0.0072\n",
      "Epoch 7349/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0103 - val_loss: 0.0633\n",
      "Epoch 7350/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0541 - val_loss: 0.0015\n",
      "Epoch 7351/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0233 - val_loss: 0.0267\n",
      "Epoch 7352/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0364 - val_loss: 0.0167\n",
      "Epoch 7353/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0214 - val_loss: 0.0154\n",
      "Epoch 7354/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0322 - val_loss: 0.0582\n",
      "Epoch 7355/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0614 - val_loss: 0.0222\n",
      "Epoch 7356/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0283 - val_loss: 0.0348\n",
      "Epoch 7357/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0174 - val_loss: 0.0141\n",
      "Epoch 7358/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0091 - val_loss: 0.0058\n",
      "Epoch 7359/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0070 - val_loss: 0.0034\n",
      "Epoch 7360/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0176 - val_loss: 0.0272\n",
      "Epoch 7361/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0147 - val_loss: 0.0232\n",
      "Epoch 7362/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0194 - val_loss: 0.0040\n",
      "Epoch 7363/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0059 - val_loss: 0.0117\n",
      "Epoch 7364/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 0.0089 - val_loss: 0.0039\n",
      "Epoch 7365/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 7366/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0017 - val_loss: 5.8075e-04\n",
      "Epoch 7367/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 7368/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.8249e-04 - val_loss: 1.7961e-04\n",
      "Epoch 7369/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3.1006e-04 - val_loss: 7.9987e-04\n",
      "Epoch 7370/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5.7505e-04 - val_loss: 3.9064e-04\n",
      "Epoch 7371/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 5.9277e-04 - val_loss: 1.9451e-04\n",
      "Epoch 7372/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.8928e-04 - val_loss: 6.4916e-04\n",
      "Epoch 7373/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0013 - val_loss: 9.4353e-04\n",
      "Epoch 7374/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 8.6612e-04 - val_loss: 7.5497e-04\n",
      "Epoch 7375/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.3990e-04 - val_loss: 8.0177e-04\n",
      "Epoch 7376/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0011 - val_loss: 2.2618e-04\n",
      "Epoch 7377/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.6245e-04 - val_loss: 1.3157e-04\n",
      "Epoch 7378/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.8410e-04 - val_loss: 6.7036e-04\n",
      "Epoch 7379/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.6059e-04 - val_loss: 2.1070e-04\n",
      "Epoch 7380/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 3.4338e-04 - val_loss: 1.0895e-04\n",
      "Epoch 7381/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.8820e-04 - val_loss: 1.1077e-04\n",
      "Epoch 7382/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.6944e-04 - val_loss: 1.8032e-04\n",
      "Epoch 7383/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.9109e-04 - val_loss: 5.3900e-04\n",
      "Epoch 7384/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 3.5699e-04 - val_loss: 3.0124e-04\n",
      "Epoch 7385/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1896e-04 - val_loss: 1.6022e-04\n",
      "Epoch 7386/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.7033e-04 - val_loss: 1.1848e-04\n",
      "Epoch 7387/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.6675e-04 - val_loss: 2.7937e-04\n",
      "Epoch 7388/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 2.3643e-04 - val_loss: 1.7909e-04\n",
      "Epoch 7389/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.4310e-04 - val_loss: 1.4003e-04\n",
      "Epoch 7390/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.4953e-04 - val_loss: 1.0246e-04\n",
      "Epoch 7391/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.4540e-04 - val_loss: 3.3422e-04\n",
      "Epoch 7392/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.7603e-04 - val_loss: 1.0667e-04\n",
      "Epoch 7393/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.0552e-04 - val_loss: 0.0012\n",
      "Epoch 7394/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0013 - val_loss: 9.2435e-04\n",
      "Epoch 7395/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0044 - val_loss: 1.4717e-04\n",
      "Epoch 7396/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0056 - val_loss: 0.0015\n",
      "Epoch 7397/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0045 - val_loss: 0.0198\n",
      "Epoch 7398/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0103 - val_loss: 0.0026\n",
      "Epoch 7399/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0101 - val_loss: 1.4872e-04\n",
      "Epoch 7400/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0083 - val_loss: 0.0299\n",
      "Epoch 7401/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0287 - val_loss: 0.0505\n",
      "Epoch 7402/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0295 - val_loss: 0.0251\n",
      "Epoch 7403/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 0.0106 - val_loss: 0.0047\n",
      "Epoch 7404/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0044 - val_loss: 0.0105\n",
      "Epoch 7405/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0058 - val_loss: 0.0012\n",
      "Epoch 7406/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0333 - val_loss: 0.0277\n",
      "Epoch 7407/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0750 - val_loss: 0.0803\n",
      "Epoch 7408/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0883 - val_loss: 0.2396\n",
      "Epoch 7409/10000\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 0.0895 - val_loss: 0.0367\n",
      "Epoch 7410/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0672 - val_loss: 0.0483\n",
      "Epoch 7411/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0627 - val_loss: 0.0538\n",
      "Epoch 7412/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0272 - val_loss: 9.3848e-04\n",
      "Epoch 7413/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0330 - val_loss: 0.0153\n",
      "Epoch 7414/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0222 - val_loss: 0.0144\n",
      "Epoch 7415/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0155 - val_loss: 0.0173\n",
      "Epoch 7416/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0262 - val_loss: 0.0591\n",
      "Epoch 7417/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0921 - val_loss: 0.1346\n",
      "Epoch 7418/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2062 - val_loss: 0.5160\n",
      "Epoch 7419/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7688 - val_loss: 1.4477\n",
      "Epoch 7420/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.4655 - val_loss: 3.8204\n",
      "Epoch 7421/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.6614 - val_loss: 0.4438\n",
      "Epoch 7422/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 14.7176 - val_loss: 11.9858\n",
      "Epoch 7423/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 10.6257 - val_loss: 11.9228\n",
      "Epoch 7424/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.2720 - val_loss: 7.7887\n",
      "Epoch 7425/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.7297 - val_loss: 0.6094\n",
      "Epoch 7426/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8545 - val_loss: 3.6341\n",
      "Epoch 7427/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1220 - val_loss: 0.2326\n",
      "Epoch 7428/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1438 - val_loss: 0.2819\n",
      "Epoch 7429/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6038 - val_loss: 2.7356\n",
      "Epoch 7430/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.5800 - val_loss: 5.8129\n",
      "Epoch 7431/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 4.9618 - val_loss: 7.2153\n",
      "Epoch 7432/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.3891 - val_loss: 5.9918\n",
      "Epoch 7433/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.2839 - val_loss: 16.1996\n",
      "Epoch 7434/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16.9709 - val_loss: 23.5212\n",
      "Epoch 7435/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 57.2322 - val_loss: 4.7324\n",
      "Epoch 7436/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 2.279 - 0s 132us/sample - loss: 65.6047 - val_loss: 62.9218\n",
      "Epoch 7437/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 88.8318 - val_loss: 33.4686\n",
      "Epoch 7438/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 67.9371 - val_loss: 121.2063\n",
      "Epoch 7439/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 288.6817 - val_loss: 247.0418\n",
      "Epoch 7440/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 146.2980 - val_loss: 290.7567\n",
      "Epoch 7441/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 89.3691 - val_loss: 116.6358\n",
      "Epoch 7442/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 62.8139 - val_loss: 167.9866\n",
      "Epoch 7443/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 85.7261 - val_loss: 5.6166\n",
      "Epoch 7444/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 30.9743 - val_loss: 150.6626\n",
      "Epoch 7445/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 121.2182 - val_loss: 341.7423\n",
      "Epoch 7446/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 255.8359 - val_loss: 127.0007\n",
      "Epoch 7447/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 55.7196 - val_loss: 38.4388\n",
      "Epoch 7448/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.0462 - val_loss: 8.8480\n",
      "Epoch 7449/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 3.1099 - val_loss: 0.5055\n",
      "Epoch 7450/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1288 - val_loss: 2.3244\n",
      "Epoch 7451/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.0641 - val_loss: 5.4536\n",
      "Epoch 7452/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.0845 - val_loss: 0.0112\n",
      "Epoch 7453/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 2.4154 - val_loss: 4.5569\n",
      "Epoch 7454/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 17.4149 - val_loss: 33.2087\n",
      "Epoch 7455/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 127.3847 - val_loss: 9.2125\n",
      "Epoch 7456/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 155.7798 - val_loss: 141.0214\n",
      "Epoch 7457/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 184.8823 - val_loss: 19.2439\n",
      "Epoch 7458/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 219.8685 - val_loss: 788.6553\n",
      "Epoch 7459/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 572.6002 - val_loss: 578.8902\n",
      "Epoch 7460/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 187.5237 - val_loss: 56.9871\n",
      "Epoch 7461/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 229.1497 - val_loss: 260.7932\n",
      "Epoch 7462/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 380.9335 - val_loss: 14.6854\n",
      "Epoch 7463/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 508.0707 - val_loss: 868.8340\n",
      "Epoch 7464/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1297.3994 - val_loss: 528.6358\n",
      "Epoch 7465/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 595.9222 - val_loss: 31.6473\n",
      "Epoch 7466/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 931.5198 - val_loss: 497.2554\n",
      "Epoch 7467/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2631.4671 - val_loss: 941.5368\n",
      "Epoch 7468/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 2846.9036 - val_loss: 2127.4096\n",
      "Epoch 7469/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1794.3446 - val_loss: 1195.4426\n",
      "Epoch 7470/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 338.3042 - val_loss: 4.7869\n",
      "Epoch 7471/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 57.0240 - val_loss: 63.9485\n",
      "Epoch 7472/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 55.1913 - val_loss: 65.5955\n",
      "Epoch 7473/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 41.9157 - val_loss: 40.4258\n",
      "Epoch 7474/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 25.1562 - val_loss: 21.1077\n",
      "Epoch 7475/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 16.0119 - val_loss: 5.7330\n",
      "Epoch 7476/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.9782 - val_loss: 7.3667\n",
      "Epoch 7477/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.5982 - val_loss: 1.3387\n",
      "Epoch 7478/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.1977 - val_loss: 0.3312\n",
      "Epoch 7479/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1026 - val_loss: 0.1748\n",
      "Epoch 7480/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1598 - val_loss: 0.0353\n",
      "Epoch 7481/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0419 - val_loss: 0.0328\n",
      "Epoch 7482/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0627 - val_loss: 0.2136\n",
      "Epoch 7483/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0875 - val_loss: 0.1856\n",
      "Epoch 7484/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1088 - val_loss: 0.0353\n",
      "Epoch 7485/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0320 - val_loss: 0.0158\n",
      "Epoch 7486/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0074 - val_loss: 0.0050\n",
      "Epoch 7487/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0040 - val_loss: 0.0017\n",
      "Epoch 7488/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0028 - val_loss: 0.0019\n",
      "Epoch 7489/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 7490/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 7491/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0019 - val_loss: 0.0165\n",
      "Epoch 7492/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0141 - val_loss: 0.0336\n",
      "Epoch 7493/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0306 - val_loss: 0.0068\n",
      "Epoch 7494/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0123 - val_loss: 0.0358\n",
      "Epoch 7495/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0120 - val_loss: 0.0126\n",
      "Epoch 7496/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0062 - val_loss: 0.0103\n",
      "Epoch 7497/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0058 - val_loss: 0.0025\n",
      "Epoch 7498/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0052 - val_loss: 0.0080\n",
      "Epoch 7499/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0036 - val_loss: 0.0028\n",
      "Epoch 7500/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0046 - val_loss: 0.0074\n",
      "Epoch 7501/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0050 - val_loss: 0.0044\n",
      "Epoch 7502/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0531 - val_loss: 0.0015\n",
      "Epoch 7503/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0423 - val_loss: 0.0786\n",
      "Epoch 7504/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0428 - val_loss: 0.0734\n",
      "Epoch 7505/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0314 - val_loss: 0.0098\n",
      "Epoch 7506/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0201 - val_loss: 0.0173\n",
      "Epoch 7507/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0131 - val_loss: 0.0266\n",
      "Epoch 7508/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0421 - val_loss: 0.0585\n",
      "Epoch 7509/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0735 - val_loss: 0.0179\n",
      "Epoch 7510/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0916 - val_loss: 0.0044\n",
      "Epoch 7511/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1237 - val_loss: 0.2268\n",
      "Epoch 7512/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1193 - val_loss: 0.0692\n",
      "Epoch 7513/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3302 - val_loss: 0.5250\n",
      "Epoch 7514/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1735 - val_loss: 0.0396\n",
      "Epoch 7515/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2821 - val_loss: 0.8166\n",
      "Epoch 7516/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2648 - val_loss: 2.2397\n",
      "Epoch 7517/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2992 - val_loss: 4.0628\n",
      "Epoch 7518/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.1245 - val_loss: 6.3886\n",
      "Epoch 7519/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.0140 - val_loss: 7.1218\n",
      "Epoch 7520/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.5542 - val_loss: 2.9726\n",
      "Epoch 7521/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8353 - val_loss: 0.8371\n",
      "Epoch 7522/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9363 - val_loss: 1.3047\n",
      "Epoch 7523/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.8546 - val_loss: 1.0361\n",
      "Epoch 7524/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4923 - val_loss: 0.4243\n",
      "Epoch 7525/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1934 - val_loss: 0.2034\n",
      "Epoch 7526/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1898 - val_loss: 0.0261\n",
      "Epoch 7527/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1302 - val_loss: 0.1405\n",
      "Epoch 7528/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0654 - val_loss: 0.0353\n",
      "Epoch 7529/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0120 - val_loss: 0.0082\n",
      "Epoch 7530/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0077 - val_loss: 0.0606\n",
      "Epoch 7531/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0265 - val_loss: 0.0535\n",
      "Epoch 7532/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0524 - val_loss: 4.7866e-04\n",
      "Epoch 7533/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0180 - val_loss: 0.0067\n",
      "Epoch 7534/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0090 - val_loss: 0.0187\n",
      "Epoch 7535/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0128 - val_loss: 0.0288\n",
      "Epoch 7536/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0218 - val_loss: 0.0025\n",
      "Epoch 7537/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0101 - val_loss: 0.0119\n",
      "Epoch 7538/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0052 - val_loss: 0.0241\n",
      "Epoch 7539/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0127 - val_loss: 0.0648\n",
      "Epoch 7540/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0388 - val_loss: 0.0105\n",
      "Epoch 7541/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0078 - val_loss: 0.0113\n",
      "Epoch 7542/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0042 - val_loss: 0.0068\n",
      "Epoch 7543/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0057 - val_loss: 0.0064\n",
      "Epoch 7544/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0035 - val_loss: 0.0012\n",
      "Epoch 7545/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0134 - val_loss: 0.0265\n",
      "Epoch 7546/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0087 - val_loss: 0.0141\n",
      "Epoch 7547/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0060 - val_loss: 0.0065\n",
      "Epoch 7548/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0036 - val_loss: 0.0083\n",
      "Epoch 7549/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0033 - val_loss: 0.0044\n",
      "Epoch 7550/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0047 - val_loss: 0.0061\n",
      "Epoch 7551/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0072 - val_loss: 0.0285\n",
      "Epoch 7552/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0268 - val_loss: 0.0585\n",
      "Epoch 7553/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1453 - val_loss: 0.1096\n",
      "Epoch 7554/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1630 - val_loss: 0.1278\n",
      "Epoch 7555/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.9078 - val_loss: 6.3560\n",
      "Epoch 7556/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4538 - val_loss: 10.1909\n",
      "Epoch 7557/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.0504 - val_loss: 11.3281\n",
      "Epoch 7558/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.2833 - val_loss: 0.9095\n",
      "Epoch 7559/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0385 - val_loss: 0.8897\n",
      "Epoch 7560/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.0031 - val_loss: 9.6723\n",
      "Epoch 7561/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 6.1384 - val_loss: 1.2347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7562/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6754 - val_loss: 0.8448\n",
      "Epoch 7563/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4943 - val_loss: 0.5445\n",
      "Epoch 7564/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3642 - val_loss: 0.6443\n",
      "Epoch 7565/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4247 - val_loss: 0.1695\n",
      "Epoch 7566/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0554 - val_loss: 0.0297\n",
      "Epoch 7567/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0228 - val_loss: 0.0571\n",
      "Epoch 7568/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0260 - val_loss: 0.0283\n",
      "Epoch 7569/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0081 - val_loss: 0.0785\n",
      "Epoch 7570/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2323 - val_loss: 1.0013\n",
      "Epoch 7571/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.4539 - val_loss: 0.0936\n",
      "Epoch 7572/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1809 - val_loss: 0.1162\n",
      "Epoch 7573/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.7661 - val_loss: 0.5872\n",
      "Epoch 7574/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0750 - val_loss: 0.9889\n",
      "Epoch 7575/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.2225 - val_loss: 4.0307\n",
      "Epoch 7576/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 29.6425 - val_loss: 6.1653\n",
      "Epoch 7577/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.1396 - val_loss: 30.4402\n",
      "Epoch 7578/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 23.8939 - val_loss: 6.5863\n",
      "Epoch 7579/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.6601 - val_loss: 0.0281\n",
      "Epoch 7580/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.0433 - val_loss: 7.1313\n",
      "Epoch 7581/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.5177 - val_loss: 0.1249\n",
      "Epoch 7582/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.4380 - val_loss: 9.4101\n",
      "Epoch 7583/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 30.1941 - val_loss: 34.3228\n",
      "Epoch 7584/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 383.9304 - val_loss: 15.7769\n",
      "Epoch 7585/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 455.4821 - val_loss: 923.1736\n",
      "Epoch 7586/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 439.9822 - val_loss: 1599.5913\n",
      "Epoch 7587/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 691.9074 - val_loss: 152.6487\n",
      "Epoch 7588/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 82.8680 - val_loss: 118.5991\n",
      "Epoch 7589/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 81.3887 - val_loss: 291.8296\n",
      "Epoch 7590/10000\n",
      "68/68 [==============================] - 0s 544us/sample - loss: 354.7820 - val_loss: 462.7650\n",
      "Epoch 7591/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 437.8320 - val_loss: 323.4496\n",
      "Epoch 7592/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 364.9864 - val_loss: 75.3239\n",
      "Epoch 7593/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1079.4625 - val_loss: 6964.3862\n",
      "Epoch 7594/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7811.6777 - val_loss: 14713.4808\n",
      "Epoch 7595/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9155.3592 - val_loss: 1656.2242\n",
      "Epoch 7596/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4157.0668 - val_loss: 2982.8665\n",
      "Epoch 7597/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2975.6963 - val_loss: 4121.7021\n",
      "Epoch 7598/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2627.3472 - val_loss: 6835.1456\n",
      "Epoch 7599/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 5767.7741 - val_loss: 6139.5357\n",
      "Epoch 7600/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3085.8189 - val_loss: 1056.1739\n",
      "Epoch 7601/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1477.9529 - val_loss: 230.7966\n",
      "Epoch 7602/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1808.7157 - val_loss: 3659.9408\n",
      "Epoch 7603/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1513.8998 - val_loss: 2766.9549\n",
      "Epoch 7604/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1410.7251 - val_loss: 1466.4143\n",
      "Epoch 7605/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 704.1263 - val_loss: 990.4726\n",
      "Epoch 7606/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 641.0955 - val_loss: 1032.2694\n",
      "Epoch 7607/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1067.3803 - val_loss: 80.3735\n",
      "Epoch 7608/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 484.6363 - val_loss: 283.9480\n",
      "Epoch 7609/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 310.9693 - val_loss: 135.8390\n",
      "Epoch 7610/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 85.3065 - val_loss: 37.5460\n",
      "Epoch 7611/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 89.9602 - val_loss: 2.2659\n",
      "Epoch 7612/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 24.1786 - val_loss: 3.1472\n",
      "Epoch 7613/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.3003 - val_loss: 1.1666\n",
      "Epoch 7614/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8976 - val_loss: 0.9089\n",
      "Epoch 7615/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.4264 - val_loss: 0.4035\n",
      "Epoch 7616/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1579 - val_loss: 0.4402\n",
      "Epoch 7617/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0366 - val_loss: 1.6877\n",
      "Epoch 7618/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2586 - val_loss: 0.4932\n",
      "Epoch 7619/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8401 - val_loss: 0.3675\n",
      "Epoch 7620/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 2.1357 - val_loss: 1.0562\n",
      "Epoch 7621/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1868 - val_loss: 0.5272\n",
      "Epoch 7622/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.4584 - val_loss: 0.1291\n",
      "Epoch 7623/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2417 - val_loss: 0.2176\n",
      "Epoch 7624/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2044 - val_loss: 0.4793\n",
      "Epoch 7625/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2588 - val_loss: 0.4984\n",
      "Epoch 7626/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2612 - val_loss: 0.4425\n",
      "Epoch 7627/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1999 - val_loss: 0.0595\n",
      "Epoch 7628/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1066 - val_loss: 0.1378\n",
      "Epoch 7629/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1245 - val_loss: 0.2915\n",
      "Epoch 7630/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2155 - val_loss: 0.4056\n",
      "Epoch 7631/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1497 - val_loss: 0.0623\n",
      "Epoch 7632/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0253 - val_loss: 0.0303\n",
      "Epoch 7633/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0115 - val_loss: 0.0137\n",
      "Epoch 7634/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0089 - val_loss: 0.0088\n",
      "Epoch 7635/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0025 - val_loss: 0.0044\n",
      "Epoch 7636/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 0.0027\n",
      "Epoch 7637/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 7638/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0011 - val_loss: 1.5135e-05\n",
      "Epoch 7639/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.3203e-04 - val_loss: 2.6076e-04\n",
      "Epoch 7640/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.5696e-04 - val_loss: 2.2813e-04\n",
      "Epoch 7641/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 3.1293e-04 - val_loss: 2.2490e-05\n",
      "Epoch 7642/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.5912e-04 - val_loss: 1.1259e-04\n",
      "Epoch 7643/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.7522e-04 - val_loss: 9.9632e-05\n",
      "Epoch 7644/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1347e-04 - val_loss: 4.9333e-05\n",
      "Epoch 7645/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.8051e-04 - val_loss: 5.2499e-04\n",
      "Epoch 7646/10000\n",
      "68/68 [==============================] - 0s 735us/sample - loss: 5.1384e-04 - val_loss: 0.0011\n",
      "Epoch 7647/10000\n",
      "68/68 [==============================] - 0s 779us/sample - loss: 4.6282e-04 - val_loss: 4.5070e-04\n",
      "Epoch 7648/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.8197e-04 - val_loss: 0.0014\n",
      "Epoch 7649/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.0599e-04 - val_loss: 0.0033\n",
      "Epoch 7650/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0016 - val_loss: 0.0029\n",
      "Epoch 7651/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.0026 - val_loss: 3.2335e-05\n",
      "Epoch 7652/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0012 - val_loss: 1.5867e-04\n",
      "Epoch 7653/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.5265e-04 - val_loss: 3.6404e-04\n",
      "Epoch 7654/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.5183e-04 - val_loss: 0.0019\n",
      "Epoch 7655/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0012 - val_loss: 1.9959e-04\n",
      "Epoch 7656/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0014 - val_loss: 0.0026\n",
      "Epoch 7657/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0020 - val_loss: 0.0063\n",
      "Epoch 7658/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0035 - val_loss: 0.0064\n",
      "Epoch 7659/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0082 - val_loss: 0.0011\n",
      "Epoch 7660/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0032 - val_loss: 0.0014\n",
      "Epoch 7661/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0010 - val_loss: 1.2929e-04\n",
      "Epoch 7662/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.0859e-04 - val_loss: 3.3719e-04\n",
      "Epoch 7663/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.0052e-04 - val_loss: 5.4532e-04\n",
      "Epoch 7664/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 4.4632e-04 - val_loss: 6.6155e-05\n",
      "Epoch 7665/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.2511e-04 - val_loss: 2.0011e-04\n",
      "Epoch 7666/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 1.2817e-0 - 0s 250us/sample - loss: 1.2738e-04 - val_loss: 1.1458e-04\n",
      "Epoch 7667/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.2143e-05 - val_loss: 1.9880e-05\n",
      "Epoch 7668/10000\n",
      "68/68 [==============================] - 0s 853us/sample - loss: 3.4555e-05 - val_loss: 8.7409e-05\n",
      "Epoch 7669/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 5.0316e-05 - val_loss: 6.8690e-05\n",
      "Epoch 7670/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.4645e-05 - val_loss: 6.8335e-05\n",
      "Epoch 7671/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 1.2071e-04 - val_loss: 1.2812e-05\n",
      "Epoch 7672/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 1.1600e-04 - val_loss: 1.4046e-04\n",
      "Epoch 7673/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4445e-04 - val_loss: 1.8380e-05\n",
      "Epoch 7674/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.3832e-05 - val_loss: 1.5561e-05\n",
      "Epoch 7675/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.6914e-05 - val_loss: 2.7902e-05\n",
      "Epoch 7676/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 1.4042e-05 - val_loss: 1.2868e-05\n",
      "Epoch 7677/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.2418e-06 - val_loss: 5.6103e-06\n",
      "Epoch 7678/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.9716e-05 - val_loss: 4.5247e-05\n",
      "Epoch 7679/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.9762e-05 - val_loss: 4.1454e-05\n",
      "Epoch 7680/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 3.7960e-05 - val_loss: 2.1129e-05\n",
      "Epoch 7681/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.4643e-05 - val_loss: 7.5072e-06\n",
      "Epoch 7682/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.2908e-05 - val_loss: 2.1339e-05\n",
      "Epoch 7683/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9641e-05 - val_loss: 1.5305e-05\n",
      "Epoch 7684/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.2616e-05 - val_loss: 2.2298e-04\n",
      "Epoch 7685/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 1.0899e-04 - val_loss: 1.7054e-05\n",
      "Epoch 7686/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.7741e-05 - val_loss: 1.4581e-04\n",
      "Epoch 7687/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.4945e-05 - val_loss: 2.2261e-05\n",
      "Epoch 7688/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.8680e-05 - val_loss: 1.6953e-05\n",
      "Epoch 7689/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3712e-05 - val_loss: 1.1052e-05\n",
      "Epoch 7690/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.5028e-05 - val_loss: 5.3077e-05\n",
      "Epoch 7691/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 5.8401e-05 - val_loss: 1.8197e-05\n",
      "Epoch 7692/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 7.5574e-05 - val_loss: 3.4482e-04\n",
      "Epoch 7693/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.3556e-04 - val_loss: 8.0103e-04\n",
      "Epoch 7694/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.5154e-04 - val_loss: 2.2180e-04\n",
      "Epoch 7695/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.6572e-04 - val_loss: 7.7603e-04\n",
      "Epoch 7696/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0012 - val_loss: 9.1695e-04\n",
      "Epoch 7697/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.8147e-04 - val_loss: 3.6860e-04\n",
      "Epoch 7698/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.7698e-04 - val_loss: 3.1072e-04\n",
      "Epoch 7699/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4938e-04 - val_loss: 9.8481e-05\n",
      "Epoch 7700/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.5664e-05 - val_loss: 1.4314e-04\n",
      "Epoch 7701/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.7871e-05 - val_loss: 1.3281e-04\n",
      "Epoch 7702/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.2372e-04 - val_loss: 1.0949e-04\n",
      "Epoch 7703/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.2489e-04 - val_loss: 8.6500e-04\n",
      "Epoch 7704/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 7705/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0063 - val_loss: 0.0041\n",
      "Epoch 7706/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0115 - val_loss: 0.0051\n",
      "Epoch 7707/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0053 - val_loss: 0.0068\n",
      "Epoch 7708/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0062 - val_loss: 0.0084\n",
      "Epoch 7709/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0092 - val_loss: 0.0067\n",
      "Epoch 7710/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0047 - val_loss: 7.0498e-04\n",
      "Epoch 7711/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 7712/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0033 - val_loss: 0.0177\n",
      "Epoch 7713/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0124 - val_loss: 0.0112\n",
      "Epoch 7714/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0212 - val_loss: 0.0452\n",
      "Epoch 7715/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0285 - val_loss: 0.0152\n",
      "Epoch 7716/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0123 - val_loss: 0.0163\n",
      "Epoch 7717/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0127 - val_loss: 0.0076\n",
      "Epoch 7718/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0133 - val_loss: 0.0136\n",
      "Epoch 7719/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0055 - val_loss: 0.0058\n",
      "Epoch 7720/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0032 - val_loss: 0.0082\n",
      "Epoch 7721/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0036 - val_loss: 0.0088\n",
      "Epoch 7722/10000\n",
      "68/68 [==============================] - 0s 882us/sample - loss: 0.0071 - val_loss: 0.0050\n",
      "Epoch 7723/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0097 - val_loss: 0.0117\n",
      "Epoch 7724/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 0.0299 - val_loss: 0.0213\n",
      "Epoch 7725/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0439 - val_loss: 0.1651\n",
      "Epoch 7726/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2346 - val_loss: 0.1898\n",
      "Epoch 7727/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0549 - val_loss: 0.0449\n",
      "Epoch 7728/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0221 - val_loss: 0.0664\n",
      "Epoch 7729/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0489 - val_loss: 0.0481\n",
      "Epoch 7730/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.2369 - val_loss: 0.3650\n",
      "Epoch 7731/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.2442 - val_loss: 0.3914\n",
      "Epoch 7732/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.1926 - val_loss: 0.3625\n",
      "Epoch 7733/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.4454 - val_loss: 0.6519\n",
      "Epoch 7734/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.2271 - val_loss: 0.1574\n",
      "Epoch 7735/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0577 - val_loss: 0.1809\n",
      "Epoch 7736/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0990 - val_loss: 0.1294\n",
      "Epoch 7737/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1503 - val_loss: 0.1569\n",
      "Epoch 7738/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3434 - val_loss: 1.4308\n",
      "Epoch 7739/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.5864 - val_loss: 0.2388\n",
      "Epoch 7740/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.4143 - val_loss: 4.2783\n",
      "Epoch 7741/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 8.2052 - val_loss: 11.1831\n",
      "Epoch 7742/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 3.8585 - val_loss: 2.4094\n",
      "Epoch 7743/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 2.8125 - val_loss: 3.0540\n",
      "Epoch 7744/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 3.5246 - val_loss: 0.4889\n",
      "Epoch 7745/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1.2516 - val_loss: 0.8697\n",
      "Epoch 7746/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.3791 - val_loss: 2.3257\n",
      "Epoch 7747/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 6.0148 - val_loss: 14.8747\n",
      "Epoch 7748/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 16.64 - 0s 206us/sample - loss: 13.9179 - val_loss: 12.7351\n",
      "Epoch 7749/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 6.2385 - val_loss: 5.5559\n",
      "Epoch 7750/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.5463 - val_loss: 10.2969\n",
      "Epoch 7751/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 7.3856 - val_loss: 10.9989\n",
      "Epoch 7752/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 8.7707 - val_loss: 5.4467\n",
      "Epoch 7753/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 2.5089 - val_loss: 2.6992\n",
      "Epoch 7754/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1.8911 - val_loss: 2.0226\n",
      "Epoch 7755/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.2148 - val_loss: 2.5698\n",
      "Epoch 7756/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 1.6549 - val_loss: 2.5574\n",
      "Epoch 7757/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 2.8318 - val_loss: 0.0957\n",
      "Epoch 7758/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1.5894 - val_loss: 1.0695\n",
      "Epoch 7759/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 0.6401 - val_loss: 0.5932\n",
      "Epoch 7760/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.2589 - val_loss: 1.3156\n",
      "Epoch 7761/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.3049 - val_loss: 2.8961\n",
      "Epoch 7762/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 1.7831 - val_loss: 6.5144\n",
      "Epoch 7763/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 4.5080 - val_loss: 0.5073\n",
      "Epoch 7764/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.8712 - val_loss: 0.4350\n",
      "Epoch 7765/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 10.1397 - val_loss: 23.4156\n",
      "Epoch 7766/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 10.8905 - val_loss: 19.3168\n",
      "Epoch 7767/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 17.1928 - val_loss: 57.4014\n",
      "Epoch 7768/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 69.7276 - val_loss: 86.2089\n",
      "Epoch 7769/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 289.1955 - val_loss: 240.2427\n",
      "Epoch 7770/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 109.2021 - val_loss: 124.3096\n",
      "Epoch 7771/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 29.7976 - val_loss: 47.9833\n",
      "Epoch 7772/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 49.3806 - val_loss: 1.4899\n",
      "Epoch 7773/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 148.5984 - val_loss: 245.8898\n",
      "Epoch 7774/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 227.0817 - val_loss: 134.4464\n",
      "Epoch 7775/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 125.2361 - val_loss: 21.7962\n",
      "Epoch 7776/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 30.1862 - val_loss: 2.6083\n",
      "Epoch 7777/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 10.3614 - val_loss: 2.5799\n",
      "Epoch 7778/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 7.5968 - val_loss: 2.8241\n",
      "Epoch 7779/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.3650 - val_loss: 8.0375\n",
      "Epoch 7780/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.8998 - val_loss: 14.2788\n",
      "Epoch 7781/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.5995 - val_loss: 4.8662\n",
      "Epoch 7782/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8464 - val_loss: 16.5446\n",
      "Epoch 7783/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 42.7484 - val_loss: 19.4984\n",
      "Epoch 7784/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 176.3722 - val_loss: 25.1666\n",
      "Epoch 7785/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 181.4821 - val_loss: 739.8295\n",
      "Epoch 7786/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 340.4172 - val_loss: 719.4140\n",
      "Epoch 7787/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 369.4693 - val_loss: 202.5870\n",
      "Epoch 7788/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 119.8389 - val_loss: 140.0278\n",
      "Epoch 7789/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 141.4277 - val_loss: 43.7069\n",
      "Epoch 7790/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 21.7702 - val_loss: 10.7819\n",
      "Epoch 7791/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 27.3723 - val_loss: 100.0347\n",
      "Epoch 7792/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 91.2112 - val_loss: 218.9411\n",
      "Epoch 7793/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 143.3291 - val_loss: 260.8849\n",
      "Epoch 7794/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 596.2404 - val_loss: 1002.1091\n",
      "Epoch 7795/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 648.3169 - val_loss: 676.2376\n",
      "Epoch 7796/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 211.5729 - val_loss: 798.5543\n",
      "Epoch 7797/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1472.6696 - val_loss: 1417.8023\n",
      "Epoch 7798/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 619.9814 - val_loss: 137.4160\n",
      "Epoch 7799/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 908.1696 - val_loss: 840.4777\n",
      "Epoch 7800/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1066.3088 - val_loss: 643.8388\n",
      "Epoch 7801/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 421.2814 - val_loss: 172.3398\n",
      "Epoch 7802/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 161.4666 - val_loss: 424.4452\n",
      "Epoch 7803/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 314.7242 - val_loss: 278.5410\n",
      "Epoch 7804/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 231.2455 - val_loss: 117.3417\n",
      "Epoch 7805/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 89.5469 - val_loss: 172.5478\n",
      "Epoch 7806/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 116.7966 - val_loss: 118.2873\n",
      "Epoch 7807/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 162.5942 - val_loss: 354.3591\n",
      "Epoch 7808/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 162.1297 - val_loss: 131.3610\n",
      "Epoch 7809/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 68.1054 - val_loss: 71.1846\n",
      "Epoch 7810/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 41.2685 - val_loss: 97.7677\n",
      "Epoch 7811/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 34.1227 - val_loss: 5.2968\n",
      "Epoch 7812/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 9.0584 - val_loss: 9.2831\n",
      "Epoch 7813/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.9538 - val_loss: 3.2338\n",
      "Epoch 7814/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.9902 - val_loss: 1.4985\n",
      "Epoch 7815/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.5221 - val_loss: 0.0660\n",
      "Epoch 7816/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1400 - val_loss: 0.2139\n",
      "Epoch 7817/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.1251 - val_loss: 0.2032\n",
      "Epoch 7818/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0908 - val_loss: 0.0990\n",
      "Epoch 7819/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0743 - val_loss: 0.0412\n",
      "Epoch 7820/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0194 - val_loss: 0.0127\n",
      "Epoch 7821/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0357 - val_loss: 0.0078\n",
      "Epoch 7822/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0622 - val_loss: 0.1174\n",
      "Epoch 7823/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0395 - val_loss: 0.0104\n",
      "Epoch 7824/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0130 - val_loss: 0.0221\n",
      "Epoch 7825/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.0084 - val_loss: 0.0074\n",
      "Epoch 7826/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0346 - val_loss: 0.0504\n",
      "Epoch 7827/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0434 - val_loss: 0.0377\n",
      "Epoch 7828/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0197 - val_loss: 0.0614\n",
      "Epoch 7829/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 0.0871 - val_loss: 0.0322\n",
      "Epoch 7830/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1005 - val_loss: 0.0826\n",
      "Epoch 7831/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1425 - val_loss: 0.0770\n",
      "Epoch 7832/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0390 - val_loss: 0.0115\n",
      "Epoch 7833/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0204 - val_loss: 0.0818\n",
      "Epoch 7834/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0411 - val_loss: 0.0455\n",
      "Epoch 7835/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0413 - val_loss: 0.1333\n",
      "Epoch 7836/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0483 - val_loss: 0.0879\n",
      "Epoch 7837/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.1536 - val_loss: 0.1250\n",
      "Epoch 7838/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1743 - val_loss: 0.0073\n",
      "Epoch 7839/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5102 - val_loss: 1.2374\n",
      "Epoch 7840/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3808 - val_loss: 0.1243\n",
      "Epoch 7841/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0659 - val_loss: 0.0581\n",
      "Epoch 7842/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0557 - val_loss: 0.1702\n",
      "Epoch 7843/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.1762 - val_loss: 0.0286\n",
      "Epoch 7844/10000\n",
      "68/68 [==============================] - 0s 544us/sample - loss: 0.1556 - val_loss: 0.0143\n",
      "Epoch 7845/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0574 - val_loss: 0.0122\n",
      "Epoch 7846/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0070 - val_loss: 0.0142\n",
      "Epoch 7847/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0185 - val_loss: 0.0107\n",
      "Epoch 7848/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0150 - val_loss: 0.0438\n",
      "Epoch 7849/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.0757 - val_loss: 0.1168\n",
      "Epoch 7850/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.4491 - val_loss: 0.1752\n",
      "Epoch 7851/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6144 - val_loss: 1.7760\n",
      "Epoch 7852/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 0.9534 - val_loss: 1.7026\n",
      "Epoch 7853/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.9847 - val_loss: 2.4823\n",
      "Epoch 7854/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.9953 - val_loss: 1.4668\n",
      "Epoch 7855/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.1847 - val_loss: 1.8566\n",
      "Epoch 7856/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 12.4477 - val_loss: 6.5946\n",
      "Epoch 7857/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 324us/sample - loss: 35.9166 - val_loss: 65.7389\n",
      "Epoch 7858/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 63.9668 - val_loss: 59.3066\n",
      "Epoch 7859/10000\n",
      "68/68 [==============================] - 0s 765us/sample - loss: 209.7141 - val_loss: 284.9765\n",
      "Epoch 7860/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 124.5893 - val_loss: 67.9097\n",
      "Epoch 7861/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 277.1401 - val_loss: 286.4950\n",
      "Epoch 7862/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1331.9655 - val_loss: 14386.6111\n",
      "Epoch 7863/10000\n",
      "68/68 [==============================] - 0s 721us/sample - loss: 8192.1844 - val_loss: 7131.7724\n",
      "Epoch 7864/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4442.0870 - val_loss: 4320.8690\n",
      "Epoch 7865/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3002.3984 - val_loss: 589.8615\n",
      "Epoch 7866/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2615.2142 - val_loss: 3625.7433\n",
      "Epoch 7867/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 2165.2898 - val_loss: 1920.5665\n",
      "Epoch 7868/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1257.4509 - val_loss: 1173.9190\n",
      "Epoch 7869/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 545.7647 - val_loss: 433.6948\n",
      "Epoch 7870/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 196.8179 - val_loss: 33.4150\n",
      "Epoch 7871/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 147.9045 - val_loss: 228.4139\n",
      "Epoch 7872/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 146.5762 - val_loss: 166.9970\n",
      "Epoch 7873/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 116.5025 - val_loss: 304.5296\n",
      "Epoch 7874/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 143.2626 - val_loss: 2.4360\n",
      "Epoch 7875/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 45.8247 - val_loss: 13.0881\n",
      "Epoch 7876/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.9413 - val_loss: 14.2824\n",
      "Epoch 7877/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16.3368 - val_loss: 11.1594\n",
      "Epoch 7878/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.4824 - val_loss: 3.1263\n",
      "Epoch 7879/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1441 - val_loss: 0.5963\n",
      "Epoch 7880/10000\n",
      "68/68 [==============================] - 0s 735us/sample - loss: 0.5215 - val_loss: 0.1213\n",
      "Epoch 7881/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4846 - val_loss: 0.1833\n",
      "Epoch 7882/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.5961 - val_loss: 0.0632\n",
      "Epoch 7883/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.4985 - val_loss: 0.2310\n",
      "Epoch 7884/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1796 - val_loss: 0.1279\n",
      "Epoch 7885/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 0.0962 - val_loss: 0.0945\n",
      "Epoch 7886/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0530 - val_loss: 0.0714\n",
      "Epoch 7887/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0473 - val_loss: 0.0294\n",
      "Epoch 7888/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0180 - val_loss: 0.0211\n",
      "Epoch 7889/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0300 - val_loss: 0.0171\n",
      "Epoch 7890/10000\n",
      "68/68 [==============================] - 0s 544us/sample - loss: 0.0348 - val_loss: 0.1517\n",
      "Epoch 7891/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0681 - val_loss: 0.0214\n",
      "Epoch 7892/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.1201 - val_loss: 0.1715\n",
      "Epoch 7893/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.1473 - val_loss: 0.1978\n",
      "Epoch 7894/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0700 - val_loss: 0.0386\n",
      "Epoch 7895/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0294 - val_loss: 0.0091\n",
      "Epoch 7896/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.0245 - val_loss: 0.0426\n",
      "Epoch 7897/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.0163 - val_loss: 0.0114\n",
      "Epoch 7898/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.0074 - val_loss: 0.0110\n",
      "Epoch 7899/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0078 - val_loss: 0.0015\n",
      "Epoch 7900/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0084 - val_loss: 0.0071\n",
      "Epoch 7901/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0059 - val_loss: 0.0088\n",
      "Epoch 7902/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0058 - val_loss: 0.0064\n",
      "Epoch 7903/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0043 - val_loss: 0.0026\n",
      "Epoch 7904/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 0.0047 - val_loss: 0.0036\n",
      "Epoch 7905/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0037 - val_loss: 0.0013\n",
      "Epoch 7906/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0021 - val_loss: 0.0111\n",
      "Epoch 7907/10000\n",
      "68/68 [==============================] - 0s 735us/sample - loss: 0.0068 - val_loss: 0.0079\n",
      "Epoch 7908/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0087 - val_loss: 0.0024\n",
      "Epoch 7909/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 7910/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 7911/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.0027 - val_loss: 0.0015\n",
      "Epoch 7912/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0024 - val_loss: 0.0012\n",
      "Epoch 7913/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0029 - val_loss: 0.0041\n",
      "Epoch 7914/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.0048 - val_loss: 0.0038\n",
      "Epoch 7915/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0028 - val_loss: 0.0021\n",
      "Epoch 7916/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0050 - val_loss: 0.0020\n",
      "Epoch 7917/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0047 - val_loss: 0.0038\n",
      "Epoch 7918/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0026 - val_loss: 0.0032\n",
      "Epoch 7919/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 7920/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0029 - val_loss: 0.0092\n",
      "Epoch 7921/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 7922/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0036 - val_loss: 0.0015\n",
      "Epoch 7923/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0043 - val_loss: 9.8309e-04\n",
      "Epoch 7924/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0022 - val_loss: 8.4430e-04\n",
      "Epoch 7925/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0022 - val_loss: 0.0047\n",
      "Epoch 7926/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0045 - val_loss: 0.0011\n",
      "Epoch 7927/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0017 - val_loss: 0.0037\n",
      "Epoch 7928/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0054 - val_loss: 8.9456e-04\n",
      "Epoch 7929/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0032 - val_loss: 0.0038\n",
      "Epoch 7930/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0061 - val_loss: 8.6819e-04\n",
      "Epoch 7931/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0028 - val_loss: 0.0049\n",
      "Epoch 7932/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0117 - val_loss: 0.0662\n",
      "Epoch 7933/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0365 - val_loss: 0.0675\n",
      "Epoch 7934/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.0449 - val_loss: 0.0510\n",
      "Epoch 7935/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0466 - val_loss: 0.0392\n",
      "Epoch 7936/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0177 - val_loss: 0.0078\n",
      "Epoch 7937/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0041 - val_loss: 0.0017\n",
      "Epoch 7938/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0015 - val_loss: 4.8681e-04\n",
      "Epoch 7939/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 7940/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0043 - val_loss: 0.0027\n",
      "Epoch 7941/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0026 - val_loss: 7.7579e-04\n",
      "Epoch 7942/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0012 - val_loss: 0.0045\n",
      "Epoch 7943/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0022 - val_loss: 8.8673e-04\n",
      "Epoch 7944/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0011 - val_loss: 7.2784e-04\n",
      "Epoch 7945/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0040 - val_loss: 0.0075\n",
      "Epoch 7946/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0078 - val_loss: 0.0145\n",
      "Epoch 7947/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0068 - val_loss: 0.0058\n",
      "Epoch 7948/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0031 - val_loss: 0.0070\n",
      "Epoch 7949/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0057 - val_loss: 0.0050\n",
      "Epoch 7950/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0044 - val_loss: 0.0076\n",
      "Epoch 7951/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0053 - val_loss: 0.0083\n",
      "Epoch 7952/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0057 - val_loss: 0.0107\n",
      "Epoch 7953/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0044 - val_loss: 3.8450e-04\n",
      "Epoch 7954/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 7955/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0013 - val_loss: 3.3420e-04\n",
      "Epoch 7956/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.3649e-04 - val_loss: 0.0018\n",
      "Epoch 7957/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0014 - val_loss: 0.0038\n",
      "Epoch 7958/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0040 - val_loss: 4.4738e-04\n",
      "Epoch 7959/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0133 - val_loss: 0.0073\n",
      "Epoch 7960/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0138 - val_loss: 0.0273\n",
      "Epoch 7961/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0141 - val_loss: 0.0168\n",
      "Epoch 7962/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0167 - val_loss: 0.0534\n",
      "Epoch 7963/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0388 - val_loss: 0.0457\n",
      "Epoch 7964/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0141 - val_loss: 0.0359\n",
      "Epoch 7965/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0142 - val_loss: 7.4422e-04\n",
      "Epoch 7966/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.5726e-04 - val_loss: 3.9320e-04\n",
      "Epoch 7967/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.6331e-04 - val_loss: 7.1627e-04\n",
      "Epoch 7968/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0014 - val_loss: 5.8570e-04\n",
      "Epoch 7969/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 6.1375e-04 - val_loss: 5.0163e-04\n",
      "Epoch 7970/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.0743e-04 - val_loss: 4.5949e-04\n",
      "Epoch 7971/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0043 - val_loss: 0.0029\n",
      "Epoch 7972/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0018 - val_loss: 5.8477e-04\n",
      "Epoch 7973/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 9.7250e-04 - val_loss: 9.0505e-04\n",
      "Epoch 7974/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0047 - val_loss: 0.0031\n",
      "Epoch 7975/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0049 - val_loss: 0.0142\n",
      "Epoch 7976/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0332 - val_loss: 0.0474\n",
      "Epoch 7977/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0171 - val_loss: 0.0024\n",
      "Epoch 7978/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0047 - val_loss: 0.0075\n",
      "Epoch 7979/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.0051 - val_loss: 5.1134e-04\n",
      "Epoch 7980/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0013 - val_loss: 0.0047\n",
      "Epoch 7981/10000\n",
      "68/68 [==============================] - 0s 868us/sample - loss: 0.0034 - val_loss: 0.0096\n",
      "Epoch 7982/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0051 - val_loss: 0.0025\n",
      "Epoch 7983/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 7984/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0015 - val_loss: 4.5486e-04\n",
      "Epoch 7985/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0025 - val_loss: 0.0020\n",
      "Epoch 7986/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0019 - val_loss: 0.0068\n",
      "Epoch 7987/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0072 - val_loss: 0.0120\n",
      "Epoch 7988/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0030 - val_loss: 0.0087\n",
      "Epoch 7989/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.0032 - val_loss: 0.0015\n",
      "Epoch 7990/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.0011 - val_loss: 0.0061\n",
      "Epoch 7991/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0029 - val_loss: 0.0143\n",
      "Epoch 7992/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0071 - val_loss: 2.8790e-04\n",
      "Epoch 7993/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 1.3609e-0 - 0s 779us/sample - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 7994/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0034 - val_loss: 0.0069\n",
      "Epoch 7995/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0034 - val_loss: 0.0060\n",
      "Epoch 7996/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 7997/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 0.0029 - val_loss: 0.0046\n",
      "Epoch 7998/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0487 - val_loss: 0.4799\n",
      "Epoch 7999/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.7166 - val_loss: 0.6264\n",
      "Epoch 8000/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 8.4290 - val_loss: 52.8812\n",
      "Epoch 8001/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 22.4925 - val_loss: 52.9807\n",
      "Epoch 8002/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 41.0306 - val_loss: 0.8262\n",
      "Epoch 8003/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 19.3098 - val_loss: 10.8479\n",
      "Epoch 8004/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 6.3300 - val_loss: 0.9158\n",
      "Epoch 8005/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 4.3899 - val_loss: 8.4803\n",
      "Epoch 8006/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 16.0783 - val_loss: 4.1264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8007/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 39.7565 - val_loss: 8.1126\n",
      "Epoch 8008/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 119.2695 - val_loss: 1659.3698\n",
      "Epoch 8009/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1357.2058 - val_loss: 323.3287\n",
      "Epoch 8010/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5235.8027 - val_loss: 2629.5239\n",
      "Epoch 8011/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 15755.6768 - val_loss: 56227.2148\n",
      "Epoch 8012/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 27239.0808 - val_loss: 18934.2342\n",
      "Epoch 8013/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 19055.5489 - val_loss: 3451.1947\n",
      "Epoch 8014/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 14057.6460 - val_loss: 41640.6604\n",
      "Epoch 8015/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 19358.6974 - val_loss: 18214.3896\n",
      "Epoch 8016/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 7705.2899 - val_loss: 354.3341\n",
      "Epoch 8017/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4610.4394 - val_loss: 1886.3381\n",
      "Epoch 8018/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4114.6748 - val_loss: 2939.8230\n",
      "Epoch 8019/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4090.3973 - val_loss: 2318.7488\n",
      "Epoch 8020/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1765.0683 - val_loss: 4205.2163\n",
      "Epoch 8021/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1745.1458 - val_loss: 412.6116\n",
      "Epoch 8022/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 595.8484 - val_loss: 433.5002\n",
      "Epoch 8023/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 305.2417 - val_loss: 96.2063\n",
      "Epoch 8024/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 82.1997 - val_loss: 46.2632\n",
      "Epoch 8025/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 41.8836 - val_loss: 5.1297\n",
      "Epoch 8026/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 18.5710 - val_loss: 19.8495\n",
      "Epoch 8027/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 11.6343 - val_loss: 14.0917\n",
      "Epoch 8028/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.3482 - val_loss: 1.0988\n",
      "Epoch 8029/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 1.7550 - val_loss: 0.6278\n",
      "Epoch 8030/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.9463 - val_loss: 0.7513\n",
      "Epoch 8031/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.4163 - val_loss: 0.2403\n",
      "Epoch 8032/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.1060 - val_loss: 0.1505\n",
      "Epoch 8033/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 0.0710 - val_loss: 0.0441\n",
      "Epoch 8034/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0483 - val_loss: 0.0142\n",
      "Epoch 8035/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0270 - val_loss: 0.0177\n",
      "Epoch 8036/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0174 - val_loss: 0.0118\n",
      "Epoch 8037/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0192 - val_loss: 0.0138\n",
      "Epoch 8038/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.0184 - val_loss: 0.0233\n",
      "Epoch 8039/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0221 - val_loss: 0.0090\n",
      "Epoch 8040/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0276 - val_loss: 0.0327\n",
      "Epoch 8041/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0262 - val_loss: 0.0212\n",
      "Epoch 8042/10000\n",
      "68/68 [==============================] - 0s 956us/sample - loss: 0.0172 - val_loss: 0.0195\n",
      "Epoch 8043/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0171 - val_loss: 0.0094\n",
      "Epoch 8044/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0143 - val_loss: 0.0099\n",
      "Epoch 8045/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0138 - val_loss: 0.0260\n",
      "Epoch 8046/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 0.0189 - val_loss: 0.0086\n",
      "Epoch 8047/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0211 - val_loss: 0.0317\n",
      "Epoch 8048/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0508 - val_loss: 0.0146\n",
      "Epoch 8049/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0265 - val_loss: 0.0222\n",
      "Epoch 8050/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0482 - val_loss: 0.0924\n",
      "Epoch 8051/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.0761 - val_loss: 0.0415\n",
      "Epoch 8052/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0210 - val_loss: 0.0122\n",
      "Epoch 8053/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0250 - val_loss: 0.0173\n",
      "Epoch 8054/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0238 - val_loss: 0.0195\n",
      "Epoch 8055/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.0291 - val_loss: 0.0259\n",
      "Epoch 8056/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0213 - val_loss: 0.0323\n",
      "Epoch 8057/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0173 - val_loss: 0.0156\n",
      "Epoch 8058/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0151 - val_loss: 0.0080\n",
      "Epoch 8059/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0103 - val_loss: 0.0149\n",
      "Epoch 8060/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0127 - val_loss: 0.0093\n",
      "Epoch 8061/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 8062/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0113 - val_loss: 0.0103\n",
      "Epoch 8063/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0098 - val_loss: 0.0065\n",
      "Epoch 8064/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 8065/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0168 - val_loss: 0.0126\n",
      "Epoch 8066/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0122 - val_loss: 0.0087\n",
      "Epoch 8067/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0111 - val_loss: 0.0155\n",
      "Epoch 8068/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0157 - val_loss: 0.0089\n",
      "Epoch 8069/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0119 - val_loss: 0.0202\n",
      "Epoch 8070/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0213 - val_loss: 0.0116\n",
      "Epoch 8071/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0252 - val_loss: 0.0163\n",
      "Epoch 8072/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0136 - val_loss: 0.0215\n",
      "Epoch 8073/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.0142 - val_loss: 0.0058\n",
      "Epoch 8074/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0101 - val_loss: 0.0074\n",
      "Epoch 8075/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0091 - val_loss: 0.0149\n",
      "Epoch 8076/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0119 - val_loss: 0.0255\n",
      "Epoch 8077/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.0238 - val_loss: 0.0073\n",
      "Epoch 8078/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0118 - val_loss: 0.0186\n",
      "Epoch 8079/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0191 - val_loss: 0.0067\n",
      "Epoch 8080/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0149 - val_loss: 0.0167\n",
      "Epoch 8081/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0109 - val_loss: 0.0365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8082/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0375 - val_loss: 0.0255\n",
      "Epoch 8083/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0218 - val_loss: 0.0168\n",
      "Epoch 8084/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0209 - val_loss: 0.0104\n",
      "Epoch 8085/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.0203 - val_loss: 0.0455\n",
      "Epoch 8086/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0162 - val_loss: 0.0062\n",
      "Epoch 8087/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0079 - val_loss: 0.0084\n",
      "Epoch 8088/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0076 - val_loss: 0.0095\n",
      "Epoch 8089/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0157 - val_loss: 0.0047\n",
      "Epoch 8090/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0161 - val_loss: 0.0259\n",
      "Epoch 8091/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0171 - val_loss: 0.0113\n",
      "Epoch 8092/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0121 - val_loss: 0.0141\n",
      "Epoch 8093/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0143 - val_loss: 0.0114\n",
      "Epoch 8094/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0074 - val_loss: 0.0077\n",
      "Epoch 8095/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 8096/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0078 - val_loss: 0.0042\n",
      "Epoch 8097/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 8098/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0074 - val_loss: 0.0107\n",
      "Epoch 8099/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0093 - val_loss: 0.0040\n",
      "Epoch 8100/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0086 - val_loss: 0.0051\n",
      "Epoch 8101/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0118 - val_loss: 0.0094\n",
      "Epoch 8102/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0059 - val_loss: 0.0050\n",
      "Epoch 8103/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0104 - val_loss: 0.0113\n",
      "Epoch 8104/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0066 - val_loss: 0.0035\n",
      "Epoch 8105/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0088 - val_loss: 0.0039\n",
      "Epoch 8106/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0098 - val_loss: 0.0365\n",
      "Epoch 8107/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0183 - val_loss: 0.0245\n",
      "Epoch 8108/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0210 - val_loss: 0.0144\n",
      "Epoch 8109/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0223 - val_loss: 0.0437\n",
      "Epoch 8110/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0204 - val_loss: 0.0410\n",
      "Epoch 8111/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0326 - val_loss: 0.0634\n",
      "Epoch 8112/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0224 - val_loss: 0.0117\n",
      "Epoch 8113/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0149 - val_loss: 0.0038\n",
      "Epoch 8114/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0084 - val_loss: 0.0047\n",
      "Epoch 8115/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 8116/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0064 - val_loss: 0.0092\n",
      "Epoch 8117/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0107 - val_loss: 0.0178\n",
      "Epoch 8118/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.020 - 0s 279us/sample - loss: 0.0132 - val_loss: 0.0308\n",
      "Epoch 8119/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0135 - val_loss: 0.0117\n",
      "Epoch 8120/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0119 - val_loss: 0.0064\n",
      "Epoch 8121/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.005 - 0s 132us/sample - loss: 0.0093 - val_loss: 0.0029\n",
      "Epoch 8122/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0060 - val_loss: 0.0146\n",
      "Epoch 8123/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0106 - val_loss: 0.0076\n",
      "Epoch 8124/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0093 - val_loss: 0.0026\n",
      "Epoch 8125/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0071 - val_loss: 0.0033\n",
      "Epoch 8126/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0077 - val_loss: 0.0057\n",
      "Epoch 8127/10000\n",
      "68/68 [==============================] - 0s 544us/sample - loss: 0.0101 - val_loss: 0.0118\n",
      "Epoch 8128/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0064 - val_loss: 0.0026\n",
      "Epoch 8129/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0035 - val_loss: 0.0037\n",
      "Epoch 8130/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 8131/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0050 - val_loss: 0.0033\n",
      "Epoch 8132/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0052 - val_loss: 0.0039\n",
      "Epoch 8133/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.0074 - val_loss: 0.0161\n",
      "Epoch 8134/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0127 - val_loss: 0.0090\n",
      "Epoch 8135/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0072 - val_loss: 0.0224\n",
      "Epoch 8136/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0099 - val_loss: 0.0271\n",
      "Epoch 8137/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0132 - val_loss: 0.0055\n",
      "Epoch 8138/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0094 - val_loss: 0.0024\n",
      "Epoch 8139/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0049 - val_loss: 0.0033\n",
      "Epoch 8140/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0033 - val_loss: 0.0025\n",
      "Epoch 8141/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 8142/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0037 - val_loss: 0.0043\n",
      "Epoch 8143/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 8144/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0033 - val_loss: 0.0019\n",
      "Epoch 8145/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0027 - val_loss: 0.0020\n",
      "Epoch 8146/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.0049 - val_loss: 0.0022\n",
      "Epoch 8147/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0086 - val_loss: 0.0087\n",
      "Epoch 8148/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0125 - val_loss: 0.0059\n",
      "Epoch 8149/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0066 - val_loss: 0.0020\n",
      "Epoch 8150/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0047 - val_loss: 0.0073\n",
      "Epoch 8151/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0038 - val_loss: 0.0019\n",
      "Epoch 8152/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0063 - val_loss: 0.0018\n",
      "Epoch 8153/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0070 - val_loss: 0.0050\n",
      "Epoch 8154/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0106 - val_loss: 0.0095\n",
      "Epoch 8155/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 0.0084 - val_loss: 0.0016\n",
      "Epoch 8156/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0069 - val_loss: 0.0084\n",
      "Epoch 8157/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0073 - val_loss: 0.0055\n",
      "Epoch 8158/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0034 - val_loss: 0.0030\n",
      "Epoch 8159/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 8160/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0039 - val_loss: 0.0021\n",
      "Epoch 8161/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0040 - val_loss: 0.0019\n",
      "Epoch 8162/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0059 - val_loss: 0.0081\n",
      "Epoch 8163/10000\n",
      "68/68 [==============================] - 0s 559us/sample - loss: 0.0068 - val_loss: 0.0051\n",
      "Epoch 8164/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0084 - val_loss: 0.0053\n",
      "Epoch 8165/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0034 - val_loss: 0.0020\n",
      "Epoch 8166/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 8167/10000\n",
      "68/68 [==============================] - 0s 735us/sample - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 8168/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 8169/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 8170/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0031 - val_loss: 0.0013\n",
      "Epoch 8171/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 8172/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 8173/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 8174/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 8175/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 8176/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0019 - val_loss: 0.0071\n",
      "Epoch 8177/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0042 - val_loss: 0.0020\n",
      "Epoch 8178/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.0032 - val_loss: 0.0012\n",
      "Epoch 8179/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 8180/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0018 - val_loss: 0.0040\n",
      "Epoch 8181/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0022 - val_loss: 0.0043\n",
      "Epoch 8182/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 8183/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0025 - val_loss: 0.0036\n",
      "Epoch 8184/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0047 - val_loss: 0.0072\n",
      "Epoch 8185/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.0035 - val_loss: 0.0021\n",
      "Epoch 8186/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0036 - val_loss: 0.0030\n",
      "Epoch 8187/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 8188/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0038 - val_loss: 0.0127\n",
      "Epoch 8189/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0055 - val_loss: 0.0034\n",
      "Epoch 8190/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0016 - val_loss: 0.0029\n",
      "Epoch 8191/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0026 - val_loss: 0.0029\n",
      "Epoch 8192/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0029 - val_loss: 0.0085\n",
      "Epoch 8193/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0040 - val_loss: 8.1741e-04\n",
      "Epoch 8194/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0017 - val_loss: 7.0511e-04\n",
      "Epoch 8195/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.7433e-04 - val_loss: 0.0011\n",
      "Epoch 8196/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0017 - val_loss: 0.0035\n",
      "Epoch 8197/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 8198/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 8199/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0022 - val_loss: 5.3282e-04\n",
      "Epoch 8200/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 8201/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0011 - val_loss: 7.2000e-04\n",
      "Epoch 8202/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0024 - val_loss: 0.0014\n",
      "Epoch 8203/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0013 - val_loss: 6.5622e-04\n",
      "Epoch 8204/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.1519e-04 - val_loss: 0.0019\n",
      "Epoch 8205/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0011 - val_loss: 4.8737e-04\n",
      "Epoch 8206/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 8207/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.001 - 0s 235us/sample - loss: 0.0017 - val_loss: 5.7669e-04\n",
      "Epoch 8208/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 8.9383e-04 - val_loss: 5.4145e-04\n",
      "Epoch 8209/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0015 - val_loss: 5.7974e-04\n",
      "Epoch 8210/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 8211/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 0.0028 - val_loss: 0.0024\n",
      "Epoch 8212/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0035 - val_loss: 0.0134\n",
      "Epoch 8213/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0150 - val_loss: 0.0044\n",
      "Epoch 8214/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0255 - val_loss: 0.0443\n",
      "Epoch 8215/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0452 - val_loss: 0.0687\n",
      "Epoch 8216/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0241 - val_loss: 8.2716e-04\n",
      "Epoch 8217/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 5.5063e-0 - 0s 147us/sample - loss: 0.0061 - val_loss: 0.0029\n",
      "Epoch 8218/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0083 - val_loss: 0.0152\n",
      "Epoch 8219/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0088 - val_loss: 0.0132\n",
      "Epoch 8220/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0337 - val_loss: 0.0175\n",
      "Epoch 8221/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0297 - val_loss: 0.0814\n",
      "Epoch 8222/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0743 - val_loss: 0.1700\n",
      "Epoch 8223/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1259 - val_loss: 0.0998\n",
      "Epoch 8224/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0536 - val_loss: 0.0145\n",
      "Epoch 8225/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0217 - val_loss: 0.0301\n",
      "Epoch 8226/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0137 - val_loss: 0.0234\n",
      "Epoch 8227/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 0.0280 - val_loss: 0.0081\n",
      "Epoch 8228/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0143 - val_loss: 0.0278\n",
      "Epoch 8229/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0483 - val_loss: 0.0447\n",
      "Epoch 8230/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0617 - val_loss: 0.0920\n",
      "Epoch 8231/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 0.0384 - val_loss: 0.0939\n",
      "Epoch 8232/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0430 - val_loss: 0.0783\n",
      "Epoch 8233/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0219 - val_loss: 0.0103\n",
      "Epoch 8234/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0073 - val_loss: 0.0024\n",
      "Epoch 8235/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0014 - val_loss: 4.8836e-04\n",
      "Epoch 8236/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.8152e-04 - val_loss: 8.4587e-04\n",
      "Epoch 8237/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.9334e-04 - val_loss: 3.3617e-04\n",
      "Epoch 8238/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.1163e-04 - val_loss: 2.8164e-04\n",
      "Epoch 8239/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.7627e-04 - val_loss: 5.6329e-04\n",
      "Epoch 8240/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3.5499e-04 - val_loss: 1.9337e-04\n",
      "Epoch 8241/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3.0413e-04 - val_loss: 3.7631e-04\n",
      "Epoch 8242/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 4.2578e-04 - val_loss: 9.2361e-04\n",
      "Epoch 8243/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0010 - val_loss: 0.0023\n",
      "Epoch 8244/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0035 - val_loss: 0.0048\n",
      "Epoch 8245/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0025 - val_loss: 0.0017\n",
      "Epoch 8246/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0031 - val_loss: 0.0037\n",
      "Epoch 8247/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0041 - val_loss: 0.0032\n",
      "Epoch 8248/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0037 - val_loss: 0.0060\n",
      "Epoch 8249/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0051 - val_loss: 0.0044\n",
      "Epoch 8250/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.0080 - val_loss: 0.0388\n",
      "Epoch 8251/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0202 - val_loss: 0.0100\n",
      "Epoch 8252/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0234 - val_loss: 0.0293\n",
      "Epoch 8253/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0163 - val_loss: 0.0215\n",
      "Epoch 8254/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0069 - val_loss: 0.0044\n",
      "Epoch 8255/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 8256/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 8.6204e-04 - val_loss: 1.7591e-04\n",
      "Epoch 8257/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 5.0662e-04 - val_loss: 5.0791e-04\n",
      "Epoch 8258/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 7.3545e-04 - val_loss: 0.0018\n",
      "Epoch 8259/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 7.6690e-04 - val_loss: 4.2927e-04\n",
      "Epoch 8260/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 4.0381e-04 - val_loss: 3.5924e-04\n",
      "Epoch 8261/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.2650e-04 - val_loss: 7.8145e-04\n",
      "Epoch 8262/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0023 - val_loss: 0.0053\n",
      "Epoch 8263/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.0041 - val_loss: 0.0043\n",
      "Epoch 8264/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0031 - val_loss: 0.0034\n",
      "Epoch 8265/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 8266/10000\n",
      "68/68 [==============================] - 0s 750us/sample - loss: 7.6446e-04 - val_loss: 7.5523e-04\n",
      "Epoch 8267/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.7984e-04 - val_loss: 3.1294e-04\n",
      "Epoch 8268/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3.9819e-04 - val_loss: 9.6656e-04\n",
      "Epoch 8269/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.5297e-04 - val_loss: 4.7992e-04\n",
      "Epoch 8270/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 3.1585e-04 - val_loss: 1.0672e-04\n",
      "Epoch 8271/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.3034e-04 - val_loss: 3.3221e-04\n",
      "Epoch 8272/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0017 - val_loss: 0.0070\n",
      "Epoch 8273/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0053 - val_loss: 0.0156\n",
      "Epoch 8274/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0154 - val_loss: 0.0144\n",
      "Epoch 8275/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0289 - val_loss: 0.0562\n",
      "Epoch 8276/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0233 - val_loss: 0.0419\n",
      "Epoch 8277/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0170 - val_loss: 6.0090e-04\n",
      "Epoch 8278/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0079 - val_loss: 0.0029\n",
      "Epoch 8279/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0020 - val_loss: 0.0068\n",
      "Epoch 8280/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.0081 - val_loss: 0.0061\n",
      "Epoch 8281/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0039 - val_loss: 0.0033\n",
      "Epoch 8282/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0047 - val_loss: 0.0040\n",
      "Epoch 8283/10000\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 9.0497e-04 - val_loss: 9.8720e-04\n",
      "Epoch 8284/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.4088e-04 - val_loss: 6.3882e-05\n",
      "Epoch 8285/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.7047e-04 - val_loss: 0.0017\n",
      "Epoch 8286/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.5370e-04 - val_loss: 6.9605e-04\n",
      "Epoch 8287/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 4.8360e-04 - val_loss: 5.9549e-04\n",
      "Epoch 8288/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 3.5251e-04 - val_loss: 9.4223e-04\n",
      "Epoch 8289/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.8278e-04 - val_loss: 0.0038\n",
      "Epoch 8290/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0012 - val_loss: 5.7580e-04\n",
      "Epoch 8291/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9990e-04 - val_loss: 5.5590e-04\n",
      "Epoch 8292/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 2.8858e-04 - val_loss: 1.6123e-04\n",
      "Epoch 8293/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 1.8933e-04 - val_loss: 2.7593e-04\n",
      "Epoch 8294/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.0824e-04 - val_loss: 8.6950e-05\n",
      "Epoch 8295/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1.0126e-04 - val_loss: 1.6273e-04\n",
      "Epoch 8296/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.5061e-04 - val_loss: 2.2099e-04\n",
      "Epoch 8297/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.6293e-04 - val_loss: 7.8525e-05\n",
      "Epoch 8298/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.2758e-04 - val_loss: 4.4631e-05\n",
      "Epoch 8299/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.5813e-05 - val_loss: 1.3880e-04\n",
      "Epoch 8300/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 1.4553e-0 - 0s 221us/sample - loss: 1.0690e-04 - val_loss: 4.9572e-05\n",
      "Epoch 8301/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.7568e-05 - val_loss: 1.2664e-04\n",
      "Epoch 8302/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.1080e-04 - val_loss: 1.7925e-04\n",
      "Epoch 8303/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.2950e-04 - val_loss: 5.4534e-04\n",
      "Epoch 8304/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.3470e-04 - val_loss: 4.5899e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8305/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.9920e-04 - val_loss: 5.2474e-05\n",
      "Epoch 8306/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.2939e-04 - val_loss: 1.0036e-04\n",
      "Epoch 8307/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.8299e-04 - val_loss: 0.0011\n",
      "Epoch 8308/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 5.0709e-04 - val_loss: 7.7156e-04\n",
      "Epoch 8309/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.3401e-04 - val_loss: 4.3274e-04\n",
      "Epoch 8310/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.2408e-04 - val_loss: 0.0025\n",
      "Epoch 8311/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 8312/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0011 - val_loss: 3.7173e-05\n",
      "Epoch 8313/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0022 - val_loss: 2.7791e-05\n",
      "Epoch 8314/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0015 - val_loss: 7.0340e-04\n",
      "Epoch 8315/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 8316/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.1125e-04 - val_loss: 3.3996e-04\n",
      "Epoch 8317/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 8318/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0032 - val_loss: 3.9300e-04\n",
      "Epoch 8319/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0036 - val_loss: 6.3744e-04\n",
      "Epoch 8320/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.9659e-04 - val_loss: 2.7913e-05\n",
      "Epoch 8321/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.9346e-04 - val_loss: 0.0017\n",
      "Epoch 8322/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.1934e-04 - val_loss: 2.5890e-04\n",
      "Epoch 8323/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.8981e-04 - val_loss: 3.1239e-04\n",
      "Epoch 8324/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.5369e-04 - val_loss: 1.6577e-04\n",
      "Epoch 8325/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.6811e-04 - val_loss: 8.7906e-05\n",
      "Epoch 8326/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.1794e-04 - val_loss: 8.5636e-05\n",
      "Epoch 8327/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 4.1717e-04 - val_loss: 0.0038\n",
      "Epoch 8328/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0027 - val_loss: 0.0060\n",
      "Epoch 8329/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0037 - val_loss: 0.0476\n",
      "Epoch 8330/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0479 - val_loss: 0.0298\n",
      "Epoch 8331/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0253 - val_loss: 0.0089\n",
      "Epoch 8332/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0142 - val_loss: 0.0165\n",
      "Epoch 8333/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.019 - 0s 206us/sample - loss: 0.0817 - val_loss: 0.0428\n",
      "Epoch 8334/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.2487 - val_loss: 0.4242\n",
      "Epoch 8335/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.2423 - val_loss: 0.8319\n",
      "Epoch 8336/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.5142 - val_loss: 3.5383\n",
      "Epoch 8337/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.8539 - val_loss: 31.7056\n",
      "Epoch 8338/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 16.5554 - val_loss: 2.3560\n",
      "Epoch 8339/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 4.3381 - val_loss: 1.3801\n",
      "Epoch 8340/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.3949 - val_loss: 1.9845\n",
      "Epoch 8341/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1.9114 - val_loss: 1.0969\n",
      "Epoch 8342/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 11.4631 - val_loss: 121.1194\n",
      "Epoch 8343/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 135.6368 - val_loss: 308.4170\n",
      "Epoch 8344/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 140.6278 - val_loss: 185.0401\n",
      "Epoch 8345/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 71.1763 - val_loss: 93.3030\n",
      "Epoch 8346/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 146.0446 - val_loss: 73.9950\n",
      "Epoch 8347/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 130.7039 - val_loss: 172.0263\n",
      "Epoch 8348/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1178.3047 - val_loss: 9244.8002\n",
      "Epoch 8349/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 7305.9206 - val_loss: 12754.1006\n",
      "Epoch 8350/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6114.8016 - val_loss: 2051.6916\n",
      "Epoch 8351/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3077.3545 - val_loss: 2049.7423\n",
      "Epoch 8352/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1082.0369 - val_loss: 682.6399\n",
      "Epoch 8353/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2782.3925 - val_loss: 1145.0766\n",
      "Epoch 8354/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 788.6049 - val_loss: 614.8327\n",
      "Epoch 8355/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 456.6370 - val_loss: 361.9907\n",
      "Epoch 8356/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 138.8527 - val_loss: 184.2489\n",
      "Epoch 8357/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 83.0576 - val_loss: 102.2982\n",
      "Epoch 8358/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 52.5602 - val_loss: 105.4366\n",
      "Epoch 8359/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 66.6849 - val_loss: 13.0450\n",
      "Epoch 8360/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 18.6748 - val_loss: 8.1662\n",
      "Epoch 8361/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 28.3804 - val_loss: 43.9012\n",
      "Epoch 8362/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 35.6472 - val_loss: 164.8698\n",
      "Epoch 8363/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 62.1829 - val_loss: 42.1548\n",
      "Epoch 8364/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 56.2691 - val_loss: 18.2827\n",
      "Epoch 8365/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 55.6871 - val_loss: 253.7767\n",
      "Epoch 8366/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 201.5884 - val_loss: 133.3601\n",
      "Epoch 8367/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 107.9151 - val_loss: 182.1214\n",
      "Epoch 8368/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 133.7255 - val_loss: 6.3394\n",
      "Epoch 8369/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 35.5742 - val_loss: 48.7921\n",
      "Epoch 8370/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 48.9565 - val_loss: 105.7276\n",
      "Epoch 8371/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 52.4286 - val_loss: 50.4787\n",
      "Epoch 8372/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 49.1805 - val_loss: 93.7233\n",
      "Epoch 8373/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 68.3201 - val_loss: 29.0263\n",
      "Epoch 8374/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 81.4509 - val_loss: 257.7243\n",
      "Epoch 8375/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 78.8345 - val_loss: 38.7069\n",
      "Epoch 8376/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 63.3452 - val_loss: 9.7907\n",
      "Epoch 8377/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 94.7863 - val_loss: 230.8680\n",
      "Epoch 8378/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 191us/sample - loss: 95.7622 - val_loss: 89.7813\n",
      "Epoch 8379/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 62.4565 - val_loss: 60.1604\n",
      "Epoch 8380/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 72.5012 - val_loss: 140.3162\n",
      "Epoch 8381/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 72.3222 - val_loss: 27.1309\n",
      "Epoch 8382/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 54.5735 - val_loss: 15.9487\n",
      "Epoch 8383/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 37.5729 - val_loss: 41.7839\n",
      "Epoch 8384/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 13.4068 - val_loss: 33.6377\n",
      "Epoch 8385/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 14.5511 - val_loss: 22.5302\n",
      "Epoch 8386/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 25.8773 - val_loss: 63.6928\n",
      "Epoch 8387/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 24.7135 - val_loss: 7.6232\n",
      "Epoch 8388/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 13.6197 - val_loss: 24.4145\n",
      "Epoch 8389/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 11.8027 - val_loss: 4.4553\n",
      "Epoch 8390/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.4556 - val_loss: 0.7745\n",
      "Epoch 8391/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 1.8249 - val_loss: 1.4592\n",
      "Epoch 8392/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.3041 - val_loss: 0.4504\n",
      "Epoch 8393/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.5327 - val_loss: 0.4881\n",
      "Epoch 8394/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.9599 - val_loss: 0.2650\n",
      "Epoch 8395/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.5062 - val_loss: 0.5130\n",
      "Epoch 8396/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.3523 - val_loss: 0.2301\n",
      "Epoch 8397/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3299 - val_loss: 0.1004\n",
      "Epoch 8398/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1446 - val_loss: 0.3676\n",
      "Epoch 8399/10000\n",
      "68/68 [==============================] - 0s 956us/sample - loss: 0.1854 - val_loss: 0.0492\n",
      "Epoch 8400/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1125 - val_loss: 0.2274\n",
      "Epoch 8401/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1479 - val_loss: 0.0094\n",
      "Epoch 8402/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0942 - val_loss: 0.1710\n",
      "Epoch 8403/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 0.0746 - val_loss: 0.0487\n",
      "Epoch 8404/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.0413 - val_loss: 0.1037\n",
      "Epoch 8405/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0446 - val_loss: 0.0592\n",
      "Epoch 8406/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0432 - val_loss: 0.0581\n",
      "Epoch 8407/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 0.0175 - val_loss: 0.0330\n",
      "Epoch 8408/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0119 - val_loss: 0.0069\n",
      "Epoch 8409/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0056 - val_loss: 0.0075\n",
      "Epoch 8410/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0054 - val_loss: 0.0021\n",
      "Epoch 8411/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 0.0037 - val_loss: 0.0181\n",
      "Epoch 8412/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0151 - val_loss: 0.0114\n",
      "Epoch 8413/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0107 - val_loss: 0.0342\n",
      "Epoch 8414/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0175 - val_loss: 0.0058\n",
      "Epoch 8415/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0324 - val_loss: 0.0483\n",
      "Epoch 8416/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0478 - val_loss: 0.0552\n",
      "Epoch 8417/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0205 - val_loss: 0.0078\n",
      "Epoch 8418/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0217 - val_loss: 0.0504\n",
      "Epoch 8419/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0308 - val_loss: 0.0698\n",
      "Epoch 8420/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.0465 - val_loss: 0.0018\n",
      "Epoch 8421/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0293 - val_loss: 0.0207\n",
      "Epoch 8422/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0223 - val_loss: 0.0678\n",
      "Epoch 8423/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0299 - val_loss: 0.0011\n",
      "Epoch 8424/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.0130 - val_loss: 0.0060\n",
      "Epoch 8425/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0067 - val_loss: 0.0015\n",
      "Epoch 8426/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.6550e-04 - val_loss: 0.0015\n",
      "Epoch 8427/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 7.0477e-04 - val_loss: 5.1312e-04\n",
      "Epoch 8428/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.1601e-04 - val_loss: 4.4093e-04\n",
      "Epoch 8429/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.2243e-04 - val_loss: 8.9650e-04\n",
      "Epoch 8430/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.6900e-04 - val_loss: 3.1470e-04\n",
      "Epoch 8431/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.6734e-04 - val_loss: 5.1331e-04\n",
      "Epoch 8432/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.1308e-04 - val_loss: 3.6386e-04\n",
      "Epoch 8433/10000\n",
      "68/68 [==============================] - 0s 544us/sample - loss: 6.5274e-04 - val_loss: 4.3041e-04\n",
      "Epoch 8434/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.9307e-04 - val_loss: 9.0567e-04\n",
      "Epoch 8435/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.4163e-04 - val_loss: 4.3601e-04\n",
      "Epoch 8436/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.6944e-04 - val_loss: 7.0608e-04\n",
      "Epoch 8437/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.8015e-04 - val_loss: 0.0021\n",
      "Epoch 8438/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0010 - val_loss: 9.0077e-04\n",
      "Epoch 8439/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0018 - val_loss: 0.0074\n",
      "Epoch 8440/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0085 - val_loss: 0.0098\n",
      "Epoch 8441/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0109 - val_loss: 0.0019\n",
      "Epoch 8442/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0080 - val_loss: 0.0508\n",
      "Epoch 8443/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0609 - val_loss: 0.0168\n",
      "Epoch 8444/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0340 - val_loss: 0.0722\n",
      "Epoch 8445/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0947 - val_loss: 0.0689\n",
      "Epoch 8446/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0478 - val_loss: 0.0423\n",
      "Epoch 8447/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0553 - val_loss: 0.0799\n",
      "Epoch 8448/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0261 - val_loss: 0.0603\n",
      "Epoch 8449/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0407 - val_loss: 0.0078\n",
      "Epoch 8450/10000\n",
      "68/68 [==============================] - 0s 647us/sample - loss: 0.0217 - val_loss: 0.0036\n",
      "Epoch 8451/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0062 - val_loss: 0.0053\n",
      "Epoch 8452/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0059 - val_loss: 0.0189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8453/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0073 - val_loss: 0.0123\n",
      "Epoch 8454/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0239 - val_loss: 0.0422\n",
      "Epoch 8455/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0393 - val_loss: 0.0740\n",
      "Epoch 8456/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1460 - val_loss: 0.0655\n",
      "Epoch 8457/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0255 - val_loss: 0.0624\n",
      "Epoch 8458/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0258 - val_loss: 0.0228\n",
      "Epoch 8459/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 8460/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0561 - val_loss: 0.0774\n",
      "Epoch 8461/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.2355 - val_loss: 1.0228\n",
      "Epoch 8462/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1.3233 - val_loss: 1.1101\n",
      "Epoch 8463/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7319 - val_loss: 0.2396\n",
      "Epoch 8464/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.6206 - val_loss: 0.6601\n",
      "Epoch 8465/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.570 - 0s 471us/sample - loss: 0.2910 - val_loss: 0.6735\n",
      "Epoch 8466/10000\n",
      "68/68 [==============================] - 0s 853us/sample - loss: 0.2929 - val_loss: 0.3105\n",
      "Epoch 8467/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0150 - val_loss: 0.3048\n",
      "Epoch 8468/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2341 - val_loss: 0.0416\n",
      "Epoch 8469/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2354 - val_loss: 0.5383\n",
      "Epoch 8470/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1845 - val_loss: 0.1986\n",
      "Epoch 8471/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 0.0936 - val_loss: 0.1415\n",
      "Epoch 8472/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1033 - val_loss: 0.2065\n",
      "Epoch 8473/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0812 - val_loss: 0.1107\n",
      "Epoch 8474/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0546 - val_loss: 0.0133\n",
      "Epoch 8475/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1996 - val_loss: 0.1235\n",
      "Epoch 8476/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3481 - val_loss: 0.2250\n",
      "Epoch 8477/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1468 - val_loss: 0.2211\n",
      "Epoch 8478/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2688 - val_loss: 0.7448\n",
      "Epoch 8479/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1642 - val_loss: 0.0093\n",
      "Epoch 8480/10000\n",
      "68/68 [==============================] - 0s 706us/sample - loss: 0.0188 - val_loss: 0.0251\n",
      "Epoch 8481/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.0071 - val_loss: 5.8017e-04\n",
      "Epoch 8482/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0110 - val_loss: 0.0167\n",
      "Epoch 8483/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0051 - val_loss: 0.0025\n",
      "Epoch 8484/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0059 - val_loss: 0.0110\n",
      "Epoch 8485/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0260 - val_loss: 0.0218\n",
      "Epoch 8486/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0497 - val_loss: 0.0971\n",
      "Epoch 8487/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0433 - val_loss: 0.0075\n",
      "Epoch 8488/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0068 - val_loss: 0.0082\n",
      "Epoch 8489/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0179 - val_loss: 0.0319\n",
      "Epoch 8490/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.0293 - val_loss: 0.1513\n",
      "Epoch 8491/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0600 - val_loss: 4.2396e-04\n",
      "Epoch 8492/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0170 - val_loss: 0.0599\n",
      "Epoch 8493/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.0161 - val_loss: 0.0120\n",
      "Epoch 8494/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0101 - val_loss: 0.0297\n",
      "Epoch 8495/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0113 - val_loss: 0.0268\n",
      "Epoch 8496/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0185 - val_loss: 0.0160\n",
      "Epoch 8497/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 0.0226 - val_loss: 0.1625\n",
      "Epoch 8498/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0621 - val_loss: 0.0056\n",
      "Epoch 8499/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0441 - val_loss: 0.0656\n",
      "Epoch 8500/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.8698 - val_loss: 2.7057\n",
      "Epoch 8501/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.7230 - val_loss: 6.2490\n",
      "Epoch 8502/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.9266 - val_loss: 11.1686\n",
      "Epoch 8503/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 4.2800 - val_loss: 2.5105\n",
      "Epoch 8504/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.9523 - val_loss: 8.9369\n",
      "Epoch 8505/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.4704 - val_loss: 6.7702\n",
      "Epoch 8506/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.3343 - val_loss: 3.6641\n",
      "Epoch 8507/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.7845 - val_loss: 2.8667\n",
      "Epoch 8508/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.2024 - val_loss: 4.7615\n",
      "Epoch 8509/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5.7787 - val_loss: 51.1690\n",
      "Epoch 8510/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 24.6897 - val_loss: 14.4199\n",
      "Epoch 8511/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.6146 - val_loss: 33.4488\n",
      "Epoch 8512/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 23.9339 - val_loss: 35.1646\n",
      "Epoch 8513/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 61.3459 - val_loss: 83.7418\n",
      "Epoch 8514/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 228.5097 - val_loss: 710.5345\n",
      "Epoch 8515/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2081.0009 - val_loss: 599.4745\n",
      "Epoch 8516/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 635.3151 - val_loss: 319.5878\n",
      "Epoch 8517/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 433.7884 - val_loss: 2915.6598\n",
      "Epoch 8518/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3201.6608 - val_loss: 17678.3905\n",
      "Epoch 8519/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 14988.8363 - val_loss: 159.7512\n",
      "Epoch 8520/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5226.7977 - val_loss: 525.8293\n",
      "Epoch 8521/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1145.1189 - val_loss: 313.0469\n",
      "Epoch 8522/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1299.7909 - val_loss: 1915.4383\n",
      "Epoch 8523/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2132.4182 - val_loss: 3183.2036\n",
      "Epoch 8524/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3118.4029 - val_loss: 6520.6326\n",
      "Epoch 8525/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4152.1494 - val_loss: 4231.8162\n",
      "Epoch 8526/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4747.5789 - val_loss: 1472.4333\n",
      "Epoch 8527/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2516.0019 - val_loss: 700.5454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8528/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2775.5007 - val_loss: 525.0096\n",
      "Epoch 8529/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2250.4145 - val_loss: 3171.1760\n",
      "Epoch 8530/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2825.3654 - val_loss: 3752.9436\n",
      "Epoch 8531/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 2874.4088 - val_loss: 1651.4765\n",
      "Epoch 8532/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2557.8847 - val_loss: 1395.7335\n",
      "Epoch 8533/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2215.9464 - val_loss: 1554.4083\n",
      "Epoch 8534/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 570.3919 - val_loss: 865.0420\n",
      "Epoch 8535/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 416.6173 - val_loss: 65.2241\n",
      "Epoch 8536/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 596.5667 - val_loss: 1052.7750\n",
      "Epoch 8537/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 845.3628 - val_loss: 1605.9721\n",
      "Epoch 8538/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 847.3290 - val_loss: 1789.8358\n",
      "Epoch 8539/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1151.5456 - val_loss: 724.6170\n",
      "Epoch 8540/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 940.5572 - val_loss: 696.7504\n",
      "Epoch 8541/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 631.5904 - val_loss: 865.7833\n",
      "Epoch 8542/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 313.8938 - val_loss: 272.5445\n",
      "Epoch 8543/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 293.3637 - val_loss: 540.5316\n",
      "Epoch 8544/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 164.9748 - val_loss: 50.4153\n",
      "Epoch 8545/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 96.7689 - val_loss: 59.4919\n",
      "Epoch 8546/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 86.4835 - val_loss: 226.7286\n",
      "Epoch 8547/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 99.7431 - val_loss: 87.3476\n",
      "Epoch 8548/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 54.1832 - val_loss: 62.6068\n",
      "Epoch 8549/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 61.7301 - val_loss: 6.7757\n",
      "Epoch 8550/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 57.8560 - val_loss: 111.0583\n",
      "Epoch 8551/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 56.2620 - val_loss: 24.4411\n",
      "Epoch 8552/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 24.0388 - val_loss: 62.1294\n",
      "Epoch 8553/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 18.7125 - val_loss: 19.0589\n",
      "Epoch 8554/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 8.3706 - val_loss: 2.6306\n",
      "Epoch 8555/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 1.8469 - val_loss: 0.3107\n",
      "Epoch 8556/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3765 - val_loss: 1.6128\n",
      "Epoch 8557/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.9512 - val_loss: 0.8812\n",
      "Epoch 8558/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3455 - val_loss: 0.1556\n",
      "Epoch 8559/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1170 - val_loss: 0.1467\n",
      "Epoch 8560/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.1273 - val_loss: 0.0599\n",
      "Epoch 8561/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0914 - val_loss: 0.0832\n",
      "Epoch 8562/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0798 - val_loss: 0.1073\n",
      "Epoch 8563/10000\n",
      "68/68 [==============================] - 0s 809us/sample - loss: 0.1225 - val_loss: 0.1826\n",
      "Epoch 8564/10000\n",
      "68/68 [==============================] - 0s 750us/sample - loss: 0.1714 - val_loss: 0.0762\n",
      "Epoch 8565/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2024 - val_loss: 0.0744\n",
      "Epoch 8566/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1014 - val_loss: 0.0748\n",
      "Epoch 8567/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0766 - val_loss: 0.0555\n",
      "Epoch 8568/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0764 - val_loss: 0.0779\n",
      "Epoch 8569/10000\n",
      "68/68 [==============================] - 0s 824us/sample - loss: 0.1613 - val_loss: 0.2793\n",
      "Epoch 8570/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4632 - val_loss: 0.2412\n",
      "Epoch 8571/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5082 - val_loss: 0.7646\n",
      "Epoch 8572/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5461 - val_loss: 1.2111\n",
      "Epoch 8573/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 0.8691 - val_loss: 0.7070\n",
      "Epoch 8574/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4411 - val_loss: 0.4451\n",
      "Epoch 8575/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2321 - val_loss: 0.1628\n",
      "Epoch 8576/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1560 - val_loss: 0.0518\n",
      "Epoch 8577/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1611 - val_loss: 0.0427\n",
      "Epoch 8578/10000\n",
      "68/68 [==============================] - 0s 868us/sample - loss: 0.1150 - val_loss: 0.1274\n",
      "Epoch 8579/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1298 - val_loss: 0.2075\n",
      "Epoch 8580/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1777 - val_loss: 0.1436\n",
      "Epoch 8581/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1991 - val_loss: 0.2801\n",
      "Epoch 8582/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.3110 - val_loss: 0.0731\n",
      "Epoch 8583/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1469 - val_loss: 0.1008\n",
      "Epoch 8584/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0976 - val_loss: 0.3192\n",
      "Epoch 8585/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2789 - val_loss: 0.1480\n",
      "Epoch 8586/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.3198 - val_loss: 0.1085\n",
      "Epoch 8587/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1137 - val_loss: 0.0403\n",
      "Epoch 8588/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 0.0771 - val_loss: 0.0488\n",
      "Epoch 8589/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1093 - val_loss: 0.0839\n",
      "Epoch 8590/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0629 - val_loss: 0.0596\n",
      "Epoch 8591/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0576 - val_loss: 0.0363\n",
      "Epoch 8592/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0458 - val_loss: 0.0525\n",
      "Epoch 8593/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0532 - val_loss: 0.0397\n",
      "Epoch 8594/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 0.0506 - val_loss: 0.0884\n",
      "Epoch 8595/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0598 - val_loss: 0.0523\n",
      "Epoch 8596/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.0433 - val_loss: 0.0339\n",
      "Epoch 8597/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.0673 - val_loss: 0.0524\n",
      "Epoch 8598/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0641 - val_loss: 0.1012\n",
      "Epoch 8599/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0659 - val_loss: 0.0355\n",
      "Epoch 8600/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0762 - val_loss: 0.0426\n",
      "Epoch 8601/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1322 - val_loss: 0.0710\n",
      "Epoch 8602/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1472 - val_loss: 0.0709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8603/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1514 - val_loss: 0.1807\n",
      "Epoch 8604/10000\n",
      "68/68 [==============================] - 0s 956us/sample - loss: 0.1287 - val_loss: 0.1841\n",
      "Epoch 8605/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1108 - val_loss: 0.0451\n",
      "Epoch 8606/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1516 - val_loss: 0.1505\n",
      "Epoch 8607/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0430 - val_loss: 0.0333\n",
      "Epoch 8608/10000\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 0.0395 - val_loss: 0.0220\n",
      "Epoch 8609/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0370 - val_loss: 0.0324\n",
      "Epoch 8610/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0691 - val_loss: 0.0769\n",
      "Epoch 8611/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0758 - val_loss: 0.1106\n",
      "Epoch 8612/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0777 - val_loss: 0.1202\n",
      "Epoch 8613/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 0.1042 - val_loss: 0.0764\n",
      "Epoch 8614/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0734 - val_loss: 0.0332\n",
      "Epoch 8615/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0429 - val_loss: 0.0282\n",
      "Epoch 8616/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0502 - val_loss: 0.0237\n",
      "Epoch 8617/10000\n",
      "68/68 [==============================] - 0s 721us/sample - loss: 0.0337 - val_loss: 0.0396\n",
      "Epoch 8618/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0372 - val_loss: 0.0438\n",
      "Epoch 8619/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0386 - val_loss: 0.0620\n",
      "Epoch 8620/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0372 - val_loss: 0.0276\n",
      "Epoch 8621/10000\n",
      "68/68 [==============================] - 0s 588us/sample - loss: 0.0361 - val_loss: 0.0327\n",
      "Epoch 8622/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0356 - val_loss: 0.0568\n",
      "Epoch 8623/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1004 - val_loss: 0.0153\n",
      "Epoch 8624/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1833 - val_loss: 0.3593\n",
      "Epoch 8625/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3235 - val_loss: 0.1725\n",
      "Epoch 8626/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1350 - val_loss: 0.1926\n",
      "Epoch 8627/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.1328 - val_loss: 0.1055\n",
      "Epoch 8628/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2267 - val_loss: 0.1442\n",
      "Epoch 8629/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.3989 - val_loss: 0.0936\n",
      "Epoch 8630/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.1871 - val_loss: 0.1990\n",
      "Epoch 8631/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0930 - val_loss: 0.3318\n",
      "Epoch 8632/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 0.1035 - val_loss: 0.0340\n",
      "Epoch 8633/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0223 - val_loss: 0.0200\n",
      "Epoch 8634/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0235 - val_loss: 0.0336\n",
      "Epoch 8635/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0423 - val_loss: 0.0126\n",
      "Epoch 8636/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0201 - val_loss: 0.1034\n",
      "Epoch 8637/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0594 - val_loss: 0.0835\n",
      "Epoch 8638/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0597 - val_loss: 0.0188\n",
      "Epoch 8639/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0677 - val_loss: 0.0135\n",
      "Epoch 8640/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0933 - val_loss: 0.1455\n",
      "Epoch 8641/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1457 - val_loss: 0.2785\n",
      "Epoch 8642/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.1469 - val_loss: 0.0485\n",
      "Epoch 8643/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1389 - val_loss: 0.2090\n",
      "Epoch 8644/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1042 - val_loss: 0.1891\n",
      "Epoch 8645/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0722 - val_loss: 0.1245\n",
      "Epoch 8646/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0575 - val_loss: 0.0268\n",
      "Epoch 8647/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0500 - val_loss: 0.1225\n",
      "Epoch 8648/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1112 - val_loss: 0.1212\n",
      "Epoch 8649/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1530 - val_loss: 0.2965\n",
      "Epoch 8650/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 0.1925 - val_loss: 0.0442\n",
      "Epoch 8651/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0574 - val_loss: 0.0282\n",
      "Epoch 8652/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0386 - val_loss: 0.0482\n",
      "Epoch 8653/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0449 - val_loss: 0.0643\n",
      "Epoch 8654/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0614 - val_loss: 0.0916\n",
      "Epoch 8655/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.0481 - val_loss: 0.0292\n",
      "Epoch 8656/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0301 - val_loss: 0.0160\n",
      "Epoch 8657/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0420 - val_loss: 0.1262\n",
      "Epoch 8658/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0700 - val_loss: 0.0444\n",
      "Epoch 8659/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0249 - val_loss: 0.0153\n",
      "Epoch 8660/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0291 - val_loss: 0.0112\n",
      "Epoch 8661/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0230 - val_loss: 0.0403\n",
      "Epoch 8662/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0252 - val_loss: 0.0798\n",
      "Epoch 8663/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0419 - val_loss: 0.1047\n",
      "Epoch 8664/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0817 - val_loss: 0.0106\n",
      "Epoch 8665/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0220 - val_loss: 0.0279\n",
      "Epoch 8666/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0170 - val_loss: 0.0062\n",
      "Epoch 8667/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0088 - val_loss: 0.0150\n",
      "Epoch 8668/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0190 - val_loss: 0.0177\n",
      "Epoch 8669/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0157 - val_loss: 0.0189\n",
      "Epoch 8670/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0234 - val_loss: 0.0119\n",
      "Epoch 8671/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0179 - val_loss: 0.0209\n",
      "Epoch 8672/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0299 - val_loss: 0.0380\n",
      "Epoch 8673/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0245 - val_loss: 0.0054\n",
      "Epoch 8674/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0162 - val_loss: 0.0195\n",
      "Epoch 8675/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0199 - val_loss: 0.0356\n",
      "Epoch 8676/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0254 - val_loss: 0.0271\n",
      "Epoch 8677/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0349 - val_loss: 0.0591\n",
      "Epoch 8678/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0728 - val_loss: 0.2066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8679/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.5335 - val_loss: 0.4622\n",
      "Epoch 8680/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2308 - val_loss: 0.0120\n",
      "Epoch 8681/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.0863 - val_loss: 0.1975\n",
      "Epoch 8682/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.1150 - val_loss: 0.0368\n",
      "Epoch 8683/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0653 - val_loss: 0.1150\n",
      "Epoch 8684/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0542 - val_loss: 0.1435\n",
      "Epoch 8685/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0550 - val_loss: 0.0492\n",
      "Epoch 8686/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.1116 - val_loss: 0.1018\n",
      "Epoch 8687/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.1495 - val_loss: 0.3696\n",
      "Epoch 8688/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2066 - val_loss: 0.0654\n",
      "Epoch 8689/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.1556 - val_loss: 0.4726\n",
      "Epoch 8690/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.2022 - val_loss: 0.1092\n",
      "Epoch 8691/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0354 - val_loss: 0.0145\n",
      "Epoch 8692/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0069 - val_loss: 0.0050\n",
      "Epoch 8693/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0054 - val_loss: 0.0042\n",
      "Epoch 8694/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0168 - val_loss: 0.0300\n",
      "Epoch 8695/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0294 - val_loss: 0.0180\n",
      "Epoch 8696/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0193 - val_loss: 0.0025\n",
      "Epoch 8697/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0039 - val_loss: 0.0024\n",
      "Epoch 8698/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0055 - val_loss: 0.0042\n",
      "Epoch 8699/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0101 - val_loss: 0.0206\n",
      "Epoch 8700/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0120 - val_loss: 0.0042\n",
      "Epoch 8701/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0085 - val_loss: 0.0230\n",
      "Epoch 8702/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0284 - val_loss: 0.0549\n",
      "Epoch 8703/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0580 - val_loss: 0.0282\n",
      "Epoch 8704/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0606 - val_loss: 0.1483\n",
      "Epoch 8705/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0868 - val_loss: 0.0300\n",
      "Epoch 8706/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.2536 - val_loss: 0.5476\n",
      "Epoch 8707/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3509 - val_loss: 0.3340\n",
      "Epoch 8708/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1312 - val_loss: 0.0697\n",
      "Epoch 8709/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0889 - val_loss: 0.0152\n",
      "Epoch 8710/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0619 - val_loss: 0.1314\n",
      "Epoch 8711/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0810 - val_loss: 0.1239\n",
      "Epoch 8712/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0636 - val_loss: 0.1097\n",
      "Epoch 8713/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0298 - val_loss: 0.0186\n",
      "Epoch 8714/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0120 - val_loss: 0.0023\n",
      "Epoch 8715/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0091 - val_loss: 0.0043\n",
      "Epoch 8716/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0094 - val_loss: 0.0052\n",
      "Epoch 8717/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0119 - val_loss: 0.0216\n",
      "Epoch 8718/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0119 - val_loss: 0.0095\n",
      "Epoch 8719/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0134 - val_loss: 0.0205\n",
      "Epoch 8720/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0220 - val_loss: 0.0226\n",
      "Epoch 8721/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0180 - val_loss: 0.0462\n",
      "Epoch 8722/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0593 - val_loss: 0.2829\n",
      "Epoch 8723/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1704 - val_loss: 0.3725\n",
      "Epoch 8724/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3566 - val_loss: 0.4654\n",
      "Epoch 8725/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3382 - val_loss: 1.9803\n",
      "Epoch 8726/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.6916 - val_loss: 0.4870\n",
      "Epoch 8727/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.6461 - val_loss: 0.1362\n",
      "Epoch 8728/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3636 - val_loss: 0.0564\n",
      "Epoch 8729/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1492 - val_loss: 0.0419\n",
      "Epoch 8730/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0520 - val_loss: 0.2603\n",
      "Epoch 8731/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.2191 - val_loss: 0.0428\n",
      "Epoch 8732/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0779 - val_loss: 0.1928\n",
      "Epoch 8733/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.2132 - val_loss: 0.0062\n",
      "Epoch 8734/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.004 - 0s 206us/sample - loss: 0.1442 - val_loss: 0.1374\n",
      "Epoch 8735/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0458 - val_loss: 0.0710\n",
      "Epoch 8736/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0276 - val_loss: 0.0249\n",
      "Epoch 8737/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0361 - val_loss: 0.1200\n",
      "Epoch 8738/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.1184 - val_loss: 0.2514\n",
      "Epoch 8739/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1386 - val_loss: 0.2617\n",
      "Epoch 8740/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.2825 - val_loss: 0.5250\n",
      "Epoch 8741/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5349 - val_loss: 1.1914\n",
      "Epoch 8742/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5673 - val_loss: 1.2002\n",
      "Epoch 8743/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8864 - val_loss: 0.5017\n",
      "Epoch 8744/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 3.4735 - val_loss: 17.9617\n",
      "Epoch 8745/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 6.1966 - val_loss: 5.8122\n",
      "Epoch 8746/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.2212 - val_loss: 9.6600\n",
      "Epoch 8747/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.5507 - val_loss: 11.5941\n",
      "Epoch 8748/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 28.6492 - val_loss: 12.9498\n",
      "Epoch 8749/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 80.9174 - val_loss: 16.6955\n",
      "Epoch 8750/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 342.5802 - val_loss: 117.7741\n",
      "Epoch 8751/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 252.9317 - val_loss: 595.6691\n",
      "Epoch 8752/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 415.2281 - val_loss: 4078.3060\n",
      "Epoch 8753/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4642.6364 - val_loss: 2158.0128\n",
      "Epoch 8754/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 529us/sample - loss: 2236.7483 - val_loss: 1541.1538\n",
      "Epoch 8755/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2060.1003 - val_loss: 1770.9700\n",
      "Epoch 8756/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2581.2863 - val_loss: 1897.5311\n",
      "Epoch 8757/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1665.3452 - val_loss: 1902.6772\n",
      "Epoch 8758/10000\n",
      "68/68 [==============================] - 0s 897us/sample - loss: 1085.7817 - val_loss: 1269.7028\n",
      "Epoch 8759/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 638.2682 - val_loss: 353.3027\n",
      "Epoch 8760/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 499.6783 - val_loss: 322.6827\n",
      "Epoch 8761/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1446.4272 - val_loss: 949.5354\n",
      "Epoch 8762/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1573.7510 - val_loss: 4542.0024\n",
      "Epoch 8763/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2704.2378 - val_loss: 141.4556\n",
      "Epoch 8764/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1408.1197 - val_loss: 1898.2561\n",
      "Epoch 8765/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 694.3215 - val_loss: 1466.0738\n",
      "Epoch 8766/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 829.1536 - val_loss: 1509.3954\n",
      "Epoch 8767/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1112.9130 - val_loss: 1081.4694\n",
      "Epoch 8768/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 1574.7865 - val_loss: 2875.1619\n",
      "Epoch 8769/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2148.9575 - val_loss: 4518.2103\n",
      "Epoch 8770/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1564.2777 - val_loss: 1024.3375\n",
      "Epoch 8771/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 255.9488 - val_loss: 138.8038\n",
      "Epoch 8772/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 142.4922 - val_loss: 288.8051\n",
      "Epoch 8773/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 109.3825 - val_loss: 27.5928\n",
      "Epoch 8774/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 44.9163 - val_loss: 90.0405\n",
      "Epoch 8775/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 63.9453 - val_loss: 83.5952\n",
      "Epoch 8776/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 31.6958 - val_loss: 32.0240\n",
      "Epoch 8777/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 17.8955 - val_loss: 24.3632\n",
      "Epoch 8778/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 14.0195 - val_loss: 9.1632\n",
      "Epoch 8779/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 10.6163 - val_loss: 9.3391\n",
      "Epoch 8780/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 10.1296 - val_loss: 6.5241\n",
      "Epoch 8781/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.0411 - val_loss: 8.0420\n",
      "Epoch 8782/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.1234 - val_loss: 0.9798\n",
      "Epoch 8783/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 2.2429 - val_loss: 1.2111\n",
      "Epoch 8784/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 3.3322 - val_loss: 3.2078\n",
      "Epoch 8785/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1975 - val_loss: 3.3676\n",
      "Epoch 8786/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0210 - val_loss: 1.5137\n",
      "Epoch 8787/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 0.3918 - val_loss: 0.9735\n",
      "Epoch 8788/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.0271 - val_loss: 2.6503\n",
      "Epoch 8789/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.4565 - val_loss: 1.2147\n",
      "Epoch 8790/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0615 - val_loss: 0.1169\n",
      "Epoch 8791/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 1.1165 - val_loss: 0.1646\n",
      "Epoch 8792/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.1977 - val_loss: 0.3718\n",
      "Epoch 8793/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.4654 - val_loss: 2.9763\n",
      "Epoch 8794/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.0462 - val_loss: 5.5471\n",
      "Epoch 8795/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.1403 - val_loss: 12.9345\n",
      "Epoch 8796/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 10.3393 - val_loss: 3.9842\n",
      "Epoch 8797/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.2724 - val_loss: 0.3056\n",
      "Epoch 8798/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7042 - val_loss: 0.9673\n",
      "Epoch 8799/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7581 - val_loss: 1.2446\n",
      "Epoch 8800/10000\n",
      "68/68 [==============================] - 0s 706us/sample - loss: 0.4686 - val_loss: 1.0919\n",
      "Epoch 8801/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.7178 - val_loss: 0.0739\n",
      "Epoch 8802/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1572 - val_loss: 0.2204\n",
      "Epoch 8803/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1618 - val_loss: 0.2627\n",
      "Epoch 8804/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1068 - val_loss: 0.1130\n",
      "Epoch 8805/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1571 - val_loss: 0.0225\n",
      "Epoch 8806/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.5418 - val_loss: 0.2101\n",
      "Epoch 8807/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.8351 - val_loss: 3.6272\n",
      "Epoch 8808/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.2514 - val_loss: 3.5183\n",
      "Epoch 8809/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8594 - val_loss: 0.4168\n",
      "Epoch 8810/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2653 - val_loss: 0.0611\n",
      "Epoch 8811/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1286 - val_loss: 0.0419\n",
      "Epoch 8812/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0888 - val_loss: 0.0681\n",
      "Epoch 8813/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0558 - val_loss: 0.0436\n",
      "Epoch 8814/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0189 - val_loss: 0.0234\n",
      "Epoch 8815/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0123 - val_loss: 0.0042\n",
      "Epoch 8816/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0064 - val_loss: 0.0036\n",
      "Epoch 8817/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0081 - val_loss: 0.0073\n",
      "Epoch 8818/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0070 - val_loss: 0.0177\n",
      "Epoch 8819/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0127 - val_loss: 0.0065\n",
      "Epoch 8820/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0066 - val_loss: 0.0263\n",
      "Epoch 8821/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0191 - val_loss: 0.0070\n",
      "Epoch 8822/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0091 - val_loss: 0.0039\n",
      "Epoch 8823/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0054 - val_loss: 0.0044\n",
      "Epoch 8824/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.004 - 0s 176us/sample - loss: 0.0063 - val_loss: 0.0049\n",
      "Epoch 8825/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0089 - val_loss: 0.0034\n",
      "Epoch 8826/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0057 - val_loss: 0.0027\n",
      "Epoch 8827/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0111 - val_loss: 0.0318\n",
      "Epoch 8828/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0176 - val_loss: 0.0083\n",
      "Epoch 8829/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0179 - val_loss: 0.0283\n",
      "Epoch 8830/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0917 - val_loss: 0.1163\n",
      "Epoch 8831/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.0476 - val_loss: 0.0499\n",
      "Epoch 8832/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0247 - val_loss: 0.0208\n",
      "Epoch 8833/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0332 - val_loss: 0.0487\n",
      "Epoch 8834/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0315 - val_loss: 0.0200\n",
      "Epoch 8835/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0204 - val_loss: 0.0022\n",
      "Epoch 8836/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 8837/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0092 - val_loss: 0.0108\n",
      "Epoch 8838/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0076 - val_loss: 0.0022\n",
      "Epoch 8839/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0061 - val_loss: 0.0210\n",
      "Epoch 8840/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.0136 - val_loss: 0.0084\n",
      "Epoch 8841/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0046 - val_loss: 0.0025\n",
      "Epoch 8842/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0073 - val_loss: 0.0420\n",
      "Epoch 8843/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0377 - val_loss: 0.0161\n",
      "Epoch 8844/10000\n",
      "68/68 [==============================] - 0s 824us/sample - loss: 0.0655 - val_loss: 0.0822\n",
      "Epoch 8845/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.058 - 0s 162us/sample - loss: 0.0527 - val_loss: 0.1545\n",
      "Epoch 8846/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0486 - val_loss: 0.0301\n",
      "Epoch 8847/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0261 - val_loss: 0.0052\n",
      "Epoch 8848/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0230 - val_loss: 0.0112\n",
      "Epoch 8849/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0152 - val_loss: 0.0250\n",
      "Epoch 8850/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0103 - val_loss: 0.0157\n",
      "Epoch 8851/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0051 - val_loss: 0.0085\n",
      "Epoch 8852/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0076 - val_loss: 0.0198\n",
      "Epoch 8853/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0073 - val_loss: 0.0026\n",
      "Epoch 8854/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0065 - val_loss: 0.0100\n",
      "Epoch 8855/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0088 - val_loss: 0.0034\n",
      "Epoch 8856/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0100 - val_loss: 0.0151\n",
      "Epoch 8857/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0233 - val_loss: 0.0121\n",
      "Epoch 8858/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0218 - val_loss: 0.0381\n",
      "Epoch 8859/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0173 - val_loss: 0.0275\n",
      "Epoch 8860/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0180 - val_loss: 0.0013\n",
      "Epoch 8861/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0365 - val_loss: 0.0469\n",
      "Epoch 8862/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0300 - val_loss: 0.0126\n",
      "Epoch 8863/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0134 - val_loss: 0.0246\n",
      "Epoch 8864/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0100 - val_loss: 0.0137\n",
      "Epoch 8865/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0091 - val_loss: 0.0095\n",
      "Epoch 8866/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0067 - val_loss: 9.8768e-04\n",
      "Epoch 8867/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0041 - val_loss: 0.0023\n",
      "Epoch 8868/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0035 - val_loss: 0.0218\n",
      "Epoch 8869/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0079 - val_loss: 0.0203\n",
      "Epoch 8870/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0163 - val_loss: 6.1481e-04\n",
      "Epoch 8871/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0038 - val_loss: 0.0011\n",
      "Epoch 8872/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0049 - val_loss: 0.0018\n",
      "Epoch 8873/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 8874/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0017 - val_loss: 6.2309e-04\n",
      "Epoch 8875/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.5519e-04 - val_loss: 6.3337e-04\n",
      "Epoch 8876/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0031 - val_loss: 0.0084\n",
      "Epoch 8877/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0035 - val_loss: 8.7391e-04\n",
      "Epoch 8878/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0029 - val_loss: 0.0017\n",
      "Epoch 8879/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0012 - val_loss: 0.0027\n",
      "Epoch 8880/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 8881/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0095 - val_loss: 0.0110\n",
      "Epoch 8882/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0075 - val_loss: 0.0021\n",
      "Epoch 8883/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 8884/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 8885/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0016 - val_loss: 0.0077\n",
      "Epoch 8886/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0059 - val_loss: 0.0053\n",
      "Epoch 8887/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 8888/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.001 - 0s 147us/sample - loss: 9.9385e-04 - val_loss: 0.0020\n",
      "Epoch 8889/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0016 - val_loss: 0.0038\n",
      "Epoch 8890/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0039 - val_loss: 0.0124\n",
      "Epoch 8891/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0041 - val_loss: 0.0067\n",
      "Epoch 8892/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0495 - val_loss: 0.0112\n",
      "Epoch 8893/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0553 - val_loss: 0.0441\n",
      "Epoch 8894/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2444 - val_loss: 0.4016\n",
      "Epoch 8895/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2440 - val_loss: 0.0059\n",
      "Epoch 8896/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2969 - val_loss: 1.5669\n",
      "Epoch 8897/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.5203 - val_loss: 3.7983\n",
      "Epoch 8898/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.1572 - val_loss: 2.1454\n",
      "Epoch 8899/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 9.1504 - val_loss: 17.7121\n",
      "Epoch 8900/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 25.3288 - val_loss: 1.5237\n",
      "Epoch 8901/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 41.1780 - val_loss: 12.6843\n",
      "Epoch 8902/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 76.7789 - val_loss: 186.8845\n",
      "Epoch 8903/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 426us/sample - loss: 320.4935 - val_loss: 802.9033\n",
      "Epoch 8904/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1593.5669 - val_loss: 883.9648\n",
      "Epoch 8905/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 3182.6215 - val_loss: 14656.1593\n",
      "Epoch 8906/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6074.2478 - val_loss: 221.6173\n",
      "Epoch 8907/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 503.1398 - val_loss: 868.5804\n",
      "Epoch 8908/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 747.8894 - val_loss: 822.4550\n",
      "Epoch 8909/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 1410.3074 - val_loss: 1242.0564\n",
      "Epoch 8910/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 673.6424 - val_loss: 531.3134\n",
      "Epoch 8911/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 322.5022 - val_loss: 347.3053\n",
      "Epoch 8912/10000\n",
      "68/68 [==============================] - 0s 544us/sample - loss: 219.4413 - val_loss: 180.0284\n",
      "Epoch 8913/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 90.1548 - val_loss: 131.1702\n",
      "Epoch 8914/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 116.2791 - val_loss: 61.8009\n",
      "Epoch 8915/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 94.5230 - val_loss: 75.3125\n",
      "Epoch 8916/10000\n",
      "68/68 [==============================] - 0s 868us/sample - loss: 35.7930 - val_loss: 1.3246\n",
      "Epoch 8917/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.3683 - val_loss: 0.9925\n",
      "Epoch 8918/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.0008 - val_loss: 0.6888\n",
      "Epoch 8919/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.0635 - val_loss: 1.4472\n",
      "Epoch 8920/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.0717 - val_loss: 1.6433\n",
      "Epoch 8921/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 2.9798 - val_loss: 1.7346\n",
      "Epoch 8922/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 2.4909 - val_loss: 0.0957\n",
      "Epoch 8923/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.4654 - val_loss: 1.9441\n",
      "Epoch 8924/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8081 - val_loss: 0.9712\n",
      "Epoch 8925/10000\n",
      "68/68 [==============================] - 0s 559us/sample - loss: 0.5552 - val_loss: 0.4003\n",
      "Epoch 8926/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.9079 - val_loss: 0.5152\n",
      "Epoch 8927/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1771 - val_loss: 0.0375\n",
      "Epoch 8928/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1536 - val_loss: 0.2309\n",
      "Epoch 8929/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1050 - val_loss: 0.1163\n",
      "Epoch 8930/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.1426 - val_loss: 0.4105\n",
      "Epoch 8931/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1414 - val_loss: 0.0240\n",
      "Epoch 8932/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0863 - val_loss: 0.0645\n",
      "Epoch 8933/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0536 - val_loss: 0.0398\n",
      "Epoch 8934/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0596 - val_loss: 0.0964\n",
      "Epoch 8935/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0748 - val_loss: 0.1702\n",
      "Epoch 8936/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1285 - val_loss: 0.0811\n",
      "Epoch 8937/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.3560 - val_loss: 0.4250\n",
      "Epoch 8938/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 0.3076 - val_loss: 0.1150\n",
      "Epoch 8939/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2756 - val_loss: 0.7175\n",
      "Epoch 8940/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3940 - val_loss: 0.2895\n",
      "Epoch 8941/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.2759 - val_loss: 0.1650\n",
      "Epoch 8942/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.1283 - val_loss: 0.3699\n",
      "Epoch 8943/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 0.6118 - val_loss: 0.4754\n",
      "Epoch 8944/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.2610 - val_loss: 0.1958\n",
      "Epoch 8945/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8414 - val_loss: 2.4006\n",
      "Epoch 8946/10000\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 0.4380 - val_loss: 0.2986\n",
      "Epoch 8947/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3274 - val_loss: 0.4859\n",
      "Epoch 8948/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3046 - val_loss: 0.1886\n",
      "Epoch 8949/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1513 - val_loss: 0.0898\n",
      "Epoch 8950/10000\n",
      "68/68 [==============================] - 0s 941us/sample - loss: 0.1340 - val_loss: 0.2565\n",
      "Epoch 8951/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0874 - val_loss: 0.4262\n",
      "Epoch 8952/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2201 - val_loss: 0.3844\n",
      "Epoch 8953/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4658 - val_loss: 0.1941\n",
      "Epoch 8954/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1246 - val_loss: 0.3370\n",
      "Epoch 8955/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.3920 - val_loss: 0.4524\n",
      "Epoch 8956/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3641 - val_loss: 0.2198\n",
      "Epoch 8957/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.9483 - val_loss: 3.9474\n",
      "Epoch 8958/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.5215 - val_loss: 0.3342\n",
      "Epoch 8959/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.6624 - val_loss: 0.0597\n",
      "Epoch 8960/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1728 - val_loss: 0.0781\n",
      "Epoch 8961/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0915 - val_loss: 0.0171\n",
      "Epoch 8962/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0423 - val_loss: 0.0408\n",
      "Epoch 8963/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.0385 - val_loss: 0.0786\n",
      "Epoch 8964/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0448 - val_loss: 0.0514\n",
      "Epoch 8965/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0296 - val_loss: 0.0353\n",
      "Epoch 8966/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0389 - val_loss: 0.0460\n",
      "Epoch 8967/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0500 - val_loss: 0.0839\n",
      "Epoch 8968/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0554 - val_loss: 0.0086\n",
      "Epoch 8969/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0225 - val_loss: 0.0730\n",
      "Epoch 8970/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0277 - val_loss: 0.0336\n",
      "Epoch 8971/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0171 - val_loss: 0.0080\n",
      "Epoch 8972/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0120 - val_loss: 0.0100\n",
      "Epoch 8973/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 0.0140 - val_loss: 0.0130\n",
      "Epoch 8974/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0106 - val_loss: 0.0324\n",
      "Epoch 8975/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0282 - val_loss: 0.0163\n",
      "Epoch 8976/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0108 - val_loss: 0.0046\n",
      "Epoch 8977/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0200 - val_loss: 0.0048\n",
      "Epoch 8978/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0262 - val_loss: 0.0128\n",
      "Epoch 8979/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.0150 - val_loss: 0.0073\n",
      "Epoch 8980/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0077 - val_loss: 0.0239\n",
      "Epoch 8981/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.0099 - val_loss: 0.0052\n",
      "Epoch 8982/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0101 - val_loss: 0.0082\n",
      "Epoch 8983/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0235 - val_loss: 0.0321\n",
      "Epoch 8984/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0345 - val_loss: 0.0459\n",
      "Epoch 8985/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0378 - val_loss: 0.0044\n",
      "Epoch 8986/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0210 - val_loss: 0.0313\n",
      "Epoch 8987/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0151 - val_loss: 0.0301\n",
      "Epoch 8988/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0227 - val_loss: 0.0037\n",
      "Epoch 8989/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.0102 - val_loss: 0.0209\n",
      "Epoch 8990/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.0319 - val_loss: 0.0917\n",
      "Epoch 8991/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2176 - val_loss: 0.0278\n",
      "Epoch 8992/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3974 - val_loss: 0.3289\n",
      "Epoch 8993/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 2.0548 - val_loss: 1.7702\n",
      "Epoch 8994/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.8280 - val_loss: 1.1181\n",
      "Epoch 8995/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 0.3011 - val_loss: 0.1467\n",
      "Epoch 8996/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.3211 - val_loss: 0.6797\n",
      "Epoch 8997/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.3283 - val_loss: 5.1310\n",
      "Epoch 8998/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 1.5387 - val_loss: 3.1150\n",
      "Epoch 8999/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.4053 - val_loss: 2.7199\n",
      "Epoch 9000/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 2.5055 - val_loss: 4.7501\n",
      "Epoch 9001/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 2.8512 - val_loss: 0.2651\n",
      "Epoch 9002/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.8719 - val_loss: 0.6958\n",
      "Epoch 9003/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.1724 - val_loss: 1.9370\n",
      "Epoch 9004/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.2312 - val_loss: 11.8581\n",
      "Epoch 9005/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 13.9533 - val_loss: 88.6656\n",
      "Epoch 9006/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 134.3103 - val_loss: 203.0074\n",
      "Epoch 9007/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 73.5071 - val_loss: 6.8792\n",
      "Epoch 9008/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 19.8222 - val_loss: 7.1716\n",
      "Epoch 9009/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 11.8696 - val_loss: 11.6915\n",
      "Epoch 9010/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 41.1472 - val_loss: 9.7503\n",
      "Epoch 9011/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 46.4586 - val_loss: 3.5594\n",
      "Epoch 9012/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 104.9047 - val_loss: 569.5333\n",
      "Epoch 9013/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 432.6495 - val_loss: 1616.6059\n",
      "Epoch 9014/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2394.2302 - val_loss: 3763.3790\n",
      "Epoch 9015/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 7343.6762 - val_loss: 7964.9560\n",
      "Epoch 9016/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3832.0563 - val_loss: 1205.2862\n",
      "Epoch 9017/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2527.3069 - val_loss: 5011.9172\n",
      "Epoch 9018/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5987.5544 - val_loss: 620.0558\n",
      "Epoch 9019/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1810.1109 - val_loss: 799.9611\n",
      "Epoch 9020/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 912.3855 - val_loss: 1095.3804\n",
      "Epoch 9021/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 765.4662 - val_loss: 1848.6069\n",
      "Epoch 9022/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1727.1576 - val_loss: 157.0501\n",
      "Epoch 9023/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2037.7785 - val_loss: 793.9326\n",
      "Epoch 9024/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1286.8587 - val_loss: 438.1654\n",
      "Epoch 9025/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 791.7753 - val_loss: 368.7617\n",
      "Epoch 9026/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 349.7830 - val_loss: 466.7038\n",
      "Epoch 9027/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 971.5993 - val_loss: 703.8586\n",
      "Epoch 9028/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 304.9646 - val_loss: 326.7113\n",
      "Epoch 9029/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 287.2818 - val_loss: 576.0556\n",
      "Epoch 9030/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 304.5296 - val_loss: 194.4123\n",
      "Epoch 9031/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 90.7125 - val_loss: 95.6372\n",
      "Epoch 9032/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 70.0118 - val_loss: 191.2530\n",
      "Epoch 9033/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 75.7024 - val_loss: 110.7729\n",
      "Epoch 9034/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 110.4915 - val_loss: 54.8561\n",
      "Epoch 9035/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 33.6823 - val_loss: 14.3901\n",
      "Epoch 9036/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 9.5467 - val_loss: 11.9746\n",
      "Epoch 9037/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.9372 - val_loss: 5.9532\n",
      "Epoch 9038/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.9814 - val_loss: 4.2517\n",
      "Epoch 9039/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.5575 - val_loss: 2.3615\n",
      "Epoch 9040/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.9372 - val_loss: 0.2344\n",
      "Epoch 9041/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5927 - val_loss: 0.2190\n",
      "Epoch 9042/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.3299 - val_loss: 0.6661\n",
      "Epoch 9043/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.5338 - val_loss: 0.2602\n",
      "Epoch 9044/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4256 - val_loss: 0.3688\n",
      "Epoch 9045/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1777 - val_loss: 0.2464\n",
      "Epoch 9046/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1676 - val_loss: 0.3939\n",
      "Epoch 9047/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2139 - val_loss: 0.4306\n",
      "Epoch 9048/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2025 - val_loss: 0.1830\n",
      "Epoch 9049/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1851 - val_loss: 0.1225\n",
      "Epoch 9050/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.0950 - val_loss: 0.2454\n",
      "Epoch 9051/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1264 - val_loss: 0.1283\n",
      "Epoch 9052/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1073 - val_loss: 0.0557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9053/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0869 - val_loss: 0.0931\n",
      "Epoch 9054/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1416 - val_loss: 0.1058\n",
      "Epoch 9055/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0794 - val_loss: 0.0486\n",
      "Epoch 9056/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0701 - val_loss: 0.0712\n",
      "Epoch 9057/10000\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 0.0878 - val_loss: 0.0367\n",
      "Epoch 9058/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0650 - val_loss: 0.0790\n",
      "Epoch 9059/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1141 - val_loss: 0.1571\n",
      "Epoch 9060/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0972 - val_loss: 0.2251\n",
      "Epoch 9061/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 0.1240 - val_loss: 0.2946\n",
      "Epoch 9062/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1141 - val_loss: 0.0571\n",
      "Epoch 9063/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0950 - val_loss: 0.2353\n",
      "Epoch 9064/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.2027 - val_loss: 0.3853\n",
      "Epoch 9065/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.2411 - val_loss: 0.0927\n",
      "Epoch 9066/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2299 - val_loss: 0.0656\n",
      "Epoch 9067/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0875 - val_loss: 0.0565\n",
      "Epoch 9068/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0882 - val_loss: 0.0589\n",
      "Epoch 9069/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 0.1127 - val_loss: 0.0396\n",
      "Epoch 9070/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0626 - val_loss: 0.0244\n",
      "Epoch 9071/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0525 - val_loss: 0.0955\n",
      "Epoch 9072/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0467 - val_loss: 0.0303\n",
      "Epoch 9073/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0487 - val_loss: 0.0424\n",
      "Epoch 9074/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0348 - val_loss: 0.0277\n",
      "Epoch 9075/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0329 - val_loss: 0.0495\n",
      "Epoch 9076/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0612 - val_loss: 0.0237\n",
      "Epoch 9077/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1032 - val_loss: 0.1571\n",
      "Epoch 9078/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.1376 - val_loss: 0.1200\n",
      "Epoch 9079/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0785 - val_loss: 0.0217\n",
      "Epoch 9080/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0575 - val_loss: 0.0489\n",
      "Epoch 9081/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0847 - val_loss: 0.0669\n",
      "Epoch 9082/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0929 - val_loss: 0.0463\n",
      "Epoch 9083/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0334 - val_loss: 0.0204\n",
      "Epoch 9084/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0565 - val_loss: 0.0677\n",
      "Epoch 9085/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0483 - val_loss: 0.0224\n",
      "Epoch 9086/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0295 - val_loss: 0.0252\n",
      "Epoch 9087/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 0.0281 - val_loss: 0.1140\n",
      "Epoch 9088/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1235 - val_loss: 0.1078\n",
      "Epoch 9089/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1065 - val_loss: 0.1769\n",
      "Epoch 9090/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0764 - val_loss: 0.0915\n",
      "Epoch 9091/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0528 - val_loss: 0.0315\n",
      "Epoch 9092/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0305 - val_loss: 0.0155\n",
      "Epoch 9093/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0216 - val_loss: 0.0238\n",
      "Epoch 9094/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0265 - val_loss: 0.0608\n",
      "Epoch 9095/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 0.0329 - val_loss: 0.0203\n",
      "Epoch 9096/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0456 - val_loss: 0.0362\n",
      "Epoch 9097/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0778 - val_loss: 0.0534\n",
      "Epoch 9098/10000\n",
      "68/68 [==============================] - 0s 4ms/sample - loss: 0.1413 - val_loss: 0.3109\n",
      "Epoch 9099/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1464 - val_loss: 0.1423\n",
      "Epoch 9100/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0804 - val_loss: 0.1711\n",
      "Epoch 9101/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.1512 - val_loss: 0.7440\n",
      "Epoch 9102/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.0587 - val_loss: 1.2795\n",
      "Epoch 9103/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7067 - val_loss: 0.1151\n",
      "Epoch 9104/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2864 - val_loss: 0.9391\n",
      "Epoch 9105/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7999 - val_loss: 1.3377\n",
      "Epoch 9106/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.6730 - val_loss: 0.1683\n",
      "Epoch 9107/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1740 - val_loss: 0.0343\n",
      "Epoch 9108/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0558 - val_loss: 0.0122\n",
      "Epoch 9109/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0804 - val_loss: 0.1602\n",
      "Epoch 9110/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 0.0927 - val_loss: 0.1367\n",
      "Epoch 9111/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1104 - val_loss: 0.2992\n",
      "Epoch 9112/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2279 - val_loss: 0.4468\n",
      "Epoch 9113/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1722 - val_loss: 0.4954\n",
      "Epoch 9114/10000\n",
      "68/68 [==============================] - 0s 588us/sample - loss: 0.2517 - val_loss: 0.0570\n",
      "Epoch 9115/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0638 - val_loss: 0.1078\n",
      "Epoch 9116/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0526 - val_loss: 0.0339\n",
      "Epoch 9117/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0196 - val_loss: 0.0080\n",
      "Epoch 9118/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0480 - val_loss: 0.1211\n",
      "Epoch 9119/10000\n",
      "68/68 [==============================] - 0s 985us/sample - loss: 0.1363 - val_loss: 0.0581\n",
      "Epoch 9120/10000\n",
      "68/68 [==============================] - 0s 765us/sample - loss: 0.0426 - val_loss: 0.0159\n",
      "Epoch 9121/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0187 - val_loss: 0.0166\n",
      "Epoch 9122/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0236 - val_loss: 0.0658\n",
      "Epoch 9123/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.0329 - val_loss: 0.0094\n",
      "Epoch 9124/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.0180 - val_loss: 0.0090\n",
      "Epoch 9125/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0148 - val_loss: 0.0166\n",
      "Epoch 9126/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0147 - val_loss: 0.0086\n",
      "Epoch 9127/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0085 - val_loss: 0.0095\n",
      "Epoch 9128/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0202 - val_loss: 0.0185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9129/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.0218 - val_loss: 0.0169\n",
      "Epoch 9130/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.0126 - val_loss: 0.0060\n",
      "Epoch 9131/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.0134 - val_loss: 0.0329\n",
      "Epoch 9132/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.0238 - val_loss: 0.0378\n",
      "Epoch 9133/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 0.0558 - val_loss: 0.1297\n",
      "Epoch 9134/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.1687 - val_loss: 0.0078\n",
      "Epoch 9135/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.1437 - val_loss: 0.2736\n",
      "Epoch 9136/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 0.1981 - val_loss: 0.1073\n",
      "Epoch 9137/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.1487 - val_loss: 0.1238\n",
      "Epoch 9138/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.0898 - val_loss: 0.0137\n",
      "Epoch 9139/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.0785 - val_loss: 0.0099\n",
      "Epoch 9140/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.2060 - val_loss: 0.0747\n",
      "Epoch 9141/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3627 - val_loss: 0.0117\n",
      "Epoch 9142/10000\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 0.0976 - val_loss: 0.0705\n",
      "Epoch 9143/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1059 - val_loss: 0.0522\n",
      "Epoch 9144/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1540 - val_loss: 0.0794\n",
      "Epoch 9145/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2450 - val_loss: 0.4433\n",
      "Epoch 9146/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1274 - val_loss: 0.3167\n",
      "Epoch 9147/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0967 - val_loss: 0.3167\n",
      "Epoch 9148/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1559 - val_loss: 0.1539\n",
      "Epoch 9149/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.1578 - val_loss: 0.1886\n",
      "Epoch 9150/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.1036 - val_loss: 0.1508\n",
      "Epoch 9151/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.0917 - val_loss: 0.1097\n",
      "Epoch 9152/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0659 - val_loss: 0.2498\n",
      "Epoch 9153/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.1150 - val_loss: 0.6803\n",
      "Epoch 9154/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.8382 - val_loss: 1.8834\n",
      "Epoch 9155/10000\n",
      "68/68 [==============================] - 0s 426us/sample - loss: 1.9927 - val_loss: 7.8012\n",
      "Epoch 9156/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 3.6181 - val_loss: 3.3783\n",
      "Epoch 9157/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1.3897 - val_loss: 1.4887\n",
      "Epoch 9158/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.1529 - val_loss: 0.1515\n",
      "Epoch 9159/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.5524 - val_loss: 0.7624\n",
      "Epoch 9160/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.2540 - val_loss: 2.0740\n",
      "Epoch 9161/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.2574 - val_loss: 11.5666\n",
      "Epoch 9162/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.4986 - val_loss: 11.5360\n",
      "Epoch 9163/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.2933 - val_loss: 3.4782\n",
      "Epoch 9164/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.5981 - val_loss: 1.2399\n",
      "Epoch 9165/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.6013 - val_loss: 18.1433\n",
      "Epoch 9166/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.0811 - val_loss: 20.1591\n",
      "Epoch 9167/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 42.9634 - val_loss: 263.7411\n",
      "Epoch 9168/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 83.6262 - val_loss: 0.7931\n",
      "Epoch 9169/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 115.1312 - val_loss: 56.7602\n",
      "Epoch 9170/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 639.4667 - val_loss: 3.9942\n",
      "Epoch 9171/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2755.3313 - val_loss: 3861.4797\n",
      "Epoch 9172/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12514.7867 - val_loss: 21401.7094\n",
      "Epoch 9173/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 15786.8609 - val_loss: 21812.3190\n",
      "Epoch 9174/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9801.1442 - val_loss: 6061.1169\n",
      "Epoch 9175/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7740.4767 - val_loss: 2315.3385\n",
      "Epoch 9176/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7719.0906 - val_loss: 3351.3250\n",
      "Epoch 9177/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4802.5844 - val_loss: 3761.8805\n",
      "Epoch 9178/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6890.9854 - val_loss: 22546.7846\n",
      "Epoch 9179/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 11449.5671 - val_loss: 1100.1485\n",
      "Epoch 9180/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3471.1486 - val_loss: 4147.7508\n",
      "Epoch 9181/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1390.4073 - val_loss: 187.5360\n",
      "Epoch 9182/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 550.4925 - val_loss: 750.7971\n",
      "Epoch 9183/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 713.6674 - val_loss: 350.2921\n",
      "Epoch 9184/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 479.9616 - val_loss: 668.2747\n",
      "Epoch 9185/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 465.8144 - val_loss: 453.1798\n",
      "Epoch 9186/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 226.5372 - val_loss: 231.0058\n",
      "Epoch 9187/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 220.9243 - val_loss: 141.4116\n",
      "Epoch 9188/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 214.0422 - val_loss: 133.8962\n",
      "Epoch 9189/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 72.8971 - val_loss: 13.9504\n",
      "Epoch 9190/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 22.0182 - val_loss: 29.9086\n",
      "Epoch 9191/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 9.3780 - val_loss: 2.3333\n",
      "Epoch 9192/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.2500 - val_loss: 6.2211\n",
      "Epoch 9193/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1224 - val_loss: 3.8191\n",
      "Epoch 9194/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.0478 - val_loss: 1.4369\n",
      "Epoch 9195/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.4284 - val_loss: 1.6508\n",
      "Epoch 9196/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.1468 - val_loss: 0.1271\n",
      "Epoch 9197/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3285 - val_loss: 0.0699\n",
      "Epoch 9198/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1060 - val_loss: 0.0243\n",
      "Epoch 9199/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0560 - val_loss: 0.0127\n",
      "Epoch 9200/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0299 - val_loss: 0.0037\n",
      "Epoch 9201/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0091 - val_loss: 0.0097\n",
      "Epoch 9202/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0093 - val_loss: 6.1021e-04\n",
      "Epoch 9203/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0057 - val_loss: 0.0011\n",
      "Epoch 9204/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0029 - val_loss: 2.4773e-04\n",
      "Epoch 9205/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 8.3488e-04 - val_loss: 9.2772e-04\n",
      "Epoch 9206/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.0012 - val_loss: 0.0047\n",
      "Epoch 9207/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0016 - val_loss: 7.7067e-04\n",
      "Epoch 9208/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.5300e-04 - val_loss: 7.8542e-04\n",
      "Epoch 9209/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.2548e-04 - val_loss: 1.0008e-04\n",
      "Epoch 9210/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.6075e-04 - val_loss: 5.9459e-05\n",
      "Epoch 9211/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.5850e-04 - val_loss: 2.6938e-04\n",
      "Epoch 9212/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1.9459e-04 - val_loss: 2.1519e-04\n",
      "Epoch 9213/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.2259e-04 - val_loss: 5.3181e-05\n",
      "Epoch 9214/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.8700e-04 - val_loss: 3.1063e-04\n",
      "Epoch 9215/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.6669e-04 - val_loss: 5.4862e-04\n",
      "Epoch 9216/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.7570e-04 - val_loss: 9.5778e-05\n",
      "Epoch 9217/10000\n",
      "68/68 [==============================] - 0s 544us/sample - loss: 2.0751e-04 - val_loss: 2.6535e-04\n",
      "Epoch 9218/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 2.7814e-04 - val_loss: 4.9266e-04\n",
      "Epoch 9219/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.7131e-04 - val_loss: 4.6069e-04\n",
      "Epoch 9220/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9206e-04 - val_loss: 1.4187e-04\n",
      "Epoch 9221/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.2700e-04 - val_loss: 1.8006e-04\n",
      "Epoch 9222/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.2376e-04 - val_loss: 7.2197e-05\n",
      "Epoch 9223/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.3226e-04 - val_loss: 1.1817e-04\n",
      "Epoch 9224/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.9457e-04 - val_loss: 7.6553e-05\n",
      "Epoch 9225/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.9239e-04 - val_loss: 4.1951e-04\n",
      "Epoch 9226/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8050e-04 - val_loss: 1.4957e-04\n",
      "Epoch 9227/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 2.5324e-04 - val_loss: 2.4806e-04\n",
      "Epoch 9228/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 3.6478e-04 - val_loss: 0.0010\n",
      "Epoch 9229/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.1344e-04 - val_loss: 0.0010\n",
      "Epoch 9230/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 6.8145e-04 - val_loss: 0.0013\n",
      "Epoch 9231/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 8.0123e-04 - val_loss: 8.4600e-05\n",
      "Epoch 9232/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.6140e-04 - val_loss: 2.2412e-04\n",
      "Epoch 9233/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.7399e-04 - val_loss: 2.0113e-04\n",
      "Epoch 9234/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1.5397e-04 - val_loss: 1.2375e-04\n",
      "Epoch 9235/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.1285e-04 - val_loss: 5.7590e-05\n",
      "Epoch 9236/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.2317e-04 - val_loss: 2.6579e-04\n",
      "Epoch 9237/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 1.3411e-04 - val_loss: 6.7879e-05\n",
      "Epoch 9238/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.0470e-05 - val_loss: 7.5677e-05\n",
      "Epoch 9239/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2582e-04 - val_loss: 2.9009e-04\n",
      "Epoch 9240/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.0230e-04 - val_loss: 9.5512e-04\n",
      "Epoch 9241/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 7.5126e-04 - val_loss: 0.0024\n",
      "Epoch 9242/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.1821e-04 - val_loss: 2.8370e-04\n",
      "Epoch 9243/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.8232e-04 - val_loss: 1.9896e-04\n",
      "Epoch 9244/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 2.9226e-04 - val_loss: 1.9767e-04\n",
      "Epoch 9245/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.3247e-04 - val_loss: 2.8792e-04\n",
      "Epoch 9246/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.9449e-04 - val_loss: 7.5195e-05\n",
      "Epoch 9247/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 1.4993e-04 - val_loss: 4.2164e-05\n",
      "Epoch 9248/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.2321e-04 - val_loss: 4.6227e-04\n",
      "Epoch 9249/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.4223e-04 - val_loss: 3.5092e-04\n",
      "Epoch 9250/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1200e-04 - val_loss: 2.8385e-04\n",
      "Epoch 9251/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1.1775e-04 - val_loss: 7.7298e-05\n",
      "Epoch 9252/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 1.2602e-04 - val_loss: 1.7511e-04\n",
      "Epoch 9253/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.2247e-04 - val_loss: 3.3929e-04\n",
      "Epoch 9254/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.0217e-05 - val_loss: 2.5265e-05\n",
      "Epoch 9255/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.6167e-05 - val_loss: 7.7514e-05\n",
      "Epoch 9256/10000\n",
      "68/68 [==============================] - 0s 515us/sample - loss: 6.9724e-05 - val_loss: 7.7183e-05\n",
      "Epoch 9257/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.0576e-04 - val_loss: 1.5881e-04\n",
      "Epoch 9258/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.1949e-05 - val_loss: 9.5289e-05\n",
      "Epoch 9259/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.8751e-05 - val_loss: 7.8368e-05\n",
      "Epoch 9260/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 7.2787e-05 - val_loss: 3.0578e-05\n",
      "Epoch 9261/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.9800e-05 - val_loss: 5.3018e-05\n",
      "Epoch 9262/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.9607e-05 - val_loss: 1.0081e-04\n",
      "Epoch 9263/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.0339e-04 - val_loss: 3.1461e-05\n",
      "Epoch 9264/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.2250e-05 - val_loss: 1.9370e-04\n",
      "Epoch 9265/10000\n",
      "68/68 [==============================] - 0s 706us/sample - loss: 1.0618e-04 - val_loss: 7.5288e-05\n",
      "Epoch 9266/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 6.0273e-05 - val_loss: 3.4252e-05\n",
      "Epoch 9267/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.4609e-05 - val_loss: 4.6081e-05\n",
      "Epoch 9268/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.2153e-04 - val_loss: 1.1629e-04\n",
      "Epoch 9269/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 2.5936e-04 - val_loss: 7.8863e-05\n",
      "Epoch 9270/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.5499e-05 - val_loss: 4.0975e-05\n",
      "Epoch 9271/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 7.3639e-05 - val_loss: 6.6296e-05\n",
      "Epoch 9272/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.4307e-05 - val_loss: 6.4431e-05\n",
      "Epoch 9273/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.7211e-05 - val_loss: 1.1520e-04\n",
      "Epoch 9274/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0801e-04 - val_loss: 5.9971e-05\n",
      "Epoch 9275/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 6.3563e-05 - val_loss: 3.4888e-05\n",
      "Epoch 9276/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.9451e-05 - val_loss: 4.0557e-05\n",
      "Epoch 9277/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.2804e-05 - val_loss: 4.8442e-05\n",
      "Epoch 9278/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1031e-04 - val_loss: 3.1279e-04\n",
      "Epoch 9279/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.7994e-04 - val_loss: 1.5657e-04\n",
      "Epoch 9280/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.0637e-04 - val_loss: 3.8945e-04\n",
      "Epoch 9281/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.7639e-04 - val_loss: 6.4215e-04\n",
      "Epoch 9282/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.3463e-04 - val_loss: 7.1813e-04\n",
      "Epoch 9283/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.1798e-04 - val_loss: 5.4071e-04\n",
      "Epoch 9284/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.3706e-04 - val_loss: 7.8335e-05\n",
      "Epoch 9285/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.5803e-05 - val_loss: 3.6128e-04\n",
      "Epoch 9286/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.2957e-04 - val_loss: 3.8078e-04\n",
      "Epoch 9287/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.9824e-04 - val_loss: 1.6681e-04\n",
      "Epoch 9288/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1172e-04 - val_loss: 3.5991e-05\n",
      "Epoch 9289/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 8.0579e-05 - val_loss: 3.8214e-05\n",
      "Epoch 9290/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.8692e-05 - val_loss: 7.1410e-05\n",
      "Epoch 9291/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 6.1093e-05 - val_loss: 5.2375e-05\n",
      "Epoch 9292/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.7384e-05 - val_loss: 6.4850e-05\n",
      "Epoch 9293/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 6.2708e-05 - val_loss: 4.4262e-05\n",
      "Epoch 9294/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.2348e-05 - val_loss: 2.8161e-05\n",
      "Epoch 9295/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.9683e-05 - val_loss: 4.0728e-05\n",
      "Epoch 9296/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.8977e-05 - val_loss: 3.1285e-05\n",
      "Epoch 9297/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 4.6483e-05 - val_loss: 2.2643e-05\n",
      "Epoch 9298/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.5821e-05 - val_loss: 2.3692e-04\n",
      "Epoch 9299/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.0478e-04 - val_loss: 5.2950e-05\n",
      "Epoch 9300/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 4.4756e-05 - val_loss: 2.8926e-05\n",
      "Epoch 9301/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 6.2978e-05 - val_loss: 6.7900e-05\n",
      "Epoch 9302/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.7666e-05 - val_loss: 2.3744e-05\n",
      "Epoch 9303/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.1738e-05 - val_loss: 1.5007e-05\n",
      "Epoch 9304/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.5161e-05 - val_loss: 9.7541e-05\n",
      "Epoch 9305/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 5.3343e-05 - val_loss: 1.0110e-04\n",
      "Epoch 9306/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 9.5326e-05 - val_loss: 9.6876e-05\n",
      "Epoch 9307/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.3848e-04 - val_loss: 3.4659e-05\n",
      "Epoch 9308/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.9855e-05 - val_loss: 5.0019e-05\n",
      "Epoch 9309/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.2312e-05 - val_loss: 7.3840e-05\n",
      "Epoch 9310/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.5797e-05 - val_loss: 3.9242e-05\n",
      "Epoch 9311/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 9.2399e-05 - val_loss: 6.4234e-05\n",
      "Epoch 9312/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 5.4553e-05 - val_loss: 3.2356e-05\n",
      "Epoch 9313/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.4568e-05 - val_loss: 1.8985e-04\n",
      "Epoch 9314/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.3363e-04 - val_loss: 1.4344e-04\n",
      "Epoch 9315/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.0019e-05 - val_loss: 1.0064e-04\n",
      "Epoch 9316/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 7.1700e-05 - val_loss: 9.1519e-05\n",
      "Epoch 9317/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 7.3693e-05 - val_loss: 1.0688e-04\n",
      "Epoch 9318/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.8521e-05 - val_loss: 5.3042e-05\n",
      "Epoch 9319/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.4125e-04 - val_loss: 2.2052e-05\n",
      "Epoch 9320/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 2.9535e-04 - val_loss: 2.3583e-04\n",
      "Epoch 9321/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 2.0062e-04 - val_loss: 3.0265e-04\n",
      "Epoch 9322/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.1982e-04 - val_loss: 1.3000e-04\n",
      "Epoch 9323/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 1.6653e-04 - val_loss: 2.0211e-05\n",
      "Epoch 9324/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.9457e-05 - val_loss: 2.8865e-05\n",
      "Epoch 9325/10000\n",
      "68/68 [==============================] - 0s 765us/sample - loss: 6.8697e-05 - val_loss: 1.1507e-04\n",
      "Epoch 9326/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 5.0272e-05 - val_loss: 7.9311e-05\n",
      "Epoch 9327/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.4638e-05 - val_loss: 9.0068e-05\n",
      "Epoch 9328/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2885e-04 - val_loss: 3.1825e-05\n",
      "Epoch 9329/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.8788e-05 - val_loss: 1.3658e-04\n",
      "Epoch 9330/10000\n",
      "68/68 [==============================] - 0s 750us/sample - loss: 9.4846e-05 - val_loss: 8.0174e-05\n",
      "Epoch 9331/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.3972e-04 - val_loss: 5.5599e-04\n",
      "Epoch 9332/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4150e-04 - val_loss: 3.2764e-05\n",
      "Epoch 9333/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6761e-05 - val_loss: 3.8276e-05\n",
      "Epoch 9334/10000\n",
      "68/68 [==============================] - 0s 809us/sample - loss: 2.1501e-05 - val_loss: 1.5886e-05\n",
      "Epoch 9335/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.6648e-05 - val_loss: 5.0436e-05\n",
      "Epoch 9336/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.9593e-05 - val_loss: 7.8012e-06\n",
      "Epoch 9337/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.4902e-06 - val_loss: 9.7783e-05\n",
      "Epoch 9338/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.0924e-04 - val_loss: 1.4196e-05\n",
      "Epoch 9339/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 3.9243e-05 - val_loss: 4.9524e-05\n",
      "Epoch 9340/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.4089e-05 - val_loss: 9.7506e-06\n",
      "Epoch 9341/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.5576e-05 - val_loss: 2.3242e-05\n",
      "Epoch 9342/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.1198e-05 - val_loss: 1.2080e-04\n",
      "Epoch 9343/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 7.8687e-05 - val_loss: 1.9330e-05\n",
      "Epoch 9344/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 5.4411e-05 - val_loss: 1.9379e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9345/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.8448e-05 - val_loss: 7.9745e-05\n",
      "Epoch 9346/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.3623e-04 - val_loss: 2.9175e-04\n",
      "Epoch 9347/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 9.8493e-05 - val_loss: 8.9730e-05\n",
      "Epoch 9348/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 6.4160e-05 - val_loss: 1.1575e-04\n",
      "Epoch 9349/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.5049e-05 - val_loss: 1.3090e-05\n",
      "Epoch 9350/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.2453e-05 - val_loss: 9.7897e-05\n",
      "Epoch 9351/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.0248e-05 - val_loss: 1.8756e-05\n",
      "Epoch 9352/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.6584e-05 - val_loss: 1.6344e-05\n",
      "Epoch 9353/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 1.1615e-04 - val_loss: 7.8569e-05\n",
      "Epoch 9354/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.0628e-04 - val_loss: 1.1623e-04\n",
      "Epoch 9355/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.5525e-04 - val_loss: 5.9679e-04\n",
      "Epoch 9356/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 2.6331e-04 - val_loss: 9.0946e-06\n",
      "Epoch 9357/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.9083e-04 - val_loss: 3.0551e-04\n",
      "Epoch 9358/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.7230e-04 - val_loss: 1.7701e-04\n",
      "Epoch 9359/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 7.9997e-05 - val_loss: 1.3421e-04\n",
      "Epoch 9360/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1.7637e-04 - val_loss: 1.8467e-04\n",
      "Epoch 9361/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.5394e-04 - val_loss: 4.7139e-05\n",
      "Epoch 9362/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 1.4815e-05 - val_loss: 1.0359e-05\n",
      "Epoch 9363/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.9136e-05 - val_loss: 1.1720e-05\n",
      "Epoch 9364/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.5917e-05 - val_loss: 1.4300e-04\n",
      "Epoch 9365/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.3123e-05 - val_loss: 6.9698e-05\n",
      "Epoch 9366/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.4419e-05 - val_loss: 3.4205e-05\n",
      "Epoch 9367/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.4302e-05 - val_loss: 1.7566e-04\n",
      "Epoch 9368/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 8.8409e-05 - val_loss: 2.5114e-04\n",
      "Epoch 9369/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.4638e-04 - val_loss: 7.2273e-04\n",
      "Epoch 9370/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.8913e-04 - val_loss: 4.0769e-04\n",
      "Epoch 9371/10000\n",
      "68/68 [==============================] - 0s 735us/sample - loss: 2.2627e-04 - val_loss: 4.1398e-04\n",
      "Epoch 9372/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 3.7661e-04 - val_loss: 5.8638e-04\n",
      "Epoch 9373/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.7019e-04 - val_loss: 3.7645e-04\n",
      "Epoch 9374/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.4509e-04 - val_loss: 4.6881e-05\n",
      "Epoch 9375/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 1.5745e-04 - val_loss: 7.7918e-05\n",
      "Epoch 9376/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7.1908e-05 - val_loss: 1.1273e-05\n",
      "Epoch 9377/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2460e-04 - val_loss: 9.1684e-05\n",
      "Epoch 9378/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.7807e-05 - val_loss: 1.0863e-04\n",
      "Epoch 9379/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 9.6655e-0 - 0s 191us/sample - loss: 5.4329e-05 - val_loss: 2.9792e-04\n",
      "Epoch 9380/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.3226e-04 - val_loss: 1.9864e-05\n",
      "Epoch 9381/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.5953e-04 - val_loss: 5.1405e-04\n",
      "Epoch 9382/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0013 - val_loss: 0.0025\n",
      "Epoch 9383/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0014 - val_loss: 8.1159e-05\n",
      "Epoch 9384/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 5.0893e-04 - val_loss: 6.7934e-05\n",
      "Epoch 9385/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1.4859e-04 - val_loss: 6.4185e-05\n",
      "Epoch 9386/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.5237e-04 - val_loss: 5.8140e-04\n",
      "Epoch 9387/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3.3015e-04 - val_loss: 1.1367e-04\n",
      "Epoch 9388/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.7928e-04 - val_loss: 2.0458e-04\n",
      "Epoch 9389/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.6073e-04 - val_loss: 3.1558e-04\n",
      "Epoch 9390/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.0240e-04 - val_loss: 0.0011\n",
      "Epoch 9391/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.1765e-04 - val_loss: 8.7571e-04\n",
      "Epoch 9392/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 9393/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 9394/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 5.5588e-04 - val_loss: 0.0013\n",
      "Epoch 9395/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.1006e-04 - val_loss: 6.6320e-04\n",
      "Epoch 9396/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.5615e-04 - val_loss: 5.4214e-04\n",
      "Epoch 9397/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 4.2324e-04 - val_loss: 1.3110e-04\n",
      "Epoch 9398/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.6770e-04 - val_loss: 4.1008e-05\n",
      "Epoch 9399/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.9361e-04 - val_loss: 2.1766e-05\n",
      "Epoch 9400/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.3030e-05 - val_loss: 2.3617e-05\n",
      "Epoch 9401/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.3666e-04 - val_loss: 3.1741e-04\n",
      "Epoch 9402/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.3634e-04 - val_loss: 1.2762e-04\n",
      "Epoch 9403/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6.2885e-04 - val_loss: 9.6545e-05\n",
      "Epoch 9404/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0023 - val_loss: 3.7799e-04\n",
      "Epoch 9405/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0020 - val_loss: 4.1008e-04\n",
      "Epoch 9406/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0011 - val_loss: 0.0035\n",
      "Epoch 9407/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0026 - val_loss: 0.0032\n",
      "Epoch 9408/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0033 - val_loss: 0.0074\n",
      "Epoch 9409/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0042 - val_loss: 0.0062\n",
      "Epoch 9410/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0024 - val_loss: 0.0077\n",
      "Epoch 9411/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0103 - val_loss: 0.0257\n",
      "Epoch 9412/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0190 - val_loss: 0.0014\n",
      "Epoch 9413/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0053 - val_loss: 0.0186\n",
      "Epoch 9414/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0086 - val_loss: 0.0148\n",
      "Epoch 9415/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0081 - val_loss: 0.0352\n",
      "Epoch 9416/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0222 - val_loss: 0.0370\n",
      "Epoch 9417/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0494 - val_loss: 0.0733\n",
      "Epoch 9418/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2056 - val_loss: 1.0262\n",
      "Epoch 9419/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.6240 - val_loss: 1.1879\n",
      "Epoch 9420/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.0019 - val_loss: 4.4761\n",
      "Epoch 9421/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 5.7969 - val_loss: 11.4608\n",
      "Epoch 9422/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 6.5360 - val_loss: 24.9293\n",
      "Epoch 9423/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 6.1533 - val_loss: 1.4827\n",
      "Epoch 9424/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.5968 - val_loss: 0.6758\n",
      "Epoch 9425/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.9036 - val_loss: 1.9476\n",
      "Epoch 9426/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5939 - val_loss: 0.3631\n",
      "Epoch 9427/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2965 - val_loss: 0.1319\n",
      "Epoch 9428/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1141 - val_loss: 0.1509\n",
      "Epoch 9429/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.2080 - val_loss: 1.8989\n",
      "Epoch 9430/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.0025 - val_loss: 0.6304\n",
      "Epoch 9431/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2352 - val_loss: 0.1925\n",
      "Epoch 9432/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3067 - val_loss: 0.1456\n",
      "Epoch 9433/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.9305 - val_loss: 2.6059\n",
      "Epoch 9434/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 2.0635 - val_loss: 0.3892\n",
      "Epoch 9435/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.0162 - val_loss: 1.9871\n",
      "Epoch 9436/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 3.5651 - val_loss: 3.0652\n",
      "Epoch 9437/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.4493 - val_loss: 4.8766\n",
      "Epoch 9438/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.4818 - val_loss: 1.8292\n",
      "Epoch 9439/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.0130 - val_loss: 8.6737\n",
      "Epoch 9440/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.0875 - val_loss: 0.6762\n",
      "Epoch 9441/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.1710 - val_loss: 0.5570\n",
      "Epoch 9442/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 0.7497 - val_loss: 1.0943\n",
      "Epoch 9443/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 0.3407 - val_loss: 0.2567\n",
      "Epoch 9444/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1412 - val_loss: 0.2603\n",
      "Epoch 9445/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0921 - val_loss: 0.0846\n",
      "Epoch 9446/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0868 - val_loss: 0.1895\n",
      "Epoch 9447/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4060 - val_loss: 0.7616\n",
      "Epoch 9448/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.3199 - val_loss: 4.6751\n",
      "Epoch 9449/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.2702 - val_loss: 5.6229\n",
      "Epoch 9450/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 10.4670 - val_loss: 17.3773\n",
      "Epoch 9451/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 68.9040 - val_loss: 163.1391\n",
      "Epoch 9452/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 124.2168 - val_loss: 158.6688\n",
      "Epoch 9453/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 191.0578 - val_loss: 433.9254\n",
      "Epoch 9454/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 778.7664 - val_loss: 937.4923\n",
      "Epoch 9455/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1935.3791 - val_loss: 2253.7465\n",
      "Epoch 9456/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 3123.0628 - val_loss: 7708.6004\n",
      "Epoch 9457/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 2208.9929 - val_loss: 1101.1984\n",
      "Epoch 9458/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 464.4628 - val_loss: 31.2877\n",
      "Epoch 9459/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 190.9643 - val_loss: 442.2407\n",
      "Epoch 9460/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 246.7289 - val_loss: 391.1058\n",
      "Epoch 9461/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 345.6165 - val_loss: 595.9479\n",
      "Epoch 9462/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 269.0939 - val_loss: 344.7927\n",
      "Epoch 9463/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 441.8334 - val_loss: 673.7216\n",
      "Epoch 9464/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 343.4544 - val_loss: 245.2981\n",
      "Epoch 9465/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 454.8822 - val_loss: 767.7190\n",
      "Epoch 9466/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 366.1918 - val_loss: 177.0188\n",
      "Epoch 9467/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 66.0053 - val_loss: 5.4870\n",
      "Epoch 9468/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 8.3204 - val_loss: 29.2075\n",
      "Epoch 9469/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 47.9582 - val_loss: 80.0281\n",
      "Epoch 9470/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 47.1441 - val_loss: 57.3472\n",
      "Epoch 9471/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 32.5464 - val_loss: 162.2051\n",
      "Epoch 9472/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 52.5739 - val_loss: 32.3376\n",
      "Epoch 9473/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 18.2917 - val_loss: 3.1931\n",
      "Epoch 9474/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 8.5132 - val_loss: 9.3426\n",
      "Epoch 9475/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 3.1791 - val_loss: 9.0539\n",
      "Epoch 9476/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.5925 - val_loss: 6.9335\n",
      "Epoch 9477/10000\n",
      "68/68 [==============================] - 0s 735us/sample - loss: 22.1862 - val_loss: 58.5373\n",
      "Epoch 9478/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 41.7613 - val_loss: 23.0033\n",
      "Epoch 9479/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 13.4106 - val_loss: 24.1487\n",
      "Epoch 9480/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 58.0893 - val_loss: 127.1216\n",
      "Epoch 9481/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 91.3375 - val_loss: 146.7535\n",
      "Epoch 9482/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 94.7535 - val_loss: 41.6738\n",
      "Epoch 9483/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.5735 - val_loss: 0.7220\n",
      "Epoch 9484/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.2479 - val_loss: 8.1236\n",
      "Epoch 9485/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 8.3270 - val_loss: 17.5711\n",
      "Epoch 9486/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 11.9196 - val_loss: 5.3351\n",
      "Epoch 9487/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.7539 - val_loss: 7.1673\n",
      "Epoch 9488/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 5.7598 - val_loss: 3.8447\n",
      "Epoch 9489/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 10.9440 - val_loss: 41.6569\n",
      "Epoch 9490/10000\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 16.4564 - val_loss: 24.5428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9491/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 11.3429 - val_loss: 3.2973\n",
      "Epoch 9492/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3.3176 - val_loss: 3.9067\n",
      "Epoch 9493/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.1136 - val_loss: 11.8127\n",
      "Epoch 9494/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 7.4226 - val_loss: 26.0816\n",
      "Epoch 9495/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.4704 - val_loss: 7.9689\n",
      "Epoch 9496/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 2.8117 - val_loss: 1.2465\n",
      "Epoch 9497/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5936 - val_loss: 0.1532\n",
      "Epoch 9498/10000\n",
      "68/68 [==============================] - 0s 559us/sample - loss: 0.3907 - val_loss: 2.5594\n",
      "Epoch 9499/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.8774 - val_loss: 1.7846\n",
      "Epoch 9500/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2382 - val_loss: 3.9937\n",
      "Epoch 9501/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.8569 - val_loss: 6.6774\n",
      "Epoch 9502/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.4645 - val_loss: 0.0359\n",
      "Epoch 9503/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 0.1819 - val_loss: 0.2853\n",
      "Epoch 9504/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6099 - val_loss: 0.2807\n",
      "Epoch 9505/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2052 - val_loss: 0.0422\n",
      "Epoch 9506/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.2142 - val_loss: 0.2608\n",
      "Epoch 9507/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.6766 - val_loss: 0.1360\n",
      "Epoch 9508/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.7652 - val_loss: 0.1269\n",
      "Epoch 9509/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.5061 - val_loss: 0.5038\n",
      "Epoch 9510/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.5767 - val_loss: 3.2679\n",
      "Epoch 9511/10000\n",
      "68/68 [==============================] - 0s 559us/sample - loss: 1.9590 - val_loss: 3.1044\n",
      "Epoch 9512/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 8.6219 - val_loss: 28.8995\n",
      "Epoch 9513/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 14.2816 - val_loss: 11.7042\n",
      "Epoch 9514/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 2.4205 - val_loss: 1.6318\n",
      "Epoch 9515/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.3985 - val_loss: 0.3006\n",
      "Epoch 9516/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.3448 - val_loss: 0.2954\n",
      "Epoch 9517/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.1949 - val_loss: 0.3271\n",
      "Epoch 9518/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1425 - val_loss: 0.0629\n",
      "Epoch 9519/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.1554 - val_loss: 0.1761\n",
      "Epoch 9520/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.5285 - val_loss: 0.7904\n",
      "Epoch 9521/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.3722 - val_loss: 0.1500\n",
      "Epoch 9522/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.9214 - val_loss: 0.7481\n",
      "Epoch 9523/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 2.2151 - val_loss: 4.9045\n",
      "Epoch 9524/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.4667 - val_loss: 0.0128\n",
      "Epoch 9525/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3232 - val_loss: 2.2772\n",
      "Epoch 9526/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.1217 - val_loss: 3.0622\n",
      "Epoch 9527/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2.4310 - val_loss: 12.2647\n",
      "Epoch 9528/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 35.6736 - val_loss: 7.6774\n",
      "Epoch 9529/10000\n",
      "68/68 [==============================] - 0s 618us/sample - loss: 22.5125 - val_loss: 18.6776\n",
      "Epoch 9530/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 11.6151 - val_loss: 27.7895\n",
      "Epoch 9531/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 11.4729 - val_loss: 8.9455\n",
      "Epoch 9532/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 27.8852 - val_loss: 91.4237\n",
      "Epoch 9533/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 46.0782 - val_loss: 340.6957\n",
      "Epoch 9534/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 66.6967 - val_loss: 9.1801\n",
      "Epoch 9535/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 24.1109 - val_loss: 197.4863\n",
      "Epoch 9536/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 227.5715 - val_loss: 32.3676\n",
      "Epoch 9537/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 493.3681 - val_loss: 1598.9800\n",
      "Epoch 9538/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 840.3306 - val_loss: 1440.6972\n",
      "Epoch 9539/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 666.9229 - val_loss: 422.2805\n",
      "Epoch 9540/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 1388.0842 - val_loss: 607.1617\n",
      "Epoch 9541/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 920.0339 - val_loss: 1846.4848\n",
      "Epoch 9542/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 667.2451 - val_loss: 1640.1143\n",
      "Epoch 9543/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1144.1588 - val_loss: 3583.6792\n",
      "Epoch 9544/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 3332.9883 - val_loss: 4551.2984\n",
      "Epoch 9545/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 2722.8500 - val_loss: 3080.6648\n",
      "Epoch 9546/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 7110.8446 - val_loss: 1906.3012\n",
      "Epoch 9547/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 6195.1235 - val_loss: 4724.7322\n",
      "Epoch 9548/10000\n",
      "68/68 [==============================] - 0s 691us/sample - loss: 9496.4263 - val_loss: 13369.8284\n",
      "Epoch 9549/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 7178.6950 - val_loss: 5163.8969\n",
      "Epoch 9550/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3474.7159 - val_loss: 285.3554\n",
      "Epoch 9551/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 560.1945 - val_loss: 1387.3176\n",
      "Epoch 9552/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1125.2267 - val_loss: 1687.3540\n",
      "Epoch 9553/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1101.2829 - val_loss: 993.1571\n",
      "Epoch 9554/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 568.7225 - val_loss: 128.4598\n",
      "Epoch 9555/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 231.4441 - val_loss: 113.2151\n",
      "Epoch 9556/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 161.7751 - val_loss: 167.4746\n",
      "Epoch 9557/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 96.7391 - val_loss: 96.2102\n",
      "Epoch 9558/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 53.7638 - val_loss: 49.3283\n",
      "Epoch 9559/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 39.9103 - val_loss: 26.2101\n",
      "Epoch 9560/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 14.8925 - val_loss: 2.2758\n",
      "Epoch 9561/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.9175 - val_loss: 2.8652\n",
      "Epoch 9562/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2.9286 - val_loss: 7.3572\n",
      "Epoch 9563/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3.5218 - val_loss: 5.1872\n",
      "Epoch 9564/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.2086 - val_loss: 1.9060\n",
      "Epoch 9565/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 176us/sample - loss: 2.2171 - val_loss: 3.5362\n",
      "Epoch 9566/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 4.2245 - val_loss: 1.2822\n",
      "Epoch 9567/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.2666 - val_loss: 0.0802\n",
      "Epoch 9568/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.4652 - val_loss: 0.5104\n",
      "Epoch 9569/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.7899 - val_loss: 0.6585\n",
      "Epoch 9570/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.7489 - val_loss: 0.1061\n",
      "Epoch 9571/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.9063 - val_loss: 1.4505\n",
      "Epoch 9572/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.7160 - val_loss: 2.4775\n",
      "Epoch 9573/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1.1672 - val_loss: 0.3429\n",
      "Epoch 9574/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.5297 - val_loss: 0.1847\n",
      "Epoch 9575/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.2088 - val_loss: 0.0276\n",
      "Epoch 9576/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0779 - val_loss: 0.0381\n",
      "Epoch 9577/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0608 - val_loss: 0.1522\n",
      "Epoch 9578/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0865 - val_loss: 0.1149\n",
      "Epoch 9579/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.0922 - val_loss: 0.2338\n",
      "Epoch 9580/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0860 - val_loss: 0.1344\n",
      "Epoch 9581/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1100 - val_loss: 0.2199\n",
      "Epoch 9582/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1636 - val_loss: 0.3169\n",
      "Epoch 9583/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.1161 - val_loss: 0.0662\n",
      "Epoch 9584/10000\n",
      "68/68 [==============================] - 0s 706us/sample - loss: 0.0522 - val_loss: 0.0379\n",
      "Epoch 9585/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0618 - val_loss: 0.0754\n",
      "Epoch 9586/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1051 - val_loss: 0.0162\n",
      "Epoch 9587/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1584 - val_loss: 0.1418\n",
      "Epoch 9588/10000\n",
      "68/68 [==============================] - 0s 647us/sample - loss: 0.0666 - val_loss: 0.1537\n",
      "Epoch 9589/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1294 - val_loss: 0.1842\n",
      "Epoch 9590/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.1385 - val_loss: 0.1205\n",
      "Epoch 9591/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1021 - val_loss: 0.0595\n",
      "Epoch 9592/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.1040 - val_loss: 0.2130\n",
      "Epoch 9593/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0932 - val_loss: 0.0859\n",
      "Epoch 9594/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0525 - val_loss: 0.0413\n",
      "Epoch 9595/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0395 - val_loss: 0.0369\n",
      "Epoch 9596/10000\n",
      "68/68 [==============================] - 0s 544us/sample - loss: 0.0302 - val_loss: 0.0613\n",
      "Epoch 9597/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0361 - val_loss: 0.0142\n",
      "Epoch 9598/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0309 - val_loss: 0.0170\n",
      "Epoch 9599/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0249 - val_loss: 0.0168\n",
      "Epoch 9600/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0363 - val_loss: 0.0140\n",
      "Epoch 9601/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0228 - val_loss: 0.0215\n",
      "Epoch 9602/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0247 - val_loss: 0.0434\n",
      "Epoch 9603/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0352 - val_loss: 0.0331\n",
      "Epoch 9604/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0568 - val_loss: 0.0154\n",
      "Epoch 9605/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.0241 - val_loss: 0.0166\n",
      "Epoch 9606/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0251 - val_loss: 0.0136\n",
      "Epoch 9607/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0210 - val_loss: 0.0111\n",
      "Epoch 9608/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0172 - val_loss: 0.0170\n",
      "Epoch 9609/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0258 - val_loss: 0.0272\n",
      "Epoch 9610/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0186 - val_loss: 0.0218\n",
      "Epoch 9611/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 0.0222 - val_loss: 0.0554\n",
      "Epoch 9612/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.0341 - val_loss: 0.0127\n",
      "Epoch 9613/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0375 - val_loss: 0.0432\n",
      "Epoch 9614/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.0384 - val_loss: 0.0624\n",
      "Epoch 9615/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.0293 - val_loss: 0.0145\n",
      "Epoch 9616/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0145 - val_loss: 0.0129\n",
      "Epoch 9617/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.0146 - val_loss: 0.0102\n",
      "Epoch 9618/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0126 - val_loss: 0.0121\n",
      "Epoch 9619/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 0.0162 - val_loss: 0.0269\n",
      "Epoch 9620/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0299 - val_loss: 0.0238\n",
      "Epoch 9621/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0377 - val_loss: 0.0164\n",
      "Epoch 9622/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0172 - val_loss: 0.0111\n",
      "Epoch 9623/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.014 - 0s 471us/sample - loss: 0.0233 - val_loss: 0.0542\n",
      "Epoch 9624/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0727 - val_loss: 0.1545\n",
      "Epoch 9625/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1094 - val_loss: 0.0268\n",
      "Epoch 9626/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0463 - val_loss: 0.0785\n",
      "Epoch 9627/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.0498 - val_loss: 0.0697\n",
      "Epoch 9628/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0430 - val_loss: 0.1175\n",
      "Epoch 9629/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1594 - val_loss: 0.0372\n",
      "Epoch 9630/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.1356 - val_loss: 0.2158\n",
      "Epoch 9631/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.3270 - val_loss: 0.5745\n",
      "Epoch 9632/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2551 - val_loss: 0.0862\n",
      "Epoch 9633/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1267 - val_loss: 0.1625\n",
      "Epoch 9634/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1641 - val_loss: 0.4350\n",
      "Epoch 9635/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 0.2428 - val_loss: 0.1415\n",
      "Epoch 9636/10000\n",
      "68/68 [==============================] - 0s 706us/sample - loss: 0.1897 - val_loss: 0.3699\n",
      "Epoch 9637/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.1840 - val_loss: 0.2614\n",
      "Epoch 9638/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3328 - val_loss: 0.0113\n",
      "Epoch 9639/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.1588 - val_loss: 0.1715\n",
      "Epoch 9640/10000\n",
      "68/68 [==============================] - 0s 677us/sample - loss: 0.1162 - val_loss: 0.0822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9641/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0758 - val_loss: 0.0905\n",
      "Epoch 9642/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1509 - val_loss: 0.1480\n",
      "Epoch 9643/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1030 - val_loss: 0.0171\n",
      "Epoch 9644/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.0456 - val_loss: 0.0192\n",
      "Epoch 9645/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0178 - val_loss: 0.0168\n",
      "Epoch 9646/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 0.0116 - val_loss: 0.0090\n",
      "Epoch 9647/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0083 - val_loss: 0.0079\n",
      "Epoch 9648/10000\n",
      "68/68 [==============================] - 0s 471us/sample - loss: 0.0103 - val_loss: 0.0209\n",
      "Epoch 9649/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0111 - val_loss: 0.0088\n",
      "Epoch 9650/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0099 - val_loss: 0.0126\n",
      "Epoch 9651/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0247 - val_loss: 0.0140\n",
      "Epoch 9652/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0315 - val_loss: 0.0488\n",
      "Epoch 9653/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0249 - val_loss: 0.0446\n",
      "Epoch 9654/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.0393 - val_loss: 0.1389\n",
      "Epoch 9655/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.0696 - val_loss: 0.0176\n",
      "Epoch 9656/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 0.0939 - val_loss: 0.1668\n",
      "Epoch 9657/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2258 - val_loss: 0.8931\n",
      "Epoch 9658/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.632 - 0s 309us/sample - loss: 0.7744 - val_loss: 0.9162\n",
      "Epoch 9659/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 0.7388 - val_loss: 0.9775\n",
      "Epoch 9660/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.6226 - val_loss: 0.2473\n",
      "Epoch 9661/10000\n",
      "68/68 [==============================] - 0s 559us/sample - loss: 0.2209 - val_loss: 0.4302\n",
      "Epoch 9662/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.2618 - val_loss: 0.1510\n",
      "Epoch 9663/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.4509 - val_loss: 0.7081\n",
      "Epoch 9664/10000\n",
      "68/68 [==============================] - 0s 735us/sample - loss: 0.7855 - val_loss: 1.4292\n",
      "Epoch 9665/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.8944 - val_loss: 0.1195\n",
      "Epoch 9666/10000\n",
      "68/68 [==============================] - 0s 412us/sample - loss: 0.6328 - val_loss: 1.2229\n",
      "Epoch 9667/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7868 - val_loss: 0.5541\n",
      "Epoch 9668/10000\n",
      "68/68 [==============================] - 0s 721us/sample - loss: 0.5539 - val_loss: 0.5546\n",
      "Epoch 9669/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.2080 - val_loss: 0.1600\n",
      "Epoch 9670/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3752 - val_loss: 0.7405\n",
      "Epoch 9671/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.9941 - val_loss: 1.8749\n",
      "Epoch 9672/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8716 - val_loss: 0.4176\n",
      "Epoch 9673/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2768 - val_loss: 0.3539\n",
      "Epoch 9674/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3757 - val_loss: 0.5286\n",
      "Epoch 9675/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.2653 - val_loss: 0.1503\n",
      "Epoch 9676/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0923 - val_loss: 0.1280\n",
      "Epoch 9677/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 0.1159 - val_loss: 0.0394\n",
      "Epoch 9678/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0324 - val_loss: 0.0421\n",
      "Epoch 9679/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0655 - val_loss: 0.1428\n",
      "Epoch 9680/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.169 - 0s 176us/sample - loss: 0.1249 - val_loss: 0.1957\n",
      "Epoch 9681/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 0.1212 - val_loss: 0.0092\n",
      "Epoch 9682/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2783 - val_loss: 0.5035\n",
      "Epoch 9683/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 1.1630 - val_loss: 4.8060\n",
      "Epoch 9684/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 10.3341 - val_loss: 9.3217\n",
      "Epoch 9685/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 7.3565 - val_loss: 12.0167\n",
      "Epoch 9686/10000\n",
      "68/68 [==============================] - 0s 765us/sample - loss: 8.9738 - val_loss: 9.3773\n",
      "Epoch 9687/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.4138 - val_loss: 4.1110\n",
      "Epoch 9688/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 4.2218 - val_loss: 0.5813\n",
      "Epoch 9689/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 3.6654 - val_loss: 6.7893\n",
      "Epoch 9690/10000\n",
      "68/68 [==============================] - 0s 588us/sample - loss: 16.9314 - val_loss: 50.4438\n",
      "Epoch 9691/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 19.0523 - val_loss: 1.7792\n",
      "Epoch 9692/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 6.5840 - val_loss: 18.0643\n",
      "Epoch 9693/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 14.3472 - val_loss: 67.7573\n",
      "Epoch 9694/10000\n",
      "68/68 [==============================] - 0s 603us/sample - loss: 46.2704 - val_loss: 32.1453\n",
      "Epoch 9695/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 17.7794 - val_loss: 8.6830\n",
      "Epoch 9696/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.0589 - val_loss: 4.9108\n",
      "Epoch 9697/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.5727 - val_loss: 12.4836\n",
      "Epoch 9698/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 20.1454 - val_loss: 8.9358\n",
      "Epoch 9699/10000\n",
      "68/68 [==============================] - 0s 574us/sample - loss: 13.0001 - val_loss: 45.2720\n",
      "Epoch 9700/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 34.2699 - val_loss: 44.9701\n",
      "Epoch 9701/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 64.7623 - val_loss: 4.7244\n",
      "Epoch 9702/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 43.1700 - val_loss: 226.1715\n",
      "Epoch 9703/10000\n",
      "68/68 [==============================] - 0s 882us/sample - loss: 123.2938 - val_loss: 163.3305\n",
      "Epoch 9704/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 94.6015 - val_loss: 175.3847\n",
      "Epoch 9705/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 99.8608 - val_loss: 210.5310\n",
      "Epoch 9706/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 101.5493 - val_loss: 13.2821\n",
      "Epoch 9707/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 39.6035 - val_loss: 20.4930\n",
      "Epoch 9708/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 11.6475 - val_loss: 24.3641\n",
      "Epoch 9709/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 19.5720 - val_loss: 49.1810\n",
      "Epoch 9710/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 24.5020 - val_loss: 36.0055\n",
      "Epoch 9711/10000\n",
      "68/68 [==============================] - 0s 721us/sample - loss: 25.4441 - val_loss: 60.0664\n",
      "Epoch 9712/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 67.5577 - val_loss: 93.3167\n",
      "Epoch 9713/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 55.8802 - val_loss: 107.5008\n",
      "Epoch 9714/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 75.1831 - val_loss: 35.3866\n",
      "Epoch 9715/10000\n",
      "68/68 [==============================] - 0s 750us/sample - loss: 24.2293 - val_loss: 252.9983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9716/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 192.8062 - val_loss: 442.2780\n",
      "Epoch 9717/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 254.0120 - val_loss: 212.8775\n",
      "Epoch 9718/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 346.5423 - val_loss: 240.9923\n",
      "Epoch 9719/10000\n",
      "68/68 [==============================] - 0s 529us/sample - loss: 336.0379 - val_loss: 683.6287\n",
      "Epoch 9720/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 681.1783 - val_loss: 46.6410\n",
      "Epoch 9721/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 761.6321 - val_loss: 267.7523\n",
      "Epoch 9722/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1347.1020 - val_loss: 452.0306\n",
      "Epoch 9723/10000\n",
      "68/68 [==============================] - 0s 794us/sample - loss: 182.0030 - val_loss: 442.1129\n",
      "Epoch 9724/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 198.5123 - val_loss: 127.3715\n",
      "Epoch 9725/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 118.8095 - val_loss: 148.4897\n",
      "Epoch 9726/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 74.3481 - val_loss: 12.4693\n",
      "Epoch 9727/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 41.2000 - val_loss: 23.1174\n",
      "Epoch 9728/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 84.4014 - val_loss: 148.6609\n",
      "Epoch 9729/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 66.6899 - val_loss: 104.8465\n",
      "Epoch 9730/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 77.0990 - val_loss: 147.3106\n",
      "Epoch 9731/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 98.0360 - val_loss: 95.9887\n",
      "Epoch 9732/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 95.2741 - val_loss: 20.8062\n",
      "Epoch 9733/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 21.6348 - val_loss: 3.0434\n",
      "Epoch 9734/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 5.1110 - val_loss: 25.0036\n",
      "Epoch 9735/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 18.2858 - val_loss: 64.0519\n",
      "Epoch 9736/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 24.7544 - val_loss: 12.0940\n",
      "Epoch 9737/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 7.4270 - val_loss: 20.0958\n",
      "Epoch 9738/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 15.4632 - val_loss: 28.0979\n",
      "Epoch 9739/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 11.6168 - val_loss: 2.5954\n",
      "Epoch 9740/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 35.1497 - val_loss: 7.8809\n",
      "Epoch 9741/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 20.3391 - val_loss: 9.1866\n",
      "Epoch 9742/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 27.1847 - val_loss: 15.1411\n",
      "Epoch 9743/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 17.8841 - val_loss: 31.3912\n",
      "Epoch 9744/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 54.9684 - val_loss: 17.9227\n",
      "Epoch 9745/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 52.3077 - val_loss: 71.6712\n",
      "Epoch 9746/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 325.8029 - val_loss: 963.4139\n",
      "Epoch 9747/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 823.1786 - val_loss: 1633.5093\n",
      "Epoch 9748/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 478.0888 - val_loss: 61.9061\n",
      "Epoch 9749/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 238.7674 - val_loss: 59.5221\n",
      "Epoch 9750/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 179.4380 - val_loss: 139.8560\n",
      "Epoch 9751/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 126.5641 - val_loss: 189.2920\n",
      "Epoch 9752/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 114.7570 - val_loss: 396.4567\n",
      "Epoch 9753/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 125.8811 - val_loss: 60.7469\n",
      "Epoch 9754/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 118.1748 - val_loss: 11.8724\n",
      "Epoch 9755/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 48.2138 - val_loss: 11.6696\n",
      "Epoch 9756/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 37.1619 - val_loss: 40.1354\n",
      "Epoch 9757/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 28.8745 - val_loss: 16.3582\n",
      "Epoch 9758/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 12.5583 - val_loss: 9.3893\n",
      "Epoch 9759/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 8.3243 - val_loss: 8.7771\n",
      "Epoch 9760/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 3.2054 - val_loss: 2.7396\n",
      "Epoch 9761/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.8684 - val_loss: 0.0638\n",
      "Epoch 9762/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1147 - val_loss: 0.2875\n",
      "Epoch 9763/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.4520 - val_loss: 1.3397\n",
      "Epoch 9764/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2257 - val_loss: 0.3299\n",
      "Epoch 9765/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1852 - val_loss: 0.0590\n",
      "Epoch 9766/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3478 - val_loss: 0.9369\n",
      "Epoch 9767/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.4845 - val_loss: 0.6577\n",
      "Epoch 9768/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.4346 - val_loss: 0.2048\n",
      "Epoch 9769/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1332 - val_loss: 0.5443\n",
      "Epoch 9770/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.3237 - val_loss: 0.3031\n",
      "Epoch 9771/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1180 - val_loss: 0.0904\n",
      "Epoch 9772/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.1343 - val_loss: 0.5229\n",
      "Epoch 9773/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.8566 - val_loss: 0.6343\n",
      "Epoch 9774/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4690 - val_loss: 0.2466\n",
      "Epoch 9775/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.6739 - val_loss: 0.3241\n",
      "Epoch 9776/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.6786 - val_loss: 6.7284\n",
      "Epoch 9777/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.3488 - val_loss: 0.3922\n",
      "Epoch 9778/10000\n",
      "68/68 [==============================] - 0s 353us/sample - loss: 0.3422 - val_loss: 0.4692\n",
      "Epoch 9779/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2443 - val_loss: 0.0944\n",
      "Epoch 9780/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.045 - 0s 176us/sample - loss: 0.1251 - val_loss: 0.0715\n",
      "Epoch 9781/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.3323 - val_loss: 0.1411\n",
      "Epoch 9782/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.2166 - val_loss: 1.3783\n",
      "Epoch 9783/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.4128 - val_loss: 1.9255\n",
      "Epoch 9784/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5961 - val_loss: 0.5907\n",
      "Epoch 9785/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1672 - val_loss: 0.0895\n",
      "Epoch 9786/10000\n",
      "68/68 [==============================] - 0s 485us/sample - loss: 0.0336 - val_loss: 0.0820\n",
      "Epoch 9787/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0747 - val_loss: 0.0115\n",
      "Epoch 9788/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.0448 - val_loss: 0.1076\n",
      "Epoch 9789/10000\n",
      "68/68 [==============================] - 0s 809us/sample - loss: 0.0445 - val_loss: 0.0243\n",
      "Epoch 9790/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 191us/sample - loss: 0.0425 - val_loss: 0.0848\n",
      "Epoch 9791/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.0708 - val_loss: 0.3342\n",
      "Epoch 9792/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2136 - val_loss: 0.0780\n",
      "Epoch 9793/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 0.0945 - val_loss: 0.1150\n",
      "Epoch 9794/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 0.0585 - val_loss: 0.0074\n",
      "Epoch 9795/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 1.2804 - val_loss: 0.2283\n",
      "Epoch 9796/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.5639 - val_loss: 6.2673\n",
      "Epoch 9797/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 9.2366 - val_loss: 18.7388\n",
      "Epoch 9798/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 21.8915 - val_loss: 17.5707\n",
      "Epoch 9799/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 46.2755 - val_loss: 25.7311\n",
      "Epoch 9800/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 41.9913 - val_loss: 137.6808\n",
      "Epoch 9801/10000\n",
      "68/68 [==============================] - 0s 588us/sample - loss: 141.3502 - val_loss: 218.0863\n",
      "Epoch 9802/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 314.0085 - val_loss: 71.8622\n",
      "Epoch 9803/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 440.2441 - val_loss: 93.4326\n",
      "Epoch 9804/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1381.0552 - val_loss: 1500.1566\n",
      "Epoch 9805/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1117.5966 - val_loss: 1946.0043\n",
      "Epoch 9806/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3177.6343 - val_loss: 12174.0290\n",
      "Epoch 9807/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 6391.7530 - val_loss: 13236.4953\n",
      "Epoch 9808/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 6471.4166 - val_loss: 3330.2814\n",
      "Epoch 9809/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 3885.6045 - val_loss: 6673.4621\n",
      "Epoch 9810/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 2441.5264 - val_loss: 1994.3725\n",
      "Epoch 9811/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1063.0754 - val_loss: 661.9151\n",
      "Epoch 9812/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 577.3309 - val_loss: 123.0382\n",
      "Epoch 9813/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 394.5676 - val_loss: 166.7913\n",
      "Epoch 9814/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 247.2104 - val_loss: 1046.7669\n",
      "Epoch 9815/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 495.7835 - val_loss: 1180.6485\n",
      "Epoch 9816/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 475.8431 - val_loss: 687.1491\n",
      "Epoch 9817/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 264.8258 - val_loss: 177.3960\n",
      "Epoch 9818/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 66.8750 - val_loss: 13.2820\n",
      "Epoch 9819/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 10.9745 - val_loss: 6.3832\n",
      "Epoch 9820/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 14.9125 - val_loss: 12.1513\n",
      "Epoch 9821/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 9.1798 - val_loss: 7.3172\n",
      "Epoch 9822/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 6.8299 - val_loss: 10.5296\n",
      "Epoch 9823/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 6.8457 - val_loss: 5.8078\n",
      "Epoch 9824/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 2.1267 - val_loss: 0.7456\n",
      "Epoch 9825/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 1.7035 - val_loss: 2.2552\n",
      "Epoch 9826/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.8220 - val_loss: 0.9278\n",
      "Epoch 9827/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.6841 - val_loss: 1.0145\n",
      "Epoch 9828/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.8328 - val_loss: 0.6892\n",
      "Epoch 9829/10000\n",
      "68/68 [==============================] - 0s 750us/sample - loss: 0.5896 - val_loss: 0.5194\n",
      "Epoch 9830/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2880 - val_loss: 0.1383\n",
      "Epoch 9831/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.1628 - val_loss: 0.1221\n",
      "Epoch 9832/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2237 - val_loss: 0.1370\n",
      "Epoch 9833/10000\n",
      "68/68 [==============================] - 0s 544us/sample - loss: 0.4741 - val_loss: 0.9910\n",
      "Epoch 9834/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7630 - val_loss: 1.3593\n",
      "Epoch 9835/10000\n",
      "68/68 [==============================] - 0s 118us/sample - loss: 0.4319 - val_loss: 0.1316\n",
      "Epoch 9836/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.1636 - val_loss: 0.1818\n",
      "Epoch 9837/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2102 - val_loss: 0.0982\n",
      "Epoch 9838/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.1345 - val_loss: 0.1454\n",
      "Epoch 9839/10000\n",
      "68/68 [==============================] - 0s 1ms/sample - loss: 0.1887 - val_loss: 0.1394\n",
      "Epoch 9840/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1624 - val_loss: 0.1899\n",
      "Epoch 9841/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.2251 - val_loss: 0.3486\n",
      "Epoch 9842/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.6600 - val_loss: 0.1533\n",
      "Epoch 9843/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1.2552 - val_loss: 2.2282\n",
      "Epoch 9844/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1.7120 - val_loss: 0.9432\n",
      "Epoch 9845/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.0523 - val_loss: 0.3912\n",
      "Epoch 9846/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.7029 - val_loss: 1.2903\n",
      "Epoch 9847/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 2.8400 - val_loss: 6.0429\n",
      "Epoch 9848/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 14.0834 - val_loss: 17.9883\n",
      "Epoch 9849/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 47.4184 - val_loss: 97.1987\n",
      "Epoch 9850/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 42.1733 - val_loss: 9.7554\n",
      "Epoch 9851/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 10.9018 - val_loss: 30.8943\n",
      "Epoch 9852/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 21.5087 - val_loss: 51.0798\n",
      "Epoch 9853/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 23.7577 - val_loss: 1.3006\n",
      "Epoch 9854/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 5.1883 - val_loss: 4.1649\n",
      "Epoch 9855/10000\n",
      "68/68 [==============================] - 0s 500us/sample - loss: 2.9413 - val_loss: 0.5236\n",
      "Epoch 9856/10000\n",
      "68/68 [==============================] - 0s 368us/sample - loss: 2.4736 - val_loss: 1.7606\n",
      "Epoch 9857/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 5.9753 - val_loss: 8.9716\n",
      "Epoch 9858/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 10.2897 - val_loss: 88.9581\n",
      "Epoch 9859/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 42.5652 - val_loss: 44.5545\n",
      "Epoch 9860/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 21.3448 - val_loss: 5.6280\n",
      "Epoch 9861/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 6.0988 - val_loss: 14.2577\n",
      "Epoch 9862/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 14.9240 - val_loss: 14.1500\n",
      "Epoch 9863/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 5.1806 - val_loss: 2.5403\n",
      "Epoch 9864/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 2.8553 - val_loss: 3.9872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9865/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1.2774 - val_loss: 0.6399\n",
      "Epoch 9866/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.7423 - val_loss: 0.5004\n",
      "Epoch 9867/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 1.7145 - val_loss: 6.2796\n",
      "Epoch 9868/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 2.7224 - val_loss: 3.2612\n",
      "Epoch 9869/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 4.5995 - val_loss: 2.8365\n",
      "Epoch 9870/10000\n",
      "68/68 [==============================] - 0s 338us/sample - loss: 2.0813 - val_loss: 3.4809\n",
      "Epoch 9871/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3.1268 - val_loss: 1.3062\n",
      "Epoch 9872/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1.8419 - val_loss: 0.5281\n",
      "Epoch 9873/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 0.7634 - val_loss: 1.3140\n",
      "Epoch 9874/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.5871 - val_loss: 0.0807\n",
      "Epoch 9875/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.1363 - val_loss: 0.2674\n",
      "Epoch 9876/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.2360 - val_loss: 0.1559\n",
      "Epoch 9877/10000\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 0.1890 - val_loss: 0.2387\n",
      "Epoch 9878/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.1151 - val_loss: 0.0568\n",
      "Epoch 9879/10000\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 0.1145 - val_loss: 0.1083\n",
      "Epoch 9880/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.132 - 0s 162us/sample - loss: 0.1366 - val_loss: 0.1984\n",
      "Epoch 9881/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.1550 - val_loss: 0.4559\n",
      "Epoch 9882/10000\n",
      "68/68 [==============================] - 0s 397us/sample - loss: 0.2865 - val_loss: 0.1606\n",
      "Epoch 9883/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 0.2882 - val_loss: 0.0253\n",
      "Epoch 9884/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.2130 - val_loss: 0.1048\n",
      "Epoch 9885/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.0476 - val_loss: 0.0298\n",
      "Epoch 9886/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.0595 - val_loss: 0.0240\n",
      "Epoch 9887/10000\n",
      "68/68 [==============================] - 0s 456us/sample - loss: 0.0741 - val_loss: 0.2017\n",
      "Epoch 9888/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5502 - val_loss: 0.1505\n",
      "Epoch 9889/10000\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 2.1178 - val_loss: 2.2110\n",
      "Epoch 9890/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 6.1003 - val_loss: 7.8085\n",
      "Epoch 9891/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 2.7648 - val_loss: 1.2011\n",
      "Epoch 9892/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.4888 - val_loss: 0.5485\n",
      "Epoch 9893/10000\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 0.1752 - val_loss: 0.1019\n",
      "Epoch 9894/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.2043 - val_loss: 0.1580\n",
      "Epoch 9895/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.2828 - val_loss: 0.3809\n",
      "Epoch 9896/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 0.1567 - val_loss: 1.0528\n",
      "Epoch 9897/10000\n",
      "68/68 [==============================] - 0s 382us/sample - loss: 0.4020 - val_loss: 0.3482\n",
      "Epoch 9898/10000\n",
      "68/68 [==============================] - 0s 441us/sample - loss: 0.3972 - val_loss: 1.1660\n",
      "Epoch 9899/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.4792 - val_loss: 1.5492\n",
      "Epoch 9900/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 1.5579 - val_loss: 0.6582\n",
      "Epoch 9901/10000\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 1.2780 - val_loss: 0.5308\n",
      "Epoch 9902/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.5327 - val_loss: 1.8916\n",
      "Epoch 9903/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.8370 - val_loss: 1.3444\n",
      "Epoch 9904/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 0.3816 - val_loss: 0.3279\n",
      "Epoch 9905/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.2603 - val_loss: 0.0356\n",
      "Epoch 9906/10000\n",
      "68/68 [==============================] - 0s 132us/sample - loss: 0.2652 - val_loss: 0.2085\n",
      "Epoch 9907/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.4813 - val_loss: 0.1280\n",
      "Epoch 9908/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 0.5201 - val_loss: 1.4234\n",
      "Epoch 9909/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 0.7742 - val_loss: 1.2300\n",
      "Epoch 9910/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 1.061 - 0s 250us/sample - loss: 0.4481 - val_loss: 1.2629\n",
      "Epoch 9911/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1.2580 - val_loss: 0.6603\n",
      "Epoch 9912/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1.1823 - val_loss: 1.6565\n",
      "Epoch 9913/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 0.8463 - val_loss: 2.1837\n",
      "Epoch 9914/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1.4671 - val_loss: 0.2755\n",
      "Epoch 9915/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 1.2677 - val_loss: 1.0019\n",
      "Epoch 9916/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.7871 - val_loss: 0.3338\n",
      "Epoch 9917/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.7910 - val_loss: 0.2714\n",
      "Epoch 9918/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2.4616 - val_loss: 0.4168\n",
      "Epoch 9919/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 17.1486 - val_loss: 3.8197\n",
      "Epoch 9920/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 65.7511 - val_loss: 68.4780\n",
      "Epoch 9921/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 113.1312 - val_loss: 550.2103\n",
      "Epoch 9922/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 324.6239 - val_loss: 1569.6202\n",
      "Epoch 9923/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 734.5862 - val_loss: 350.6871\n",
      "Epoch 9924/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 260.8896 - val_loss: 164.3665\n",
      "Epoch 9925/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 110.9778 - val_loss: 96.0160\n",
      "Epoch 9926/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 83.5672 - val_loss: 219.1702\n",
      "Epoch 9927/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 264.3388 - val_loss: 1311.5354\n",
      "Epoch 9928/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 465.9173 - val_loss: 580.3094\n",
      "Epoch 9929/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 319.8586 - val_loss: 454.4974\n",
      "Epoch 9930/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 199.8831 - val_loss: 33.9978\n",
      "Epoch 9931/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 80.7655 - val_loss: 2.9203\n",
      "Epoch 9932/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 31.7093 - val_loss: 18.5590\n",
      "Epoch 9933/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 31.6561 - val_loss: 39.6076\n",
      "Epoch 9934/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 13.3485 - val_loss: 45.6914\n",
      "Epoch 9935/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 78.6433 - val_loss: 265.1482\n",
      "Epoch 9936/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 123.9386 - val_loss: 138.5442\n",
      "Epoch 9937/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 267.6443 - val_loss: 243.7148\n",
      "Epoch 9938/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 106.9055 - val_loss: 14.1672\n",
      "Epoch 9939/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 66.3588 - val_loss: 16.5656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9940/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 46.6684 - val_loss: 34.1304\n",
      "Epoch 9941/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 44.1419 - val_loss: 4.4347\n",
      "Epoch 9942/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 14.6090 - val_loss: 9.0803\n",
      "Epoch 9943/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.6221 - val_loss: 5.8229\n",
      "Epoch 9944/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 7.0158 - val_loss: 0.1082\n",
      "Epoch 9945/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 1.6836 - val_loss: 1.4054\n",
      "Epoch 9946/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.0099 - val_loss: 0.5766\n",
      "Epoch 9947/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 8.3409 - val_loss: 4.7352\n",
      "Epoch 9948/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 11.1686 - val_loss: 2.2252\n",
      "Epoch 9949/10000\n",
      "68/68 [==============================] - 0s 309us/sample - loss: 2.3395 - val_loss: 1.5331\n",
      "Epoch 9950/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 0.4490 - val_loss: 0.7419\n",
      "Epoch 9951/10000\n",
      "68/68 [==============================] - 0s 250us/sample - loss: 0.5298 - val_loss: 1.0059\n",
      "Epoch 9952/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.7821 - val_loss: 0.0162\n",
      "Epoch 9953/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 1.1453 - val_loss: 1.2083\n",
      "Epoch 9954/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 3.5762 - val_loss: 3.4236\n",
      "Epoch 9955/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.9276 - val_loss: 1.8963\n",
      "Epoch 9956/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 0.7249 - val_loss: 0.3163\n",
      "Epoch 9957/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.9500 - val_loss: 0.3944\n",
      "Epoch 9958/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 0.5421 - val_loss: 0.7459\n",
      "Epoch 9959/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 4.0882 - val_loss: 9.1678\n",
      "Epoch 9960/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 5.8399 - val_loss: 16.2251\n",
      "Epoch 9961/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 23.2829 - val_loss: 44.6578\n",
      "Epoch 9962/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 121.4326 - val_loss: 35.1174\n",
      "Epoch 9963/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 28.2853 - val_loss: 24.1907\n",
      "Epoch 9964/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 11.9859 - val_loss: 30.2127\n",
      "Epoch 9965/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 15.6242 - val_loss: 9.9096\n",
      "Epoch 9966/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 40.0667 - val_loss: 120.3709\n",
      "Epoch 9967/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 55.7991 - val_loss: 131.6608\n",
      "Epoch 9968/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 69.5677 - val_loss: 128.0669\n",
      "Epoch 9969/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 44.3502 - val_loss: 84.9506\n",
      "Epoch 9970/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 42.0431 - val_loss: 353.2822\n",
      "Epoch 9971/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 794.8190 - val_loss: 204.6190\n",
      "Epoch 9972/10000\n",
      "68/68 [==============================] - 0s 294us/sample - loss: 761.7364 - val_loss: 2871.6742\n",
      "Epoch 9973/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 746.2048 - val_loss: 1349.0214\n",
      "Epoch 9974/10000\n",
      "68/68 [==============================] - 0s 235us/sample - loss: 1081.1159 - val_loss: 559.6267\n",
      "Epoch 9975/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3122.9986 - val_loss: 5694.1374\n",
      "Epoch 9976/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 2570.3449 - val_loss: 936.5963\n",
      "Epoch 9977/10000\n",
      "68/68 [==============================] - 0s 324us/sample - loss: 432.4649 - val_loss: 8.1547\n",
      "Epoch 9978/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 128.7844 - val_loss: 144.9335\n",
      "Epoch 9979/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 73.8208 - val_loss: 190.2953\n",
      "Epoch 9980/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 89.3578 - val_loss: 185.1333\n",
      "Epoch 9981/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 112.9308 - val_loss: 100.1047\n",
      "Epoch 9982/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 113.2784 - val_loss: 362.9755\n",
      "Epoch 9983/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 182.9697 - val_loss: 145.2754\n",
      "Epoch 9984/10000\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 134.5407 - val_loss: 205.7615\n",
      "Epoch 9985/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 126.0458 - val_loss: 78.4487\n",
      "Epoch 9986/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 192.2118 - val_loss: 1050.6097\n",
      "Epoch 9987/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 927.4444 - val_loss: 5212.5113\n",
      "Epoch 9988/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 3039.1163 - val_loss: 60.6257\n",
      "Epoch 9989/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 3046.7607 - val_loss: 2922.9874\n",
      "Epoch 9990/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 1608.9124 - val_loss: 2697.1350\n",
      "Epoch 9991/10000\n",
      "68/68 [==============================] - 0s 162us/sample - loss: 1971.8104 - val_loss: 76.4992\n",
      "Epoch 9992/10000\n",
      "68/68 [==============================] - ETA: 0s - loss: 151.665 - 0s 235us/sample - loss: 1394.2794 - val_loss: 2584.5334\n",
      "Epoch 9993/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 876.5704 - val_loss: 1489.2964\n",
      "Epoch 9994/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 581.1337 - val_loss: 383.7885\n",
      "Epoch 9995/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 277.3147 - val_loss: 84.5600\n",
      "Epoch 9996/10000\n",
      "68/68 [==============================] - 0s 176us/sample - loss: 115.0560 - val_loss: 129.0496\n",
      "Epoch 9997/10000\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 81.8505 - val_loss: 244.4696\n",
      "Epoch 9998/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 83.3013 - val_loss: 133.7258\n",
      "Epoch 9999/10000\n",
      "68/68 [==============================] - 0s 147us/sample - loss: 61.6881 - val_loss: 7.0864\n",
      "Epoch 10000/10000\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 40.5012 - val_loss: 108.2070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x52e3a860>"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(5, activation='relu', input_shape=(2,)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer=Adam(lr=0.01), loss='mse')\n",
    "model.fit(X_train, Y_train, validation_split = 0.2, batch_size=10, epochs=10000)\n",
    "# Note: sometimes this works with 5 nodes in the first layer, sometimes it doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "15/15 [==============================] - 0s 67us/sample - loss: 107.1612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "107.16120147705078"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict = model.predict(X_test)\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_test[:,0], Y_test, linestyle='none', marker='.', color='green', label='Test data')\n",
    "plt.plot(X_test[:,0], Y_predict, linestyle='none', marker='x', color='red', label='Prediction')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_test[:,1], Y_test, linestyle='none', marker='.', color='green', label='Test data')\n",
    "plt.plot(X_test[:,1], Y_predict, linestyle='none', marker='x', color='red', label='Prediction')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 5.274257  , -0.01414301,  5.9599366 , -0.52464306, -0.03330344],\n",
      "       [ 1.01935   , -0.5586614 , -0.05862324, -0.90254873,  5.4761662 ]],\n",
      "      dtype=float32), array([ 3.497665  , -0.31180364,  3.3919716 ,  0.        ,  9.473638  ],\n",
      "      dtype=float32)]\n",
      "[array([[ 5.1476245 ],\n",
      "       [-0.26826772],\n",
      "       [ 5.643872  ],\n",
      "       [ 0.17518163],\n",
      "       [-6.3791127 ]], dtype=float32), array([2.2337697], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.get_weights())\n",
    "\n",
    "#blob1 = np.dot(X_test[0,:], model.layers[0].get_weights()[0][0]) + model.layers[0].get_weights()[1][0]\n",
    "#blob1 = np.dot(X_test[0,:], model.layers[0].get_weights()[0][1]) + model.layers[0].get_weights()[1][1]\n",
    "#blob3 = blob1*model.layers[1].get_weights()[0][0] + blob2*model.layers[1].get_weights()[0][1] + model.layers[1].get_weights()[1]\n",
    "#print(blob3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x4775d3c8>"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=p.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.scatter(xs=X_test[:,0], ys=X_test[:,1], zs=Y_test, zdir='z', s=20, c=None, depthshade=True, color='blue')\n",
    "ax.scatter(xs=X_test[:,0], ys=X_test[:,1], zs=Y_predict, zdir='z', s=20, c=None, depthshade=True, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
